{
    "data_collection": [
        {
            "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "This dataset contains over\n200h of classical piano music, recorded over nine years of\nthe International Piano-e-Competition.",
                "The harmonic progressions in many of the generated sam-\nples follow the patterns common for western classical mu-\nsic."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Hierarchical Recurrent Neural Networks for Conditional Melody Generation with Long-term Structure",
            " homophonic ": [],
            " polyphonic ": [
                "Looking at \u201cpure\u201d neural networks that do not leverage\nslower optimization techniques, [11] use a novel Tonnetz\nrepresentation to train an LSTM that is better able to gen-\nerate polyphonic music with repeated patterns than a similar\nnetwork with a more traditional piano roll representation.",
                "2.[11] C.-H. Chuan and D. Herremans, \u201cModeling temporal tonal relations\nin polyphonic music through deep networks with a novel image-based\nrepresentation,\u201d in Proc.",
                "[25] S. Lattner, M. Grachten, and G. Widmer, \u201cImposing higher-level struc-\nture in polyphonic music generation using convolutional restricted\nboltzmann machines and constraints,\u201d J. of Creative Music Syst. , vol. 2,\np. 1, 2018."
            ],
            " monophonic ": [
                "Two\nrecent RNN-based systems, LookbackRNN and AttentionRNN\n[1], aim to generate monophonic melodies with long term\nstructure in an autoagressive way.",
                "By combining the generated pitch and rhythmic pattern, the\ngenerated monophonic melodies are qualitatively evaluated\nthrough multiple listening tests and are shown to outperform\nthe LookbackRNN",
                "For simplicity, we included only monophonic melodies with\na time signature of 4/4, along with their chords."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "Composing melodies from a given set of chords is a task\nfaced by many musicians in the real world, e.g. in pop music\ncomposition or jazz improvisation."
            ],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "Composing melodies from a given set of chords is a task\nfaced by many musicians in the real world, e.g. in pop music\ncomposition or jazz improvisation."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "LEARNING TO GENERATE MUSIC WITH SENTIMENT",
            " homophonic ": [],
            " polyphonic ": [
                "LEARNING TO GENERATE MUSIC WITH SENTIMENT\nLucas N. Ferreira\nUniversity of California, Santa Cruz\nDepartment of Computational MediaJim Whitehead\nUniversity of California, Santa Cruz\nDepartment of Computational Media\nABSTRACT\nDeep Learning models have shown very promising re-\nsults in automatically composing polyphonic music pieces."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "For example, one can\u2019t easily con-\ntrol a model trained on classical piano pieces to compose\na tense piece for a horror scene of a movie."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Personalized Popular Music Generation Using Imitation and Structure",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "Part I Music Background\n\u2022Music listening and Practice (select one from the following)\n{I am an expert, e.g. more than 5 years of vocal or instrumental training and\npractice."
            ],
            " vocal ": [
                "Part I Music Background\n\u2022Music listening and Practice (select one from the following)\n{I am an expert, e.g. more than 5 years of vocal or instrumental training and\npractice."
            ],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "Deep neural models for\npersonalized jazz improvisations."
            ],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?"
            ],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "An evaluation using 10 pop songs shows\nthat our new representations and methods are able to create high-quality stylistic mu-\nsic that is similar to a given input song.",
                "We introduce a stylistic music generation model that is able to capture melody, chord\nand bass style from a single pop song and imitate them with structure information in a new\ncomplete piece.",
                "We note that imitation of performance, orchestration,\nand production (especially in pop music) are also important aspects of style and perceived\nsimilarity, but we leave these to future work.",
                "Thus, we are able to learn enough from\na single input song to form an imitation, even when seeds vary from Chinese Pop to Western\nPop songs.",
                "We identify distinc-\ntive chord sequences in the seed song by comparing statistical features between the seed song\nand a general dataset consisting of 300 annotated pop songs.",
                "The other ten songs, \fve Western pop songs and\n\fve Chinese pop songs, are used as seed songs for evaluation.",
                "One application of our system is to imitate Parkinson patients'\nfavorite pop songs, so we wanted to imitate songs that are already popular and familiar.",
                "6 Conclusions\nWe have described techniques to automatically generate stylistic melody, harmony and bass\nlines for pop songs with a logical and hierarchical music structure."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network",
            " homophonic ": [],
            " polyphonic ": [
                "Mogren [42] proposed an RNN GAN for generating single voice polyphonic music.",
                "[43] introduced a novel multi-track polyphonic symbolic music generator using GANs.",
                "[39] Generate melodies based on lyrics Conditional GAN Cross entropy LSTM generators and discriminators No evaluation was provided  [41] Generate melodies based on lyrics Conditional LSTM GAN Cross entropy Dense layer followed by 2 LSTM followed by a dense layer for generator, 2 LSTM followed by dense for discriminator BLEU-2 score of 0.735, scores of about 3.8, 3.5, 4.1 respectively out of 5 for lyrics, rhythm, and melody by evaluators [42] Generate single voice polyphonic music RNN GAN Cross entropy and Squared error loss 2 LSTM layers for generator, 2 Bi-directional LSTM layers followed by a dense for discriminator No evaluation was provided  [43] Generate multi-track, polyphonic music Conditional GAN Wasserstein Generators contain 1D transposed CONV, discriminators 5 contain 1D Conv layers followed by one dense layer The highest score for conditional generation was 3.1 and non-conditional was 3.16 out of 5 by \u2018non-pro\u2019 evaluators."
            ],
            " monophonic ": [
                "The authors in [37] presented an adversarial and convolutional based architecture known as MidiNet for generating pop music monophonic melodies using 1022 pop music from an online MIDI database called TheoryTab [38].",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds"
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "In this work, five genres were considered, namely folk music, Arabic, jazz, metal rock, and classical.",
                "For intra-track metrics, jamming model performed best [44] Generate folk music RL GAN Cross entropy and policy gradient RNN generators, CNN discriminators BLEU score of 0.94 and MSE of 20.6 outperformed baseline maximum likelihood estimation D. Poetry and Literary Text Generation using GANs Researchers have also focused on literary text generation using GANs."
            ],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "The dataset used consists of 3697 classical music from 160 different composers.",
                "The dataset used for poetry generation consists of 740 classical and contemporary English poems.",
                "740 classical and contemporary English poems and 1500 song lyrics across various genres Perplexity score of 42.5 for poetry and 9.02 for lyrics generations IV."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "The authors in [37] presented an adversarial and convolutional based architecture known as MidiNet for generating pop music monophonic melodies using 1022 pop music from an online MIDI database called TheoryTab [38].",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds"
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "CONTROLLABLE DEEP MELODY GENERATION VIA HIERARCHICAL MUSIC STRUCTURE REPRESENTATION",
            " homophonic ": [],
            " polyphonic ": [
                "[3], polyphonic performance generation [4] and drum\npattern generation"
            ],
            " monophonic ": [
                "Research has tackled this question\nfrom many angles, including monophonic melody genera-\n\u00a9 S. Dai, Z. Jin, C. Gomes and R. Dannenberg."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "[7] S. H. Hakimi, N. Bhonker, and R. El-Yaniv, \u201cBebop-\nnet: Deep neural models for personalized jazz impro-\nvisations,\u201d in Proc. of the 21st International Society for\nMusic Information Retrieval Conference, ISMIR ."
            ],
            " rock ": [],
            " classical ": [
                "There are a few\nmodels using rule-based and statistical methods to con-\nstruct long-term repetitive structure in classical music [19]\nand pop music"
            ],
            " electronic ": [
                "[9] L. Hiller and L. Isaacson, Experimental Music: Com-\nposition with an Electronic Computer ."
            ],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "For this \u201csanity check,\u201d we randomly picked 20 test\nsongs and generated 500 alternative basic melodies and\nrhythm forms."
            ],
            " k-pop ": [],
            " pop ": [
                "There are a few\nmodels using rule-based and statistical methods to con-\nstruct long-term repetitive structure in classical music [19]\nand pop music",
                "We describe a controllable melody generation system that\nuses hierarchical music representation to generate full-\nlength pop song melodies with multi-level repetition and\nstructure.",
                "Our work is with pop music because structures are rel-\natively simple and listeners are generally familiar with the\nstyle and thus able to evaluate compositions.",
                "We use a Chi-\nnese pop song dataset, POP909",
                "Sections contain multi-\nple phrases, e.g., the illustrated song has an intro, a main\ntheme section (phrase A as verse and phrase B as chorus),\na bridge section followed by a repeat of the theme, and an\noutro section, which is a typical pop song structure.",
                "The demographics information about the\nlisteners are as follows:\nGender male: 120, female: 75, other: 1;\nAge distribution 0-10: 0, 11-20: 17, 21-30: 149, 31-40:\n28, 41-50: 0, 51-60: 2, >60: 0;\nMusic pro\ufb01ciency levels lowest (listen to music <1\nhour/week): 16, low (listen to music 1\u201315 hours/week):\n62, medium (listen to music >15 hours/week): 21, high\n(studied music for 1\u20135 years): 52, expert ( >5 years of\nmusic practice): 44;\nNationality Chinese: 180, Others: 16 (note that the\nPOP909 dataset is primarily Chinese pop songs, and lis-\nteners who are more familiar with this style are likely to be\nmore reliable and discriminating raters.)",
                "The human-composed songs used in this study are from\nthe most popular ones in Chinese pop history.",
                "[24] J. Wu, X. Liu, X. Hu, and J. Zhu, \u201cPopmnet: Gener-\nating structured pop music melodies using neural net-\nworks,\u201d Arti\ufb01cial Intelligence , vol."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer",
            " homophonic ": [],
            " polyphonic ": [
                "We report on objective and subjective evaluations of variants of\nthe proposed Theme Transformer and the conventional prompt-\nbased baseline, showing that our best model can generate, to\nsome extent, polyphonic pop piano music with repetition and\nplausible variations of a given condition.",
                "We present an empirical performance comparison between\nthe proposed theme-conditioned Transformer and the con-\nventional prompt-conditioned Transformer [13], [17] for gen-\nerating polyphonic piano music.",
                "[13] greatly improved\nupon this by presenting the \ufb01rst Transformer decoder model\nfor symbolic music generation, showing that a Transformer\nmodel can generate coherent minute-long polyphonic piano\nmusic with local repetitions and variations.",
                "In the objective evaluation, we let each model generate 64-\nbar polyphonic piano music using the thematic conditions\nretrieved from each of the 29 testing songs of POP909.",
                "Zhang, and G. Xia, \u201cLearning interpretable\nrepresentation for controllable polyphonic music generation,\u201d in Proc.\nInt."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "Ren, \u201cClosed patterns in folk music and other genres,\u201d in Proc.",
                "Workshop on Folk Music Analysis , 2016."
            ],
            " blues ": [],
            " jazz ": [
                "Yang, \u201cThe Jazz Transformer on the front line:\nExploring the shortcomings of AI-composed music through quantitative\nmeasures,\u201d in Proc."
            ],
            " rock ": [],
            " classical ": [
                "A Transformer decoder can take a random seed\nand generate a piece from scratch, or take any user-provided\nmusic fragment as the \u201cprompt\u201d and generate a continuation\n1In Western classical music, both themes and motifs can be considered\nshort, salient recurring patterns.",
                "[3],\nis only for Western classical music and does not specify all\npositions where the themes occur in compositions.",
                "Moreover,\nboth of them consist only of classical music."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "In this\npaper, we propose an alternative conditioning approach, called\ntheme-based conditioning , that explicitly trains the Transformer\nto treat the conditioning sequence as a thematic material that\nhas to manifest itself multiple times in its generation result.",
                "[50]\u2013[52] as\na plausible alternative thematic material."
            ],
            " k-pop ": [],
            " pop ": [
                "We report on objective and subjective evaluations of variants of\nthe proposed Theme Transformer and the conventional prompt-\nbased baseline, showing that our best model can generate, to\nsome extent, polyphonic pop piano music with repetition and\nplausible variations of a given condition.",
                "I MPLEMENTATION DETAILS\nA. Dataset\nWe train our models using the piano covers of Mandarin\npop music from the POP909 dataset [25], which is composed\nof the piano covers of 909 Mandarin pop songs originally\ncomposed by 462 artists, released from the earliest in 1950s\nto the latest around 2010",
                "The \ufb01rst user group\nis familiar with Mandarin pop music, while the second group does not.",
                "Therefore, we require\nour subjects to self-report whether they are familiar with\nMandarin pop music, and only ask those who are unfamiliar\nto listen to the original pieces.",
                "We ask the\nsubjects to indicate (yes or no) whether they are familiar\nwith Mandarin pop music by the question: \u201cDo you listen\nto Mandarin pop songs?\u201d",
                "However, as\nmentioned in Section IX-B, a music piece (e.g., a sonata, or\na pop song in verse-chorus form) can often contain multiple\nthemes, which may be similar or in stark contrast to each\nother.",
                "Yang, \u201cPop Music Transformer: Beat-based\nmodeling and generation of expressive pop piano compositions,\u201d in Proc."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            " homophonic ": [],
            " polyphonic ": [
                "Here we explore\nits ability to reconstruct polyphonic music .",
                "Fig-\nure 2 indicates that DDSP generalised best to all classes, im-\nplying that polyphonic music, reconstructed by DDSP, main-\ntains much of its meaningful information."
            ],
            " monophonic ": [
                "While the former two have been shownarXiv:2201.00052v1  [cs.SD]  31 Dec 2021to generate commercial music [1], [3], [8], the latter was\nproposed in the context of monophonic audio.",
                "Although DDSP was\ndesigned for monophonic music, it can also reconstruct poly-\nphonic music since it is trained to reconstruct a spectrogram."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [
                "[8] C. Carr and Z. Zukowski, \u201cGenerating albums with samplernn to imitate\nmetal, rock, and punk bands,\u201d arXiv preprint arXiv:1811.06633 , 2018."
            ],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "Brunner et al. (2018) propose MIDI-\nV AE as a method for conditional generation as well as genre\ntransfer between pop and jazz music."
            ],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "Brunner et al. (2018) propose MIDI-\nV AE as a method for conditional generation as well as genre\ntransfer between pop and jazz music.",
                "Pop Music Transformer: Beat-\nbased Modeling and Generation of Expressive Pop Piano\nCompositions."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Music Generation Using an LSTM",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "The songs in our data are a mix of classical music and video game soundtracks consisting \nof single -track piano music."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Prote\u00e7\u00e3o intelectual de obras produzidas por sistemas baseados em intelig\u00eancia artificial: uma vis\u00e3o tecnicista sobre o tema",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "An adaptive music generation architecture for games based on the deep learning Transformer model",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "And, perhaps even more importantly,\nwe should seek a model to support instrumental layers used in video game composition (such as the practice\nof \\striping\", which is to record orchestral sections separately for future mixing according to the whims of the\ncomposer).",
                "Some musical features (e.g., adding or removing instrumental layers (such as for striping),\nchanging the tempo, adding or removing processing, changing the pitch content. . . ) are linked to game\nplay variables."
            ],
            " vocal ": [],
            " choral ": [],
            "orchestral": [
                "And, perhaps even more importantly,\nwe should seek a model to support instrumental layers used in video game composition (such as the practice\nof \\striping\", which is to record orchestral sections separately for future mixing according to the whims of the\ncomposer).",
                "4.3 Layering\nWe consider layers of music, analogous to the production of orchestral music for games [37], with currently up\nto 4 layers:\n\u20221st layer, the most conservative and neutral;\n\u20222nd layer, to add more excitement, e.g., though some additional instrument;\n\u20223rd and 4th layers, to intensify the immersion and the tension.",
                "It allows de\fning some kind of\n\\orchestral blueprint\" (actually, some cartography of possible paths) for activating various musical components\nof a piece of music.",
                "Fig. 7 shows an example of visual orchestral blueprint (musical \row) in Skini.\nFigure 7: Example of orchestral \row in Skini (Opus1 Piece by Bertrand Petit)."
            ],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "3(as for a musician improvising in some jazz context, balancing between constructing and following some musical\ndiscourse and \ftting into the dynamic context, in the \frst place, harmonic modulations)."
            ],
            " rock ": [],
            " classical ": [
                "Its main novelty is a self-attention\nmechanism (as a full alternative to more classical mechanisms such as recurrence or convolution), to focus on\ncontributing elements of an input sequence."
            ],
            " electronic ": [
                "Experimental Music: Composition with an Electronic Computer ."
            ],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "Its main novelty is a self-attention\nmechanism (as a full alternative to more classical mechanisms such as recurrence or convolution), to focus on\ncontributing elements of an input sequence.",
                "Plain arrows represent \fxed\npaths and bold arrows represent alternative paths which may be decided by the public."
            ],
            " k-pop ": [],
            " pop ": [
                "From Pac-Man to Pop Music Interactive Audio in Games and New Media ."
            ],
            " ambient ": [
                "The proposal by Je\u000bries for ambient music generation based on the Transformer [17] has also been a source\nof inspiration.",
                "In the experiment described\nin this paper, we have chosen a corpus of ambient music, more precisely a Spotify playlist named \\Ambient\nsongs for creativity and calm\", curated by Je\u000bries, and containing approximately 20 hours and 165 titles [18].",
                "4.5 Strategy and Control Model\nWhile planning for the future some more advanced state machine for mapping the emotions into generation\ncontrol strategies (as will be detailed in Section 5.1), in current prototype we have implemented 9 pre-de\fned\nstrategies (corresponding to the 9 emotions shown in Fig. 4 with for each one di\u000berent values corresponding to\nthe parameters for the generation: which layers are activated, which instruments (sampled or synthetic sounds,\ncurrently selected from some instruments library for ambient music within Ableton Live) are used and which\n5Figure 3: The 4 Layers (each one with a di\u000berent color) in the Ableton Live session view window\nFigure 4: Strategy/Layer/Emotion model, with the 9 pre-de\fned emotions based on the arousal/valence emotion\nmodel\n6Figure 5: Adding a new strategy named angry.",
                "4.8 Evaluation\nCurrent architecture has been tested with an emulated game model and with music generated from a corpus\nof ambient music.",
                "It has\nbeen tested with an emulated game model and with music generated from a corpus of ambient music."
            ],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "WHAT IS MISSING IN DEEP MUSIC GENERATION? A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC",
            " homophonic ": [],
            " polyphonic ": [
                "[31] C.-H. Chuan and D. Herremans, \u201cModeling tempo-\nral tonal relations in polyphonic music through deep\nnetworks with a novel image-based representation,\u201d in\nProc. of the AAAI Conference on Arti\ufb01cial Intelligence ,\nvol."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "[1] W. E. Caplan, Classical Form: A Theory of For-\nmal Functions for the Instrumental Music of Haydn,\nMozart, and Beethoven, Revised Edition ."
            ],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "For ex-\nample, listening experiments with reordered Classical and\nPopular music have shown that listeners are rather insensi-\ntive to restructuring, but these results are subtle and some-\nwhat ambiguous [17].",
                "Pop music, with its nearly exact repetitions, seems easier\nto study than Classical music where we might expect more\nvariation, development and modulation, which make repe-\ntition less obvious.",
                "Perhaps\nthis approach will be of use in Classical and other music.",
                "[1] W. E. Caplan, Classical Form: A Theory of For-\nmal Functions for the Instrumental Music of Haydn,\nMozart, and Beethoven, Revised Edition ."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "Figure 1 : Structure hierarchy in pop music.",
                "[36], which has 909 pop song per-\nformances in MIDI, and an American pop song dataset\nPDSA"
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "VIS2MUS: EXPLORING MULTIMODAL REPRESENTATION MAPPING FOR CONTROLLABLE MUSIC GENERATION",
            " homophonic ": [],
            " polyphonic ": [
                "In order to generate music from the latent space, we use a pre-\ntrained polyphonic music representation learning model (the\nbottom half of Figure 2)",
                "An illustration of the polyphonic disentanglement\nmodel and how to apply cross-modal transformation gto the\nlatent texture feature.\nof such design is that the intermediate representation of the\nmusic texture ztxt;mis a two-dimensional feature map, which\nmakes it easy to perform various image-like operations on it,\nsuch as changing the \u201dbrightness\u201d and \u201dcontrast\u201d of the fea-\nture map.",
                "An illustration of applying cross-modal style trans-\nformationgon the images (v1;v2)and the music pieces\n(m1;m2), respectively, in which (v1;m1)and(v2;m2)are\ntwo matched image-music pairs.\nis a sketched contour, v2is a styled image (with colored tex-\nture),m1is a melody contour, and m2is a styled accompa-\nniment (with polyphonic texture).",
                "[16] Ziyu Wang, Dingsu Wang, Yixiao Zhang, and Gus\nXia, \u201cLearning interpretable representation for con-\ntrollable polyphonic music generation,\u201d arXiv preprint\narXiv:2008.07122 , 2020."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "GENERATING MUSIC WITH SENTIMENT USING TRANSFORMER-GANS",
            " homophonic ": [],
            " polyphonic ": [
                "In [31], Biax-\nial LSTM networks are used to produce polyphonic music,and the generation can be conditioned by emotion via 4 pa-\nrameters originating from the valence and arousal dimen-\nsions."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [
                "[31] K. Zhao, S. Li, J. Cai, H. Wang, and J. Wang, \u201cAn emo-\ntional symbolic music generation system based on lstm\nnetworks,\u201d in 2019 IEEE 3rd Information Technology,\nNetworking, Electronic and Automation Control Con-\nference (ITNEC) ."
            ],
            "hip-hop": [
                "For contemporary styles,\nlike Pop or Hip-Hop, where a rigid metrical grid is often\nfollowed, it is desirable to incorporate data about the rhyth-\nmic structure of the songs into the representation."
            ],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "The excerpts on\nthe dataset are from piano transcriptions of pop songs that\nwere labeled by its authors.",
                "For contemporary styles,\nlike Pop or Hip-Hop, where a rigid metrical grid is often\nfollowed, it is desirable to incorporate data about the rhyth-\nmic structure of the songs into the representation.",
                "The\nAILABS17k dataset [38] contains over 108hours of piano\ncovers of pop songs automatically transcribed by a state-of\nthe art piano transcription model",
                "H. Yang, \u201cEMOPIA: A multi-modal pop piano dataset\nfor emotion recognition and emotion-based music gen-\neration,\u201d in Proc."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [
                "4.2 Symbolic melody representation\nIn this work, we adopted a modi\ufb01ed version of the \u201cMuMIDI\u201d symbolic music representation (18)\nto encode a piece of monophonic melody into discrete musical event sequences."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "So far, however, there is still\nan insuf\ufb01cient investigation into an alternative order of melody generation and the difference in the\nrelative structural importance among musical notes."
            ],
            " k-pop ": [],
            " pop ": [
                "2.6 Comparisons with other melody generation methods\nTo prove the effectiveness of the proposed hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, we compared WuYun-RS (i.e., using the rhythmic\nskeleton setting) to \ufb01ve public SOTA Transformer-based melody generation models, namely, Music\nTransformer (15), Pop Music Transformer (17), Compound Word Transformer (19), Melons (33)\nand MeMIDI, that follow an end-to-end left-to-right note-by-note generative paradigm and treat\neach note equally.",
                "Additionally, the MIDI quantization level of the Pop MusicTransformer, Compound\nWord Transformer, and Melons only considered the 16th note time grid.",
                "Overall, WuYun-RS (No. 6) and WuYun-RRS (No. 7) outperformed the\nother \ufb01ve current SOTA end-to-end left-to-right note-by-note melody generation models on all metrics,\nincluding MusicTransformer (No. 1), Pop Music Transformer (No. 2), Compound Word Transformer\n(No. 4), Melons (No. 5), and MeMIDI (No. 3).",
                "2.84\u00060.89 2.68\u00060.95 2.75\u00060.87 2.67\u00060.87 2.71\u00060.88\n6 WuYun-RS 3.13\u00060.88 3.07\u00060.87 3.13\u00060.86 3.02\u00060.92 3.00\u00060.87\n7 WuYun-RRS 3.20\u00060.81 3.11\u00060.85 3.15\u00060.88 3.00\u00060.96 3.02\u00060.88\n8 Human 3.54\u00060.82 3.65\u00060.76 3.68\u00060.89 3.55\u00060.92 3.57\u00060.84\nMT, PMT, and CWT stand for Music Transformer, Pop Music Transformer, and Compound Word Trans-\nformer, respectively.",
                "Yang, Pop music transformer: Beat-based modeling and generation of expressive pop piano\ncompositions, in Proceedings of the 28th ACM International Conference on Multimedia (MM, 2020), pp.",
                "Ren, J. He, X. Tan, T. Qin, Z. Zhao, T. Liu, Popmag: Pop music accompaniment generation, in\nProceedings of the 28th ACM International Conference on Multimedia (MM, 2020), pp.",
                "Generating structured pop music melodies using neural networks.",
                "3:43\u000210\u000075:37\u000210\u000092:44\u000210\u0000126:50\u000210\u000062:21\u000210\u000010\nPMT 2:18\u000210\u000081:21\u000210\u000091:20\u000210\u000062:55\u000210\u000061:13\u000210\u00007\nMeMIDI 1:88\u000210\u000063:09\u000210\u000066:86\u000210\u000072:31\u000210\u000056:29\u000210\u00007\nCWT 3:31\u000210\u000048:47\u000210\u000043:02\u000210\u000042:03\u000210\u000031:69\u000210\u00003\nMelons 4:60\u000210\u000031:54\u000210\u000038:02\u000210\u000042:12\u000210\u000037:41\u000210\u00003\nWuYun-RRS 0.26 0.36 0.42 0.42 0.41\nMT, PMT, and CWT stand for Music Transformer, Pop Music Transformer, and Compound Word Trans-\nformer, respectively.\n19"
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "An investigation of the reconstruction capacity of stacked convolutional autoencoders for log-mel-spectrograms",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [
                "In an unsupervised manner, the\nnetwork is able to reconstruct a monophonic and harmonic sound\nbased on latent representations.",
                "The network attempts to\nproject the log-mel-spectrogram of monophonic and harmonic\nsounds to a lower dimensional space.",
                "More-\nover, we presented an evaluation method for calculating the\naccuracy of predicted frequencies in monophonic and har-\nmonic musical sounds."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "The design of the network is\nillustrated in Fig. 1, where an encoder based on convolu-\ntional networks aims to compress instrumental sounds to a\nlower dimensional space and a mirrored decoder attempts to\nreconstruct the samples from this high-level representation.",
                "Based on the results above,\nwe can conclude that stacked convolutional autoencoders with\na rough pooling approach, such as max pooling, can generate\na more accurate audio time-frequency representation from a\ncompressed low dimensional space of instrumental pitched\nsound.",
                "Furthermore, using Wavenet [24] as a vocoder may not pre-\nserve the phase continuity in the waveform and therefore\ngenerate more noisy instrumental sounds."
            ],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Byte Pair Encoding for Symbolic Music",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "Sturm, B. L., Santos, J. F., and Korshunova, I. Folk mu-\nsic style modelling by recurrent neural networks with\nlong short-term memory units."
            ],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "The POP909 dataset (Wang et al., 2020b) is composed of\n909 piano tracks of Pop musics, with aligned MIDI and\naudio versions."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "A Symbolic-domain Music Generation Method Based on Leak-GAN",
            " homophonic ": [],
            " polyphonic ": [
                "[10] proposed three \ndifferent methods combined with GAN to handle the \ninteraction among tracks and generate polyphonic music with \nharmonic and rhythmic structure."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "Based on the understanding  \nof how pop music is composed, Dong et al.",
                "The reason we choose the length of 150 is that in our representation of pop song melodies, the verse and chorus of a song are approximately 150-word sequence."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "AI Music Therapist: A Study on Generating Specific Therapeutic Music based on Deep Generative Adversarial Network Approach",
            " homophonic ": [],
            " polyphonic ": [
                "Finally, in polyphonic \nmusic, notes are usually divided into chords or melodies."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [
                "2022 IEEE 2nd International Conference on Electronic Technology, Communication and Information (ICETCI) \nAI Music Therapist: A Study on Generating \nSpecific Therapeutic Music based on Deep \nGenerative Adversarial Network Approach \nYurui Hou \nWalnut Hill School for The Arts \nNatick, MA 01760, United States \nyurui.hou2023 @walnuthillarts.org",
                "International Conference on Electronic Technology, Communication and Information (ICETCI)"
            ],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "Wang Yuhua et al. played pop songs or \nlight music to conscious patients during surgery, and \nobserved the changes of their heart rate and blood pressure, \nand scored their anxiety."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "An intelligent music generation based on Variational Autoencoder",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [
                "[10] established a \nbacktracking Backtracking specification language (BSL), \nwhich is used to implement CHORAL, a rule-based expert \nsystem that can construct a four-part chorus with Bach style \nfor the monophonic main melody, and has certain practical \nvalue."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "Waltzes in classical music, \nfor example, are characterized by its triple meter style, in \nwhich the dance steps rise and fall with the beat, giving one \na magnificent and elegant enjoyment.",
                "Experimental data \nThe data used in this paper is the classical piano MIDI \ndataset, which contains music files of different formats for \nall classical music."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "APE-GAN: A Novel Active Learning Based Music Generation Model With Pre-Embedding",
            " homophonic ": [],
            " polyphonic ": [
                "Notably this\nmodel can generate multi-track polyphonic musics and reason-\nable accompaniments.",
                "Convolutional generative adversarial\nnetworks with binary neurons for polyphonic music generation."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [
                "\u000fWe use active learning technology to ef\ufb01ciently train\nour APE-GAN to achieve high performance under the\n123\nICEICT 2021 \nIEEE 4th International Conference on Electronic Information and Communication Technology\n978-1-6654-3203-0/21/$31.00 \u00a92021 IEEE\nXi'an, China \u2022",
                "August 18-20, 20212021 IEEE 4th International Conference on Electronic Information and Communication Technology (ICEICT) | 978-1-6654-3203-0/21/$31.00",
                "Can GAN originate new electronic dance music\ngenres?\u2013Generating novel rhythm patterns using GAN with Genre\nAmbiguity Loss."
            ],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "A mu-\nsically plausible network for pop music generation."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Automatic Music Generation System based on RNN Architecture",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [
                "Dataset A GAN-based model that was trained on a dataset of Bach's \norchestral symphonies produced the desired outcomes."
            ],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "The \nmost classical track becomes composed of sophisticated \nshapes and arrangements to deliver ideas and feelings",
                "11 Tianyu Jiang et al., \n2019 Bidirectional LSTM \nnetwork  Classical \nPiano \nDataset Bidirectional LSTM network was presented to produce harmonic \nmusic."
            ],
            " electronic ": [
                "[15] Lang, Runnan, Songsong Wu, Songhao Zhu, and Zuoyong Li, \n\u201cSSCL: Music Generation in Long-term with Cluster Learning,\u201d In \n4th IEEE Information Technology, Networking, Electronic and \nAutomation Control Conference (ITNEC), pp. 77-81, 2020."
            ],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Development of Application Software for Generating Music Composition Inspired by Nature Using Deep Learning",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            " homophonic ": [],
            " polyphonic ": [
                "Here we explore\nits ability to reconstruct polyphonic music .",
                "Fig-\nure 2 indicates that DDSP generalised best to all classes, im-\nplying that polyphonic music, reconstructed by DDSP, main-\ntains much of its meaningful information."
            ],
            " monophonic ": [
                "to generate commercial music [1], [3], [8], the latter was\nproposed in the context of monophonic audio.",
                "Although DDSP was\ndesigned for monophonic music, it can also reconstruct poly-\nphonic music since it is trained to reconstruct a spectrogram."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [
                "[8] C. Carr and Z. Zukowski, \u201cGenerating albums with samplernn to imitate\nmetal, rock, and punk bands,\u201d arXiv preprint arXiv:1811.06633 , 2018."
            ],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Generating Music Algorithm with Deep Convolutional Generative Adversarial Networks",
            " homophonic ": [],
            " polyphonic ": [
                "Music can \nbe separated into main tone music and polyphonic music according to the tone classification.",
                "In order to solve a problem that can solve polyphonic music and multi-track orbit generation, this paper proposes a scheme consistent \nwith this algorithm."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Generating Music with Emotions",
            " homophonic ": [],
            " polyphonic ": [
                "[21] model to\nreproduce polyphonic music sequences.",
                "Similarly, recurrent neural network\n(RNN) based GAN is proposed in C-RNN-GAN [23], which\ncan generate polyphonic continuous music sequence.",
                "Collect large-scale polyphonic music dataset\nwith emotion labels is a valuable further work for us."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "In human life, the contem-\nporary pop music is often used to express and share\nemotions.",
                "Therefore, we think the deep learning\nmodel trained on EMOPIA cannot be used to annatate our\ndataset because of the following reasons: 1) There\u2019s a domain\ngap between piano solo performances and pop songs\u2019 melodies\nin our dataset.",
                "Yang,\n\u201cEmopia: A multi-modal pop piano dataset for emotion recognition\nand emotion-based music generation,\u201d arXiv preprint arXiv:2108.01374,\n2021."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Generating Music with Generative Adversarial Networks and Long Short-Term Memory",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles.",
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            " classical ": [],
            " electronic ": [
                "The author found that the melodies generated by this model were  2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI) | 978-1-6654-3881-0/21/$31.00 \u00a92021 IEEE | DOI: 10.1109/CEI52496.2021.9574491\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "MIDI is a technical standard that describes a communications protocol, \ndigital interface, and electrical connectors that connect a wid e \nvariety of electronic musical instruments, computers, and \nrelated audio devices for playing, editing, and recording music  \n[11]."
            ],
            "hip-hop": [],
            " reggae ": [
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Generation of Music With Dynamics Using Deep Convolutional Generative Adversarial Network",
            " homophonic ": [],
            " polyphonic ": [
                "PROPOSED METHOD  \nIn this paper, we designed and implemented a music \ngeneration system that can generate polyphonic music with \ndynamic using Deep Convolutional Generative Adversarial \nNetwork (DCGAN)."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Monophonic Music Generation With a Given Emotion Using Conditional Variational Autoencoder",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [
                "For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nsystems",
                "In [34], Hadjeres et al. proposed geodesic latent space reg-\nularization for the variational autoencoder, which enhances\nVOLUME 9, 2021 129089J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nlatent space navigation with the change of the attributes of the\ndecoded sequences.",
                "The music generation system created in this work should\ngenerate monophonic sequences, therefore the original\nJ. S. Bach music21 Dataset underwent several transforma-\ntions (Fig. 1).",
                "Happy, angry, sad, relaxed, these\n129090 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 2.",
                "There are four\n3https://github.com/grekowj/musgenvae\nVOLUME 9, 2021 129091J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nsixteenth notes for each quarter note, dividing the segment\nwith the shortest note (sixteenth note) we get 64 time steps,\n4(bar)\u00024(quarter note )\u00024(sixteenth note ).",
                "Latent\nloss is calculated using the Kullback-Leibler divergence,\nwhich calculates the distance between the target distribution\n(the Gaussian distribution) and the actual distribution in latent\nvector z:\nLLD\u00001\n2KX\niD1(1Clog\u001b2\ni\u0000\u001b2\ni\u0000\u00162\ni) (1)\nwhere Kis the dimensionality of latent vector z,\u0016iand\u001bi\nare mean and standard deviation of idimension of latent\nvector z.\nB. TRAINING OF THE NETWORK\nFor our classi\u001ccation task, which is the prediction of one\ncategory (one pitch of note), the softmax function was used as\n4https://keras.io\n5https://www.tensor\u001dow.org\n129092 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 5.",
                "Testing how the use of\nthe baseline model (CVAE CDense) and the proposed model\n(CVAECGRU) affects the obtained metrics for the generated\nVOLUME 9, 2021 129093J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 6.",
                "A set of generated MIDI examples can be found\n129094 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 8.",
                "The sequences with emotions happy (e1)\nVOLUME 9, 2021 129095J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 10.",
                "Table 8 shows which of the training sets are closest to\n129096 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 12.",
                "The diagonal values are\nVOLUME 9, 2021 129097J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nTABLE 4.",
                "The\nannotated examples were mixed up so that their order was\n129098 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nnot grouped by emotion.",
                "This article presents the stages of creating a system gen-\nerating monophonic musical sequences with one of four\nbasic emotions.",
                "The limitations of this study include the emotional model\nwe adopt, the musical area used in the training set, and the\nlength of the monophonic pieces.",
                "Another potential application of the system is music therapy,\nVOLUME 9, 2021 129099J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nwhere the generated melodies with a speci\u001cc emotion could\nbe used to change or enhance the emotional state of the\npatient.",
                "13\u001524.\n129100 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\n[42] A. P. Oliveira and A. Cardoso, ``Towards affective-psychophysiological\nfoundations for music production,'' in Affective Computing and Intelligent\nInteraction ."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Music Deep Learning: A Survey on Deep Learning Methods for Music Processing",
            " homophonic ": [],
            " polyphonic ": [
                "In [33] a novel RNN model, DeepBach, is proposed aimed\nat modeling polyphonic music and specifically hymn-like\npieces, while in [34] the model produces only drums\u2019 sounds.",
                "Recurrent neural\nnetworks for polyphonic sound event detection in real life recordings.",
                "Automatic Instrument Recogni-\ntion in Polyphonic Music Using Convolutional Neural Networks.",
                "Generating Polyphonic Music Using Tied\nParallel Networks."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "Finally, in [8] classical ML and DL methods\nare reviewed for the task of music genre classification.",
                "Variants of classical transformer\nare designed in [52], [53] reducing the required memory, A\nsparse factorization of the attention matrix was proposed in\n[54], reducing the computation time and producing longer\nsequences of data, including music.",
                "The\nraw audio data were first compressed into compressed codes\nusing Vector Quantization - Variational Autoencoders (VQ-\nV AE), a variant of classical V AE which produces discrete data."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "C. Alternative approaches\nSome other alternative approaches have been utilized\nthrough the years in various MIR tasks, providing new ways\nto extract useful information."
            ],
            " k-pop ": [],
            " pop ": [
                "The authors of [54]\npropose Pop Music Transformer to compose pop piano music,\nachieving better rhythmic structure than other models.",
                "Pop Music Transformer:\nBeat-based Modeling and Generation of Expressive Pop Piano Com-\npositions."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Music Generation using Deep Generative Modelling",
            " homophonic ": [],
            " polyphonic ": [
                "[8] J.  A.   Henning,  A.  Umakantha   and   R.  C.   Williamson,  \u201cA classifying \nvariational autoencoder with the application to polyphonic music generation,\u201d\narXiv preprint arXiv:1711.07050, 2017."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "VI.METHODOLOGY\nWe are focusing on classical music for training of the model \nbecause of the availability of MIDI files for classical music as \nwell as classical music is relatively more standardized in \ncontrast t o other genres."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "[3] A. Shin and L. Crestel, \u201cMelody Generation for Pop Music via word\nrepresentation of musical properties,\u201d arXiv, vol."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Music Generation using Deep Learning with Spectrogram Analysis",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Music Generation with AI technology: Is It Possible?",
            " homophonic ": [],
            " polyphonic ": [
                "The Biaxial-LSTM can \ngenerate polyphonic music, and the model evaluation mainly \nuses the Turing test.",
                "DeepJ can be seen as an improvement based \non Biaxial-LSTM, generating specific-style polyphonic music.",
                "While LSTM \nmodel can only be used to generate monophonic music, Biaxial \nLSTM can be used to generate polyphonic music which is a kind \nof music texture whe re multiple voices play at the same time.",
                "[0 0\n1 10 0\n0 0\n0\n10\n10\n00\n0]  \nAlso, while holding a note is not the same as replaying a note, \nwe need to distinguish these two events, so \ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66  is also \nneeded which looks similar to \ud835\udc61\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66. \n2) Architecture \nBiaxial LSTM generates polyphonic music by modeling \nevery note in every time step as a probability, using all previous \ntime steps and all notes already generated in the current time step \nas the condition.",
                "LSTM model performs weak \nwhen generating polyphonic music.",
                "As this model is \ndeveloped based on Biaxial LSTM, it has retained the features \nof genera ting polyphonic music texture and transposition \ninvariance.",
                "Biaxial LSTM can \ndistinguish relative pitches and therefore can handle polyphonic \nmusic, which is the biggest improvement over the LSTM model.",
                "This model is related to musical style, but \nis more suitable for polyphonic music generation with multiple \ntracks.",
                "The soundtracks are all polyphonic piano \npieces.",
                "This model \nhas the ability to generate polyphonic music, but since the \nalgorithm still does not have the ability to filter music styles in \norder to customize music generated, two cutting -edge researches \non AI music gen eration  were  further explored, which are the \nDeepJ and MuseGAN models.  \n \nFig. 12."
            ],
            " monophonic ": [
                "While LSTM \nmodel can only be used to generate monophonic music, Biaxial \nLSTM can be used to generate polyphonic music which is a kind \nof music texture whe re multiple voices play at the same time."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "In DeepJ model, we use a dataset of 23 different composers \nfrom different classical music periods.",
                "In this paper, the DeepJ model is mainly used for the \ngeneration of classical music, and the generated music style also \nfocus on classical music.",
                "C. Features  in Datasets  \n1) DeepJ model  \nThe dataset of DeepJ includes MIDI files of pieces \ncomposed by 23 well -known composers in the three major \nperiods of class ical music (Baroque period, classical period, and \nromantic period).",
                "20 people with musical background are \nselected to classify the classical period for the specific piece they \nheard.",
                "Beethoven, who is the yellow point right next to the red point, is \na representation for the transition between classical and \nromantic periods, which proved our observation.",
                "We also made \nanother 3 -D visualization with x -axis, y -axis, and z -axis \nrepresenting baroque, classical and romantic period, \nrespectively.  \n \nFig.",
                "In theory, the DeepJ model can be generalized to other types \nof music, but modific ations to the model are required, so the \nmusic generated here is classified into three categories of \nclassical music which are Baroque, Classical and Romantic \nperiods."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "MuseGAN \nis an innovational approach based on Generative Adversarial \nNetwork that can be used to generate pop music with multiple \ntracks.",
                "For instance, in pop music, there is more \nthan one instrument playing at the same time and have their own \nmelody and rhythmic patterns.",
                "Both of these two models are related to \nmusic style, while the main difference is, the DeepJ model is \napplicable to all musi c styles, but the MuseGAN model is mostly \nused to generate pop music which has multiple instruments and tracks.",
                "4) Apply GAN into pop music generation  \nPop music is usually composed of multiple \ninstruments/tracks.",
                "Due to the existence of a long -term independent  structure of \nmultitrack pop music, the authors used bars as the most basi c \nunit for model training.",
                "The MuseGAN model, on the other \nhand, is mainly targeted at modern pop music and usually \ncontains a variety of instruments such as drums, bass, guitar, \norchestra and piano.",
                "2) MuseGAN model  \nThe goal of Mu seGAN is to generate pop music of multiple \ntracks in piano -roll format.",
                "The \nMuseGAN model is another style -related model that is used to \ngenerate music with multiple instrument tracks, normally used \nto generate modern pop music.",
                "Compared with DeepJ model, the MuseGAN model is \nmore suitable for generating pop music w ith multiple \ninstruments and tracks, and can be used in the future for game \nsoundtracks, etc."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Music Generation with Bi-Directional Long Short Term Memory Neural Networks",
            " homophonic ": [
                "This paper works with the generation of homophonic \nmusic for which the piano channel of the input MI DI file is \nconverted to a piano roll format and is read over two axes."
            ],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "Firstly, the file \nname is passed as the pa rameter to load the song in the \nvariable followed by extracting the data of the first channel, \ni.e., the piano instrumental of the song."
            ],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "RE-RLTuner: A topic-based music generation method",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "[8] Sturm B, Santos J F, Korshunova I. Folk music style modelling by\nrecurrent neural networks with long short term memory units[C]//16thInternational Society for Music Information Retrieval Conference.2015."
            ],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "Inspiredby RLTuner, we treated the music generation problem asa sequential decision problem using the classical DQNalgorithm."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Some Reflections on the Potential and Limitations of Deep Learning for Automated Music Generation",
            " homophonic ": [],
            " polyphonic ": [
                "Bengio, and P. Vincent,\n\u201cModeling temporal dependencies in high-dimensional se-\nquences: Application to polyphonic music generation and\ntranscription,\u201d arXiv preprint arXiv:1206.6392, 2012."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "This notation was introduced\nin the early 90s to share Irish folk tunes on the\nInternet and has since become the most popular\nformat alternative to midi (it even has a MIME type).",
                "Bach-styled\nChorales generated in [14] and Irish folk music\ngenerated in [15] are an example of how a having\na corpus with rigid structural rules makes it easier\nfor the model to generate convincing results, even\nthough expert ears can still identify them as arti\ufb01cial\ndue to some dubious style decision or technical\nerror."
            ],
            " blues ": [
                "[12] D. Eck and J. Schmidhuber, \u201cFinding temporal structure\nin music: Blues improvisation with LSTM recurrent net-\nworks,\u201d in Neural Networks for Signal Processing, 2002."
            ],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "Both models are based on CNNs,\nbut while MidiNet uses a fairly classical CNN\narchitecture, MuseGan uses an unprecedented model\nwith a strong hierarchy that is based not on the\nsingle not but on the bar as a unit, featuring a bar\ngenerator controlled by a phrase generator."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "This notation was introduced\nin the early 90s to share Irish folk tunes on the\nInternet and has since become the most popular\nformat alternative to midi (it even has a MIME type)."
            ],
            " k-pop ": [],
            " pop ": [
                "Available:\nhttp://arxiv.org/abs/1704.01279\n[28] M. Marchini, F. Pachet, and B. Carr \u00b4e, \u201cRethinking Re\ufb02exive\nLooper for structured pop music,\u201d in Proceedings of the\nInternational Conference on New Interfaces for Musical\nExpression, 2017, pp."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation",
            " homophonic ": [],
            " polyphonic ": [
                "[6] S. Lattner, M. Grachten and G. Widmer, Imposing higher-level Structure\nin Polyphonic Music Generation using Convolutional Restricted Boltz-\nmann Machines and Constraints, Journal of Creative Music Systems,\nvol. 2, Issue 1, March 2018"
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "[35], which consists of\n941 folk songs, and each contains both melody and chords."
            ],
            " blues ": [
                "They\ntested the Blues improvisation performance of LSTM by\ninputting note slices in real time."
            ],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "Formally:\nH0:\u03bcA=\u03bcB=\u03bcC (11)\nthe alternative hypothesis is that:\nH1:\u2203i,j\u2208{A,B,C}:\u03bci/negationslash=\u03bcj (12)",
                "Formally:\nH0:\u03bci=\u03bcj (13)\nthe alternative hypothesis is that:\nH1:\u03bci/negationslash=\u03bcj (14)\nD. Survey Evaluation\nA total of n= 106 people (42 females and 64 males) have\ncompleted the survey."
            ],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "PopMNet: Generating structured pop music melodies using neural networks",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [
                "In a monophonic  melody,  all notes and \nrests are represented  by 128 \u201cnote-on\u201d  tokens  and one \u201cnote-off\u201d  token."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [
                "Over 10 years ago, it was used to learn a form of blues music [14]."
            ],
            " jazz ": [
                "GenJam  is a genetic  algorithm-based  model  that pro-\nduces  jazz solos over a given chord  progression [ 12].",
                "[12]J.A. Biles, et al., GenJam:  a genetic  algorithm  for generating  jazz solos, in: International  Computer  Music Conference,  vol."
            ],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "Arti\ufb01cial Intelligence 286 (2020) 103303\nContents lists available at ScienceDirect\nArti\ufb01cial  Intelligence\nwww.elsevier.com/locate/artint\nPopMNet:  Generating  structured  pop  music  melodies  using  \nneural  networks\nJian Wua, Xiaoguang Liub, Xiaolin Hua,\u2217, Jun Zhua\naInstitute  for Arti\ufb01cial  Intelligence,  Beijing National  Research  Center for Information  Science and Technology  (BNRist),  the State Key Laboratory  \nof Intelligent  Technology  and Systems,  and Department  of Computer  Science and Technology,  Tsinghua  University,  Beijing 100084,  China\nbLingDongYin  Technoloy  Co., Ltd., Beijing, 100084,  China\na r t",
                "However,  generating  pop music melodies  with well organized  structures  remains  to be \nchallenging.",
                "In this paper, we present  a melody  structure-based  model  called PopMNet  \nto generate  structured  pop music melodies.",
                "Fig.1shows  the melody  of a pop song \u201cSimple  Love\u201d.",
                "Speci\ufb01cally,  we consider  two important  relations  \u2014 repetition and sequence , which  play critical  \nroles in the formation  of pop music  melody  structures",
                "1.A piece of melody of \u201cSimple Love\u201d, which is a pop song by Chinese singer Jay Chou, released on 14 September 2001.",
                "Many types of music  such as pop music  have high-level  units such as phrases  and periods  \u2014 a \nphrase  consists  of several  bars and a period  consists  of several  phrases.",
                "Discussion\nWe present  the PopMNet  to generate  structured  pop music  melodies,  which  integrates  melody  structure  into the gen-\neration  process.",
                "This is only the \ufb01rst step towards  generating  rich and compelling  pop music  melodies.",
                "First, only repetition  and rhythmic  \nsequence  between  bars were considered  as structures,  while real, human-composed  pop music  melodies  contain  much more \ncomplex  relations  between  melody  segments,  which  will certainly  be studied.",
                "[5]H. Zhu, Q. Liu, N.J. Yuan, C. Qin, J. Li, K. Zhang, G. Zhou, F. Wei, Y. Xu, E. Chen, Xiaoice  band: a melody  and arrangement  generation  framework  \nfor pop music, in: Proceedings  of the 24th ACM SIGKDD  International  Conference  on Knowledge  Discovery,  Data Mining,  New York, NY, USA, 2018, \npp.",
                "[16]H. Chu, R. Urtasun,  S. Fidler, Song from pi: a musically  plausible  network  for pop music generation,  in: International  Conference  on Learning  Repre-\nsentations  (ICLR), 2017, workshop  track."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Singability-enhanced lyric generator with music style transfer",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [
                "The second dataset is the audio of the original lyrics,\nand the DALI dataset [24] was chosen for this study, which is a large\ndataset of audio full sections synchronized with audio, lyrics and notes,\nwith lyrics and notes (of the vocal melody) aligned in time."
            ],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [
                "Then the rock lyrics dataset\nis trained for model migration and the lyric text is modified using a\npost-processing module to ensure that every line and word in the lyrics\nmatches the audio.",
                "Then the rock lyrics dataset\nis trained for model migration and the lyric text is modified using a\npost-processing module to ensure that every line and word in the lyrics\nmatches the audio.",
                "Experiments were conducted on an English music\ndataset containing pop and rock music.",
                "Experiments were conducted on an English music\ndataset containing pop and rock music.",
                "The different lyric styles\nuse different expressions and lyricism, e.g. pop lyrics tend to emphasize\nromantic love, whereas rock lyrics tend to emphasize social or political\naspects.",
                "The different lyric styles\nuse different expressions and lyricism, e.g. pop lyrics tend to emphasize\nromantic love, whereas rock lyrics tend to emphasize social or political\naspects.",
                "We had some\nlimitations song lyric dataset size, so we focused on pop and rock styles,\ncombining pop and rock lyric data to form a text file with over 8000\n40J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "We had some\nlimitations song lyric dataset size, so we focused on pop and rock styles,\ncombining pop and rock lyric data to form a text file with over 8000\n40J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "For example, the lyrics could be constrained to be\nlike rock style, with the key sentence \u2018\u2018I\u2019ve seen your shadow in the dark\nI\u2019ve seen this struggle in your life\u2019\u2019 as a prefix.",
                "For example, the lyrics could be constrained to be\nlike rock style, with the key sentence \u2018\u2018I\u2019ve seen your shadow in the dark\nI\u2019ve seen this struggle in your life\u2019\u2019 as a prefix.",
                "Thus, pop\nstyle and rock style models were generated by fine-tuning GPT-2.",
                "Thus, pop\nstyle and rock style models were generated by fine-tuning GPT-2.",
                "Suppose we have two lyric text datasets X = {x(1),x(2),\u2026,x(m)}\n(original lyrics) and Y = {y(1),y(2),\u2026,y(n)}(output lyrics after GPT-2\nprocessing), with styles SxandSy, respectively (e.g. Sxis pop style and\nSyis rock style).",
                "Suppose we have two lyric text datasets X = {x(1),x(2),\u2026,x(m)}\n(original lyrics) and Y = {y(1),y(2),\u2026,y(n)}(output lyrics after GPT-2\nprocessing), with styles SxandSy, respectively (e.g. Sxis pop style and\nSyis rock style).",
                "Suppose pop music style lyrics are transferred to rock music.",
                "Suppose pop music style lyrics are transferred to rock music.",
                "Then\nX(structure template) is the provided pop music lyrics and Yis the\ntarget rock music lyrics.",
                "Then\nX(structure template) is the provided pop music lyrics and Yis the\ntarget rock music lyrics.",
                "A few lines from the target lyrics are chosen\nas the key sentence Kso GPT-2 can include rock style elements in its\ngeneration ( G).",
                "A few lines from the target lyrics are chosen\nas the key sentence Kso GPT-2 can include rock style elements in its\ngeneration ( G).",
                "Table 3-4 shows the lyrics change to \u2018\u2018You make river\nwas a black sea\u2019\u2019, which has same dependencies between words as in\nthe original lyrics and adds a rock element to the sentence.",
                "Table 3-4 shows the lyrics change to \u2018\u2018You make river\nwas a black sea\u2019\u2019, which has same dependencies between words as in\nthe original lyrics and adds a rock element to the sentence.",
                "We chose a sample of 100 stylized\nlyrics texts generated by the proposed system for each transfer task\nfor users to cross-rate, with 50 each being pop to rock and rock to\npop conversion, respectively.",
                "We chose a sample of 100 stylized\nlyrics texts generated by the proposed system for each transfer task\nfor users to cross-rate, with 50 each being pop to rock and rock to\npop conversion, respectively.",
                "Fig. 4-3 and Fig. 4-6 show stylized lyrics using\nGPT-2 from pop to rock and rock to pop, respectively.",
                "Fig. 4-3 and Fig. 4-6 show stylized lyrics using\nGPT-2 from pop to rock and rock to pop, respectively.",
                "We set the key\nsentence as \u2018\u2018Bury every word I\u2019ve said in the city of the dead and drown\nthis masterpiece in red\u2019\u2019 (rock style) and conditionally transferred from\npop to rock style.",
                "We set the key\nsentence as \u2018\u2018Bury every word I\u2019ve said in the city of the dead and drown\nthis masterpiece in red\u2019\u2019 (rock style) and conditionally transferred from\npop to rock style.",
                "The generated results are divided into two\nparts, one is Pop to Rock (Pop2Rock) and the other is Rock to Pop\n(Rock2Pop).",
                "The generated results are divided into two\nparts, one is Pop to Rock (Pop2Rock) and the other is Rock to Pop\n(Rock2Pop).",
                "A total of 100 songs in the experimental section, 50\nPop to Rock and 50 Rock to Pop, will be evaluated for each of\nthe three different approaches.",
                "A total of 100 songs in the experimental section, 50\nPop to Rock and 50 Rock to Pop, will be evaluated for each of\nthe three different approaches.",
                "In Pop2Rock, GPT-2+DP has an overlap of 0.6683 \u00b10.1351, and\nin Rock2Pop, GPT-2+DP has an overlap of 0.6962 \u00b10.0996, which\nis even higher than GPT-2 alone because GPT-2+DP draws on the\nstructure of the original pop or rock lyrics, thus increasing the overlap\nscore, but the overlap is within acceptable limits and the content still\nretains the results produced by GPT-2.",
                "In Pop2Rock, GPT-2+DP has an overlap of 0.6683 \u00b10.1351, and\nin Rock2Pop, GPT-2+DP has an overlap of 0.6962 \u00b10.0996, which\nis even higher than GPT-2 alone because GPT-2+DP draws on the\nstructure of the original pop or rock lyrics, thus increasing the overlap\nscore, but the overlap is within acceptable limits and the content still\nretains the results produced by GPT-2.",
                "The result of the GPT-2 processing of the lyrics from Rock to Pop.",
                "The result of the GPT-2 processing of the lyrics from Rock to Pop.",
                "The result of the GPT-2+DP processing of the lyrics from Rock to Pop.\nTable 4-3\nResults of human evaluation (Pop2Rock).",
                "The result of the GPT-2+DP processing of the lyrics from Rock to Pop.\nTable 4-3\nResults of human evaluation (Pop2Rock).",
                "The result of the GPT-2+DP+RM processing of the lyrics from Rock to Pop.\nFig. 4-9. Results of human evaluation (Pop2Rock).",
                "The result of the GPT-2+DP+RM processing of the lyrics from Rock to Pop.\nFig. 4-9. Results of human evaluation (Pop2Rock)."
            ],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [
                "This study captured pop, rap, country, rock, and reggae lyric styles\nfrom the Genius website [22], which is non-parallel data, to overcome\nsparse data problems when generating different lyric styles."
            ],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "Experiments were conducted on an English music\ndataset containing pop and rock music.",
                "For example,\nsuppose pop song lyrics were used as training data, then and all the data\nis combined into a single text, splitting each lyric with < |endoftext |>.",
                "The different lyric styles\nuse different expressions and lyricism, e.g. pop lyrics tend to emphasize\nromantic love, whereas rock lyrics tend to emphasize social or political\naspects.",
                "The different styles often use different words, e.g. pop songs\nusually contain \u2018\u2018love\u2019\u2019, \u2018\u2018feel\u2019\u2019, \u2018\u2018live\u2019\u2019, \u2018\u2018heart\u2019\u2019, and \u2018\u2018tell\u2019\u2019; whereas rock\nsongs usually contain \u2018\u2018oh\u2019\u2019, \u2018\u2018never\u2019\u2019, \u2018\u2018burn\u2019\u2019, and \u2018\u2018rock\u2019\u2019.",
                "We had some\nlimitations song lyric dataset size, so we focused on pop and rock styles,\ncombining pop and rock lyric data to form a text file with over 8000\n40J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "Suppose we have two lyric text datasets X = {x(1),x(2),\u2026,x(m)}\n(original lyrics) and Y = {y(1),y(2),\u2026,y(n)}(output lyrics after GPT-2\nprocessing), with styles SxandSy, respectively (e.g. Sxis pop style and\nSyis rock style).",
                "Suppose pop music style lyrics are transferred to rock music.",
                "Then\nX(structure template) is the provided pop music lyrics and Yis the\ntarget rock music lyrics.",
                "We chose a sample of 100 stylized\nlyrics texts generated by the proposed system for each transfer task\nfor users to cross-rate, with 50 each being pop to rock and rock to\npop conversion, respectively.",
                "Fig. 4-3 and Fig. 4-6 show stylized lyrics using\nGPT-2 from pop to rock and rock to pop, respectively.",
                "The generated results are divided into two\nparts, one is Pop to Rock (Pop2Rock) and the other is Rock to Pop\n(Rock2Pop).",
                "In Pop2Rock, GPT-2+DP has an overlap of 0.6683 \u00b10.1351, and\nin Rock2Pop, GPT-2+DP has an overlap of 0.6962 \u00b10.0996, which\nis even higher than GPT-2 alone because GPT-2+DP draws on the\nstructure of the original pop or rock lyrics, thus increasing the overlap\nscore, but the overlap is within acceptable limits and the content still\nretains the results produced by GPT-2.",
                "The result of the GPT-2 processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP+RM processing of the lyrics from Pop to Rock."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Human, I wrote a song for you: An experiment testing the influence of machines\u2019 attributes on the AI-composed music evaluation",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [
                "In this case, people will see this machine less as a musician but rather as \na musical instrument, such as software that electronic dance music \n(EDM) musicians use.",
                "Journal of Broadcasting & Electronic Media, 64(4), 566\u2013591. https://doi. \norg/10.1080/08838151.2020.1835136 \nYang, L., Chou, S., & Yang, Y. (2017)."
            ],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "As a result, the concept of \nAI-generated art has emerged as an alternative art form (Smith & Leymarie, 2017 )."
            ],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Rethinking musicality in dementia as embodied and relational",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "Research on the impact of music programs is dominated by studies\nthat evaluate music as a therapeutic tool to achieve instrumental out-\ncomes ( DeNora & Ansdell, 2014).",
                "Lack of engagement with this sub-\ufb01eld has not only impoverished understandings of musicality, but has\nalso restricted music in dementia care to its application as a therapeutictool to achieve instrumental outcomes such as improving cognitive\nfunctioning.",
                "Further, given that our analysis demonstrates\nthat musicality is embodied and persists despite even severe cognitive\nimpairment, it is egregious that music is restricted in dementia care to\nits instrumental application as a therapeutic tool to improve \u2018beha-\nviours \u2019and cognitive functioning."
            ],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [
                "Such relationships are rarely\nexplicitly discussed in dementia care, despite the fact that the \u201crules,\nlaws and policies of the country or jurisdiction in which a person lives\u201d\n(Bartlett & O'Connor, 2010, p. 30) will inevitably in \ufb02uence the ex-\nperiences and opportunities associated with the health and social care aperson with dementia receives ( Kontos, Grigorovich, et al., 2016;\nKontos, Miller, & Kontos, 2017 ;Miller & Kontos, 2016; Reid, Ryan, &\nEnderby, 2001 )."
            ],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [
                "and Bennett (2015) in drawing on empirical data gen-\nerated through research on the punk scene in Southeast Queensland,Australia, retheorize the concept of \u2018music scene \u2019to highlight the cri-\ntically important role of embodiment for how music scenes are con-structed, enacted, and maintained.",
                "Crossley\n(2015) takes up this theme in his exploration of the links between body\ntechniques and music scenes, or what he refers to as \u2018music worlds\u2019 ,\nfocusing on early UK punk in London."
            ],
            " alternative ": [
                "In response to critique regarding the overreliance on pharma-\ncotherapies, non-pharmacological approaches are now recommendedas an alternative to psychotropic medication ( Fossey et al., 2006;\nMoniz-Cook, Woods, & Richards, 2001)."
            ],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "deepsing: Generating sentiment-aware visual stories using cross-modal music translation",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [
                "More recent works also\nprovided tools for better understanding the harmonic structures in mu-\nsic (Malandrino et al. , 2015 ), the semantic structure in classical music\nworks ( Chan, Qu, & Mak , 2009 ) or even improving the understanding\nof music compositions ( De Prisco, Malandrino, Pirozzi, Zaccagnino, &\nZaccagnino , 2017 ), allowing for assisting the learning process ( Malan-\ndrino, Pirozzi, & Zaccagnino , 2019 ).",
                "Another\nquite interesting observation is that the effectiveness of the proposed\nmethod varies for different music genres, e.g., classical music leads to\nthe best results, followed by jazz.",
                "Visualizing the semantic structure\nin classical music works.",
                "Music in India: The classical traditions ."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [
                "The generated images match this sentiment,\nsince they are quite neutral and with low arousal (a car mirror and\ncastle in the country side)."
            ],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "Uehara, Misa, & Itoh, Takayuki Pop music visualization based on acoustic features and\nchord progression patterns applying dual scatterplots."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [
                "On the other hand metal, disco and\nrock consistently led to the lowest precision compared to the rest of the\nevaluated methods."
            ],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "ComposeInStyle: Music composition with and without Style Transfer",
            " homophonic ": [],
            " polyphonic ": [
                "In the paper (Johnson, 2017), a set of\nparallel, tied weight Recurrent Neural Network (RNN) is used to predict\nand generate polyphonic music compositions which is transposition\ninvariant.",
                "Description of GAN architectures \ud835\udc3a\ud835\udc34,\ud835\udc3a\ud835\udc35,\ud835\udc37\ud835\udc34and\ud835\udc37\ud835\udc35\nIn step 2, paired vanilla GAN has been trained to generate multi-\ntrack polyphonic music.",
                "Convolutional generative adversarial networks with\nbinary neurons for polyphonic music generation.",
                "Generating polyphonic music using tied parallel networks."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "In the field of genre rearrangement, (Hung, Chiang, Chen,\nYang, et al., 2019) has made an effort for arbitrary genre rearrangement\nusing instrumental real-world music.",
                "MIDI data can only\ncapture the instrumental information of musical data."
            ],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "The Luo et al. (2020) explored the field of Chinese folk song\ngeneration by capturing specific music styles using VAE.",
                "The network has been trained on British and\nAmerican folk songs.",
                "Mg-VAE: Deep Chinese folk songs generation\nwith specific regional styles."
            ],
            " blues ": [],
            " jazz ": [
                "The authors\nconvert MIDI music from one genre to another (say jazz to pop) which\nis evaluated using neural style classifiers."
            ],
            " rock ": [
                "The machine learning models which focus on clas-\nsifying music based on genre like classical, jazz, pop, rock and others,\ndo not focus on the composer specific styles.",
                "The machine learning models which focus on clas-\nsifying music based on genre like classical, jazz, pop, rock and others,\ndo not focus on the composer specific styles.",
                "Predicting TBM penetration rate in hard rock condition: A comparative study\namong six XGB-based metaheuristic techniques.",
                "Predicting TBM penetration rate in hard rock condition: A comparative study\namong six XGB-based metaheuristic techniques."
            ],
            " classical ": [
                "Nakamura, Shibata, Nishikimi, and Yoshii (2019)\nperforms genre based style conversion (say classical to pop) using\nunsupervised models."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "Wang and Tzanetakis (2018)\nperforms a singing style investigation on pop music using variants of\nneural network."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "The algorithmic composition for music copyright protection under deep learning and blockchain",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [
                "Due to the breadth of vocal frequencies, it\nis difficult to match based on the frequency corresponding to\nthenoteslikeamusicalscore[3].Incomputermusic,mostof\nthe music creation is using artificial intelligence and machine\nlearningtomakethecomputermatchthecorrespondingpattern\nto compose with original single-note score as a model."
            ],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [
                "More-\nover,theorganizationofrulesbetweendifferentelementscan\ncreatedifferentstylesofmusic,suchasclassical,rock,jazz,rap,\nandhip-hop."
            ],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Towards a Deep Improviser: a prototype deep learning post-tonal free music generator",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "Unlike the present paper, the previous deep learning\nmusic generation systems have mainly focused on gener-ation of common practice instrumental music (using sym-\nbolic representations); see reviews [ 17,21,23,24]."
            ],
            " vocal ": [],
            " choral ": [],
            "orchestral": [
                "Several of these algorithmic pieces were multi-strand in nature, that is, they have multiple simultaneous\nmelodic strands as in chamber and orchestral music, as\nFig. 1 Musical representation in the form of a single input matrix."
            ],
            " chamber ": [
                "Several of these algorithmic pieces were multi-strand in nature, that is, they have multiple simultaneous\nmelodic strands as in chamber and orchestral music, as\nFig. 1 Musical representation in the form of a single input matrix."
            ],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "Pressing J (2002) Free Jazz and the avant-garde."
            ],
            " rock ": [
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material).",
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material)."
            ],
            " classical ": [],
            " electronic ": [
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material).",
                "An audio excerpt of\none piece from the Algorithmic Corpus and another fromthe Seed Piece (to provide an example of improvised\nkeyboard playing) are provided within Electronic Supple-\nmentary Material, realised using the Pianoteq physicalsynthesis piano."
            ],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material)."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "From artificial neural networks to deep learning for music generation: history, concepts and trends",
            " homophonic ": [],
            " polyphonic ": [
                "Examples of objectives are: a monophonic melody to beplayed by a human \ufb02utist and a polyphonic accompani-\nment played by a synthesizer.",
                "Modeling temporal dependencies in high-dimensional sequences:application to polyphonic music generation and transcription.",
                "Imposing higher-level\nstructure in polyphonic music generation using convolutional\nrestricted Boltzmann machines and constraints."
            ],
            " monophonic ": [
                "Todd\u2019s objective was to generate a monophonic melody\nin some iterative way.",
                "The ABC notation is very compactbut can only represent monophonic melodies.",
                "Examples of objectives are: a monophonic melody to beplayed by a human \ufb02utist and a polyphonic accompani-\nment played by a synthesizer."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "11.\n\u2013Text\u2014A signi\ufb01cant example is the ABC notation [ 65], a\nde facto standard for folk and traditional music.",
                "It is trained\non examples selected from the folk music repository namedThe Session [ 30] and uses text (the ABC notation [ 65], see\nSect.",
                "An example generated after training\nan autoencoder on a set of Celtic melodies (selected from\nthe folk music repository The Session [ 30] introduced in\nSect. 7.2.1 ) is shown in Fig. 19(see [ 2] for more details)."
            ],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [
                "6.2.2 SymbolicThe main symbolic formats used are:\n\u2013MIDI\n28\u2014It it is a technical standard that describes a\nprotocol based on events, a digital interface and connec-\ntors for interoperability betwee n various electronic musi-\ncal instruments, softwares and devices [ 43].",
                "A tech-\nnical standard that describes a protocol, a digital\ninterface and connectors for interoperability between\nvarious electronic musical instruments, softwares anddevices.",
                "Hiller LA, Isaacson LM (1959) Experimental music: composition\nwith an electronic computer."
            ],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "2.48 Neural Computing and Applications (2021) 33:39\u201365\n1237 Main basic architectures and strategies\nFor reasons of space limitation, we will now jointly introduce\narchitectures and strategies.34For an alternative analysis\nguided by requirements (challenges), please see [ 4].\n7.1 Feedforward architecture\nThe feedforward architecture35is the most basic and\ncommon type of arti\ufb01cial neural network architecture."
            ],
            " k-pop ": [],
            " pop ": [
                "An example of the use of GAN for generating music is\nthe MidiNet system [ 67], aimed at the generation of single\nor multitrack pop music melodies."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Conditional hybrid GAN for melody generation from lyrics",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "Three stylistic categoriessuch as nursery rhymes, folk songs, and rock songs are\ncomposed when given lyrics."
            ],
            " blues ": [],
            " jazz ": [],
            " rock ": [
                "Three stylistic categoriessuch as nursery rhymes, folk songs, and rock songs are\ncomposed when given lyrics.",
                "Three stylistic categoriessuch as nursery rhymes, folk songs, and rock songs are\ncomposed when given lyrics."
            ],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Scene2Wav: a deep convolutional sequence-to-conditional SampleRNN for emotional scene musicalization",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "The affective properties of keys in instrumental music from the late nineteenth and\nearly twentieth centuries."
            ],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Attentional networks for music generation",
            " homophonic ": [],
            " polyphonic ": [
                "[ 3] attempted to man-\nage the test of learning complex polyphonic structure in music.",
                "[24] 0.2317 0.9904 0.9808\nLSTM 0.5097 1.0919 1.1924\nLSTM + Attention 0.2286 0.8924 0.7864\nBi-LSTM + Attention + LSTM 0.1069 0.6694 0.4481\nFig.3 Graph: Categorical Cross Entropy Loss\nFig. 4 Performance Evaluation Graph: (a)-(c) shows Mean Square Error for LSTM, LSTM+attention and\nBi-LSTM+attention respectively5187 Multimedia Tools and Applications (2022) 81:5179\u20135189Fig.5 a) Input music sheet b) Output music sheet generated by the proposed framework for songs Chameleon\n(Top Row) and Last farewell (Bottom row)\n4.3 Comparisonwithrelatedworks\nIn [24], the author discusses about polyphonic midi sequences using LSTM networks.",
                "Modeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic music generation and transcription.",
                "Johnson DD (2017) Generating polyphonic music using tied parallel networks.",
                "A study on lstm networks for polyphonic music sequence modelling."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "In this work, we propose a deep learning based music generation\nmethod in order to produce old style music particularly JAZZ with rehashed melodic struc-\ntures utilizing a Bi-directional Long Short Term Memory (Bi-LSTM) Neural Network with\nattention.",
                "In this work, we utilize an end to end pipeline based on Bi-LSTM network with an atten-\ntion module to produce old style Jazz music with rehashed melodic structures automaticallywithout any human intervention.",
                "1 a) Sheet Music of the song: \u201cThe Last Farewell\u201d by Roger Whittaker b) Musical Notes (Extracted\nfrom MIDI file) of the song: \u201cThe Last Farewell\u201d5181 Multimedia Tools and Applications (2022) 81:5179\u20135189Table1 Batch construction for the JAZZ ML ready MIDI dataset: Batch size 64 characters\nBatch-1",
                "4 Experimentationandresults\n4.1 Datasetdiscription\nWe used Jazz ML ready MIDI dataset2to train our model.",
                "The dataset comprises of 818\ndiverse Jazz music melodies."
            ],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Monophonic music composition using genetic algorithm and Bresenham\u2019s line algorithm",
            " homophonic ": [
                "Music can have different\ntextures, namely monophonic, homophonic and polyphonic.",
                "While homophonic and poly-\nphonic textures also include harmony along with melody and rhythm."
            ],
            " polyphonic ": [],
            " monophonic ": [
                "When organized with single\nmelody and rhythm, the music has a monophonic texture.",
                "We have considered\nmelodic and rhythmic aspect of music in this work, thereby creating monophonic music.",
                "[8 ] Deep learning for monophonic music Corpus of Irish folk songs is used to generate music with same style\nGuedes et al.",
                "[ 2] Generative RNN model for sheet music Uses dataset in ABC music notation26488 Multimedia Tools and Applications (2022) 81:26483\u201326503recurrent unit (GRU) to generate convincing monophonic melodies.",
                "While proposed methodgenerates new compositions with creative exploration of the search space.\n\u2013 Proposed method vs existing evolutionary approaches: The existing methods on\ngenerating monophonic music mostly work in two directions: creating compositionsidentical to the given reference [ 24,28,37] or generating new melodic ideas",
                "Finally,both the rhythm and pitch sequences are combined to achieve monophonic music."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [
                "81:26483\u201326503F\nig.1 3/4 time signature\n2 Preliminaries\nMusic composition is the art of combining musical elements to create a piece of music,\neither vocal or instrumental.",
                "Since the individuals can have different vocal ranges, thiswork provides an option to set the range within the comfort zone."
            ],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "[8 ] Deep learning for monophonic music Corpus of Irish folk songs is used to generate music with same style\nGuedes et al."
            ],
            " blues ": [],
            " jazz ": [
                "[ 3] designed GenJam to generated jazz solos using interactive GA.",
                "[32] used algorithmic fitness function to generate jazz melodies over thegiven chord progression.",
                "[32] GA for Jazz melody and rhythm Uses input chord progression for reference\nMatic",
                "Biles JA et al (1994) Genjam: a genetic algorithm for generating jazz solos.",
                "A genetic algorithm for the generation of jazz melodies."
            ],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Polyphonic music generation generative adversarial network with Markov decision process",
            " homophonic ": [],
            " polyphonic ": [
                "Abstract\nIn the process of polyphonic music creation, it is important to combine two or more\nindependent melodies through technical treatment.",
                "Therefore, this paper proposes a novel polyphonic music creation\nmodel, combining the ideas of the Markov decision process (MDP) and Monte Carlo tree\nsearch (MCTS) and improving the Wasserstein Generative Adversarial Network\n(WGAN) theory.",
                "Keywords Polyphonic music generation .Markov decision process (MDP) .Monte",
                "Generative Adversarial Network (WGAN)\n1 Introduction\nProducing various independent melodies and combining them harmoniously through technical\nprocessing are key to creating polyphonic music.",
                "For composers, this is a very heavy task, so\nthere is hope that polyphonic music may be generated through neural networks.",
                "i n a#The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022classic Generative Adversarial Network (GAN) [ 7,13] is limited for polyphonic music\ncreation.",
                "As a polyphonic music sequence grows, the probability of the generated sequence\nsamples continuously playing the same note increases, which would destroy music coherence.",
                "Therefore, the present research team designed a GAN model based on the Markov decisionprocess (MDP) [ 21] and Monte Carlo tree search (MCTS) [ 4] to generate polyphonic music.",
                "[ 15] have also\nproposed a Morpheus music generation system, which can generate polyphonic music atgiven tension profiles, with long-term and short-term repetition pattern structures.",
                "However, in polyphonic musicgeneration, as the length of the music sequence generated by WGAN increases, sequencecoherence is broken, and the discriminator in the WGAN model finds it difficult to evaluatethe incomplete sequence.",
                "Therefore, aiming atthe problems of WGAN polyphonic music generation, this paper improves the structure of the\ngenerator and discriminator in the WGAN model and adds a policy gradient algorithm [ 29]t o\nupdate the generator parameters.",
                "Therefore, the present research team has the introduced MDP mechanism into the GAN modelto generate polyphonic music.",
                "In this way, themodel can be applied to any incomplete polyphonic music sequence at any time.",
                "By learning the characteristics of and mapping between various independentmelody sequences in a polyphonic music dataset, this model can generate polyphonic musicthat is closer to real-world music, effectively resolving the issues concerning the continuity ofpolyphonic music sequences and ensuring the diversity of generated samples \u2014making the\nmodel more powerful for unsupervised music sequence processing.",
                "This makes the networkmore quickly converge in the direction of ideal polyphonic music.",
                "1 Flowchart of polyphonic music generation in this study29868 Multimedia Tools and Applications (2022)",
                "The model has no require-ments for the length of the input polyphonic music sequence and can automatically generateexcellent music corresponding to the input sequence.",
                "(4) This paper is the first to integrateMCTS into WGAN for polyphonic music generation, removing the issues in which it isdifficult for the discriminator, in the existing GAN model, to evaluate incomplete sequences.",
                "2 Technical details\n2.1 Network construction\nIn this paper, the researchers have built a neural network based on WGAN and MDP to learnand generate polyphonic music.",
                "When learning a polyphonic music sequence, the generator\nrandomly generates the polyphonic music sequence, and then inputs the music generated bythe model into the discriminator with the original music for classification.",
                "The discriminator model includes a convolution layer and a fully-\nconnected layer, which are used to accurately identify the real polyphonic music and thepolyphonic music generated by the model in the iterative learning process.",
                "To improve thestability of the model for polyphonic music generation, the research team has added batchnormalization to all layers except the input convolution layer of the discriminator [ 16].",
                "Figure 3\ndepicts the polyphonic music generation model of this paper, including the generator anddiscriminator models.",
                "81:29865 \u201329885 29869During the training process, the generator model learns to generate polyphonic music\nsimilar to real polyphonic music, while the discriminator model learns to more accurately\ndistinguish between the real and the model generated polyphonic music.",
                "This study combines\nWGAN and MDP to generate polyphonic music.",
                "The discrim-inator must be asked to score the complete polyphonic music sequence.",
                "In the process of polyphonic music\ngeneration, the prediction of the next music sequence depends on the previous one.",
                "The discriminator network of this polyphonic music generation model is composed of a\nconvolution layer and a fully-connected layer.",
                "3 Schematic diagram of the polyphonic music generation model in this study29870 Multimedia Tools and Applications (2022)",
                "After training, thegenerator network can be used alone to generate polyphonic music.",
                "2.2 Network strategy gradient\nWhen designing the strategy gradient for the polyphonic music generation model, it wasnecessary for the authors to introduce the relevant GAN theory proposed by Goodfellow et al.[13].",
                "Therefore, the team introduced WGAN into the polyphonic music generation model.",
                "WP\nr;Pg/C0/C1\n\u00bcinf/C13/C24\u03a0\u00f0Pr;Pg\u00deEx;y\u00f0\u00de /C24 /C13x/C0yjjjj\u00bd/C138 \u00f0 2\u00de\nPris the sample distribution of a real polyphonic music sequence;",
                "Pgis the sample distribution\nof a polyphonic music sequence produced by the generator; and \u03a0\u00f0Pr;Pg\u00deis the set of all\nTable 1",
                "The calculated J\u03b8\u00f0\u00deis the function the polyphonic music generation model seeks\nto maximize.",
                "When evaluating polyphonic music\nsequences at any time, they have considered the possible long-term rewards [ 20], as well as the\noverall situation for each sequence.",
                "Therefore, it\nhas been used here for the decision-making process of polyphonic music sequence generation.",
                "3 Experimental results and analysis\n3.1 Dataset composition and preprocessing\nThe research team have constructed 6,947 original datasets composed of polyphonic music\n[19] to serve as the dataset for the polyphonic music generation model.",
                "At the\nsame time, the research team has compared the effects of the model in this study with theMuseGAN v1 model [ 10], the MuseGAN v2 model [ 11], and the LSTM music generation\nmodel [ 23] for polyphonic music generation.",
                "eM C T S\nalgorithm can balance the exploration and use of a polyphonic music sequence.",
                "3.3 Experimental result\nBy studying the existing music generation methods and analyzing the WGAN model, through\na large number of experiments, it has been proven that the model proposed in this paper is\nmore suitable for polyphonic music generation than both the WGAN model and the WGANand MDP model.",
                "The results show that,\ncompared with the music generation model proposed by Dong et al., the polyphonic musicgeneration model in this paper has better polyphonic music sequence generation ability.",
                "macro/C0P\u00bc1\nnXn\ni\u00bc1Pi \u00f017\u00de\nmacro/C0R\u00bc1\nnXn\ni\u00bc1Ri \u00f018\u00de\nmacro/C0F1\u00bc2/C2macro/C0P/C2macro/C0R\nmacro/C0P\u00femacro/C0R\u00f019\u00de\nIn this paper, all polyphonic music sequences in the original dataset are \u201ctrue\u201dsamples, while\nnoise sequences are \u201cfalse\u201dsamples.",
                "At the same time, in the polyphonic music generated by\nthe corresponding model, the regular music sequence is defined as the \u201ctrue\u201dsample predicted\nby the classifier, and the generated noise sequence is the sample predicted as \u201cfalse.",
                "During the experiments, the research team input the original polyphonic music dataset into\nthe proposed music generation model to obtain the polyphonic music generated by thealgorithm in this paper.",
                "Simultaneously, the researchers input the same polyphonic music\nd a t a s e ti n t",
                "[ 11] to obtain the polyphonic music generated in that\nstudy [ 11].",
                "After completing the above steps, the team used the polyphonic music sequence in\nthe original dataset and the formulas for precision and recall to obtain the comparison testresults of the two models, as shown in Table 2.",
                "The model proposedin this study seeks to learn polyphonic note sequences on a single-layer LSTM network.",
                "When training polyphonic music sequences in this model, there is noproblem of gradient explosion or gradient disappearance.",
                "3.6 Limitations\nThe polyphonic music generation model proposed in this paper has more hidden layers and\nhigher complexity because it contains a generator and a discriminator, adds the MDPmechanism, and integrates MCTS ideas.",
                "i o n s\nBased on the WGAN model, a novel polyphonic music generation method using MDP",
                "In addition, the polyphonic music model\nuses a GPU for training, and the team plans to collect more types of polyphonic music ofvarious styles so that the model can generate better music in subsequent training."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [
                "Conklin D, Gasser M, Oertl S (2018) Creative chord sequence generation for electronic dance music."
            ],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "A combination of multi-objective genetic algorithm and deep learning for music harmony generation",
            " homophonic ": [],
            " polyphonic ": [
                "Therefore, in this paper, we propose a Multi-Objective\nGenetic Algorithm (MO-GA) to ge nerate polyphonic music pieces, considering grammar and\nlistener satisfaction.",
                "&Considering both expert and ordinary listeners as separate objective functions.\n&Introducing a polyphonic music generation system, including the composition of melody,\nharmony, and rhythm in desired styles and lengths.\n&Modeling audience behavior in understanding the beauty of music by a Bi-LSTM model,and using it as an evaluation function.",
                "Fra nklin created a recurrent neural network with Long\nShort-Term Memory (LSTM) that generates melody or monophonic music pieces [ 13].\nHarmony or polyphonic music is a process of combining individual voices that are\nanalyzed by hearing them simultaneously.",
                "The purpose of usingharmony is to accompany the melodies by considering the relevant rules that lead to the\ncreation of polyphonic music.",
                "The objective function of this GA considers the rhythms and similaritybetween generated pieces and a standard database of polyphonic human-made pieces.",
                "At each iteration of\nGA1, the best chromosomes in the population are selected based on minimum violation of therules and the maximum similarity to a human-made polyphonic music database.",
                "82:2419\u201324355 Experimental results\nThe proposed system is implemented in MATLAB R2018b, and all the executions have been\ndone on a system with CPU Intel Core i7, 8 Gigabytes of RAM, and Windows10 O.S.\nTo provide the human-made pieces of music, we have used the Steirar database,1which\ncontains 235 polyphonic music pieces in ABC notation.",
                "In the proposed method, we first generated a collection of polyphonic music piecesusing a genetic algorithm and a database of human-made pieces as the objective function."
            ],
            " monophonic ": [
                "Fra nklin created a recurrent neural network with Long\nShort-Term Memory (LSTM) that generates melody or monophonic music pieces [ 13].\nHarmony or polyphonic music is a process of combining individual voices that are\nanalyzed by hearing them simultaneously."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "Improvising jazz chord sequences by means of formal grammars."
            ],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "A Style-Specific Music Composition Neural Network",
            " homophonic ": [],
            " polyphonic ": [
                "a polyphonic generation framework by combining RNN and general neural network.",
                "\u2013Musical texture The use of texture is \ufb02exible, and the main tone texture is combined\nwith the polyphonic texture.",
                "Imposing higher-level structure in polyphonic music generation\nusing convolutional restricted Boltzmann machines and constraints."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "Biles JA (1994) GenJam: a genetic algorithm for generating jazz solos."
            ],
            " rock ": [
                "Styles include classical, pop, jazz, rock and R&B etc.",
                "Styles include classical, pop, jazz, rock and R&B etc."
            ],
            " classical ": [
                "The so-called\u201cstyle-speci\ufb01c\u201d can be de\ufb01ned as a certain preset style such as classical music.",
                "In order to generate classical style\u2019s music, we choose several compositionprinciples from the common introduction to classical music composition.",
                "[ 37\u201339], we de\ufb01ne the reward function r\nmr(a,s)for\nthe following classical music\u2019s characteristics.",
                "We don\u2019t claim these characteristics are exhaustive, but adding the rules will guide our\nmodel towards traditional composition structure and make the generated music more struc-tural and more obvious in classical style.",
                "We employed the matched subset of the Lakh MIDI dataset (LMD) and Classical Piano MidiPage dataset as the training dataset, Lakh MIDI dataset [ 47] provides a rich collection of real-\nworld MIDI \ufb01les and some associated meta-data.",
                "In the experiment, weselect 2000 music samples of Classical Piano Midi Page dataset with speci\ufb01c composers\u2019styles in MIDI format [ 48] as test dataset, and each sample was a single orbital with an average\ntime of 2\u20134 min. To meet the requirements of the experiment, each sample was divided into20 s to obtain more than 20,000 classical music samples.",
                "In the experiment, we selected 500 training samples for training and 300 generated samples\nfor testing, and extracted 8 features of classical music as the basis for comparison, which are:range, repeated notes, vertical perfect fourths, rhythmic variability, parallel motion, verticaltritones, chord duration, number of pitches [ 39,47].",
                "When d<d0, it\nis determined that the generated sample is a classical music that meets the requirements, andlabeled as 1.",
                "Minimum distance classi\ufb01er (MDC) algorithm is used to judge whether the generated\nmusic is classical music style, and we use receiver operator characteristics (ROC) curveto judge the performance of different models.",
                "Through the establishment\nof an expert review panel, the performance of different models in generating classical musicwas evaluated from the following \ufb01ve aspects:\n1231908 C. Jin et al.\nFig. 8 ROC curve of style classi\ufb01er for 4 models\n\u2013Melody The melody of classical music sounds more balanced and symmetrical.",
                "\u2013Rhythm Classical style includes unexpected pauses, syncopation, and frequent conver-\nsion from long to short notes.",
                "\u2013Emotion The \ufb02uctuation and contrast of classical music\u2019s emotion are more obvious.",
                "The rewardfunction is utilized to optimize the probability distribution of music generating network inreal time, so as to generate smooth and beautiful classical music."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [
                "Styles include classical, pop, jazz, rock and R&B etc."
            ],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Self-Supervised Music Motion Synchronization Learning for Music-Driven Conducting Motion Generation",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "[68]Lee K, Junokas M J, Amanzadeh M, Garnett G E. An ana-\nlysis of basic expressive qualities in instrumental conduct-\ning."
            ],
            " vocal ": [],
            " choral ": [],
            "orchestral": [
                "Although\nrecent studies have successfully generated motion for singers, dancers, and musicians, few have explored motion generation\nfor orchestral conductors.",
                "2 Related Work\n2.1 Music-Driven Conducting Motion\nGeneration\nMusic-driven conducting motion generation involves\nthe generation of skeleton sequences of an orchestral\nconductor according to a given piece of music.",
                "[25] Dansereau D G, Brock N, Cooperstock J R. Predicting\nan orchestral conductor's baton movements using machine\nlearning.",
                "[51] Huang Y, Chen T, Moran N, Coleman S, Su L. Identifying\nexpressive semantics in orchestral conducting kinematics."
            ],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [
                "In a study of music-driven dance\ngeneration, Ren et al.[1]proposed pose perceptual loss,\nwhere a motion encoder pre-trained on dance genre\nclassi\fcation (distinguishing ballet, pop, and hip-hop\ndance) was used as the perceptual loss network."
            ],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [
                "Conductors, the soul of an orchestra, perform el-\negantly and charmingly in every concert."
            ],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Integration of a music generator and a song lyrics generator to create Spanish popular songs",
            " homophonic ": [],
            " polyphonic ": [],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [
                "Years of ethnomusicological education are often required \nto master the idiosincracies of the modal system and under -\nstand the vocal music in a popular context.",
                "Additionally, among the vocal songs generated, some \nwere selected for another human evaluation.",
                "On the other hand, we aim to validate the musical and lyrics results, meaning the vocal songs generated can follow the style of the Spanish popular\u00a0music."
            ],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [
                "Therefore, folk songs in Spain are as varied as its regions.",
                "In order to obtain this information and preserve popular \nmusic, some ethnomusicologists have visited different geo-graphical zones, transcribing folk songs that people sing in rituals or traditional holidays.",
                "They have studied Ethnomusicology or Musicology, or are very familiar with the Spanish tradition due to their background (for example, teachers of traditional instru-ments, or musicians of Spanish folk music).",
                "Alter -\nnatively, we also contacted people that worked as a musi-cian of Spanish folk music or teachers that teaches Span-ish traditional instruments, and they are familiar with the Spanish tradition.",
                "However, our proposal includes important novelties, such as the use of Spanish language and the use of a new corpus of folk songs, unlike the rest of the works, which are centered in English classical and pop music.",
                "Lyrics and music generation X X X\nIncludes folk songs X \u2212 \u2212\nGenerating music before lyrics X \u2212 \u2212\nLearning Machines X X X\nUse of Markov models X \u2212 X\nInteraction with the users \u2212 \u2212 X\nUse of Spanish language X \u2212 \u22124431 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n5."
            ],
            " blues ": [],
            " jazz ": [
                "This kind of music has some particular features that makes them very different to other genres like classical or jazz music.",
                "However, they are usually \nthought to generate classical or jazz music automatically, and not as a guide to generate popular songs."
            ],
            " rock ": [],
            " classical ": [
                "However, most works are centered in classical music.",
                "This kind of music has some particular features that makes them very different to other genres like classical or jazz music.",
                "They are commonly composed of melodies without musi-cal accompaniment, with complex rhythms and uncommon scales (from a classical perspective), rich in ornaments.",
                "Unlike classical music, Spanish popular music is always \nlinked to a functionality, meaning the purpose for which the melody was conceived.",
                "As we mentioned, this genre of music differs from the classical music in many aspects, including the sonority, the sounds disposition or the rhythmic formulas used.",
                "However, they are usually \nthought to generate classical or jazz music automatically, and not as a guide to generate popular songs.",
                "Unlike classical music, in popular music the harmonic tension and the use of the chords degrees are not particularly relevant, as it does not follow harmonic rules; they are only used according to the melodic course.",
                "In our case, the popular music does not have so many musical resources as classical music, for example, which makes the number of states of the MM to decrease.",
                "However, our proposal includes important novelties, such as the use of Spanish language and the use of a new corpus of folk songs, unlike the rest of the works, which are centered in English classical and pop music."
            ],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "Therefore, alternative strategies could be tested for matching the rhythm."
            ],
            " k-pop ": [],
            " pop ": [
                "However, our proposal includes important novelties, such as the use of Spanish language and the use of a new corpus of folk songs, unlike the rest of the works, which are centered in English classical and pop music.",
                "Xiaoice band: A melody and arrangement genera-tion framework for pop music."
            ],
            " ambient ": [
                "Vol.:(0123456789)1 3Journal of Ambient Intelligence and Humanized Computing (2020) 11:4421\u20134437 \nhttps://doi.org/10.1007/s12652-020-01822-5\nORIGINAL RESEARCH\nIntegration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create \nSpanish popular songs\nMar\u00eda\u00a0Navarro\u2011C\u00e1ceres1 \u00a0\u00b7 Hugo\u00a0Gon\u00e7alo\u00a0Oliveira2\u00a0\u00b7 Pedro\u00a0Martins2\u00a0\u00b7 Am\u00edlcar\u00a0Cardoso2\nReceived: 7 December 2018 / Accepted: 19 February 2020 / Published online: 11 March 2020 \n\u00a9 Springer-Verlag GmbH Germany, part of Springer Nature 2020"
            ],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
            " homophonic ": [],
            " polyphonic ": [
                "Non-\nnegative matrix factorization (NMF) model [ 6], end-to-\nend model [ 7], fusion model with spectral, temporal,\nand modulation features [ 8] can be referred to as initial\nattempts for the proposed task in a polyphonic environ-\nment.",
                "More recent works deal with instrument recogni-\ntion in polyphonic music, which is a more demanding\nand challenging problem.",
                "Both approaches were trained\nand validated by the IRMAS dataset of polyphonic music\nexcerpts.",
                "Furthermore,\nit was shown that even for shorter windows, the phase\nspectrum could contribute as much as the magnitude\nspectrumtospeechintelligibility[ 20].Inourwork,weare\nintroducingphase-basedmodgdgramasacomplementary\nfeature to magnitude-based spectrogram in recognizing\npredominant instruments from a polyphonic environ-\nment.",
                "In our work, we utilize transformer architec-\ntures to learn instrument-specific characteristics using\nMel-spectro-/modgd-/tempogram to estimate predomi-\nnant instruments from polyphonic music.",
                "5.3.2 Testingconfiguration\n2874 polyphonic files of variable length with multiple\npredominant instruments are used for the testing phase."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [
                "In music, the body of the musical\ninstrument is the counterpart of the vocal tract (system)\nin speech."
            ],
            " choral ": [],
            "orchestral": [
                "*Correspondence: clekshmir04@gmail.com\nDepartmentofElectronicsandCommunicationEngineering,Collegeof\nEngineeringTrivandrum,APJAbdulKalamTechnologicalUniversity,\nTrivandrum,IndiaThe task of identifying the leading instrument in poly-\nphonic music is challenging due to the presence of inter-\nferingpartialsintheorchestralbackground."
            ],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": []
        },
        {
            "title": "Genre Recognition from Symbolic Music with CNNs: Performance and Explainability",
            " homophonic ": [],
            " polyphonic ": [
                "Algorithms for dis-\ncovering repeated patterns in multidimensional representa-\ntions of polyphonic music."
            ],
            " monophonic ": [
                "Rizo et\u00a0al. propose a non-\nlinear representation of a melody based on trees and they \nstudy the influence of different tree representations on clas-\nsification rates in three corpora with monophonic melodies, \nconcluding that tree coding gives better results [ 35]."
            ],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [
                "The way multi-track MIDI files are handled is by averag-\ning the pianorolls of all instrumental tracks and all percus-\nsive tracks separately into two cumulative pianorolls."
            ],
            " vocal ": [
                "We chose (b) and (c) as \nexamples of certain predictions, while (d) shows an errone-\nous prediction by the CNN (Electronic), and a relatively high \nvalue of 0.06 for the vocal genre which is interesting since \nthe introduction of the song features an a capella chorus.",
                "Jazz 0.02 Vocal 0.06\nPop\u2013Rock 0.24 Country 0.037 Vocal 0.003 RnB 0.04\n2 https:// www. reddit.",
                "For instance, the choice of \nQueen\u2014We Will Rock You as a prototype for Rap  could be \ndue to the rhythmic qualities of the vocal track, the loop-\ning music, and the repeating patterns which are prevalent \nin a lot of different music, including Rap ."
            ],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [
                "It is interesting that genres such as Jazz which have lit-\ntle representation in the dataset are better classified than gen-\nres such as Electronic which has almost double the support.",
                "This could be due to distinguishing musical characteristics \nof each genre, which are apparent in symbolic representa-\ntions of music\u2014for instance, jazz music tends to have com-\nplex harmony and utilize more notes, while electronic music \ntends to contain loops of very few notes.",
                "For Here comes the Sun the first quarter of the \npianoroll seems to contribute more towards a Jazz predic-\ntion, while the third quarter contributes towards a Pop-Rock  \nprediction.",
                "0.40 Jazz 0.06 Electronic 0.03 Pop\u2013Rock 0.20\nRap 0.25 RnB 0.04"
            ],
            " rock ": [
                "For instance, the Pop-Rock genre \nis represented by a very diverse set of samples, ranging from \nHard Rock to Disco.",
                "For instance, the Pop-Rock genre \nis represented by a very diverse set of samples, ranging from \nHard Rock to Disco.",
                "For instance, the choice of \nQueen\u2014We Will Rock You as a prototype for Rap  could be \ndue to the rhythmic qualities of the vocal track, the loop-\ning music, and the repeating patterns which are prevalent \nin a lot of different music, including Rap .",
                "For instance, the choice of \nQueen\u2014We Will Rock You as a prototype for Rap  could be \ndue to the rhythmic qualities of the vocal track, the loop-\ning music, and the repeating patterns which are prevalent \nin a lot of different music, including Rap .",
                "Rap Queen - We Will Rock You (Pop-Rock) Phish - Wading In The Velvet (Pop-Rock)",
                "Rap Queen - We Will Rock You (Pop-Rock) Phish - Wading In The Velvet (Pop-Rock)",
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)",
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)"
            ],
            " classical ": [
                "In [12], Karydis \net\u00a0al. combine pattern recognition with statistical approaches \nto successfully achieve genre recognition for five sub-gen-\nres of classical music."
            ],
            " electronic ": [
                "It is interesting that genres such as Jazz which have lit-\ntle representation in the dataset are better classified than gen-\nres such as Electronic which has almost double the support.",
                "This could be due to distinguishing musical characteristics \nof each genre, which are apparent in symbolic representa-\ntions of music\u2014for instance, jazz music tends to have com-\nplex harmony and utilize more notes, while electronic music \ntends to contain loops of very few notes.",
                "Finally, for Bohemian Rhapsody, the second half of the \npianoroll contributes more towards the Electronic genre, \nafter the piano part is introduced.",
                "The \ntracks are: Beethoven\u2014Moonlight Sonata, The Beatles\u2014Here Comes the Sun, Eminem\u2014The Real Slim Shady, Queen\u2014Bohemian Rhapsody\nBeethoven Beatles Eminem Queen\nInternational 0.69 Pop\u2013Rock 0.83 Rap 0.89 Electronic 0.54\nNew Age",
                "0.40 Jazz 0.06 Electronic 0.03 Pop\u2013Rock 0.20\nRap 0.25 RnB 0.04",
                "This rule says that if both the minor third \nand the fifth appear at least half as often as the tonic, then \nthe sample is classified as Electronic, which makes intui-\ntive sense since Electronic music tends not to have complex \nharmony, and the rule represents the prevalence of a minor \ntriad in the pitches which appear in the pianoroll."
            ],
            "hip-hop": [],
            " reggae ": [
                "For instance, none of the Reggae  \nsamples in the table are actually Reggae, but would probably \nbe considered Soul/RnB/Gospel."
            ],
            " country ": [
                "Jazz 0.02 Vocal 0.06\nPop\u2013Rock 0.24 Country 0.037 Vocal 0.003 RnB 0.04\n2 https:// www. reddit."
            ],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [],
            " k-pop ": [],
            " pop ": [
                "For \nPop-Rock the prototype chosen by MMD-critic is a power \nballad by Abba which is closer to Pop than Rock, which is \ninteresting when compared to the prototype selected from the ground-truth labels: a Nine Inch Nails song which is a \nlot closer to Rock."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [
                "The Promise You Made (New Age) Slavic Soul Party!"
            ],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)"
            ],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": [
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "For Here comes the Sun the first quarter of the \npianoroll seems to contribute more towards a Jazz predic-\ntion, while the third quarter contributes towards a Pop-Rock  \nprediction.",
                "For \nHere Comes the Sun, the fifth and sixth bars, along with \ntheir repetition four bars later contribute more towards the \nPop-Rock genre.",
                "Finally, the first measure of the piano part of Bohe-\nmian Rhapsody along with its preceding measure contrib-\nute towards Electronic, while the third measure after the \npiano is introduced contributes towards Pop-Rock .",
                "For \nthe local sample set generated around Here Comes the Sun \nand The Real Slim Shady, the CNN classified 0.75 and 0.93 \nas Pop-Rock and Rap  respectively (percentage of positive \n/u1D702i ).",
                "For Here \nComes the Sun, we attribute the failure of GPX to the bias \nof the classifier towards the Pop-Rock genre which is a result \nof dataset imbalance.",
                "For instance, the Pop-Rock genre \nis represented by a very diverse set of samples, ranging from \nHard Rock to Disco.",
                "A result of this is that almost half of the Fig. 11  Final programs gener -\nated by GPX as explanations \nfor the top prediction for each \nsample\nSN Computer Science (2023) 4:106 \n Page 15 of 18 106\nSN Computer Science\nselected prototypes and criticisms are labeled as Pop-Rock  \namong other labels.",
                "For \nPop-Rock the prototype chosen by MMD-critic is a power \nballad by Abba which is closer to Pop than Rock, which is \ninteresting when compared to the prototype selected from the ground-truth labels: a Nine Inch Nails song which is a \nlot closer to Rock.",
                "In parentheses are the ground-truth \nlabels\nGenre Test set prototype Test set criticism\nPop-Rock Nine Inch Nails\u2014Piggy (Pop-Rock)",
                "In parentheses are the ground-truth labels\nGenre Black-box prototype Black-box criticism\nPop-Rock Abba - The Winner Takes It All (Pop-Rock, Vocal)"
            ]
        },
        {
            "title": "CycleDRUMS: automatic drum arrangement for bass lines using CycleGAN",
            " homophonic ": [],
            " polyphonic ": [
                "In [13], CNNs are used for generating melody as a series of MIDI notes either from scratch, by following a chord sequence, \nor by conditioning on the melody of previous bars, whereas in [14\u2013 17] LSTMs are used to generate musical notes, melo -\ndies, polyphonic music pieces, and long drum sequences under constraints imposed by metrical rhythm information and Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7 \n Research\n1 3\na given bass sequence.",
                "In [21], symbolic sequences of polyphonic music are modeled in an entirely general piano-roll representation, while the \nauthors of [22] propose a novel architecture to generate melodies satisfying positional constraints in the style of the \nsoprano parts of the J.S. Bach chorale harmonizations encoded in MIDI.",
                "In [23], RNNs are used for the prediction and \ncomposition of polyphonic music; in [24], highly convincing chorales in the style of Bach were automatically generated \nusing note names [25]; added higher-level structure on generated polyphonic music, whereas in [26] an end-to-end \ngenerative model capable of composing music conditioned on a specific mixture of composer styles was designed.",
                "Generating polyphonic music using tied parallel networks.",
                "Lattner S, Grachten M, Widmer G. Imposing higher-level structure in polyphonic music generation using convolutional restricted Boltz-\nmann machines and constraints.",
                "Sigtia S, Benetos E, Dixon S. An end-to-end neural network for polyphonic piano music transcription.",
                "Sound-object oriented analysis and note-object oriented processing of polyphonic sound recordings."
            ],
            " monophonic ": [],
            " heterophonic ": [],
            " a cappella ": [],
            " instrumental ": [],
            " vocal ": [
                "However, a potential weakness of this method is that it sometimes produces noisy separations, with watered-down \nharmonics and traces of other instruments in the vocal segment."
            ],
            " choral ": [],
            "orchestral": [],
            " chamber ": [],
            " symphonic ": [],
            " folk ": [],
            " blues ": [],
            " jazz ": [],
            " rock ": [
                "This rather small \ndataset comprises 100 tracks taken from the DSD100 dataset, 46 tracks from the MedleyDB, two tracks kindly provided by \nNative Instruments, and two tracks from the Canadian rock band The Easton Ellises.",
                "This rather small \ndataset comprises 100 tracks taken from the DSD100 dataset, 46 tracks from the MedleyDB, two tracks kindly provided by \nNative Instruments, and two tracks from the Canadian rock band The Easton Ellises."
            ],
            " classical ": [],
            " electronic ": [],
            "hip-hop": [],
            " reggae ": [],
            " country ": [],
            " r&b ": [],
            " heavy metal ": [],
            " punk ": [],
            " alternative ": [
                "Although arrangement generation has been \nextensively studied in symbolic audio, switching to mel-spectrograms allowed us to preserve the sound heritage of other \nmusical pieces and represent a valid alternative for real-case scenarios."
            ],
            " k-pop ": [],
            " pop ": [
                "Keywords Automatic music arrangement\u00a0\u00b7 Cycle-GAN\u00a0\u00b7 Deep learning\u00a0\u00b7 Source separation\u00a0\u00b7 Audio and speech \nprocessing\n1 Introduction\nThe development of home music production has brought significant innovations into the process of pop music composi-\ntion.",
                "We \nthen asked a professional guitarist who has been playing in a pop-rock band for more than ten years, a professional \ndrummer from the same band, and two pop and indie-rock music producers with more than four years of experience \nto manually annotate these samples, capturing the following musical dimensions: sound quality, contamination, \ncredibility, and whether the generated drums followed the beat.",
                "Ren Y, He J, Tan X, Qin T, Zhao Z, Liu T-Y. Popmag: pop music accompaniment generation."
            ],
            " ambient ": [],
            " gospel ": [],
            " soul ": [],
            " funk ": [],
            " disco ": [],
            " techno ": [],
            " dubstep ": [],
            " opera ": [],
            " bluegrass ": [],
            " flamenco ": [],
            "-rock ": [
                "We \nthen asked a professional guitarist who has been playing in a pop-rock band for more than ten years, a professional \ndrummer from the same band, and two pop and indie-rock music producers with more than four years of experience \nto manually annotate these samples, capturing the following musical dimensions: sound quality, contamination, \ncredibility, and whether the generated drums followed the beat."
            ]
        }
    ]
}