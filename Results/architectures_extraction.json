{
    "data_collection": [
        {
            "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
            "gan": [
                "MP3net: coherent, minute-long music generation from raw audio\nwith a simple convolutional GAN\nKorneel van den Broek1\nAbstract\nWe present a deep convolutional GAN which\nleverages techniques from MP3/V orbis audio com-\npression to produce long, high-quality audio sam-\nples with long-range coherence.",
                "[cs.SD]  12 Jan 2021MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nFigure 1.",
                "In this work, we use the MDCT amplitude as the data rep-\nresentation for raw audio in a deep 2D convolutional Gen-\nerative Adverserial Network (GAN).",
                "The architecture of our network is in-\nspired by the ProGAN model (Karras et al., 2018) with the\nnoticeable difference that we don\u2019t increase/decrease the\npixel density along the frequency axis with each successive\nmodel-block in the generator/discriminator.",
                "The discretized window function wnsatis\ufb01es\nwn=w2N\u00001\u00001 (4)\nw2\nn+w2\nn+N= 1 (5)MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nThese latter conditions ensure that the MDCT transforma-\ntion is invertible.",
                "Experimentally, these frequency ranges have been estab-\nlished as critical bands or Bark bands, with fjthe mid pointMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nof each band.",
                "As studied by Arjovsky & Bottou (2017) and Jenni\n& Favaro (2019), this process improves the stability of the\nGAN training process, since it smooths the distribution of\nboth the true and generated distributions, extending the sup-\nport of both distributions.",
                "2D convolutions to up- and downscale the MDCT\namplitude representation\nOur network architecture is based on the architecture of\nthe ProGAN network (Karras et al., 2018) with successive\nmodel blocks which scale up/down the image using strided\n2D convolutions in the generator and discriminator respec-\ntively.",
                "We\nalso note that while the number of \ufb01lter bands Nhas notMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nchanged, the corresponding frequencies of the \ufb01lter bands\nhave doubled to:\n~f0\nk=(2fs)\n2Nkwithk= 0;:::;N\u00001 (19)",
                "The\nmodel is a WGAN with gradient penalty.",
                "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nFigure 4.",
                "This model con-\n\ufb01guration has 64 million parameter for both the generator\n1Depending on the model type and implementation details 1\nto 3 NVIDIA V100s are roughly equivalent to 1 Cloud TPUv2,\nwhich has 8 cores (Wang et al., 2019)MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nFigure 5.",
                "This latter isMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nto be expected since \u00184%of the training dataset contains\natonal pieces (mostly from Alexander Scriabin).",
                "One of these techniques, Generative\nAdversarial Networks (GAN), uses two competing neural\nnetworks to generate a distribution of generated samples\nwhich closely approximates the true distribution of real sam-\nples.",
                "A lot of research in recent years has contributed to improv-\ning the stability of the training process of GANs.",
                "Arjovsky\net al. (2017) introduced the WGAN model with a loss func-\ntion based on the Wasserstein distance between distributions.",
                "Gulrajani et al. (2017) introduced a further key im-\nprovement to the WGAN technique by replacing the weight\nclipping with a gradient penalty in the loss function.",
                "Multi-\nple other GAN \ufb02avors exist with different loss functions and\nother features to stabilize the training process (Roth et al.,\n2017; Lim & Ye, 2017; Mescheder et al., 2018).",
                "Recent\nresearch on GAN stability has identi\ufb01ed the key role played\nby the singular values of the network kernels (Sedghi et al.,\n2019).",
                "GAN-based models have produced impressive results in the\n\ufb01eld of image generation.",
                "Karras et al. (2018) introduced\nthe ProGAN model.",
                "The StyleGAN model (Karras et al., 2019), is a\nfurther modi\ufb01cation to the ProGAN model allowing one to\ntune the style of the generated image at each level of detail\ngoing from \ufb01ne-grained features over middle-level styles,\nsuch as eyes, hair and lighting of the picture to high-level\nstyles like hair style and face shape.",
                "The SAGAN model\n(Zhang et al., 2019) uses a self-attention layer to increase the\nlong-range coherence of convolutional neural nets and boost\nmodel performance for images which contain geometric\nstructures.",
                "Brock et al. (2019)\nexplore these issues and give an extensive hyper-parameter\nscan for hinge-loss based model such as SAGAN.\nPart of the research into audio generation has focus on sym-\nbolic music generation such as in Hadjeres et al.",
                "GAN-based models on raw audio were introduced in Wave-\nGAN (Donahue et al., 2019).",
                "Most closely related to the work presented here is GAN-\nsynth (Engel et al., 2019) and MelNet (Vasquez & Lewis,\n2019).",
                "GANsynth produces the STFT spectrograms for 4s\naudio samples.",
                "The GANsynth architecture is based on Pro-\nGAN.",
                "The resulting GANsynth audio samples of musical\nnotes from different instruments are consistently judged by\nhuman evaluators of better \ufb01delity compared to the similar\nsamples generated by WaveNet.",
                "This training process is much faster\nsince the memory footprint of the features in the deeperMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nFigure 6.",
                "In the\ncase of images, the authors of ProGAN take blurred versions\nof the real images and present these when training the deep\nlayers.",
                "While the MP3net architecture is similar in many\nrespects to ProGAN, the characteristics of the data represen-\ntation is rather different.",
                "Another approach to controlling the gener-\nated samples is to upgrade the network structure in line with\nStyleGAN (Karras et al., 2019).",
                "7. Conclusion\nIn this paper, we introduce MP3net, a 2D convolutional\nGAN with an architecture similar to some of the most suc-\ncessful image generation GANs (Karras et al., 2018; Zhang\net al., 2019; Karras et al., 2019).",
                "Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein gan,\n2017.MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nBrandenburg, K. Mp3 and aac explained.",
                "Brock, A., Donahue, J., and Simonyan, K. Large scale gan\ntraining for high \ufb01delity natural image synthesis, 2019.",
                "Dong, H.-W., Hsiao, W.-Y ., Yang, L.-C., and Yang, Y .-H.\nMusegan: Multi-track sequential generative adversarial\nnetworks for symbolic music generation and accompani-\nment, 2017.",
                "Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue,\nC., and Roberts, A. Gansynth:",
                "Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V ., and\nCourville, A. Improved training of wasserstein gans,\n2017.",
                "Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progres-\nsive growing of gans for improved quality, stability, and\nvariation, 2018.",
                "Lim, J. H. and Ye, J. C. Geometric gan, 2017.",
                "Mescheder, L., Geiger, A., and Nowozin, S. Which training\nmethods for gans do actually converge?, 2018.",
                "Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A.\nSelf-attention generative adversarial networks, 2019.MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nZ\u00f6lzer, U. Digital audio signal processing."
            ],
            "lstm": []
        },
        {
            "title": "Hierarchical Recurrent Neural Networks for Conditional Melody Generation with Long-term Structure",
            "gan": [
                "[17] D. Herremans, S. Weisser, K. S \u00a8orensen, and D. Conklin, \u201cGenerating\nstructured music for bagana using quality metrics based on markov\nmodels,\u201d Expert Syst. with Appl. , vol."
            ],
            "lstm": [
                "Since music data is sequential, researchers have recently\nfocused on developing recurrent neural networks (RNN) and\ntheir variants (e.g., long-short term memory (LSTM) / gated\nrecurrent units (GRU)) for music generation due to theirarXiv:2102.09794v2",
                "Looking at \u201cpure\u201d neural networks that do not leverage\nslower optimization techniques, [11] use a novel Tonnetz\nrepresentation to train an LSTM that is better able to gen-\nerate polyphonic music with repeated patterns than a similar\nnetwork with a more traditional piano roll representation.",
                "[1] contains a two-layer LSTM network with\nresidual connections between different time steps.",
                "It \ufb01rst generates\nbar pro\ufb01les and beat pro\ufb01les which indicate the rhythmic\npattern within a bar and a beat using vanilla LSTMs.",
                "All tiers except for the bottom\ntier use LSTM cells to process grouped event data.",
                "frames k=(\n[e1:::eFSk];[eFSk+1:::e2\u0001FSk]:::; ifk6= 1\n[e1:::eFSk];[e2:::eFSk+1]:::; ifk= 1\n(4)\nWe de\ufb01ne the input for each tier\u2019s processing unit (2-layer\nLSTM or conv1d) ikas follows:\nik=(\nframes k; ifk=toptier or1",
                "Wfframes k+Woiok+1;otherwise(5)\nDifferent tiers receive input at different rates, hence, the output\nfrom the upper tier LSTMs needs to be upsampled before\nit is used as a condition in the lower tier generation.",
                "In the\nformula below, hindicates the hidden states of the LSTM;\notkindicates the intermediate output for tier kat thet-th\ntime step; otkrepresents the upsampled LSTM output; acct\nrepresents the accumulated time information at time step t;\n\bis used for vector concatenation; and all Ws are trainable\nweights.",
                "otk;htk=LSTM (itk;ht\u00001k); if k6= 1 (6)\notk=Woupsampleotk; ifk6=toptier or1\n(7)\not1= (Wiit1+ktopX\nk=2otk)\bacct (8)\nTo predict the next event at the t+ 1-th time step, we input\nthe intermediate output ot1to the predictive network as perFig.",
                "All models utilise a two-layer LSTM\nwith 256 nodes as the memory unit.",
                "During the data encoding stage, we changed the absolute\ntiming into delta timing due to the nature of the LSTM cells.",
                "We implemented two AttentionRNN models with the same\nLSTM setting: 2 layers (each with 256 nodes).",
                "[10] D. Eck and J. Schmidhuber, \u201cA \ufb01rst look at music composition using\nlstm recurrent neural networks,\u201d Istituto Dalle Molle Di Studi Sull\nIntelligenza Arti\ufb01ciale , vol. 103, p. 48, 2002."
            ]
        },
        {
            "title": "LEARNING TO GENERATE MUSIC WITH SENTIMENT",
            "gan": [],
            "lstm": [
                "[13] showed that a genera-\ntive Long short-term memory (LSTM) neural network can\n1Supporting strong long-term dependencies (music form) is still an\nopen problem.",
                "When\ncombined to a Logistic Regression, this LSTM achieves\nstate-of-the-art sentiment analysis accuracy on the Stan-\nford Sentiment Treebank dataset and can match the per-\nformance of previous supervised systems using 30-100x\nfewer labeled examples.",
                "This LSTM stores almost all\nof the sentiment signal in a distinct \u201csentiment neuron\u201d,\nwhich can be used to control the LSTM to generate sen-\ntences with a given sentiment.",
                "The same dataset\nalso contains another 728 non-labelled pieces, which were\nused for training the generative LSTM.",
                "We combine this generative LSTM with a Logistic\nRegression and analyse its sentiment prediction accuracy\nagainst a traditional classi\ufb01cation LSTM trained in a fully-\nsupervised way.",
                "Results showed that our model (genera-\ntive LSTM with Logistic Regression) outperformed the su-\npervised LSTM by approximately 30%.",
                "[11] tackles this problem by training an LSTM with a\nnew representation that supports tempo and velocity events\nfrom MIDI \ufb01les.",
                "For each byte, the model updates its hidden\nstate of the mLSTM and predicts a probability distribution\nover the next possible byte.",
                "This mLSTM was trained on the Amazon product re-\nview dataset, which contains over 82 million product re-\nviews from May 1996 to July 2014 amounting to over 38\nbillion training bytes",
                "[13] used the\ntrained mLSTM to encode sentences from four different\nSentiment Analysis datasets.",
                "The \ufb01nal hidden states of\nthe mLSTM are used as a feature representation.",
                "[13] discovered a single\nunit within the mLSTM that directly corresponded to sen-\ntiment.",
                "Because the mLSTM was trained as a generative\nmodel, one can simply set the value of the sentiment unit\nto be positive or negative and the model generates corre-\nsponding positive or negative reviews.",
                "3.1 Data Representation\nWe use the same combination of mLSTM and logistic re-\ngression to compose music with sentiment.",
                "[13] method to com-\npose music with sentiment, we also need a dataset of MIDI\n\ufb01les to train the LSTM and another one to train the lo-\ngistic regression.",
                "5. SENTIMENT ANALYSIS EVALUATION\nTo evaluate the sentiment classi\ufb01cation accuracy of our\nmethod (generative mLSTM + logistic regression), we\ncompare it to a baseline method which is a traditional\nclassi\ufb01cation mLSTM trained in a supervised way.",
                "Our\nmethod uses unlabelled MIDI pieces to train a generative\nmLSTM to predict the next word in a sequence.",
                "An ad-\nditional logistic regression uses the hidden states of the\ngenerative mLSTM to encode the labelled MIDI phrases\nand then predict sentiment.",
                "The baseline method uses only\nlabelled MIDI phrases to train a classi\ufb01cation mLSTM to\npredict the sentiment for the phrase.",
                "We trained the generative mLSTM with 6 different sizes\n(number of neurons in the mLSTM layer): 128, 256, 512,\n1024, 2048 and 4096.",
                "The mLSTM hidden and cell states were initialized\nto zero at the beginning of each shard.",
                "Each sequence is processed\nby an embedding layer (which is trained together with the\nmLSTM layer) with 64 neurons before passing through the\nmLSTM layer.",
                "We evaluated each variation of the generative mLSTM\nwith a forward pass on test shard using mini-batches of size\n32.",
                "Table 1 shows the average3cross entropy loss for each\nvariation of the generative mLSTM.\nmLSTM Neurons Average Cross Entropy Loss\n128 1.80\n256 1.61\n512 1.41\n1024 1.25\n2048 1.15\n4096 1.11\nTable 1 :",
                "The average cross entropy loss decreases as the size of\nthe mLSTM increases, reaching the best result (loss 1.11)\nwhen size is equal to 4096.",
                "[13], we\nre-encoded each of the 966 labelled phrases using the \ufb01nal\ncell states (a 4096 dimension vector) of the trained genera-\ntive mLSTM-4096.",
                "We\nplug a logistic regression into the mLSTM-4096 to turn it\ninto a sentiment classi\ufb01er.",
                "This ends up\nhighlighting the generative mLSTM neurons that contain\nmost of the sentiment signal.",
                "We compared this generative mLSTM + logistic regres-\nsion approach against our baseline, the supervised mL-\nSTM.",
                "This is an mLSTM with exactly the same architec-\nture and size of the generative version, but trained in a\n3Each mini-batch reports one loss.fully supervised way.",
                "To train this supervised mLSTM,\nwe used the word-based representation of the phrases, but\nwe padded each phrase with silence (the symbol \u201c.\u201d) in\norder to equalize their length.",
                "Training parameters (learn-\ning rate and decay, epochs, batch size, etc) were the same\nones of the the generative mLSTM.",
                "Method Test Accuracy\nGen. mLSTM-4096 + Log.",
                "mLSTM-4096 60.35 \u00063.52\nTable 2 : Average (10-fold cross validation) sentiment clas-\nsi\ufb01cation accuracy of both generative (with logistic regres-\nsion) and supervised mLSTMs.",
                "The generative mLSTM with logistic regression\nachieved an accuracy of 89.83%, outperforming the super-\nvised mLSTM by 29.48%.",
                "The supervised mLSTM accu-\nracy of 60.35% suggests that the amount of labelled data\n(966 phrases) was not enough to learn a good mapping be-\ntween phrases and sentiment.",
                "The accuracy of our method\nshows that the generative mLSTM is capable of learning,\nin an unsupervised way, a good representation of sentiment\nin symbolic music.",
                "First, since\nthe higher accuracy of generative mLSTM is derived from\nusing unlabeled data, it will be easier to improve this over\ntime using additional (less expensive) unlabeled data, in-\nstead of the supervised mLSTM approach which requires\nadditional (expensive) labeled data.",
                "Second, because the\ngenerative mLSTM was trained to predict the next word\nin a sequence, it can be used as a music generator.",
                "The \ufb01tness of an individual is\ncomputed by (i) adding the genes of the individual to the\nweights (vector addition) of the 161L1 neurons of the gen-\nerative mLSTM, (ii) generating Ppieces with this mL-\nSTM, (iii) using the logistic regression model to predict\nthese Pgenerated pieces and (iv) calculating the mean\nsquared error of the Ppredictions given a desired senti-\nments2S=f0;1g.",
                "We performed two independent executions of this GA,\none to optimize the mLSTM for generating positive pieces\nand another one for negative pieces.",
                "This means that if we add the genes of the\nbest individual of the \ufb01nal population to the weights of the\ngenerative mLSTM, we generate positive pieces with 84%\naccuracy and negative pieces with 67% accuracy.",
                "After these two optimization processes, the genes of\nthe best \ufb01nal individual of the positive optimization were\nadded to the weights of the 161 L1 neurons of the trained\ngenerative mLSTM.",
                "This suggests that the best negative individual (with\n\ufb01tness 0:33) encountered by the GA wasn\u2019t good enough to\ncontrol the mLSTM to generate complete negative pieces.",
                "Moreover, the challenge to optimize the L1 neurons sug-\ngests that there are more positive pieces than negative ones\nin the 3 shards used to train the generative mLSTM.",
                "7. CONCLUSION AND FUTURE WORK\nThis paper presented a generative mLSTM that can be con-\ntrolled to generate symbolic music with a given sentiment.",
                "The mLSTM is controlled by optimizing the weights of\nspeci\ufb01c neurons that are responsible for the sentiment sig-\nnal.",
                "Such neurons are found plugging a Logistic Regres-\nsion to the mLSTM and training the Logistic Regression\nto classify sentiment of symbolic music encoded with the\nmLSTM hidden states.",
                "Results showed that\nour model obtained good classi\ufb01cation accuracy, outper-\nforming a equivalent LSTM trained in a fully supervised\nway.",
                "Multiplicative LSTM for sequence modelling."
            ]
        },
        {
            "title": "Personalized Popular Music Generation Using Imitation and Structure",
            "gan": [
                "Recently, representation\nlearning has motivated the practice of music style representation and disentanglement using\nVariational Auto-Encoders (VAEs) (Kawai, Esling, & Harada, 2020; Roberts, Engel, Ra\u000bel,\nHawthorne, & Eck, 2018; Yang et al., 2019) and Generative Adversarial Networks (GANs)\n(Yu, Zhang, Wang, & Yu, 2017).",
                "Seqgan: Sequence generative adversarial\nnets with policy gradient."
            ],
            "lstm": []
        },
        {
            "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network",
            "gan": [
                "IEEE GAN Computers Generate Arts?",
                "Using generative adversarial networks (GANs), applications such as synthesizing photorealistic human faces and creating captions automatically from images were realized.",
                "This survey takes a comprehensive look at the recent works using GANs for generating visual arts, music, and literary text.",
                "A performance comparison and description of the various GAN architecture are also presented.",
                "Finally, some of the key challenges in art generation using GANs are highlighted along with recommendations for future work.",
                "Keywords\u2014GAN, deep learning, arts, generative learning, computer vision, computer art I. INTRODUCTION  Art, a manifestation of human creativity, has always been an essential component of human culture.",
                "This is where deep learning and more \nspecifically generative adversarial networks (GANs) can be effective.",
                "Organized in a layered hierarchy of concepts (the term \u2018deep\u2019 coming from the depth of the layers), complex concepts are defined in terms of simpler ones and more abstract representations are gathered using less abstract ones [9].",
                "Generative Adversarial Networks (GANs) use deep learning architectures to facilitate generative modeling.",
                "GANs primarily consist of two deep learning models, namely the generator and the discriminator.",
                "GANs have already achieved remarkable results in various applications including the generation of photorealistic images, scenes, and people which are non-recognizable as fakes even to humans.",
                "The idea of computers generating art without human supervision can be realized using GANs, unlike previous approaches.",
                "The fact that GANs can automatically learn to generate new examples from a given set of data proves to be a very useful component in computer art generation.",
                "Also, due to the advancement of technology and digitization, the two main requirements of GANs, datasets and computing power, have become widely available.",
                "Therefore, this makes GANs, a natural candidate to solve the difficult problem of computer-generated arts.",
                "GANs, on the other hand, are not restricted to any such conditions, and therefore has the potential to create more realistic arts.",
                "At the same time, we keep the paper focused by reviewing art works generated using a specific deep learning model, i.e., GANs.",
                "We hope this work will inspire researchers and artists to collaborate on advancing the field of computer-generated arts using GANs.",
                "Following are the key contributions of this paper: \u2022 It provides an overview of the primary GAN architectures for generative arts.",
                "In this Section, the necessary background information including relevant GAN architectures will be discussed.",
                "A. Generative Adversarial Netowrks GANs, first introduced by [14], can generate new content based on a min-max game between two networks, the generator and the discriminator.",
                "Figure 2 depicts a basic GAN architecture consisting of a single generator and discriminator.",
                "Meanwhile, the discriminator has access to the ground truths, whether the data came from the generator or real dataset, which it can use to minimize its error.  \n Figure 2 A Basic GAN Architecture More sophisticated GAN architectures can make use of labels to generate data points for a specific category.",
                "This is useful because in the standard GAN, data points are generated only based on the input noise.",
                "Next, various GAN architectures relevant to art generation is discussed.",
                "1) Conditional GAN: Extended from the regular GAN, it is a conditional architecture if the generator and discriminator are conditioned on auxiliary information such as class labels [15].",
                "The auxiliary information is combined in a joint representation with the noise in the basic GAN, allowing the generator to control the generation of data points based on the \ninput condition.",
                "For instance, conditional GANs can be used to generate various genres of music including jazz, rock, and classic.   2) Deep Convolutional GAN: Commonly known as DCGAN",
                "Given the success of convolutional neural network in image and video classification in recent years, DCGAN remains a suitable architecture for image generation applications.   3) Recurrent Adversarial Netwokrs: In this architecture, a recurrent computation is obtained by unrolling the gradient descent optimization [18].",
                "Other common GAN architectures include InfoGAN",
                "Laplacian GAN [20], and",
                "Wasserstein GAN",
                "For a comprehensive comparison of GAN architectures, the readers are encouraged to refer to [17] and [22].",
                "B. Common Loss Functions GAN training in a broader sense is based on a min-max game between two networks, the generator, and the discriminator.",
                "This loss function is typically used in the least square GAN (LSGAN).",
                "More recently, Wasserstein distance has gained interest as a loss function, especially in GAN training.",
                "RECENT ADVANCES IN ARTS GENERATION USING GANS In this section we provide a discussion and comparison of the recent works using GANs for generating various art forms including visual arts, music, and literary texts.",
                "A. Organization",
                "In this survey, the main organization will be based on the generation of three types of artworks.",
                "Furthermore, the loss function used, the GAN type, and architectural details will also be highlighted.",
                "Figure 3 provides a graphical illustration of the framework for organization of this survey paper.",
                "Figure 3 Framework of the Survey B. Visual Arts Generation using GANs This section presents a comparison of GAN approaches for visual arts generation.",
                "[24] presented a conditional GAN framework that can automatically generate painted cartoon images from a given sketch.",
                "A similar conditional GAN architecture using a U-Net generator for generating shoe image from a given shoe sketch is presented in [25].",
                "Similarly, the authors in [26] implemented a GAN-based solution for generating synthetic images that are fully colored and textured from a given sketch.",
                "This led the author to experiment with GANs for generating more realistic brushstrokes.",
                "Instead of giving the generator random noise as input as it is in typical GAN, the action space was provided in this approach.",
                "The brushstrokes generated by the GAN architecture were rougher and more realistic.",
                "In [28], the authors proposed a modified version of DCGAN for generating arts without any condition.",
                "When compared to the baseline DCGAN, the proposed work had better performance with 53% respondents believing that the works were generated by an artist as opposed to 35% for DCGAN.",
                "The authors then explored with an existing GAN architecture known as styleGAN",
                "Unlike regular GAN, styleGAN uses a mapping network to map points in the latent space to an intermediatory latent space to control the style of the data in the generator.",
                "Figure 6 Samples from Japanese Art Facial Expression Generated using StyleGAN",
                "A framework for generating a face photo from a given sketch as well as a sketch from a given face photo is implemented using GANs in [31].",
                "The authors trained a simple conditional GAN for both applications.",
                "[32] introduced StrokeNET, a GAN-based architecture to generate digits and character strokes.",
                "This section focused on the recent applications of GAN with regards to the generation of visual arts.",
                "Furthermore, the table also summarizes the GAN type used, the loss function as well as the generator and discriminator architectures.",
                "Recent Advances in Visual Arts Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Result",
                "[24] Generate cartoon image from sketch Conditional GAN Cross entropy, L1 distance for pixel-level loss U-Net Generator, CNN-FC discriminator Qualitative only, outperformed existing works",
                "[25] Generate shoe image from sketch Conditional GAN Binary cross-entropy loss for discriminator U-Net Generator, Deep CNN discriminator No evaluation",
                "[26] Generate fully colored synthetic images from sketch Vanilla GAN with Encoder for image style recognition Auxiliary Losses, discriminator is trained on style loss & content loss.",
                "[27] Generate painting by brushstrokes Conditional GAN Wasserstein Loss Configurations not provided No evaluation [28] Unconditional art generation DCGAN Cross entropy loss and added classification, style ambiguity losses Deep CNNs, CONV followed by LeakyReLU 53% of evaluators believe that synthesized images were by an artist.",
                "[29] Generate pre-modern Japanese art facial expression StyleGAN WGAN-GP Configurations not provided No evaluation [31] Generate face photo from a sketch and vice versa Conditional GAN Cross entropy Configurations not provided No evaluation, the loss values were reported [32] Generate digits and character strokes Modified DCGAN + agent MSE Generator contains CONV+LeakyReLU, Agent is VGG Generated images not evaluated, classification accuracy 91% on MNIST [33] Generate calligraphy and handwritten digits Modified conditional GAN with multiple encoders for style & content-encoding Binary Cross entropy discriminator, cross-entropy style loss & Kullback-Leibler content loss Two residual blocks then 4 CONV modules generator, 1 CONV layer then 6 residual blocks discriminator FID 120.1, 49.3% of the images were identified to be synthesized C. Music and Melody Generation using GANs  Next, GAN approaches for music and melody generation are presented.",
                "[34] used Musical Instrument Digital Interface (MIDI) files, which contain data to specify the musical instruction such as note\u2019s notation, pitch, and vibrato, to train a hybrid variational autoencoder (VAE) and GAN to generate musical melody for a specific genre.",
                "Similarly, [36] proposed a melody generation framework using GAN, consisting of a Bi-directional LSTM generator and an LSTM discriminator.",
                "[39] used conditional GANs for melody generation.",
                "The output of the layer is fed into the conditioned-lyrics GAN.",
                "A similar study was conducted in [41] using the exact dataset and conditional-LSTM GANs.",
                "Mogren [42] proposed an RNN GAN for generating single voice polyphonic music.",
                "[43] introduced a novel multi-track polyphonic symbolic music generator using GANs.",
                "The proposed \u2018MuseGAN\u2019 architecture utilized three different GANs namely the jamming model, the composer model, and the hybrid model.",
                "To implement the GANs, the Lakh MIDI dataset was transformed into a multi-track piano-rolls representation.",
                "This section focused on the recent applications of GAN with regards to music and melody generation.",
                "Moreover, the table also summarizes the GAN type used, the loss function as well as the generator and discriminator architectures.",
                "Recent Advances in Music and Melody Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Result",
                "[34] Generate melody for a specific genre Hybrid VAE and GAN KL Divergence Four DeCONV layers for both generator & discriminator, VAE encoder used three Conv2D No quantitative evaluation, concluded that consistency of generated melodies was not up to the same level as human composition",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds",
                "[39] Generate melodies based on lyrics Conditional GAN Cross entropy LSTM generators and discriminators No evaluation was provided  [41] Generate melodies based on lyrics Conditional LSTM GAN Cross entropy Dense layer followed by 2 LSTM followed by a dense layer for generator, 2 LSTM followed by dense for discriminator BLEU-2 score of 0.735, scores of about 3.8, 3.5, 4.1 respectively out of 5 for lyrics, rhythm, and melody by evaluators [42] Generate single voice polyphonic music RNN GAN Cross entropy and Squared error loss 2 LSTM layers for generator, 2 Bi-directional LSTM layers followed by a dense for discriminator No evaluation was provided  [43] Generate multi-track, polyphonic music Conditional GAN Wasserstein Generators contain 1D transposed CONV, discriminators 5 contain 1D Conv layers followed by one dense layer The highest score for conditional generation was 3.1 and non-conditional was 3.16 out of 5 by \u2018non-pro\u2019 evaluators.",
                "For intra-track metrics, jamming model performed best [44] Generate folk music RL GAN Cross entropy and policy gradient RNN generators, CNN discriminators BLEU score of 0.94 and MSE of 20.6 outperformed baseline maximum likelihood estimation D. Poetry and Literary Text Generation using GANs Researchers have also focused on literary text generation using GANs.",
                "This section presents a comparison of GANs in poetry and literary text generation.",
                "Most GAN architectures are restricted by several factors when it comes to text and sequential generation.",
                "[44] introduced SeqGAN to overcome these challenges by defining the generator as stochastic policy in reinforcement learning (RL).",
                "The SeqGAN architecture is illustrated in Figure 8.",
                "SeqGAN",
                "[44] \nThe discriminator in SeqGAN is trained using both the real as well as generated data.",
                "The SeqGAN generated poems outperformed the baseline maximum likelihood estimation on the BLEU score (statistically significant with p-value less than 10^-6).",
                "Additionally, 70 experts on Chinese poems were asked to evaluate between 20 real poems, 20 generated using maximum likelihood estimation, and 20 generated using SeqGAN.",
                "The SeqGAN outperformed the baseline with a 0.54 average score and a statistically significant p-value.",
                "The approach of extending GANs to generate sequences of discrete tokens is suitable for poetry and other text generations because of the sequential nature of text datasets.",
                "The proposed image to poetry GAN (I2P-GAN) was evaluated on several metrics such as relevance, novelty, and BLEU scores against different architectures including SeqGAN.",
                "The proposed I2P-GAN outperformed all the models with a 7.18 overall score which is close to the ground truth overall score of 7.37.",
                "Figure 9 Poems Generated from a Given Image of the Proposed I2P-GAN in [45] and Comparison with Baseline Models Kashyap et al.",
                "A sample generation of the poem (center) and prose (right) of the proposed model is shown in Figure 11.  Figure 11 A sample Poem and Prose Generation from [46] GANs are excellent at modeling continuous distributions, making them suitable for tasks like image generation.",
                "As a result, a maximum likelihood augmented discrete GAN was proposed in [47].",
                "The proposed model outperformed \nseveral baselines such as SeqGAN in terms of several metrics including BLUE score.",
                "For objective function, Wasserstein GAN (WGAN), as well as WGAN with gradient penalty (WGAN-GP)",
                "Both LSTM-based, as well as CNN-based GAN architectures, were experimented and the proposed model with LSTM outperformed the existing works on the aforementioned Poem-5 and Poem-7 datasets.",
                "[50] presented RankGAN by taking inspiration from the learning to rank concept from information retrieval, where given a reference, the required information is integrated into the ranking function encouraging relevant documents to be returned quicker.",
                "The overall architecture of RankGAN is presented in Figure 12.",
                "Figure 12 Proposed RankGAN Architecture in [50] The RankGAN differs from traditional GAN by including a sequence of generators and a ranker.",
                "In terms of BLUE score, the proposed RankGAN outperformed both SeqGAN and maximum likelihood estimation.",
                "Additionally, 57 native mandarin Chinese speakers were asked to rate the generations from SeqGAN, RankGAN, and human-written poems.",
                "The average scores provided by the evaluators were 3.4, 4.6, and 6.4 for SeqGAN, RankGAN, and human-written poems, respectively.",
                "The proposed RankGAN model was also applied to Shakespeare\u2019s Romeo and Juliet play to learn lexical dependency and use rare phrases in the play.",
                "The high BLEU-2 score of 0.91 indicates that RankGAN was able to capture the transition pattern among the words despite the training sentences being novel, subtle, and complex.",
                "[51] proposed a GAN architecture for creative text generation.",
                "In terms of perplexity scores, the proposed Creative-GAN outperformed the existing works.",
                "This section presented a comparison of recent applications of GAN with regards to poetry and literary text generations.",
                "Moreover, the GAN type, loss function, and architectural details are also highlighted.",
                "Recent Advances in Literary Text Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Dataset Result",
                "[44] Chinese Poetry generation RL GAN Cross entropy and policy gradient RNN Generators and CNN discriminators 16394 Chinese quatrains BLEU-2 score of 0.74, overall score of 0.54 by human evaluators [45] Generate poetry from an image Multiadversarial GAN with an embedding model Cross entropy and policy gradient RNN Generators, GRU-based discriminator, CNN image encoder and RNN poem decoder Novel dataset with paired image and poetry Overall BLEU score of 0.77, 7.18 out of 10 overall score by human evaluators [46] Generate Shakespearean prose from a painting Multiadversarial GAN with encoder and decoder Cross entropy and policy gradient RNN Generator, CNN-RNN agent for encoding and decoding painting, LSTM encoder and decoder for generating prose Two datasets for generating English poem from an image, and Shakespeare plays and their English translations for text style transfer Average scores of 3.7, 3.9, and 3.9 out of 5 by evaluators for content, creativity, and similarity to Shakespearean style respectively [47] Chinese Poetry generation RL GAN Maximum-likelihood A single layer LSTM generator, two-layer Bi-directional LSTMs discriminator Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.76 and 0.55 for the two datasets respectively",
                "[48] Chinese Poetry generation RNN GAN Wasserstein distance LSTM generator and discriminator with WGAN-GP training Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.88 and 0.67 for the two datasets respectively",
                "[50] Chinese Poetry generation GAN with a ranking function Ranking objective LSTM generator, CNN-based ranker Over 13,000 Chinese quatrains BLEU-2 score of 0.81, 4.6 out of 10 overall score by human evaluators [50] Learn rare words from Romeo and Juliet play GAN with a ranking function Ranking objective LSTM generator, CNN-based ranker Over 3000 sentences from Romeo and Juliet play BLEU-2 score of 0.914",
                "[51] Poetry and lyrics generation GAN with language model generator Cross entropy AWD-LSTM [52] and TransformerXL",
                "Although the previous section provided some promising recent developments in generating arts using GANs, there remains a few challenges.",
                "Moreover, the size of the dataset required for GAN training remains a challenge.",
                "Therefore, for novel applications, a lot of time is required in gathering the dataset first before applying the GAN training.",
                "There remain a few notable challenges in training GANs for music and melody generation.",
                "Therefore, to truly determine the ability of GAN with written arts, other applications such as novel writing and rhyme generations should be explored.",
                "This would lead to a better understanding of how well GANs can work for generating visual arts when the data is scarce.",
                "It could potentially lead to the development of a GAN framework that is well suited to dealing with such datasets.",
                "Therefore, training large scale GAN architectures will not be suitable.",
                "Therefore, future research should focus on implementing GAN architectures for generating music in raw audio format.",
                "Therefore, to find out the effectiveness of GANs in producing larger text pieces such as novels and dramas, researchers are encouraged to take on the challenge with longer texts.",
                "The previous section of this paper clearly highlighted the remarkable progress in recent years for generating various artworks utilizing GANs.",
                "In summary, for future work on art generation using GANs, we recommend the following: \u2022 Experiment with smaller dataset and GAN architectures for visual arts generation.",
                "Implement GANs to generate music in raw audio format as opposed to MIDI file format.",
                "Fortunately, due to the advancement in the field of deep learning, GANs emerged as a promising technology for computer generated arts.",
                "After a brief overview on different types of GANs and loss functions, the paper presented the recent works on generating visual arts, music, and literary texts using GANs.",
                "[10] J. Brownlee, \u201cA gentle introduction to generative adversarial networks (GANs),\u201d Retrieved June, vol. 17, p. 2019, 2019.",
                "[19] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, \u201cInfogan: Interpretable representation learning by information maximizing generative adversarial nets,\u201d in Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016, pp.",
                "[33] L. Kang, P. Riba, Y. Wang, M. Rusi\u00f1ol, A. Forn\u00e9s, and M. Villegas, \u201cGANwriting: Content-Conditioned Generation of Styled Handwritten Word Images,\u201d in Computer Vision \u2013 ECCV 2020, Cham, 2020, pp.",
                "[36] Y. Xu, X. Yang, Y. Gan, W. Zhou, H. Cheng, and X. He, \u201cA Music Generation Model Based on Generative Adversarial Networks with Bayesian Optimization,\u201d in Chinese Intelligent Systems Conference, 2020, pp.",
                "[41] Y. Yu, A. Srivastava, and S. Canales, \u201cConditional lstm-gan for melody generation from lyrics,\u201d arXiv preprint arXiv:1908.05551, 2019.",
                "[42] O. Mogren, \u201cC-RNN-GAN: Continuous recurrent neural networks with adversarial training,\u201d arXiv preprint arXiv:1611.09904, 2016.",
                "Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang, \u201cMusegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, 2018, vol.",
                "[44] L. Yu, W. Zhang, J. Wang, and Y. Yu, \u201cSeqgan: Sequence generative adversarial nets with policy gradient,\u201d in Proceedings of the AAAI conference on artificial intelligence, 2017, vol.",
                "[49] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville, \u201cImproved training of wasserstein gans,\u201d arXiv preprint arXiv:1704.00028, 2017.",
                "[51] A. Saeed, S. Ili\u0107, and E. Zangerle, \u201cCreative GANs for generating poems, lyrics, and metaphors,\u201d arXiv preprint arXiv:1909.09534, 2019."
            ],
            "lstm": [
                "Similarly, [36] proposed a melody generation framework using GAN, consisting of a Bi-directional LSTM generator and an LSTM discriminator.",
                "The generator uses LSTM to study the sequential alignment between the lyrics and melody and the real MIDI samples' distribution.",
                "A similar study was conducted in [41] using the exact dataset and conditional-LSTM GANs.",
                "The architecture of both the generator and discriminator contains 2 LSTM layers of 350 hidden units.",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds",
                "[39] Generate melodies based on lyrics Conditional GAN Cross entropy LSTM generators and discriminators No evaluation was provided  [41] Generate melodies based on lyrics Conditional LSTM GAN Cross entropy Dense layer followed by 2 LSTM followed by a dense layer for generator, 2 LSTM followed by dense for discriminator BLEU-2 score of 0.735, scores of about 3.8, 3.5, 4.1 respectively out of 5 for lyrics, rhythm, and melody by evaluators [42] Generate single voice polyphonic music RNN GAN Cross entropy and Squared error loss 2 LSTM layers for generator, 2 Bi-directional LSTM layers followed by a dense for discriminator No evaluation was provided  [43] Generate multi-track, polyphonic music Conditional GAN Wasserstein Generators contain 1D transposed CONV, discriminators 5 contain 1D Conv layers followed by one dense layer The highest score for conditional generation was 3.1 and non-conditional was 3.16 out of 5 by \u2018non-pro\u2019 evaluators.",
                "To obtain the final prose, a sequence-to-sequence model with a unidirectional single layer LSTM encoder and a single layer LSTM decoder with pre-trained word embeddings were used.",
                "The generator is a single layer LSTM and discriminators are two-layered Bi-directional LSTMs.",
                "Both LSTM-based, as well as CNN-based GAN architectures, were experimented and the proposed model with LSTM outperformed the existing works on the aforementioned Poem-5 and Poem-7 datasets.",
                "[44] Chinese Poetry generation RL GAN Cross entropy and policy gradient RNN Generators and CNN discriminators 16394 Chinese quatrains BLEU-2 score of 0.74, overall score of 0.54 by human evaluators [45] Generate poetry from an image Multiadversarial GAN with an embedding model Cross entropy and policy gradient RNN Generators, GRU-based discriminator, CNN image encoder and RNN poem decoder Novel dataset with paired image and poetry Overall BLEU score of 0.77, 7.18 out of 10 overall score by human evaluators [46] Generate Shakespearean prose from a painting Multiadversarial GAN with encoder and decoder Cross entropy and policy gradient RNN Generator, CNN-RNN agent for encoding and decoding painting, LSTM encoder and decoder for generating prose Two datasets for generating English poem from an image, and Shakespeare plays and their English translations for text style transfer Average scores of 3.7, 3.9, and 3.9 out of 5 by evaluators for content, creativity, and similarity to Shakespearean style respectively [47] Chinese Poetry generation RL GAN Maximum-likelihood A single layer LSTM generator, two-layer Bi-directional LSTMs discriminator Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.76 and 0.55 for the two datasets respectively",
                "[48] Chinese Poetry generation RNN GAN Wasserstein distance LSTM generator and discriminator with WGAN-GP training Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.88 and 0.67 for the two datasets respectively",
                "[50] Chinese Poetry generation GAN with a ranking function Ranking objective LSTM generator, CNN-based ranker Over 13,000 Chinese quatrains BLEU-2 score of 0.81, 4.6 out of 10 overall score by human evaluators [50] Learn rare words from Romeo and Juliet play GAN with a ranking function Ranking objective LSTM generator, CNN-based ranker Over 3000 sentences from Romeo and Juliet play BLEU-2 score of 0.914",
                "[51] Poetry and lyrics generation GAN with language model generator Cross entropy AWD-LSTM [52] and TransformerXL",
                "[41] Y. Yu, A. Srivastava, and S. Canales, \u201cConditional lstm-gan for melody generation from lyrics,\u201d arXiv preprint arXiv:1908.05551, 2019.",
                "[52] S. Merity, N. S. Keskar, and R. Socher, \u201cRegularizing and optimizing LSTM language models,\u201d arXiv preprint arXiv:1708.02182, 2017."
            ]
        },
        {
            "title": "CONTROLLABLE DEEP MELODY GENERATION VIA HIERARCHICAL MUSIC STRUCTURE REPRESENTATION",
            "gan": [
                "We \ufb01rst organize\nthe full melody with section and phrase-level structure.",
                "One such approach is\nto use Generative Adversarial Networks (GANs) to model\nthe distribution of music [25\u201327].",
                "GANs learn a mapping\nfrom a point zsampled from a prior distribution to an in-\nstance of generated music xand hence represents the dis-\ntribution of music with z.",
                "Yu, \u201cSeqgan: Se-\nquence generative adversarial nets with policy gradi-\nent,\u201d in Thirty-First AAAI Conference on Arti\ufb01cial In-\ntelligence , 2017.",
                "Yang,\n\u201cMusegan: Multi-track sequential generative adversar-\nial networks for symbolic music generation and accom-\npaniment,\u201d Proc. of the AAAI Conference on Arti\ufb01cial\nIntelligence , vol."
            ],
            "lstm": [
                "Machine learning models with\nmemory and the ability to associate context have also been\npopular in this area and include LSTMs and Transformers\n[4, 6, 22, 23], which operate by generating music one or a\nfew notes at a time, based on information from previously\ngenerated notes.",
                "3.2.2 Network Architecture\nWe use an auto-regressive model based on Transformer\nand LSTM.",
                "catenates the encoded representation and the last predicted\nnote as input and passes them through one unidirectional\nLSTM followed by two layers of 1Dconvolutions of ker-\nnel size 1.",
                "Transformer-LSTM architecture for melody, ba-\nsic melody and rhythmic pattern generation.",
                "We also use a Transformer-LSTM architecture (Figure\n5), but with different model settings (size).",
                "3.2.5 Realized Melody Generation\nWe generate melody from a basic melody, a rhythm and\na chord progression using another Transformer-LSTM ar-\nchitecture similar to generating basic melody in Figure 5.",
                "To sample a good sounding\nmelody, we randomly generate 100 sequences by samplingBasic Melody Rhythm Melody\nTrans-LSTM 38.7% 50.1% 55.2%\nLSTM 39.8% 43.6% 51.2%\nTransformer 30.9% 25.8% 39.3%",
                "The \ufb01rst line in Ta-\nble 1 represents the Transformer-LSTM models introduced\nin Section 3.",
                "In all three networks, the projection size and\nfeed forward channels are 128; there are 8 heads in the\nmulti-head encoder attention layer; LSTM hidden size is\n64; dropout rate for basic melody and realized melody gen-\neration is 0.2, dropout rate for rhythm generation is 0.1; de-\ncoder input projection size is 8 for rhythm generation and\n17 for others.",
                "We compared this model with several alternatives: the\nsecond model is a bi-directional LSTM followed by a uni-\ndirectional LSTM (model size is 64 in both).",
                "The third\nmodel is a Transformer with two layers of encoder and\ntwo layers of decoder (with same parameter settings as\nTransformer-LSTM), and we \ufb01rst pre-trained the encoder\nwith 10% of random masking of input (similar to training\nin BERT [37]), and then trained the encoder and decoder\ntogether.",
                "No music frameworks (the fourth line) means\ngenerate without basic melody or basic rhythm form, us-\ning a Transformer-LSTM model.",
                "The results in Table 1\nshow that the Transformer-LSTM achieved the best accu-\nracy.",
                "4.2 Objective Evaluation\nWe use Transformer-LSTM model for all further evalua-\ntions."
            ]
        },
        {
            "title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer",
            "gan": [
                "[51], a geometric-based musical pattern min-\ning method that views the melody notes as dots in a\n2-dimensional space of fonset, pitchgand then extracts\nthe \u201ctranslational equivalence class\u201d (TEC) groups from\nthe melody.",
                "[1] B. L. Jacob, \u201cAlgorithmic composition as a model of creativity,\u201d Organ-\nised Sounds , vol.",
                "[23] A. Muhamed, L. Li, X. Shi, S. Yaddanapudi, W. Chi, D. Jackson,\nR. Suresh, Z. C. Lipton, and A. J. Smola, \u201cSymbolic music generation\nwith Transformer-GANs,\u201d in Proc.",
                "International Joint Conferences on Arti\ufb01cial Intelligence\nOrganization, 7 2019, pp.",
                "Informatics and Semiotics\nin Organisations , 2011, pp.",
                "Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sar-\nlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser et al. , \u201cRethinking\nattention with performers,\u201d in Proc."
            ],
            "lstm": []
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "gan": [
                "[11] E. Parada-Cabaleiro, A. Baird, A. Batliner, N. Cummins, S. Hantke, and\nB. W. Schuller, \u201cThe perception of emotion in the singing voice: The\nunderstanding of music mood for music organisation,\u201d in Proceedings\nof the 4th International Workshop on Digital Libraries for Musicology ,\n2017, pp.",
                "[13] F. Bao, M. Neumann, and N. T. Vu, \u201cCyclegan-based emotion style\ntransfer as data augmentation for speech emotion recognition.\u201d",
                "[14] G. Rizos, A. Baird, M. Elliott, and B. Schuller, \u201cStargan for emotional\nspeech conversion: Validated by data augmentation of end-to-end emo-\ntion recognition,\u201d in ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) .",
                "[21] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra,\n\u201cThe mtg-jamendo dataset for automatic music tagging,\u201d in Machine\nLearning for Music Discovery Workshop, International Conference on\nMachine Learning (ICML 2019) , Long Beach, CA, United States,\n2019.",
                "Available: http://hdl.handle.net/10230/42015\n[22] D. Bogdanov, A. Porter, P. Tovstogan, and M. Won, \u201cMediaeval 2019:\nEmotion and theme recognition in music using jamendo,\u201d in MediaEval\n2019 Workshop , 2019.",
                "Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio-\nlette, M. Marchand, and V ."
            ],
            "lstm": []
        },
        {
            "title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control",
            "gan": [],
            "lstm": []
        },
        {
            "title": "Music Generation Using an LSTM",
            "gan": [
                "1  \n  \n \nMusic Generation Using an LSTM  \n \nMichael Conner, Lucas Gral,  Kevin Adams  \nDavid Hunger, Reagan Strelow, and Alexander Neuwirth  \nDepartment of Electrical Engineering and Computer Science  \nMilwaukee School of Engineering  \n{connerm, grall, adamsk, hungerd, strelowr, neuwirtha}@msoe.edu  \n \n \nAbstract  \nOver the past several years, deep learning for sequence modeling has grown in popularity."
            ],
            "lstm": [
                "1  \n  \n \nMusic Generation Using an LSTM  \n \nMichael Conner, Lucas Gral,  Kevin Adams  \nDavid Hunger, Reagan Strelow, and Alexander Neuwirth  \nDepartment of Electrical Engineering and Computer Science  \nMilwaukee School of Engineering  \n{connerm, grall, adamsk, hungerd, strelowr, neuwirtha}@msoe.edu  \n \n \nAbstract  \nOver the past several years, deep learning for sequence modeling has grown in popularity.",
                "To \nachieve this goal , LSTM network structures have proven to be very useful for  making predictions \nfor the next output in a series .",
                "For instance , a smartphone  predict ing the next word of a text \nmessage  could  use an LSTM .",
                "More specifically, a Long Short -Term Memory (LSTM) \nneural network.",
                "Taking this into account,  we provide a \nbrief synopsis of the intuition, theory, and application of LSTMs in music generation, develop and \npresent the network we found to best achieve this goal, identif y and address  issues  and challenges \nfaced, and include potential future improvements for our network.",
                "To advance \nthe expanding field, we sought to construct a network built from  LSTMs which gene rates \na sequence of pitches and durations which will construct a full song.",
                "ng a LSTM Neural Network in \nKeras\u201d",
                "Enlisting a mapping \nfunction and designing a model architecture implementing fo ur-layer types, LSTM, \ndropout, dense, and the activation layer, we used the structure of the network presented \nhere as a springboard for our work.",
                "This paper presents the application of a n LSTM network for music  generation , includes a \nbrief, comprehensible overview into the intuition and theory behind LSTMs and their \napplication in music sequence modeling, provides an analysis on the benefits and \ndisadvantages of our network and training data, shows the qualitati ve impact on our \nmodel output, and discusses potential improvements for our network.",
                "[4] LSTMs were made to fix this issue, \nand they do  this by predicting what information is important to remember, and what can \nbe ignored .",
                "LSTMs were  originally introduced in 1997 by Sepp Hochreiter and J\u00fcrgen \nSchmidhuber\u2019s paper Long Short -Term Memory .",
                "The structure  of LSTMs since then has \nhad some variations made to improve the general structure for specific tasks .",
                "Ultimately, in their paper An Empirical Exploration of Recurrent Network Architectures \nthey found through a straightforward architecture search that  the existing  structure of \nLSTMs can\u2019t be beat for most applications.",
                "High level overview of an LSTM cell  \nAn LSTM cell consists of two channels running from left to right : a top channel where \ninformation  about state  flows  from one iteration to the next,  and a bottom channel that \nuses the previous output to determine how to modify the state  of the top channel .",
                "The final part of an LSTM ( labeled as 3) uses both the top \nand bottom channels to decide what the network should output .",
                "The major \ndifference we found between our structure and others, was that with the large amoun t of \nprocessing power available to us via MSOE\u2019s high performance computing cluster named \nRosie, we were able to make our network larger, and deeper with more stacked LSTMs at \nthe front of the network.",
                "In the same sense, \nour network\u2019s tiered LSTM architecture  may allow it to pick apart the various  aspects of \nmusic such as if it is building tension , speeding up, or looking at any other type of pattern \nthat may be found in music.",
                "5 Conclusion  \nIn this paper, we gave an overview of the evo lution of LSTMs and how we applied them \nto music generation.",
                "\u201cUnderstanding LSTM Networks .\u201d \nhttps://colah.github.i o/posts/2015 -08-Understanding -LSTMs / (2015)",
                "[4] Michael Phi \u201cIllustrated Guide to LSTM\u2019s and GRU\u2019s: A  Step by Step Explanation.\u201d  \nhttps://towardsdatascience.com/illustrated -guide -to-lstms",
                "[6] Sigur\u00f0ur Sk\u00fali \u201cMusic Gen eration Using a LSTM Neural Network in Keras \u201d \nhttps://towardsdatascience.com/how -to-generate -music -using -a-lstm-neural -network -in-\nkeras -68786834d4c5 (2017)"
            ]
        },
        {
            "title": "Prote\u00e7\u00e3o intelectual de obras produzidas por sistemas baseados em intelig\u00eancia artificial: uma vis\u00e3o tecnicista sobre o tema",
            "gan": [
                "At this time, the music industry began to make decisions based\non data to strategize based on predictions of listeners\u2019 habits.",
                "O restante do manuscrito encontra-se\norganizado como segue.",
                "[Russell and Norvig, 2002]\nPr\u00eamio Turing (1975) e Pr\u00eamio Nobel (1978), Hebert Simon contribuiu notavelmente para\nteorias de decis\u00e3o em organiza\u00e7\u00f5es baseadas em satisfa\u00e7\u00e3o e com racionalidade limitada.",
                "Emprestoaquialgumasde\ufb01ni\u00e7\u00f5esorganizadas\npor",
                "[Poole,1993]\nOrganizei os conceitos em ordem cronol\u00f3gica para evidenciar o distanciamento do pensa-\nmento humano e intelig\u00eancia (de\ufb01ni\u00e7\u00f5es 1 e 2), para a execu\u00e7\u00e3o de atividades (de\ufb01ni\u00e7\u00e3o 3) e\nposteriormente para o projeto de agentes inteligentes (de\ufb01ni\u00e7\u00f5es 4 e 5).",
                "O teste avalia o comportamento da m\u00e1quina - especi\ufb01camente, sua\ncapacidade de enganar um interrogador humano e lev\u00e1-lo a acreditar que ela \u00e9\nhumana.\u201d",
                "A aprendizagem profunda ganhou notoriedade por justamente alimentar os algoritmos com\nosdadosbrutos,dispensandoouminimizandootempogastonaprepara\u00e7\u00e3odosdados,reduzindo\na interven\u00e7\u00e3o humana",
                "Dentretaism\u00e9todos,destacoas Generative\nAdversarial Networks (GANs)",
                "Addo,P.M.,Guegan,D.,andHassani,B.(2018)."
            ],
            "lstm": [
                "[Creswell et al., 2018] e a Long Short-Term Memory\n(LSTM)"
            ]
        },
        {
            "title": "An adaptive music generation architecture for games based on the deep learning Transformer model",
            "gan": [
                "They consider two primary techniques:\n\u2022adaptive music (also named interactive music ), where music is organized in order to be able to react to a\ngame's state [7].",
                "It is a graph of concept nodes, connected\nby weighted edges representing the strength of the association between the concepts (and is inspired from\na semantic content organisation in cognitive science [6])."
            ],
            "lstm": [
                "4.1 Design Principles\nAfter having at \frst experimented with a recurrent neural network architecture of type LSTM (part of Google's\nMagenta project library)",
                "It uses both convolutional and recurrent (LSTM) neural networks in order to: 1) predict pitch onset\nevents; and 2) use this knowledge to condition framewise pitch predictions."
            ]
        },
        {
            "title": "WHAT IS MISSING IN DEEP MUSIC GENERATION? A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC",
            "gan": [
                "Society for Music Information Retrieval\nConf., Bengaluru, India, 2022.to organizing principles in music, which are generally hi-\nerarchical and include sections, phrases and various kinds\nof patterns.",
                "In music generation, many researchers rely on deep\nlearning models to capture music structure and organiz-\ning principles implicitly from data.",
                "By characterizing structural information\nin music, we can discover new principles of music organi-\nzation and propose new challenges and evaluation strate-\ngies for music information retrieval and music generation.",
                "[20,24], Generative Ad-\nversarial Networks (GANs)",
                "We are interested in three main problems concerning rep-\netition and structure in music: (a) How are repetition and\nstructure organized hierarchically?",
                "Whether this organization\nmatters to listeners or simply re\ufb02ects composers\u2019 inten-\ntions requires further research.",
                "Unfortunately, most deep\nlearning models can generate only a limited length: V AEs\nand GANs usually have a \ufb01xed length from 2 to 8 bars,\nabout the length of just one phrase.",
                "Yu, \u201cSeqgan: Se-\nquence generative adversarial nets with policy gradi-\nent,\u201d in Thirty-First AAAI Conference on Arti\ufb01cial In-\ntelligence , 2017."
            ],
            "lstm": [
                "Another popular trend is to use sequential models\nsuch as LSTMs and Transformers [11, 22, 27] to generate\nlonger music sequences, but they still struggle to generate\nrepetition and coherent structure on long-term time scales.",
                "Sequential models like\nTransformer and LSTM can generate longer sequences,\nbut we cannot \ufb01nd any salient repetition comparable to\nrepeated phrases by listening or by automated analysis."
            ]
        },
        {
            "title": "VIS2MUS: EXPLORING MULTIMODAL REPRESENTATION MAPPING FOR CONTROLLABLE MUSIC GENERATION",
            "gan": [
                "[3] Zihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Jun-\njie Yan, Xiaogang Wang, and Jing Shao, \u201cCAMP:\nCross-modal adaptive message passing for text-image\nretrieval,\u201d in Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision , 2019, pp. 5764\u2013\n5773.",
                "[13] Elena Rivas Ruzafa, Pix2Pitch: generating music from\npaintings by using conditionals GANs , Ph.D. thesis,\nETSI Informatica, 2020."
            ],
            "lstm": []
        },
        {
            "title": "GENERATING MUSIC WITH SENTIMENT USING TRANSFORMER-GANS",
            "gan": [
                "GENERATING MUSIC WITH SENTIMENT USING\nTRANSFORMER-GANS\nPedro L. T. Neves\nState University of Campinas\np185770@dac.unicamp.brJose Fornari\nState University of Campinas\nfornari@unicamp.brJo\u00e3o B. Florindo\nState University of Campinas\nflorindo@unicamp.br\nABSTRACT\nThe \ufb01eld of Automatic Music Generation has seen signi\ufb01-\ncant progress thanks to the advent of Deep Learning.",
                "The model is a Transformer-GAN trained with labels that\ncorrespond to different con\ufb01gurations of the valence and\narousal dimensions that quantitatively represent human af-\nfective states.",
                "Attribution: Pedro L. T. Neves, Jose\nFornari, and Jo\u00e3o Batista Florindo, \u201cGenerating music with sentiment\nusing Transformer-GANs\u201d, in Proc. of the 23rd Int.",
                "This is the motivation behind the use\nof Generative Adversarial Networks (GANs)",
                "This addition has a positive effect on the generated\nsamples, and we demonstrate, through evaluations of our\nmodel both via automatic metrics and human feedback,\nthat the proposed Transformer GAN obtains a performance\nthat competes with a current state-of-the-art model, even\nwhile having a smaller set of parameters and using a sim-\npler representation of music.",
                "To summarise, our contribu-\ntions are as following:\n\u2022 We present a neural network that, up to our knowl-\nedge, is the \ufb01rst generative model based on GANs to\nproduce symbolic music conditioned by sentiment.arXiv:2212.11134v1",
                "2. RELATED WORKS\n2.1 Generative Adversarial Networks\nGenerative Adversarial Networks, or simply GANs, con-\nsist of a theoretical framework in which two Neural Net-\nworks, the Generator and the Discriminator, through com-\npetition, optimize a model that implicitly approximates a\ndata distribution by generating samples that try to mimic\nthe features that it observes on a given set of samples orig-\ninating from that distribution.",
                "Due to the inherent instability of the adversarial pro-\ncess, several works have focused on improving the conver-\ngence and the quality of the samples generated by GANs\nvia new objective functions, regularization and normaliza-\ntion techniques, and model architectures.",
                "Of particular in-\nterest to this work is RSGAN",
                "[8], which substitutes the\nstandard GAN loss for the non-saturating Relativistic Stan-\ndard Loss.",
                "LRSGAN;D =\n\u0000E(xr;xf)\u0018(P;Q)[log( sigmoid (D(xr)\u0000D(xf)))](2)LRSGAN;G =\n\u0000E(xr;xf)\u0018(P;Q)[log( sigmoid (D(xf)\u0000D(xr)))];(3)\nin which PandQare, in this order, the real and fake disri-\nbutions.",
                "In order to provide more stability to the training process,\nWGAN-GP",
                "Nevertheless, generating\ndiscrete sequences with GANs is notoriously hard.",
                "[25] and MuseGAN",
                "[26] are GAN-based gen-\nerative models of music that use Convolutional Neural\nNetworks (CNNs) as both the Generator and Discrimina-\ntor.",
                "This tech-\nnique, often used in image generating-GANs [42], ensures\nthat the model prioritizes local structure.",
                "The networks were trained via a combination of the\nteacher forcing objective plus the RSGAN objective",
                "The overall objective for the gener-\nator in this training stage is:\nLG=LMLE+\u000bLRSGAN G-global +\fLRSGAN G-local;(9)\nwhereLmle=\u0000Ex\u0018P[logG\u0012(x)]",
                "(10)\nwhere the factorsLRSGAN G-global andLRSGAN G-local are re-\nspectively the global and local GAN losses detailed above,\n\u000band\f, which we empirically chose to be equal to 1, are\nhyperparameters controlling the relative intensity of each\nloss factor, andLMLE is the Maximum Likelihood.",
                "The\nDiscriminator was simply trained with the local and global\nRSGAN objectives given previously in Equation 2 plus the\nglobal and local gradient penalties based on Equation 4\nand regulated by a hyperparameter \u0015(which as per [9], we\nchose to be 10) .",
                "LD=LRSGAN D-global +\fLRSGAN D-local+\n\u0015(LGP-global +LGP-local ):(11)4.",
                "To achieve this purpose, we used both the automatic evalu-\nation metrics proposed in [26,45] and a set of human eval-\nuation metrics to compare our GAN with the system that,\nas far as we know, corresponds to a state-of-the-art gen-\nerative model of symbolic music conditioned by sentiment\ncurrently available in the literature.",
                "[37] 49:76 8:52 4:36\nTransformer 48:79 8 :65 4 :37\nTransformer GAN 50:73 9:45 4:43\nTable 1 : Comparison between the samples generated by\nours and a state-of-art model.",
                "Since the focus\nof our work was to implement the GAN framework within\nthe context of symbolic music generation conditioned by\nsentiment, and given the complexity of this framework, es-\npecially when it is applied to the discrete domain, we chose\nto use a simpler representation in order to maintain the fo-\ncus of our work on the implementation of the GAN.",
                "These results suggest that both\nthe proposed Transformer, and the Transformer GAN, are\ncompetitive with a state-of-the-art model with respect to\nthe four qualitative metrics.",
                "The acronyms TG, T and CP corre-\nspond, respectively, to the Transformer GAN, Transformer\nand Compound-Word Transformer Baseline models.",
                "Between the\nthree, by observing the boxplots, it seems that while the\nTransformer and the Compound-Word baseline sometimes\nproduce samples that situate themselves more strongly to\nthe side to which they theoretically pertain (e.g., for the\nhigh valence and low arousal categories), the Transformer\nGAN surpasses the simple Transformer due to the fact that\nit never situates more than 50% of the excerpts on the in-\ncorrect side of the middle line, which in this case is rep-\nresented by the number 3.",
                "Overall, given the superior ratings of the Transformer\nGAN respective to the automatic metrics and its compet-\nitiveness with a state-of-the art model with respect to the\nhuman evaluations, and given the considerations above\nabout model size and representation, the Transformer GAN\nseems to be a promising model for music generation con-\nditioned by sentiment.",
                "[37] 3.32\u00061.29 2.93 \u00061.13 3.18 \u00061.30 3.49 \u00061.04\nTransformer 3.75\u00061.24 3.22 \u00061.19 3.76 \u00061.14 3.89 \u00061.14\nTransformer-GAN 3.56\u00061.34 3.06 \u00061.21 3.38 \u00061.09 3.44 \u00061.15\nTable 2 : Results of the Survey where participants were asked to rate the samples generated by several models.",
                "Yu, \u201cSeqgan: Se-\nquence generative adversarial nets with policy gradi-\nent,\u201d in Proceedings of the AAAI conference on arti\ufb01-\ncial intelligence , vol.",
                "[8] A. Jolicoeur-Martineau, \u201cThe relativistic discrim-\ninator: a key element missing from standard\nGAN,\u201d in International Conference on Learn-\ning Representations , 2019.",
                "Dumoulin,\nand A. C. Courville, \u201cImproved training of wasserstein\ngans,\u201d in Advances in Neural Information Processing\nSystems , I. Guyon, U. V .",
                "Avail-\nable: https://openreview.net/forum?id=rkE3y85ee[12] M. J. Kusner and J. M. Hern\u00e1ndez-Lobato, \u201cGans\nfor sequences of discrete elements with the gumbel-\nsoftmax distribution,\u201d 2016.",
                "[13] A. Muhamed, L. Li, X. Shi, S. Yaddanapudi, W. Chi,\nD. Jackson, R. Suresh, Z. C. Lipton, and A. J. Smola,\n\u201cSymbolic music generation with transformer-gans,\u201d\ninProceedings of the AAAI Conference on Arti\ufb01cial In-\ntelligence , vol. 35, 2021, pp.",
                "Likhosherstov, D. Dohan,\nX. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q.\nDavis, A. Mohiuddin, L. Kaiser, D. B. Belanger,\nL. J. Colwell, and A. Weller, \u201cRethinking attention\nwith performers,\u201d in International Conference on\nLearning Representations , 2021.",
                "[41] T. Miyato and M. Koyama, \u201ccGANs with projec-\ntion discriminator,\u201d in International Conference on\nLearning Representations , 2018.",
                "[44] W. Nie, N. Narodytska, and A. Patel, \u201cRelgan: Rela-\ntional generative adversarial networks for text gener-\nation,\u201d in International conference on learning repre-\nsentations , 2018.[45] L.-C. Yang and A. Lerch, \u201cOn the evaluation of gener-\native models in music,\u201d Neural Computing and Appli-\ncations , vol."
            ],
            "lstm": [
                "In [31], Biax-\nial LSTM networks are used to produce polyphonic music,and the generation can be conditioned by emotion via 4 pa-\nrameters originating from the valence and arousal dimen-\nsions.",
                "Furthermore, [35] uses a genetic al-\ngorithm to in\ufb02uence speci\ufb01c LSTM units that learn to en-\ncode sentiment in a pre-training language modeling stage,\nallowing it to steer the passages that it generates towards a\ndesired affective state.",
                "[31] K. Zhao, S. Li, J. Cai, H. Wang, and J. Wang, \u201cAn emo-\ntional symbolic music generation system based on lstm\nnetworks,\u201d in 2019 IEEE 3rd Information Technology,\nNetworking, Electronic and Automation Control Con-\nference (ITNEC) ."
            ]
        },
        {
            "title": "WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning",
            "gan": [
                "Although melodies\nappear to be a simple linear succession of notes unfolding over time, the organizational structure of\nthe melodic notes is hierarchical, like a tree resulting in intricate long-distance dependencies (9, 10).",
                "Conversely, little attention has been paid to the organizational logic of the deep structure beneath\nthe melodic surface, organized by different levels of structural importance among various musical\nevents (34\u201336) with the potential to enhance structured melody generation.",
                "WuYun follows the hierarchical organization principle of structure and prolongation (35, 37), thus\ndividing traditional single-stage end-to-end melody generation into two stages: melodic skeleton\nconstruction and melody inpainting (Fig. 1B).",
                "WuYun divides the melody generation process into melodic skeleton\nconstruction and melody inpainting stages following the hierarchical organization principle of structure and\nprolongation.",
                "32 Result\n2.1 Hierarchical organization principle of structure and prolongation\nMost AI artistic generative models differ signi\ufb01cantly from humans in their artistic creation process,\nespecially in music generation.",
                "It conforms\nto the brain\u2019s cognitive processing mechanism of structurally organizing sequential information\n(41\u201344), which makes the brain encode and process information more ef\ufb01ciently and improves\nmusical memories (45, 46).",
                "For example, musicians use this principle, consciously or unconsciously,\nto study, organize, and perform their musical works.",
                "The hierarchical structure is a key feature of the tonal musical syntax system, where musical elements\nare almost always hierarchically organized by strict rules at a fundamental level rather than unlimited\ncreative expression (36).",
                "Some researchers have investigated the patterns of structural organization\nand generalized them into music theories regarding the hierarchical structure in music from the\nperspective of the structure and prolongation principle.",
                "Schenker (47) was the \ufb01rst to introduce this\nprinciple to describe the musical structure in a hierarchically organized way.",
                "Therefore, Schenker proposed different\nlevels of structure hierarchy to organize tonal music and analyze its motion.",
                "In summary, under the surface of the music, musical\nevents are hierarchically organized based on the structural stability in rhythm and pitch dimensions\n(48, 49).",
                "5analyze, understand, and learn the logic of melodic hierarchy organization from surface to deep\nlayers.",
                "2A. Rhythm can be de\ufb01ned as the organization pattern of one downbeat with\none or more upbeat (54).",
                "Therefore, the meter provides the temporal framework for organizing music\nrhythm.",
                "Pitches are essentially organized into a distinct\nhierarchy scale based on tonal stability.",
                "In this work, we focus on designing a hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, following the hierarchical organization principle of\nstructure and prolongation.",
                "Therefore, the extraction of the hierarchical dependency structural relationship between\nmusical elements is affected by multiple musical dimensions; it will not be like a simple addition or\nsubtraction operation but a complex organic combination (63).",
                "Unlike the dominant end-to-end left-to-right note-by-note melody generation\nparadigm, we use the hierarchical organization principle of structure and prolongation to decompose\nthe melody generation process into melodic skeleton construction and melody inpainting stages.",
                "Thus, the proposed generation strategy based on the\n11hierarchical organization principle of structure and prolongation not only can maintain the long-range\ntonal coherence of generated melodies but also achieve control over the target of melodic motion by\nhuman users.",
                "Another issue is how to effectively extract an organic\nmelodic skeleton from hierarchical musical structures combining two or more musical dimensions\n(e.g., rhythm and pitch) to further improve the structure of generated melodies.",
                "[13] P. Schwaller, B. Hoover, J. L. Reymond, H. Strobelt, T. Laino, Extraction of organic chemistry grammar\nfrom unsupervised learning of chemical reactions."
            ],
            "lstm": []
        },
        {
            "title": "An investigation of the reconstruction capacity of stacked convolutional autoencoders for log-mel-spectrograms",
            "gan": [
                "[2] or Generative Adversarial Networks (GANs)",
                "The paper is organised as follows.",
                "The subsample is composed of 3750 samples\nfrom a variety of instruments: guitar, bass, brass, synth,\nkeyboard, \ufb02ute, organ, mallet, vocal, reed, and string for a\nsingle pitch."
            ],
            "lstm": [
                "The study demonstrated\nthat only deep autoencoders and LSTMs were able to ad-\nequately reconstruct the original samples.",
                "Moreover, incorporating temporal layers\nsuch as GRUs or LSTMs at the end of the encoder section\nis expected to in\ufb02uence the performance of the network."
            ]
        },
        {
            "title": "Byte Pair Encoding for Symbolic Music",
            "gan": [
                "The source code is provided for reproducibility: https:\n//github.com/Natooz/BPE-Symbolic-Music\nThe paper is organised as follows: Section 2 reviews the\nrelated work while Section 3 sheds light on the BPE tech-\nnique.",
                "Non-sequential models such as MuseGAN (Dong\net al., 2018) often represent music as pianoroll matrices.",
                "Musegan: Multi-track sequential genera-\ntive adversarial networks for symbolic music gen-\neration and accompaniment.",
                "Pair Encoding for Symbolic Music\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium."
            ],
            "lstm": [
                "Liang, F. T., Gotham, M., Johnson, M., and Shotton,\nJ. Automatic stylistic composition of bach chorales\nwith deep LSTM."
            ]
        },
        {
            "title": "A Symbolic-domain Music Generation Method Based on Leak-GAN",
            "gan": [
                "2021 3rd International Academic E xchange Conference on Science and Technology Innovation (IAECST) \n978-1-6654-0267-5/21/$31.00 \u00a92021 IEEE \n549 \n A Symbolic-domain Music Generation Method Based \non Leak-GAN \n \nZihan Li \nXihua Honor College,  \nXihua University,   \nChengdu, China,  \nkkli19@outlook.com \n \nYangcheng Liu \nSchool of Electrical and El ectronic Information, \nXihua University,  \nChengdu, China, \nluming9704@163.com Yi Guo * \nSchool of Electrical and El ectronic Information, \nXihua University,  \nChengdu, China, \nlpngy@vip.163.com \n \nQianxue Zhang \nSchool of Electrical and El ectronic Information, \nXihua University,  \nChengdu, China, \nlouxzqx@163.com",
                "In recent \nyears the deep neural network such as convolutional neural \nnetwork (CNN) and generative adversarial networks (GANs) \nhave been applied in this field [6,7,8,9,10] and have shown \ntremendous potential in their capability for content generation  \nand modification.",
                "[10] proposed three \ndifferent methods combined with GAN to handle the \ninteraction among tracks and generate polyphonic music with \nharmonic and rhythmic structure.",
                "However, to our knowledge the GAN approach is still \nlimited and suffers from exposure bias and mode collapse [12].",
                "In this paper we implement symbolic-domain music melody \ngeneration experiment on a model of LeakGAN",
                "MODEL STRUCTURE  \nWith transferring midi files to text files, the melody \ngeneration is taken as a text generation problem and then the \nLeak-GAN [14] generation model is used to generate text in \nthis work.",
                "B. LeakGAN in Music Generation \nInspired by the progressive GAN [16] approach, we choose \nLeak-GAN [14] model to generate our music.",
                "Composed of a \ndiscriminate net and a generate net, the Leak-GAN model \nallow the discriminator to leak its extracted features that is from higher level to the generator to help the guidance more distant, which effectively addresses the problem for long text-represented-music generation.",
                "Additionally, as LeakGAN model is skilled in long sequence generation, it produces promising results of adequate length as well as fine quality.",
                "Following we first introduce the general idea of generative \nadversarial network, then we take a closer look at the \nLeakGAN model. \n1) Generative Adversarial Networks \nGenerative adversarial network",
                "[1 log( ( (z))]\ndata zpx pG DVD G Dx D G\uf03d\uf045 \uf02b\uf045 \uf02d \uff0c ( 1 )  \nwheredatap is the real data\u2019s distribution andzp represent the \nprior distribution of z.   \n2) Leak-GAN On condition that we focus on symbolic-domain music \ngeneration, the sequence data of text-represented melody generation is considered as a sequential decision-making \nprocess [15].",
                "With the approval of leaking discriminator D\u2019s high level \nfeature representation to the generator who is not supposed to receive such information, the LeakGAN model addresses the problem that D\u2019s guiding signals are only available when the \nwhole melody sequence \nTs has been generated.",
                "To alleviate this situation, the LeakGAN model does not only provide generator with discriminator\u2019s information, it additionally applies a hierarchical reinforcement learning architecture to the generator in order to coordinate the leaked information with \ngeneration process of \nG\uf071.  \nWe introduce a MANAGER module and a WORKER \nmodule of the generator, which both start from an all-zero \nhidden state, denoted as 0Wh and 0Mh.",
                "FB \n2) GAN Setting \nWe choose LSTM to be the architecture of both \nMANAGER and WORKER in the generator.",
                "In this way, not only \nwe can reduce the problem of mode collapse, but also bring the GAN\u2019S solution closer to that of the supervised training.",
                "For the comparison experiment, LeakGAN model is mainly compared with LSTM.",
                "A few samples generated by LeakGAN are illustrated in Fig.",
                "2, which to a certain extent provide evidence for LeakGAN\u2019s ability of generating creative and harmonic music.",
                "The results are provided in Table 2, from which we see that the first four values for LeakGAN model is relatively high and the last is lower than that of LSTM.",
                "This indicates that the music generated by the LeakGAN tends to perform well both in statistics and imitating real music.",
                "e  \nmusic generated by LeakGAN is better.",
                "OBJECTIVE EVALUATION PERFORMANCE  \nMetrics LeanGAN LSTM Type \nWilcoxon \nTest  0.89105026  0.60805510 CB \nMWU Test 0.86661874  0.65296427 CB \nKWH)",
                "The comprehensive evaluation chart of LeakGAN and LST M. \nFig.4 (a) shows the high accuracy of our model even at the \nvery beginning, and Fig.4(b) is the training loss of D which reveals that there is a rapid decrease at the beginnings and th en \nit saturates.",
                "Musegan: Multi-track sequential generative adversarial networks  f o r  \nsymbolic music generation and ac companiment."
            ],
            "lstm": [
                "To prove good quality of the \ngenerated pieces, we conduct comparative test on LSTM model \nand evaluate the randomly-sampled music on five metrics in the \nlight of statistics and music theory.",
                "Attempts have also been made with recurrent neural networks \n[2,3] and long short-term memory (LSTM)",
                "m M\uf071\uf0d7  represents that the LSTM",
                "where W\nthis recurrent hidden vector of an LSTM and \n( ; )w W\uf071\uf0d7  represents the WORKER module.",
                "FB \n2) GAN Setting \nWe choose LSTM to be the architecture of both \nMANAGER and WORKER in the generator.",
                "For the comparison experiment, LeakGAN model is mainly compared with LSTM.",
                "We train the 8 layers LSTM model with the same dataset for 500 epochs.",
                "The results are provided in Table 2, from which we see that the first four values for LeakGAN model is relatively high and the last is lower than that of LSTM.",
                "OBJECTIVE EVALUATION PERFORMANCE  \nMetrics LeanGAN LSTM Type \nWilcoxon \nTest  0.89105026  0.60805510 CB \nMWU Test 0.86661874  0.65296427 CB \nKWH)",
                "A first look at music compos ition \nusing lstm recurrent neural networks.",
                "[5] Agrawal, P., Kaushik, S., Banga, S., Pathak, N., & Goel, S. Aut omated \nMusic Generation using LSTM."
            ]
        },
        {
            "title": "AI Music Therapist: A Study on Generating Specific Therapeutic Music based on Deep Generative Adversarial Network Approach",
            "gan": [
                "Adversarial Generative Network (GAN), \nLong-Short Time Memory (LSTM), and other neural networks \nlearn data from prepared MIDI (Musical Instrument Digital \nInterface) files with therapeutic effects.",
                "The experiments demonstrate that \nthe designed GAN neural network makes the music more \nnatural and smooth.",
                "Keywords\u2014Music therapy, Music analysis, LSTM, GAN, \nCNN \nI. INTRODUCTION \nA. Origin of Music Therapy",
                "Therefore, three dimensions need to be discussed in this \nstudy: whether the generated content is melody or \naccompaniment; whether single-track or multi-track music is \ngenerated; and whether the network architecture is CNN \n(convolutional neural network) or LSTM (recurrent neural \nnetwork), or GAN (generative adversarial neural network) \nwith CNN as the underlying architecture, which performs \nwell in various image video tasks.",
                "B. GAN \nTraining a, \nFake image Discriminator \n_",
                "GAN diagram \nAs shown in Figure 5, the simplest GAN (Adversarial \nGenerative Network) consists of two parts, G and D. The \ngenerator tries to generate data from some probability \ndistribution.",
                "6. CNN diagram \nCurrently, most of the underlying perceptrons of GANs \nuse the CNN _ framework, ie., convolutional shared \ncomputation method.",
                "The convolutional layer of the CNN is \nmainly convolutional operation, the input data is a matrix, \nand the features are extracted by multiple shared weight \nkernels/filter, and nonlinear steganography is performed.",
                "C. GAN Results and Discussions \nGAN itself is a game-like network.",
                "The GAN superparameters are as follows: the maximum \nnumber of iterations was set to 50,000, the batch size was set \nto 64, the initial learning rate was set to 0.001, and the Adam \noptimizer (betal=0.5, beta2=0.9) was chosen.",
                "Diagram of GAN music generator \nAs is shown in Fig.9.",
                "GAN multi-track generation results \nFigure 10(a) shows the output of a random sequence in \nthe dataset, with the horizontal and vertical coordinates \nindicating time and notes, respectively.",
                "The experimental platform was \nbuilt on two deep neural network models: LSTM and GAN.",
                "It paves the way for applying artificial \nintelligence music therapists to people's lives, professional \n1281 counseling organizations, and even hospitals."
            ],
            "lstm": [
                "Adversarial Generative Network (GAN), \nLong-Short Time Memory (LSTM), and other neural networks \nlearn data from prepared MIDI (Musical Instrument Digital \nInterface) files with therapeutic effects.",
                "Keywords\u2014Music therapy, Music analysis, LSTM, GAN, \nCNN \nI. INTRODUCTION \nA. Origin of Music Therapy",
                "Therefore, three dimensions need to be discussed in this \nstudy: whether the generated content is melody or \naccompaniment; whether single-track or multi-track music is \ngenerated; and whether the network architecture is CNN \n(convolutional neural network) or LSTM (recurrent neural \nnetwork), or GAN (generative adversarial neural network) \nwith CNN as the underlying architecture, which performs \nwell in various image video tasks.",
                "In 2016, Choik's team introduced a new approach to \nautomating composition with text-based Long-Short Time \nMemory (LSTM) networks.",
                "INTRODUCTION TO THE ALGORITHM \nA, LSTM \nRecurrent neural networks are specialized in processing \ntemporal information, no matter such information is textual \nor pure digital data.",
                "The three mainstays of the recurrent \nneural network family are: recurrent neural network (RNN), \nlong-short time memory model (LSTM), and gated recurrent \nunit (GRU).",
                "As is shown in Fig 1. \nRNN | LSTM     \n      \n  Fig.",
                "Schematic diagram of recurrent neural networks \nThe standard LSTM model is a special type of RNN, \nwith four special structures in each repetitive module, \ninteracting in a special way.",
                "LSTM is implemented in three gates: input gate, forget gate, \nand output gate.",
                "Diagram of LSTM forget gate \nThe forget gate is used to calculate how much of the \nmemory of the previous moment is preserved.",
                "Diagram of LSTM input gate \nThe input gate is used to calculate the memory of the \ncurrent state, which is calculated as follows: \nip =",
                "Diagram of LSTM output gate  Authorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "Ot \nhy (3) \nIt is worth noting that in LSTM, both sigmoid and tanh \nactivation functions are used.",
                "B. LSTM Results and Discussions",
                "The LSTM model was built with 4 layers and the number \nof neurons were 30, 30, 64 and 1.",
                "To enhance the \ngeneralization performance of the model, a dropout layer was \nset after each of the first three layers of the LSTM.",
                "It is evident that after 200 times of learning, \nthe LSTM model is better than 100 times of learning for the \nmusic pattern.",
                "With the above \n1280 LSTM results, we upgraded its implementation effects and \ntested whether multi-track music can be generated.",
                "The experimental platform was \nbuilt on two deep neural network models: LSTM and GAN."
            ]
        },
        {
            "title": "An intelligent music generation based on Variational Autoencoder",
            "gan": [
                "Abstract \u2014 In this paper, GAN and VAE are combined with \ndeep learning network to generate intelligent music based on \nmusic theory rules, and to explore intelligent music \ngeneration algorithm.",
                "Different from the traditional \nalgorithmic composition, it is not necessary to manually add \ncomplex rules, but trains the initial music set, evaluates and \nfilters the music collection, and ultimately generates music \nvia the RVAE-GAN neural network.",
                "On this \nbasis, the semi-supervised algorithm is used to form the chord \nstructure model, combined with the feature extraction of \nmusic, and the intelligent generation music based on GAN \nconfrontation generation network and VAE network \ncombined with music theory rules is proposed and proposed.",
                "Keywords-VAE; GAN; Music theory rules; Rhythm \nI.  INTRODUCTION  \nAlgorithmic composition, or automated composition, is \nan attempt to use a formal process to minimize the \ninvolvement of a person (or composer) in the use of \ncomputers for music creation",
                "From \neach parameter, select the possible values in turn and \norganize them into a sound column parameter value which \ncan be changed based on the current sound column or the \ninversion or inversion of the sound column.",
                "Waltzes in classical music, \nfor example, are characterized by its triple meter style, in \nwhich the dance steps rise and fall with the beat, giving one \na magnificent and elegant enjoyment.",
                "Currently, variable-point automatic \nencoders (V AEs) and generative confrontation networks \n(GANs) are widely used to generate complex structural \nmodels.",
                "Music is generated intelligently based on the V AE-\nGAN network in combination with the rules we have \nspecified.",
                "2. GAN network \nGenerative Adversarial Networks (GAN) is a deep \nlearning model and one of the most promising methods for \nunsupervised learning in complex distribution in recent \nyears.",
                "In the \ntraining process, GAN adopts a very direct alternating \noptimization method, which can be divided into two stages.",
                "MULTIMODAL NEURAL NETWORK - RVAE-GAN \n1.",
                "In view of  the above two, combined with the \nadvantages and disadvantages of each network, we propose \na new multi-modal neural network model: rule-based neural \nnetwork (RVAE-GAN).",
                "The VAE decoder and GAN generator are combined \ninto one by having them share parameters and training \ntogether.",
                "Experiment procedure \nThe VAE decoder and the GAN generator are merged \ninto one by sharing parameters and training together.",
                "This paper introduces an RVAE-GAN model for \ngenerating sequence data.",
                "Semi-Recurrent CNN-Based VAE-\nGAN for Sequential Data Generation."
            ],
            "lstm": [
                "Unsupervised learning networks are most suitable for \ngenerating data with time-series information, especially for \nthe recursive neural networks (RNN) and long-term and \nshort-term memory networks (LSTM), which perform quite \nwell in this field."
            ]
        },
        {
            "title": "APE-GAN: A Novel Active Learning Based Music Generation Model With Pre-Embedding",
            "gan": [
                "APE-GAN: A Novel Active Learning Based Music\nGeneration Model With Pre-Embedding\n1stWenyi Su\nMars Laboratory\nWhittle School\nShenzhen, China\nwsu111@whittleschool.org2ndYixuan Fang\nMars Laboratory\nWhittle School\nShenzhen, China\nyfang@whittleschool.org3rdZheng Li\u0003\nMars Laboratory\nWhittle School\nBeijing, China\nzli@whittleschool.org4thXin Steven\u0003\nEECS\nPeking University\nBeijing, China\nmengxinpku2018@gmail.org\nAbstract \u2014Being able to generate realistic musics is one of\nthe biggest challenges for Arti\ufb01cial Intelligence, and the current\nmodels do not have musical descriptive ability as humans have\nand the musics they produce are not highly realistic.",
                "This\npaper proposed an active learning based music generation model\nwith pre-embedding (APE-GAN) that can use textual inputs\nto generate musics, and have increased performance by active\nlearning and a checking mechanism with the discriminator\nin GAN model.",
                "Through experiments, this work shows that\nAPE-GAN only needs 5% to 10% humans labelled data to\nachieve relatively good music generation ability.",
                "After using Lakh\nPianoroll Dataset to train APE-GAN, the similar meaning textual\ninputs result in outputs with KL divergence approximate to 0,\nwhile different meaning textual inputs result in outputs with KL\ndivergence approximate to 1.\nKeywords \u2014Music Generation; Generative Adversarial Net-\nworks(GAN); BERT; Active Learning; KL Divergence\nI. I NTRODUCTION",
                "In the same year, the model C-RNN-GAN is proposed inwhich",
                "One of the\nclosest attempts to generate \u201creal\u201d musics was made in 2017\nby H.W.Dong [5] in the model named MuseGAN.",
                "We named our model A ctive learning based\nPre-Embeding - G enerative A dversarial N etwork (APE-GAN),\na model that can hopefully result in more realistic music\ngeneration.",
                "\u000fWe use active learning technology to ef\ufb01ciently train\nour APE-GAN to achieve high performance under the\n123\nICEICT 2021 \nIEEE 4th International Conference on Electronic Information and Communication Technology\n978-1-6654-3203-0/21/$31.00 \u00a92021 IEEE\nXi'an, China \u2022",
                "The rest of this paper is organized as follows: Section II\nintroduces the main components of our model.",
                "Section III ex-\npatiates the complete work\ufb02ow of APE-GAN.",
                "M ODEL COMPONENTS\nA. Music Generative Adversarial Network\nGenerative adversarial network, or GAN, is a kind of\ndeep neural network with two components, the generator ( G)\nand the discriminator ( D).",
                "By conducting this\ntwo player game, both GandDget better, such that eventually,\nGcan be trained to produce fairly realistic samples",
                "For generation of musics, because the music sequences for\neither real or generated musics are represented as \ufb01xed-size\nmatrix, we use CNNs for both GandD. The training process\nof GAN can be modeled by:\nmin\nGmax\nDEx\u0018pd[log(D(x))]",
                "In this work, we reference MuseGAN as our baseline\narchitecture.",
                "In MuseGAN, the generator is divided into two\nsub networks, the temporal structure generator Gtand music\nsequence generator Gm, in whichGtmaps the input vector\nzto",
                "Then, ~zis used byGmto generate melody sequences\nbar by bar:\nG(z) =fGm(~z)gT\nt=1 (3)\nB. Pre-Word-Embedding\nFor our model, instead of adding a random noise as the\ninput to let the model generate musics by itself, we decide\nto add a layer of word embedding (representation of text in\nthe form of vectors) that takes a piece of text as the input\nbefore the data enter the GAN model.",
                "The output vector z\nfrom the word embedding layer will be used as the input for\nthe GAN model.",
                "Due to the temporal correlation feature for\nmusics and the GAN model, generated musics that deviates\ntoo much from the input text will be discriminated as \u201cfake\u201d\nby the discriminator.",
                "By using BERT,\nAPE-GAN can create contextual word embedding vector zfor\nany sentences and use it as the input for the GAN model as\nreplace the random noise.",
                "As GAN model is learning in each\nround, the discriminator will label some data as \u201cuncertain\u201d\nif it \ufb01nds that it deviates only by a small amount from\nbeing either \u201creal\u201d or \u201cfake\u201d.",
                "Because of the selective nature of active\nlearning, it is rather cost ef\ufb01cient in increasing the performance\nof APE-GAN by getting a relatively small amount of labelled\ndata [18].",
                "Active Learning Process\nD. Discriminator Checking Mechanism\nFor many GAN models used in music generation, though\nboth the generator and discriminator are trained in the training\nprocess, only the generator is used to generate musics, while\nthe discriminator\u2019s function is yet to be exploited.",
                "APE-GAN I NFERENCE WORKFLOW\nA. Steps of Algorithm\nWith the knowledge in Section III, we can now introduce\nthe entire process of music generation inference of APE-GAN,\ndivided in the following steps, as illustrated in Figure 3 and\nFigure 4:\n1) Choose a piece of text that one wants to describe with\nAPE-GAN.\n2) Use word embedding with BERT as model input.",
                "3. APE-GAN Inference Work Flow\n\u000fIf the sequence is discriminated as \u201creal\u201d, then\noutput the sequence.",
                "4. APE-GAN Inference Work Flow\nIV.",
                "[21]\nC. Results\nFor our results, Figure 5 demonstrates the pianorolls of\n4 generated sequences of APE-GAN.",
                "In this paper, we proposed a model\nnamed \u201cAPE-GAN\u201d that can improve the performance of\nmusic generation models in three ways.",
                "To evaluate APE-GAN more objectively, we\nmeasure APE-GAN\u2019s descriptive ability by comparing the\nmusic sequences generated by different textual inputs with\nthe metric of KL divergence.",
                "Therefore, APE-GAN is able to learn semantic\nprior knowledge ef\ufb01ciently, which allows it to generate more\nrealistic and creative musics.",
                "C-RNN-GAN: Continuous recurrent neural networks\nwith adversarial training.",
                "Musegan: Multi-track sequential generative adversarial networks for\nsymbolic music generation and accompaniment.",
                "MIDI-Sandwich: Multi-model\nMulti-task Hierarchical Conditional V AE-GAN networks for Symbolic\nSingle-track Music Generation.",
                "[8] Malekzadeh, S., Samami, M., RezazadehAzar, S. and Rayegan, M.,\n2019.",
                "Classical Music Generation in Distinct Dastgahs with AlimNet\nACGAN. arXiv preprint arXiv:1901.04696 .",
                "Can GAN originate new electronic dance music\ngenres?\u2013Generating novel rhythm patterns using GAN with Genre\nAmbiguity Loss."
            ],
            "lstm": [
                "Before Transformers, Long-\nShort-Term-Memory (LSTM)-based models are very popular\nin constructing Seq2Seq models because the LSTM model can\nmap sequences of data to vectors while selectively remember-\ning the parts of the sequences it \ufb01nds important.",
                "Fundamentals of recurrent neural network (RNN)\nand long short-term memory (LSTM) network.",
                "Understanding LSTM\u2013a\ntutorial into Long Short-Term Memory Recurrent Neural Networks.\narXiv preprint arXiv:1909.09586 ."
            ]
        },
        {
            "title": "Automatic Music Generation System based on RNN Architecture",
            "gan": [
                "TABLE I. C OMPARATIVE ANALYSIS OF DIFFERENT METHODS \nS.NO Author Name& Year \nof Publication Methodology Database Remarks \n1 Chih-Fang Huang  et \nal., 2020 CVAE-GAN Model Self database Scoring Statistics Table for the Generated Music: \n                A        B     C      D \nAverage 2.52 1.52 2.27 4.10  \nSD1.24",
                "RVAE-GAN neural \nnetwork \n Self database Objective Evaluation: The average probability of a beat, abbreviated \nas APB, the standard deviation of the beat length, the standard \ndeviation of the bar length.",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n296 \n 9 Sarthak Agarwal et al., \n2018 PRECON-LSTM  Nottingham \nMus\nic \nDatabase  Sco\nreMeanMean Realness \nArtif3.120 2.747 \nGen \nHum3.613 3.516 \nComp  \n10 AdvaitMaduskar et al. \n2020 Autoregressive GAN \nmodel Bach\u2019s \nmusical \nsymphonies \ndataset Outlined a generation model for note sequence generation using the \nGAN framework.",
                "12 Hongyu Chen et al., \n2019 DCGAN model The Lakh \nMIDI",
                "Dataset A GAN-based model that was trained on a dataset of Bach's \norchestral symphonies produced the desired outcomes.",
                "14 Belinda M. Dungan et \nal. 2020 Generative model GAN-based \nMidinet PERFORMANCE OF THE MELODY GENERATION",
                "Fig. 2 depicts the proposed method's \norganizational structure.",
                "REFERENCES  \n \n[1] Huang, Chih-Fang, and Cheng-Yuan Huang, \u201cEmotion-based AI \nmusi\nc generation system with CVAE-GAN,\u201d In IEEE Eurasia \nConference on IoT, Communication and Engineering (ECICE), pp. \n220-222, 2020.",
                "[14] Dungan, Belinda M., and Proceso L. Fernandez, \u201cNext Bar Predictor: \nAn Architecture in Automated Music Generation,\u201d In IEEE \nInternational Conference on Communication and Signal Processing \n(ICCSP), pp. 109-113, 2020.",
                "[21]  Sandeep Kumar, Arpit Jain, Anand Prakash Shukla, Satyendra \nSingh, Rohit Raja, Shilpa Rani, G. Harshitha, Mohammed A. AlZain, \nMehediMasud, \"A Comparative Analysis of Machine Learning \nAlgorithms for Detection of Organic and Non-Organic Cotton \nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ],
            "lstm": [
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n296 \n 9 Sarthak Agarwal et al., \n2018 PRECON-LSTM  Nottingham \nMus\nic \nDatabase  Sco\nreMeanMean Realness \nArtif3.120 2.747 \nGen \nHum3.613 3.516 \nComp  \n10 AdvaitMaduskar et al. \n2020 Autoregressive GAN \nmodel Bach\u2019s \nmusical \nsymphonies \ndataset Outlined a generation model for note sequence generation using the \nGAN framework.",
                "11 Tianyu Jiang et al., \n2019 Bidirectional LSTM \nnetwork  Classical \nPiano \nDataset Bidirectional LSTM network was presented to produce harmonic \nmusic.",
                "13 Muhammad Nadeem et \nal., 2019 LSTM  Nottingham \nMusicDataba\nse The majority of participants enjoyed listening to the music created by \nthe proposed musical data structure and RNN architecture.",
                "MODELS: \nRCtimeavg \nDT        DT     0.0679     0.7436 \nKNN     KNN  0.0156    0.6524 \nMidinet         201.00    0.9682 \n15 Running Lang  et al. \n2020 \n SSCL model, which \nclusters music self-\nsimilarity matrix and \nthen LSTM network Self database Epoch-100",
                "[9] Agarwal, Sarthak, VaibhavSaxena, VaibhavSingal, and Swati \nAggarwal, \u201cLstm based music generation with dataset preprocessing \nand reconstruction techniques,\u201d In IEEE Symposium Series on \nComputational Intelligence (SSCI), pp. 455-462, 2018."
            ]
        },
        {
            "title": "Development of Application Software for Generating Music Composition Inspired by Nature Using Deep Learning",
            "gan": [
                "Deep learning, Machine learning, Computational intelligence, Mobile application \nsoftware, Artificial intelligence \nIntroduction: \nThe generative models in Deep learning like GAN (Generative Adversarial Network), Auto \nencoders are capable of generating the new data that have identical statistical characteristics of the \ntraining data.",
                "Figure 1 Illustration of (a) GAN (b) Variational En coder \n \nb) Variational Auto encoder: It is the simple feed forward network, in which the neurons are \nstacked in the layered architecture.",
                "[4] The results on \nusing Genetic algorithm for composing Music were re ported by Dragan MATIC (2010)",
                "Mobile application sofGenerative models (GAN and Auto-encoders) are used to g enerate   \nthe features that follows the statistical characteristics of the feat ures obtained from the \nnatural database.",
                "Dragan MATI\u0106 (2010), A Genetic algorithm for com posing MUSIC, Vol.20 (2010), 1, 157-177. \n6."
            ],
            "lstm": [
                "LSTM based Deep learn ing was used to classify the music composition \nbased on the composer index by V.Karthik, E.S.Gopi (2019)."
            ]
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "gan": [
                "[11] E. Parada-Cabaleiro, A. Baird, A. Batliner, N. Cummins, S. Hantke, and\nB. W. Schuller, \u201cThe perception of emotion in the singing voice: The\nunderstanding of music mood for music organisation,\u201d in Proceedings\nof the 4th International Workshop on Digital Libraries for Musicology ,\n2017, pp.",
                "[13] F. Bao, M. Neumann, and N. T. Vu, \u201cCyclegan-based emotion style\ntransfer as data augmentation for speech emotion recognition.\u201d",
                "[14] G. Rizos, A. Baird, M. Elliott, and B. Schuller, \u201cStargan for emotional\nspeech conversion: Validated by data augmentation of end-to-end emo-\ntion recognition,\u201d in ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) .",
                "[21] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra,\n\u201cThe mtg-jamendo dataset for automatic music tagging,\u201d in Machine\nLearning for Music Discovery Workshop, International Conference on\nMachine Learning (ICML 2019) , Long Beach, CA, United States,\n2019.",
                "Available: http://hdl.handle.net/10230/42015\n[22] D. Bogdanov, A. Porter, P. Tovstogan, and M. Won, \u201cMediaeval 2019:\nEmotion and theme recognition in music using jamendo,\u201d in MediaEval\n2019 Workshop , 2019.",
                "Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio-\nlette, M. Marchand, and V ."
            ],
            "lstm": []
        },
        {
            "title": "Generating Music Algorithm with Deep Convolutional Generative Adversarial Networks",
            "gan": [
                "This paper proposes an advanced arithmetic for generating music using Generative Adversarial Networks (GAN).",
                "The music is divided into tracks and the note segment of tracks is \nexpressed as a piano-roll, through trained a gan model which \ngenerator and discriminator continuous zero-sum game to generate a wonderful music integrallty.",
                "In most cases, Although GAN excel in image generation, the model adopts a \nfull-channel lateral deep convolutional network structure \naccording to the music data characteristics in this paper, generate music more in line with human hearing and aesthetics.",
                "; deep convolution GAN; full-\nchannel lateral; piano-roll; time sequence data structure algorithm optimization \nI.  INTRODUCTION  \nA.  Background  \nMusic is an art that reflects the emotions of human life.",
                "At present, the integration of AI and music has been \nstudied at home and abroad, mainly in deep learning models such as GAN\n[1], CNN[2] and LSTM[3].",
                "C. Introduction to GAN \nGAN (generative adversarial network) is an artificial \nintelligence algorithm for unsupervised learning.",
                "GAN is constituted \nby two networks, generation network G and discriminator \nnetwork D, G is responsible for generating target objects.",
                "The generator will generate objects that are very similar to the genuine objects, so that the \ndiscriminator network cannot distinguish between the \ngenerated objects and the real objects, thereby achieving GAN training.",
                "GAN Network architecture.",
                "GAN has been widely researched and apply.",
                "Here's a \nbrief list of GAN's advantage and disadvantages.",
                "GAN is a generative model that uses only \nbackpropagation compared to other generation \nmodels (Boltzmann machines\n[5] and GSNs[6]) \nwithout the need for complex Markov chains[7].",
                "GAN can produce a more clear, realistic sample \ncloser to the real object.",
                "GAN adopts an unsupervised learning style training, \nwhich can be widely used in unsupervised learning and semi-supervised learning.",
                "Compared to the variational autoencoders\n[8], GAN \ndoes not introduce any deterministic bias, and the variational method introduces deterministic bias because they optimize the lower bound of the log likelihood rather than the likelihood itself.",
                "Compared with VAE, GAN has no variation lower bound.",
                "In other words, GAN is progressive, but VAE is biased.",
                "Training GAN needs to reach Nash equilibrium\n [9], \nsometimes it can be done by gradient descent method or not, We have not found a good way to reach Nash equilibrium, so training GAN is unstable \ncompared to VAE or PixelCNN\n[10], but I think in \npractice it is still more stable than training the Boltzmann machine.",
                "GAN is not appropriate for processing discrete forms \nof data, such as text.",
                "GAN has problems of unstable training, gradient \ndisappearance, and mode collapse.",
                "DCGAN:",
                "[11], greatly improves the \nstability of GAN training and the quality of the resulting results.",
                "/g120 \n\u6fcb\u6fbb\u6fb5\u6fc2\u6fae\u6f94 Wasserstein GAN[12], solve problems such \nas mode collapse, improve learning stability, and \nprovide meaningful learning curves useful for debugging and hyperparametric searches.",
                "/g120 \n\u6fb6\u6fb9\u6fbb\u6fb5\u6fc2\u6fae  Boundary Equilibrium GAN[14], training \nauto-encoder based Generative Adversarial \nNetworks which Replace the similarity between the distribution by estimating the similarity between the distribution errors of the distribution to achieve fast \nand stable training. \nII.",
                "DCGAN generator is used for LSUN scene modeling.",
                "In addition, the discriminator of the GAN model has been \nspecially optimized and adjusted according to the nature of \nthe music data.",
                "DCGAN almost completely uses the convolutional layer \ninstead of the full-connection layer.",
                "C. GAN Model \n5HDO\n)DNH\n1RL]H5HDO\u0003GDWD\n*HQDHUDWRU\u0003\nGDWDGenerator ModelDiscriminator Model\nFigure 5.",
                "GAN train process.",
                "GAN train process follows as pseudo code: \n##start \ngenerator = Generator_build()discriminator = Discriminator_build ()\ncombiner = discriminator (generator(noise_z))\nfor epoch in range(epochs): \n# Train Discriminator # Generate a half batch of new music \ngen_musics=generator.predict(noise) \n# Train the discriminator \ndiscriminator.train(gen_musics)",
                "C. GAN Train",
                "After establishing the G-network and D-network models, \nwe started to train the network GAN and there is some tips. \n/g120When",
                "/g120Each side of the GAN can overpower the other.",
                "/g120GAN model training is a quite time-consuming \nprocess, so we can consider using parallel GPU for training under conditions, otherwise, we can only do other things while training.",
                "In this paper, we propose a generation model for \ngenerating note sequences under the GAN framework.",
                "\"C-RNN-GAN: Continuous recurrent neural networks \nwith adversarial training.\"",
                "\"Gans trained by a two time-scale update rule \nconverge to a nash equilibrium.\"",
                "\"Wasserstein \ngan.\" arXiv preprint arXiv:1701.07875 (2017).",
                "\"BEGAN: \nboundary equilibrium generative adversarial networks.\""
            ],
            "lstm": [
                "At present, the integration of AI and music has been \nstudied at home and abroad, mainly in deep learning models such as GAN\n[1], CNN[2] and LSTM[3].",
                "\"A first look at music \ncomposition using lstm recurrent neural networks.\""
            ]
        },
        {
            "title": "Generating Music with Emotions",
            "gan": [
                "Generative adversarial\nnetwork (GAN)",
                "MuseGAN",
                "[7] is a convolutional\nneural network (CNN) based GAN to compose polyphonic\nmusic with 5 sound-tracks.",
                "Similarly, recurrent neural network\n(RNN) based GAN is proposed in C-RNN-GAN [23], which\ncan generate polyphonic continuous music sequence.",
                "In [25], a model\ncalled CV AE-GAN was proposed for emotion-conditioned\nsymbolic music generation, which synthesized Conditional-\nV AE and Conditional-GAN [26].",
                "[28], [29] utilized conditional-GAN to generate\nmelody conditioned on the given lyric, in which the generator\nand discriminator were both LSTM networks with lyric as\ncondition.",
                "Yang, \u201cMusegan:\nMulti-track sequential generative adversarial networks for symbolic\nmusic generation and accompaniment,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence, vol.",
                "[23] O. Mogren, \u201cC-rnn-gan: Continuous recurrent neural networks with\nadversarial training,\u201d arXiv preprint arXiv:1611.09904, 2016.",
                "Huang, \u201cEmotion-based ai music generation\nsystem with cvae-gan,\u201d in 2020 IEEE Eurasia Conference on IOT,\nCommunication and Engineering (ECICE).",
                "Yu, A. Srivastava, and S. Canales, \u201cConditional lstm-gan for melody\ngeneration from lyrics,\u201d ACM Transactions on Multimedia Computing,\nCommunications, and Applications (TOMM), vol.",
                "Yu, A. Srivastava, and R. R. Shah, \u201cConditional hybrid gan for\nsequence generation,\u201d arXiv preprint arXiv:2009.08616, 2020.",
                "[38] C. L. Krumhansl and E. J. Kessler, \u201cTracing the dynamic changes in\nperceived tonal organization in a spatial representation of musical keys.\u201d"
            ],
            "lstm": [
                "[13] proposed a mLSTM\nbased deep generative network, which was the first work to\nexplore deep learning models for symbolic music generation\nconditioned on emotions.",
                "[28], [29] utilized conditional-GAN to generate\nmelody conditioned on the given lyric, in which the generator\nand discriminator were both LSTM networks with lyric as\ncondition.",
                "It was an encoder-decoder LSTM network where the encoder\nwas designed to generate lyric and three decoders are trained\nto generate pitch, duration and rest of melody respectively.",
                "We train a bidirectional LSTM with self-attention to classify\nthe music clips according to their valence, and achieves 83.3%\ntest accuracy on EMOPIA dataset.",
                "14, NO. 8, AUGUST 2021 5\nTABLE IV\nANNOTATION RESULTS OF THE BI-LSTM TRAINED ON\nEMOPIA FOR OUR DATASET ,ALL SEGMENTS ARE\nLABELLED TO HIGH-VALENCE OR LOW-VALENCE .",
                "Next step, bidirectional long short-\nterm memory (LSTM) network and multi-head self-attention\nTransformer",
                "TABLE VIII\nEMOTION CLASSIFICATION ACCURACY (%) OFLSTM AND\nTRANSFORMER ON DIFFERENT DATASETS WITH DIFFERENT\nLENGTH .",
                "datasetsLength\n20 50 100\nBidirectional LSTM 99.8 99.9 99.9\nSelf-attention Transformer 100.0 99.9 99.9\nA. Emotion Classifier",
                "As demonstrated in Section IV-B, both bidirectional LSTM\nand Transformer are trained on labelled data to classify the\nmusic emotion.",
                "For bidirectional LSTM, The number of layers is set to 6\nand the dimension of hidden state is set to 256.",
                "The learning rate and\nnumber of epochs are the same with bidirectional LSTM.",
                "Table VIII shows the emotion\nclassification accuracy of all datasets created in Section III-C,\nfrom it we can see that both the LSTM and Transformer based\nmodels can successfully classify the datasets.",
                "We generate\n180 segments by using the GRU based generator and LSTM\nbased classifier, in which 60 are positive, 60 are negative and\n60 are uncontrolled.",
                "We\nalso observe that EBS algorithm can applied to both traditional\nGRU or LSTM based model and Transformer based model.",
                "Yu, A. Srivastava, and S. Canales, \u201cConditional lstm-gan for melody\ngeneration from lyrics,\u201d ACM Transactions on Multimedia Computing,\nCommunications, and Applications (TOMM), vol."
            ]
        },
        {
            "title": "Generating Music with Generative Adversarial Networks and Long Short-Term Memory",
            "gan": [
                "In the paper, a deep learning m odel \ncomposed of Generative Adversarial Networks (GANs) and Long Short-Term Memory (LSTM) was introduced to implement the \ngeneration of music.",
                "The GANs are mainly composed of a \ngenerator and a discriminator, while LSTM serves as the basic \nunit of the generator and discriminator.",
                "Through analyzing the \nmusic file generated in different iteration times, the results \ndemonstrate that the pieces of music we got via the model made up \nof GAN and LSTM are in a similar pattern.",
                "In the early development of artificial intelligence technology, \nscholars began to study how to apply artificial intelligence technology to music generation.",
                "In recent years, Generative Adversarial Networks (GANs) have become one of the hottest research fields among scholars because of their outstanding performance in data generation.",
                "Thus, it is sensibl e \nto come up with the idea of how to use GAN for music creation, which is intriguing and deserves attention.",
                "The basic idea of GANs is derived from the zero-sum game, \nso generally GANs are composed of a generator and a discriminator.",
                "Inspired by the image generation [6], continuous Recurrent \nNeural Network with Generative Adversarial Network (C\u2043RNN\u2043GAN)",
                "C\u2043RNN\u2043GAN was trained by using the standard GAN loss function, and tone lengths, note frequencies, intensifies,  and \ntiming were constructed to characterize the music.",
                "The authors of GanSynth [9]  \nsuggested that, unlike image data, music is periodic.",
                "With the help of GAN, a normal person like you and me who don\u2019t have any previous experience in music is able to generate unique music through a simple click.",
                "Moreover, we would like to make some innovations by joining Long Short-term Memory (LSTM) into our GANs model.",
                "B. Generating Music based on GANs and LSTM",
                "The deep learning model used in this paper is mainly GAN, \nwhich is composed of an Encoder, a Generator and a Discriminator, as shown in Figure 1, and the Generator and \nDiscriminator are mainly composed of LSTM.",
                "The GAN structure used for music generation.",
                "The GAN mainly follows the game confrontation theory, and \nthe generator and the discriminator are balanced through a zero -\nsum game.",
                "In the GANs we designed, both the discriminator and \ngenerator are composed of two LSTM layers with one layer stacked on the top of another layer, which is suitable for retention of time-dependent patterns, especially when learning patterns cover a relatively long time.",
                "The reason why LSTMs were embedded in our GANs was \nthat the time dependent patterns could be kept through it, and the \nstructure of the LSTM can be summarized as in Figure 4 .",
                "B. Results \nThe configuration of GAN model is shown in Table 1  and \nthe structure of the signal is shown in Table 2 .",
                "TABLE 1 CONFIGURATION OF GAN  MODEL  \nGenerator \nNumber of \nEpoch Valid  \nLabel \nShape Fake  \nLabel \nShape True \nShape Fake \nShape \n5 (3, 1) (3, 1) (3, 6, 1) (3, 6, 1) \nTABLE 2 STRUCTURE OF SIGNAL  \nSample Rate Length of Signal Signal Data Type \n22050 44100",
                "Besides, the loss keeps decreasing after each epoch by definition and the validation loss is always smaller than the training loss, which means that there is some under-fitting of the model. \nTABLE 3  LOSS AND ACCURACY OF LSTM-GAN",
                "We improved the structure of the GAN model and fixed some mistakes that appeared in the \nfunction train GAN.",
                "273 \n of GAN training.",
                "From Table 5 , it could be easily noticed that \nafter upgrading to 100 music clips, both the accuracy and val_accuracy have increased gigantically, which means increasing the sample size enhances the overall accuracy.",
                "Since by lookin g \ncloser to the sequences generated by the GAN, the range and \nstyle of the sequences are very close to the sequences converte d \nfrom the sample data.",
                "This means the GAN is probably still functioning its job; however, the lack of training epochs makes  \nit hard to capture the features imbedded in the sample clips.",
                "D. Potential reasons and further improvements \nWith the increase of training times, the generated WAV files \nare broken, all of these clues point in one direction: there ma y \nbe some problems with the GAN model architecture.",
                "Future research may combine new ideas, such as Musical Instrument Digital Interface (MIDI), to train the new GAN model.",
                "In the paper, we tried to use GANs and LSTM to generate \nmusic.",
                "LSTM is the basic unit of the generator and discriminato r \nin GANs.",
                "[7] Mogren O. C-RNN-GAN: continuous recurrent neural networks with \nadversarial training [EB/OL].  (2016-11-29).",
                "[9] Engel J,Agrawal K K, Chen S,et al. GANSynth: adversarial neural  audio \nsynthesis  \n[10] https://en.wikipedia.org/wi ki/Long_short-term_memory"
            ],
            "lstm": [
                "In the paper, a deep learning m odel \ncomposed of Generative Adversarial Networks (GANs) and Long Short-Term Memory (LSTM) was introduced to implement the \ngeneration of music.",
                "The GANs are mainly composed of a \ngenerator and a discriminator, while LSTM serves as the basic \nunit of the generator and discriminator.",
                "Through analyzing the \nmusic file generated in different iteration times, the results \ndemonstrate that the pieces of music we got via the model made up \nof GAN and LSTM are in a similar pattern.",
                "Moreover, we would like to make some innovations by joining Long Short-term Memory (LSTM) into our GANs model.",
                "LSTM is an artificial cyclic neural network (RNN) structure for deep learning.",
                "Since LSTM has feedback connections, it is capable of processing individual data points , as \nwell as entire data sequences.",
                "Typically, an LSTM cell consists  \nof four elements, a cell, an input gate, an output gate, and a forgetting gate.",
                "B. Generating Music based on GANs and LSTM",
                "The deep learning model used in this paper is mainly GAN, \nwhich is composed of an Encoder, a Generator and a Discriminator, as shown in Figure 1, and the Generator and \nDiscriminator are mainly composed of LSTM.",
                "Though we had preprocessed the data, they could not fit in \nthe LSTMs, because the auto encoder received an input vector \nof constant size.",
                "In the GANs we designed, both the discriminator and \ngenerator are composed of two LSTM layers with one layer stacked on the top of another layer, which is suitable for retention of time-dependent patterns, especially when learning patterns cover a relatively long time.",
                "The generator has three layers in total, two LSTMs and one \ndense layer.",
                "Similarly, the discriminator has two LSTMs layers and one \ndense layer.",
                "The reason why LSTMs were embedded in our GANs was \nthat the time dependent patterns could be kept through it, and the \nstructure of the LSTM can be summarized as in Figure 4 .",
                "Figure 4 Structure of LSTM.",
                "Besides, the loss keeps decreasing after each epoch by definition and the validation loss is always smaller than the training loss, which means that there is some under-fitting of the model. \nTABLE 3  LOSS AND ACCURACY OF LSTM-GAN",
                "Then, we describable the results in Table 4, which shows the \ndata of the structure of LSTM model.",
                "The present study \nconfirms the method and the structure of the LSTM model that \nthere are 3 layers including 2 LSTM layer and 1 dense layer.",
                "TABLE 4 STRUCTURE OF LSTM  MODEL  \nLayer Type LSTM LSTM Dense \nOutput Shape (None, \n6, 128) (None, \n128) (None, 1) \nParametric Number 66560 131584 129 \nTotal Parametric \nNumber 198273 \nTrainable Parametric \nNumber 198273",
                "In the paper, we tried to use GANs and LSTM to generate \nmusic.",
                "LSTM is the basic unit of the generator and discriminato r \nin GANs."
            ]
        },
        {
            "title": "Generation of Music With Dynamics Using Deep Convolutional Generative Adversarial Network",
            "gan": [
                "With the piano-roll data \nrepresentation, Deep Convolutional Generative Adversarial \nNetwork (DCGAN) learned the data distribution from the \ngiven dataset and generated new data derived from the same \ndistribution.",
                "The \nevaluation results verified that DCGAN could generate \nexpressive music comprising of music dynamics and \nsyncopated rhythm.",
                "Keywords-music generation; DCGAN; dynamics; pianoroll \nI.  INTRODUCTION  \nMusic Generation is composing music with a computer \nthat uses machine learning methods.",
                "DCGAN was chosen to be the deep \nlearning architecture used in this paper.",
                "B. Generative Adverserial Network  \nGenerative Adversarial Network (GAN)",
                "MuseGAN",
                "PROPOSED METHOD  \nIn this paper, we designed and implemented a music \ngeneration system that can generate polyphonic music with \ndynamic using Deep Convolutional Generative Adversarial \nNetwork (DCGAN).",
                "With the image-like data representation \nand DCGAN ability to generate sample from the same data \ndistribution, we hypothesized that the DCGAN can generate \nmusic that incorporated with music dynamics.",
                "In this paper, a music generation system was \nimplemented under the framework of GANs, DCGAN.",
                "Future \nresearch using GAN in music generation could focus on \ngenerating arbitrary length of music or gather a larger data \nset for training.",
                "[6]  H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang, \n\"Musegan: Multi-track sequential generative adversarial \nnetworks for symbolic music generation and accompaniment,\" \nin Proceedings of the AAAI Conference on Artificial \nIntelligence , 2018, vol."
            ],
            "lstm": [
                "A variation of RNN, Long Short-Term Memory (LSTM)",
                "LSTM is used in an expressive music generation system \nPerformance RNN [4], developed by a google group \nMagenta."
            ]
        },
        {
            "title": "Monophonic Music Generation With a Given Emotion Using Conditional Variational Autoencoder",
            "gan": [
                "Music is an expression\nof human thoughts in the form of an organization of sounds in\ntime.",
                "The conducted experiments show that the model can organize\nthe latent space according to high-level genre information\nof the musical pieces, which allows you to modify the style\nof the input song.",
                "The rest of this paper is organized as follows.",
                "What affects the emotions of a music fragment is the musical\ncontent: sounds, their number, the pitch, rhythmic values,\norganization, minor/major scale [20], [42], [43].",
                "Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang, ``MuseGAN:\nMulti-track sequential generative adversarial networks for symbolic music\ngeneration and accompaniment,'' in Proc."
            ],
            "lstm": [
                "It developed the method used for gen-\nerating textual product reviews with a sentiment [33] by\nusing a single-layer multiplicative long short-term mem-\nory (mLSTM) network.",
                "A variant of this network, where\nlogistic regression uses the hidden states of the generative\nmLSTM to encode the labeled MIDI phrases, was used as\na classi\u001cer of sentiment.",
                "[9] K. Zhao, S. Li, J. Cai, H. Wang, and J. Wang, ``An emotional symbolic\nmusic generation system based on LSTM networks,'' in Proc."
            ]
        },
        {
            "title": "Music Deep Learning: A Survey on Deep Learning Methods for Music Processing",
            "gan": [
                "The most common approaches to MG are: i)\nRNNs - LSTMs , ii)Generative Adversarial Networks (GANs),\nand iii) Transformers .",
                "[36] - [42]\nGANs Symbolic music generation [44] - [51]\nTransformers Longer sequences generation",
                "B. GANs\nAnother popular approach in the field of MG is the use of\nGANs.",
                "GANs were first introduced in [43].",
                "The core idea\nbehind GANs is the existence of two antagonistic entities;\nthe generator and the discriminator.",
                "GANs have found great\nsuccess in the image generation task and since they were\nintroduced many researchers have trained GAN models for\nMG problems [44] - [51].",
                "Symbolic music is music stored in a notation-based format,\nwhich makes it easier for GANs to train on.",
                "Many different\nGANs have been applied on this task [45], [46], [48].",
                "Poly-\nphonic music generation is discussed in [47], while DRUM-\nGAN",
                "The authors of\n[49] demonstrated that GANs are able to generate high-fidelity\nand locally-coherent audio by modeling log magnitudes and\ninstantaneous frequencies with sufficient frequency resolution\nin the spectral domain.",
                "Self-attention mechanism is combined\nwith GANs in [51] in order to extract more temporal features\nto generate multi-instruments music.",
                "SeqGAN:\nsequence generative adversarial nets with policy gradient.",
                "Symbolic\nMusic Genre Transfer with CycleGAN.",
                "[48] Hao-Wen Dong,* Wen-Yi Hsiao,* Li-Chia Yang and Yi-Hsuan Yang,\n\u201dMuseGAN: Multi-track Sequential Generative Adversarial Networks\nfor Symbolic Music Generation and Accompaniment,\u201d Proceedings of\nthe 32nd AAAI Conference on Artificial Intelligence (AAAI), 2018.",
                "GANSynth:",
                "DrumGAN: Synthesis\nof drum sounds with timbral feature conditioning using Generative\nAdversarial Networks.",
                "\u2329hal-\n03233337 \u232a\n[51] F. Guan, C. Yu and S. Yang, \u201dA GAN Model With Self-attention\nMechanism To Generate Multi-instruments Symbolic Music,\u201d 2019\nInternational Joint Conference on Neural Networks (IJCNN), 2019, pp.\n1-6, doi: 10.1109/IJCNN.2019.8852291."
            ],
            "lstm": [
                "I\nDL METHODS FOR MIR\nDL Architectures Applications Research Paper\nRNNs Feature extraction [11] - [14]\nLSTMs Emotion prediction [10]\nCNNs Feature extraction [16] - [25], [27]\nUnsupervised Learning Sound representations",
                "A subset of RNNs which has been suc-\ncessfully applied in many different areas including MIR is the\nLong Short Term Memory networks (LSTM)",
                "In [10] the authors adopting the dimen-\nsional valence-arousal (V-A) emotion model to represent the\ndynamic emotion in music, managed to predict these values\nusing a Bidirectional Long Short-Term Memory (BLSTM)\nmodel.",
                "The most common approaches to MG are: i)\nRNNs - LSTMs , ii)Generative Adversarial Networks (GANs),\nand iii) Transformers .",
                "[30] - [35]\nLSTMs Style-specific music generation",
                "Instead of using simple RNNs one can test LSTM archi-\ntectures for MG tasks [36] - [42].",
                "In [36], [41] bi-directional LSTMs are used for\nthis problem, while in [42] CLSTMS, a combination of two\nLSTM models, is proposed.",
                "The authors in [37] used a\nvariation of Biaxial LSTM, designing the DeepJ model for\nstyle - specific MG.",
                "Chord Gener-\nation from Symbolic Melody Using BLSTM Networks.",
                "Polyphonic Music Mod-\nelling with LSTM-RTRBM.",
                "Singal and S. Aggarwal, \u201dLSTM based Music\nGeneration with Dataset Preprocessing and Reconstruction Techniques,\u201d\n2018 IEEE Symposium Series on Computational Intelligence (SSCI),\n2018, pp.",
                "Zhang, \u201dCLSTMS:",
                "A Combination of\nTwo LSTM Models to Generate Chords Accompaniment for Symbolic\nMelody,\u201d 2019 International Conference on High Performance Big\nData and Intelligent Systems (HPBD&IS), 2019, pp. 176-180, doi:\n10.1109/HPBDIS.2019.8735487."
            ]
        },
        {
            "title": "Music Generation using Deep Generative Modelling",
            "gan": [
                "In juxtaposition, Generative Adversarial \nNetworks  (GANs)  are effective  for modeling  globally  coherent \nsequence structures, but struggle to generate localized sequences.",
                "Through this project, we  aim to propose  a system that combines \nthe random subsampling  approach  of GANs  with  a recurrent \nautoregressive  model.",
                "The models we \nstudied were largely  based on deep generative modeling and \nconsisted of LSTM (Shin A., Crestel L.), Transformer (Cheng - \nZhi, et al.), AutoEncoders (Engel J., Resnick C., et al.) and \nGAN (Hao -Wen Dong, et al.)",
                "Based on our literature survey, we identified shortcoming s and \nopportunities in WaveNet and MuseGAN, the two models that \nproduced the most significant results.",
                "However, GANs fail when it comes to \ngenerating short term sequences due to overfitting on training \ndata and are extremely expensive from a computational \nperspective.",
                "This literature review has allowed us to propo se the \ndevelopment of a model that combines the lightweight \nautoregressive approach of WaveNet with the random \nsubsampling methodology of GANs to develop coherent \nmedium -to-long term musical subsequences without having to \nconform to computing or memory lim itations.",
                "We aim to conceptualize a \nmodel that amalgamates the properties of Auto Encoders and \nGANs in a recurrent feedback loop.",
                "The concept of GAN is fundamentally unambiguous and \nsimple.",
                "` The GAN based Autoregressive approach, which we have \nillustrated in Figure 1, can prove to be fruitful because GANs \nbeing very effective in modelling an underlying pattern will \nproduce quality results.",
                "The proposed system which combines a GAN based system with an \nautoregressive approach  \nVIII.",
                "The following results, as seen in Figures 2,3 and 4, were \nachieved when a GAN based model was trained on a dataset \nconsisting of Bach\u2019s musical symphonies.",
                "2. Representation of musical waveforms  \nHere, we have used random interpolation to generate music on \na scaled  GAN  architecture.",
                "Future scope includes development of an autoregressive GAN \nmodel  in a recurrent  feedback  loop that can be used not just for \nmusic generation, but also for detection of plagiarism in music \nby introducing original work as a sample data point and the \ndubious work as the fake samples in the discriminator function \nof the GAN.",
                "Combining Autoencoders with GANs  and allowing  the model to \nregress  on itself  can reduce  the amount  of data  required  while \nspeeding  up  computation.",
                "This  is possible due  to the \ndimensionality  reduction  being  done  by autoencoders  working \nwith the sampling  and  generative  abilities  of GANs.",
                "[2] H. W. Dong, W. Y. Hsiao, L. C. Yang and Y. H. Yang, \u201cMuseGAN\ndemonstration of a convolutional GAN based model for generating multi-track \npiano-rolls.,\u201d in International Society of Music Information Retrieva\nConference , 2017.",
                "[10] J. Engel, K. K. Agarwal, S. Chen, I. Gulrajani, C. Donahue and A.  Roberts, \n\u201cGansynth:     Adversarial     neural     audio     synthesis,\u201d     arXiv  preprint \narXiv:1902.08710, 2019."
            ],
            "lstm": [
                "The models we \nstudied were largely  based on deep generative modeling and \nconsisted of LSTM (Shin A., Crestel L.), Transformer (Cheng - \nZhi, et al.), AutoEncoders (Engel J., Resnick C., et al.) and \nGAN (Hao -Wen Dong, et al.)",
                "[4] \nEck et al use two different LSTM networks - one to learn \nchord structure and local note structure and one to learn \nlonger term dependencies in order to try to learn a tune \nand preserve it during the course of the piece."
            ]
        },
        {
            "title": "Music Generation using Deep Learning with Spectrogram Analysis",
            "gan": [
                "ZH\u0003 VWLOO\u0003 KDYH\u0003\nSURPLVLQJ\u0003UHVXOWV\u0003JHQHUDWHG\u0003IURP\u0003RXU\u0003PRGHOV\u0011\u0003)RU\u0003IXWXUH\u0003ZRUNV\u000f\u0003\nZH\u0003PLJKW\u0003EH\u0003DEOH\u0003WR\u0003JHQHUDWH\u0003XQGLVWLQJXLVKDEOH\u0003PXVLF\u0003ZLWK\u0003OLWWOH\u0003\nHIIRUW\u0011\u0003\nKeywords-music generation; LSTM; GAN; spectrogram; deep \nlearning; generative model; MIDI \n,\u0011\u0003\u0003,1752'8&7,21 \u0003\n)",
                "WR\u0003PDNLQJ\u0003SUHGLFWLRQV\u0003EDVHG\u0003RQ\u0003WLPH\u0003VHULHV\u0003GDWD\u0003>\u0016\u001b@\u0011\u0003\n2) GAN \n$\u0003 JHQHUDWLYH\u0003 DGYHUVDULDO\u0003 QHWZRUN\u0003 \u000b*$1\f\u0003 LV\u0003 D\u0003 FODVV\u0003 RI\u0003\nPDFKLQH\u0003OHDUQLQJ\u0003IUDPHZRUNV\u0003GHVLJQHG\u0003E\\\u0003,DQ\u0003*RRGIHOORZ\u0003DQG\u0003KLV\u0003FROOHDJXHV\u0003LQ\u0003\u0015\u0013\u0014\u0017\u0011\u0003,Q\u0003D\u0003JHQHUDWLYH\u0003DGYHUVDULDO\u0003QHWZRUN\u000f\u0003\nWZR\u0003QHXUDO\u0003QHWZRUNV\u0003FRQWHVW\u0003HDFK\u0003RWKHU\u0003LQ\u0003D\u0003]HUR\u0010VXP\u0003JDPH\u000f\u0003\nZKHUH\u0003RQH\nV\u0003JDLQ\u0003LV\u0003DQRWKHU\nV\u0003ORVV\u0011\u0003*$1\u0003OHDUQV\u0003WR\u0003JHQHUDWH\u0003GDWD"
            ],
            "lstm": [
                "ZH\u0003 VWLOO\u0003 KDYH\u0003\nSURPLVLQJ\u0003UHVXOWV\u0003JHQHUDWHG\u0003IURP\u0003RXU\u0003PRGHOV\u0011\u0003)RU\u0003IXWXUH\u0003ZRUNV\u000f\u0003\nZH\u0003PLJKW\u0003EH\u0003DEOH\u0003WR\u0003JHQHUDWH\u0003XQGLVWLQJXLVKDEOH\u0003PXVLF\u0003ZLWK\u0003OLWWOH\u0003\nHIIRUW\u0011\u0003\nKeywords-music generation; LSTM; GAN; spectrogram; deep \nlearning; generative model; MIDI \n,\u0011\u0003\u0003,1752'8&7,21 \u0003\n)",
                "\u0003\n)LJXUH\u0003\u0014\u0011\u0003/RVV\u0003FXUYH\u0003RI\u0003*$1\u0003PRGHO\u0003\n)LJXUH\u0003\u0015\u0011\u0003ORVV\u0003FXUYH\u0003RI\u0003/670\u0003PRGHO\u0003\n2) Data Processing \n$OO\u0003WKH\u0003PXVLF\u0003ILOHV\u0003LQ\u0003RXU\u0003UHVHDUFK\u0003DUH\u0003LQ\u0003WKH\u0003IRUPDW\u0003RI\u0003\n0,',\u0003\u000bPXVLFDO\u0003LQVWUXPHQW\u0003GLJLWDO\u0003LQWHUIDFH\f\u000f\u0003ZKLFK\u0003LV\u0003WKH\u0003PRVW\u0003\nH[WHQVLYH\u0003VWDQGDUG\u0003PXVLF\u0003IRUPDW\u0003LQ\u0003PXVLF\u0003DUUDQJHPHQW\u0011\u00030,',\u0003UHFRUGV\u0003PXVLF\u0003ZLWK\u0003GLJLWDO\u0003VLJQDOV\u0003RI\u0003QRWHV\u000f\u0003OLNH\u0003LQVWUXFWLRQV\u0003 IRU\u0003\nSLWFK\u0003DQG\u0003YROXPH\u0011\u0003+DYLQJ\u0003XSORDGHG\u0003PXVLF\u0003GDWD\u0003WR\u0003RXU\u0003PRGHO\u000f\u0003ZH\u0003WUDQVIRUP\u0003WKHP\u0003LQWR\u0003D\u0003VLQJOH\u0003DUUD\\\u0003RI\u0003QRWHV\u0003HQFRGHG\u0003DV\u0003D\u0003WXSOH\u0003RI\u0003SLWFK\u0003DQG\u0003FKRUG\u0011  \nB. Models \n1) LSTM \n/670\u0003"
            ]
        },
        {
            "title": "Music Generation with AI technology: Is It Possible?",
            "gan": [
                "These three models are Biaxial-LSTM, DeepJ and \nMuseGAN.",
                "Keywords \u2014Music Generation, LSTM, Biaxial LSTM, DeepJ, \nMuseGAN, Deep Learning, GAN \nI. INTRODUCTION  \nIn the era of big data, the demand for short videos and game \nsoundtracks has grown by leaps and bounds with the rapid \ndevelopment of streaming platforms.",
                "In this paper, we did a literature review focused on three \nmain deep learning models for music generation, Biaxial-LSTM, \nDeepJ, and MuseGAN.",
                "MuseGAN \nis an innovational approach based on Generative Adversarial \nNetwork that can be used to generate pop music with multiple \ntracks.",
                "Moreover, MuseGAN used many statistical indicators \nsuch as the ratio of empty bars and the number of used pitch \nclasses per bar to avoid the response errors from the Turing test.",
                "Therefore , we will then introduce two deep learning models, \nDeepJ and MuseGAN, with corresponding implementations in \nthe following sections.",
                "Both of these two models are related to \nmusic style, while the main difference is, the DeepJ model is \napplicable to all musi c styles, but the MuseGAN model is mostly \nused to generate pop music which has multiple instruments and tracks.",
                "Restrictions apply.  \nFig. 4. MuseGAN Framework  \nAnother improvement compared with previous Biaxial \nLSTM is that we add a 1\ud835\udc37 convolution layer with \ud835\udc58\ud835\udc52\ud835\udc5f\ud835\udc5b\ud835\udc52\ud835\udc59 =2. \nWith the rapid development of generative models in recent \nyears, generative adversarial neural networks have made very \nsignificant progress in data depth falsification.",
                "The mathematical theory that underpins GAN is extremely \ncomplex.",
                "Simply speaking, GAN is a class of machine learning \ntechniques consisting of two simultaneously trained models: a \ngenerator, which is trained to generate pseudo -data, and a \ndiscriminator, which is trained to identify pseudo -data from real \ndata.",
                "Key information for generators and discriminators  \nThe training process of GAN can be seen as a zero -sum game, \nand eventually the GAN model will reach a Nash equilibrium, \ni.e., the pseudo -samples generated by the generator are no \ndifferent from the real data in the training set.",
                "4) Apply GAN into pop music generation  \nPop music is usually composed of multiple \ninstruments/tracks.",
                "In the next experiments, the authors use three models, Jamming \nmodel, Composer model , Hybrid model, which have different \nimplementation mechanisms, but they are all based on \nGenerative Adversarial Neural Network (GAN).",
                "7. Flow chart of pre -processing  \n6) Architecture  \nGANs  implements adversarial learning mainly by \nconstructing two sub -network generators and discriminators.",
                "The \nauthors used three models based on GANs, which are Jamming \nmodel, Composer model and Hybrid model respectively.",
                "For the Jamming model, it is equivalent to each member of \nthe band having an independent random noise \ud835\udc4d. Each member \ngenerates music based on its own GAN model.",
                "Introduction of three GANs models  \nIII.",
                "COMPARISON  \nA. Between Biaxial LSTM, DeepJ and MuseGAN",
                "GAN  with temporal  struc ture \nTherefore, in the next section, we will focus on comparing \nthe DeepJ and MuseGAN models, as two models related to \nmusic styles, in order to do better processing for the music style.",
                "The MuseGAN model, on the other \nhand, is mainly targeted at modern pop music and usually \ncontains a variety of instruments such as drums, bass, guitar, \norchestra and piano.",
                "the research objectives, features in datasets and the \nevaluation methods taken by DeepJ model and the MuseGAN \nmodel.",
                "Certain \u201cmood\u201d can be generated also, to fit the \nstyle that the user wishes to compose.  \n2) MuseGAN model  \nSince the existing convolutional generative adversarial \nnetworks (GANs) models for generating music have some limits \nand cumbersome pre -processing such as, hard thresholding (HT) \nor Bernoulli sampling (BS), the research objective of MuseGAN \nis to improve a  convolutional GAN model that can use binary \nneurons to directly creates binary -valued piano -rolls.",
                "2) MuseGAN model  \nThe goal of Mu seGAN is to generate pop music of multiple \ntracks in piano -roll format.",
                "B. MuseGAN",
                "This also makes the evaluation of the model \nfor MuseGAN more reliable than the questionnaire.",
                "In MuseGAN model, the evaluation is mainly based on \nseveral indicators of the generated music, which show how \ncomplex a music piece is.",
                "This model \nhas the ability to generate polyphonic music, but since the \nalgorithm still does not have the ability to filter music styles in \norder to customize music generated, two cutting -edge researches \non AI music gen eration  were  further explored, which are the \nDeepJ and MuseGAN models.  \n \nFig. 12.",
                "The \nMuseGAN model is another style -related model that is used to \ngenerate music with multiple instrument tracks, normally used \nto generate modern pop music.",
                "Compared with DeepJ model, the MuseGAN model is \nmore suitable for generating pop music w ith multiple \ninstruments and tracks, and can be used in the future for game \nsoundtracks, etc.",
                "Compared to the \nMuseGAN model, the DeepJ model introduces human  \nevaluation and therefore has a deeper dimensionality.",
                "[4]  V. Bok, GANs in Action, 2017.",
                "[5]  H. W. Dong,  W. Y. Hsiao, L. C. Yang and Y. H. Yang  \"MuseGAN: Multi -\ntrack Sequential Generative Adversarial Networks for Symbolic Music \nGeneration and Accompaniment,\" 2019."
            ],
            "lstm": [
                "These three models are Biaxial-LSTM, DeepJ and \nMuseGAN.",
                "Keywords \u2014Music Generation, LSTM, Biaxial LSTM, DeepJ, \nMuseGAN, Deep Learning, GAN \nI. INTRODUCTION  \nIn the era of big data, the demand for short videos and game \nsoundtracks has grown by leaps and bounds with the rapid \ndevelopment of streaming platforms.",
                "In this paper, we did a literature review focused on three \nmain deep learning models for music generation, Biaxial-LSTM, \nDeepJ, and MuseGAN.",
                "The \nBiaxial-LSTM has the feature of transposition invariance, which \nmeans that the harmonies are more dependent on the relative \npitches rather than absolute pitches.",
                "The Biaxial-LSTM can \ngenerate polyphonic music, and the model evaluation mainly \nuses the Turing test.",
                "DeepJ can be seen as an improvement based \non Biaxial-LSTM, generating specific-style polyphonic music.",
                "METHODOLOGY  \nA. Biaxial LSTM \nBiaxial LSTM is a category of deep learning that can be \nconsidered as an extension to the LSTM model.",
                "While LSTM \nmodel can only be used to generate monophonic music, Biaxial \nLSTM can be used to generate polyphonic music which is a kind \nof music texture whe re multiple voices play at the same time.",
                "Compared with LSTM model, one major improvement of \nBiaxial LSTM is that it has the feature of transposition \ninvariance, which means that the harmonies are more depended \non the relative pitches rather than absolute pitches.",
                "Relative \npitches mean that when there is a key change for the same \nmelody, it should still be considered as the same music. \n1) Data representation \nIn Biaxial LSTM, a piano roll, that is a binary vector \nrepresenting the notes played at each time step, is used to \nrepresent notes.",
                "[0 0\n1 10 0\n0 0\n0\n10\n10\n00\n0]  \nAlso, while holding a note is not the same as replaying a note, \nwe need to distinguish these two events, so \ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66  is also \nneeded which looks similar to \ud835\udc61\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66. \n2) Architecture \nBiaxial LSTM generates polyphonic music by modeling \nevery note in every time step as a probability, using all previous \ntime steps and all notes already generated in the current time step \nas the condition.",
                "Each note is transformed to a tensor by itself and its \nsurrounding octave, and the feature of each note is then sent into \na LSTM unit, sharing weights on the time -axis between each \nnote.",
                "The time -axis is inspired by CNN model, and is constructed \nby two stacked layer of LSTM units.",
                "Each LSTM unit has \nweight shared between each note, so that it can force the time -\naxis to learn the transposition invariant feature.",
                "Contextua l input \nis also given for each LSTM unit on the time -axis, using the \nbinary format to find the position of time step related to a 4/4 \nmeter.",
                "The model takes the notes features generated from the time -\naxis as input, and the note -axis LSTM then scans from t he \nlowest note feature to the highest in order to predict each note \nconditioned on previous predicted notes.",
                "As shown in the Fig. 3, the \u2a01 symbol represents a \nconcatenation, \u00d72 means that the module is being stacked twice.  \n3) Limitations of biaxial LSTM",
                "LSTM model performs weak \nwhen generating polyphonic music.",
                "Biaxial LSTM Model",
                "It is developed based on \nBiaxial LSTM model.",
                "As this model is \ndeveloped based on Biaxial LSTM, it has retained the features \nof genera ting polyphonic music texture and transposition \ninvariance.",
                "The main difference of the architecture of DeepJ model \ncompared with Biaxial LSTM is that we use style conditioning  \nat each layer.",
                "For each LSTM level, we use a fully connected hidden \nlayer that is activated by tanh to connect the style.",
                "\ud835\udc89\ud835\udc8d\u2032=\ud835\udc95\ud835\udc82\ud835\udc8f\ud835\udc89 (\ud835\udc7e\ud835\udc8d\u2032\ud835\udc89) \nwhere l r epresents the LSTM layer.",
                "Restrictions apply.  \nFig. 4. MuseGAN Framework  \nAnother improvement compared with previous Biaxial \nLSTM is that we add a 1\ud835\udc37 convolution layer with \ud835\udc58\ud835\udc52\ud835\udc5f\ud835\udc5b\ud835\udc52\ud835\udc59 =2. \nWith the rapid development of generative models in recent \nyears, generative adversarial neural networks have made very \nsignificant progress in data depth falsification.",
                "COMPARISON  \nA. Between Biaxial LSTM, DeepJ and MuseGAN",
                "Biaxial LSTM is an upgraded model based on LSTM can \nbe applied to various music styles.",
                "Biaxial LSTM can \ndistinguish relative pitches and therefore can handle polyphonic \nmusic, which is the biggest improvement over the LSTM model.",
                "While for the simple LSTM  model, it can only handle \nmonophonic music.",
                "However, the Biaxial LSTM is not able to \nmake the style of the output music consistent.",
                "At the beginning of the project, Biaxial  LSTM model was \nexplored as  the initial music generation experiments.",
                "i s an optimization for the \nBiaxial LSTM model  which  is a deep learning model that can \ntheoretically be used  for various styles of music, as it \nincorporates considerations of musical style and dynamics.",
                "[2] S. Sk\u00fa li,  \u201cHow to Generate Music using a LSTM Neural Network in Keras \u201d \n2017.",
                "Available:  https://towardsdatascience.com/how -to-\ngenerate -music -using -a-lstm-neural -network -in-keras -68786834d4c5.",
                "[7]  Oinkina, \u201cUnderstanding LSTM Networks \u201d, 2015."
            ]
        },
        {
            "title": "Music Generation with Bi-Directional Long Short Term Memory Neural Networks",
            "gan": [
                "In this paper,  the training  is done  using  the MIDI  and \nAudio  Edited  for Synchronous  Tracks  and Organization \n(MAESTRO) dataset  [15].",
                "This paper is organized as follows: Section II describes, in \nbrief, the literature survey; Section III explains the network \narchitecture which includes the deep learning models \nimplemented; the methodology of implementation is \ndiscussed in section IV; Section V shows the results of the \nsurvey conducted to compare the computer -generated music \nwith human compositions; Section VI finally concludes the \npaper.",
                "Mogren et al. have generated polyphonous music using \nGenerative Adversarial Networks (GANs).",
                "GANs suffer from probl ems such as vanishing gradient, \nmode collapse and divergence mismatching",
                "This gave \nthem better results than traditional GANs [6].",
                "Further, the proposed \nmodel is able to overcome the drawbacks of the other \narchitectures such as GANs and Auto -Encoders by not being \nsusceptible to mode -collapse or generating overly repetitive \nmusic respectively.",
                "-GAN: Continuous recurrent neural networks \nwith adversarial t raining,\u201d arXiv [cs.AI], 2016.",
                "Wass erstein \nGAN. arXiv preprint arXiv:1701.07875, 2017."
            ],
            "lstm": [
                "The model is trained  on two Bidirectional  Long  Short-Term \nMemory (Bi-LSTM) layers with a self-attention layer between \nthem.",
                "LSTMs have been implemented by various researchers [2] \n[9] [11] s ince they incorporate dependencies naturally across \ntime.",
                "The LSTM model produced short melodies infinitely in \ncertain cases [2].",
                "Lackner et al. have composed a melody from \na given chord sequence by converting the musical data into \ninput and target matrices and applying an LSTM model to it.",
                "Their musical survey suggests that 40% of the LSTM \ncomposed music was misclassified as human melodies by the \nlisteners [9].",
                "Two separate LSTM models were developed in \n[11] for training dynamics and tempo separately over a dataset \nof Chopin\u2019s mazurkas.",
                "Mao et al. have bui lt a Biaxial LSTM model to generate \npolyphonic music based on a mixture of different composition \nstyles.",
                "Dawande et al. suggest the use of bi -axial LSTM to \nmaintain separate systems for tracking notes and \nprediction[23].",
                "Lastly, Azmi et al. have implemented a Bi -\ndirectional LSTM  model in which 3 hidden layers of 512 cells \neach are stacked on the input layer.",
                "They generated new \nmusic by inputting a single note and the results were \nsignificantly improved by the stacked nat ure of Bi -LSTM \n[13].",
                "This paper aims to overcome the limitations of the above \nworks by using a bi -directional LSTM model aided by a self -\nattention layer to perform training on the input music.",
                "The \nprocess of music generation involves five steps, conversion  of \nMIDI to piano roll representation, conversion of the piano roll \nto input to and target matrices, training of the Bi -LSTM \nmodel, conversion of the output matrix back to piano roll \nrepresentation and finally, piano roll to MIDI format . \n \nII.",
                "As mentioned in the methodology section, the \narchitecture of the suggested model is based on two \nbidirectional LSTM layers functioning as an encoder -decoder \nsystem with a self -attention layer between them to effectively \nutilize long sequences from the first b i-directional LSTM and \nformulate a weighted input for the second one.",
                "A. Bidirectional LSTM  \nRNNs maintain a memory based on historic sequential \ndata, where the output is determined by long -distance attributes of the incoming sequence.",
                "LSTM introduces \nsequence dependence by predicting the output on the basis of \ntwo parameters, the input at the current timestamp and the \nhidden state, which is determined by the state of an LSTM cell \nat the previous timestep.",
                "This approach to composition renders a simple LSTM \nbased architecture inadequate, and hence  our proposed system \nimplements bi -directional LSTM .",
                "Bi-directional RNNs are a method of stacking two LSTM \nnetworks into a single layer, where the first RNN processes \ndata in the forward direction while the second RNN processes \ndata in the backward direction, as shown  below  in figure 1.",
                "1.   Representation of Bi -Directional LSTM  \n \nBidirectional LSTM layers are stacked in the proposed \nsystem forming a model where the output of one layer serves \nas the input for another, which significantly increases the \nperformance of this architecture by providing a cascading \ncumulative effect while processing the data [14].",
                "In a basic LSTM network , the output is determined by the \ninput at the particular timestamp along with the hidden state \nof the LSTM cell.",
                "An attention layer is \nintroduced between two LSTM layers, where the weighted \nsum of the output of the first layer acts as the input for the \nsecond LSTM layer",
                "The training step consists of the architecture specified \nearlier in the form of a deep learning network using bi -lstm \nand self -attention layers.",
                "This work has analyzed  the musical structure of piano \ncompositions and argues for the effectiveness of bi -directional \nLSTM networks combined with the self -attention model for a \ngeneration of music.",
                "[4] Y. Huang, X. Huang, Q. Cai, \u201cMusic Generation Based on \nConvolution -LSTM\u201d, Computer and Information Science, vol.",
                "[9] K. Lackner, \u201cComposing a melody with long -short term memory ( \nLSTM )",
                "[11] M. K. J\u0119drzejewska, A. Zjawi\u0144ski and B. Stasiak, \"Generating Musical \nExpression of MIDI Music with LSTM Neural Network,\" 2018 11th \nInternational Conference on Human System Interaction (HSI),  2018, \npp.",
                "\u201cModelling \nhigh-dimensional sequences with LSTM -RTRBM: application to \npolyphonic music generation\u201d 24th International Conference on \nArtificial Intelligence (IJCAI'15).",
                "[15] F. Sh ah, T. Naik and N. Vyas, \"LSTM Based Music Generation,\" 2019 \nInternational Conference on Machine Learning and Data Engineering \n(iCMLDE), 2019, pp."
            ]
        },
        {
            "title": "RE-RLTuner: A topic-based music generation method",
            "gan": [
                "Generative AdversarialNetwork (GAN)[2], [3], [4] has also been applied to musicgeneration, where random noises serve as an input to thegenerator whose goal is to transform random noises intothe objective.",
                "GAN-based methods often achieve outstandingresults by carefully designing the generating strategy ofthe discriminator.",
                "C-RNN-GAN: Continuous recurrent neural networks with\nadversarial training[J]. 2016.",
                "[3] Dong H W , Hsiao W Y , Yang L C , et al. MuseGAN: Symbolic-\ndomain Music Generation and Accompaniment with Multi-track Se-\nquential Generative Adversarial Networks[J]. 2017.",
                "Conditional LSTM-GAN for Melody Generation\nfrom Lyrics[J]. 2019."
            ],
            "lstm": [
                "As Long Short-Term Memory (LSTM) networkarchitectures excel in modeling sequential information indata, LSTM based music generation method is proposed\n1CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems,\nShenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\nShenzhen 518055, China.",
                "Although subsequent work has optimized the LSTM[8], itsuffers both from the mismatch between training and testingconditions and the mismatch between training criterion andgeneration objectives in music generation[11].",
                "RL-Duet[11] was also put forward as a real-timeaccompaniment music generation system, which captured thecontext of music through a bidirectional LSTM on the rewardfunction.",
                "Our reward model consists of LSTM connected witha fully-connection layer.",
                "NoteRNN is trained as a single-layer LSTM network,\nconsisting of Q network, Target Q network and RewardRNN.Target Q network is the delayed copy of the Q network.",
                "Reward function: Due to the amount of parameter, we setthe LSTM hidden state to 15 and stitched a full connectionlayer to predict the notes.",
                "As for the weight of thetopics, we set 0.2,0.3, and 0.1, respectively.\nC. Generation\nConsidering the nature of the LSTM model, using maxi-\nmum likelihood estimation, repeat too many times can lead\nto the high probability of generated notes.",
                "E\nXPERIMENTS\nIn this section, we compared the performance of RLTuner,\nLSTM, and Re-RLTuner models.",
                "The LSTM we used isNoteRNN.",
                "A. Objective Evaluation\nTo verify the validity of the model, we adopted the\nfollowing indexes for quantitative analysis as in [5], [12].\nPC/bar PI IOI Auto-lag1 Auto-lag2 Auto-lag3\ndataset 1.51 25.7 1.49 0.88 0.77 0.67\nLSTM 2.73 20.0 13 -0.41 0.44 -0.37\nRLTuner 3.6 28.02 18 -0.03 -0.03 -0.03\nRE-RLTuner 2.9 17.5 11 \u22120 .10 0.003 \u22120",
                "We randomly chose four music\nsegments from each of the LSTM, RLTuner, Re-RLTuner,and a piece of original music from the dataset.",
                "RLTuner is close to the LSTM score, but RE-RLTUNER\noutperforms the baseline by subjective score.",
                "Because LSTM simply captures the context aspect of themusic, just a limited part of the learned music is repeatedin the generated music.",
                "However, due to the limitation of LSTM\u2019s own long timeseries, our future work can be carried out on reinforcementlearning agents.\nVI.",
                "Conditional LSTM-GAN for Melody Generation\nfrom Lyrics[J]. 2019.",
                "Finding temporal structure in music: Blues\nimprovisation with LSTM recurrent"
            ]
        },
        {
            "title": "Some Reflections on the Potential and Limitations of Deep Learning for Automated Music Generation",
            "gan": [
                "D. Generative Adversarial Networks\nIntroduced by Ian Goodfellow in [20] GANs are a\nnew paradigm that is based on an adversarial game\nbetween a Generator network and a Discriminator\nnetwork.",
                "GANs output is more sharp and de\ufb01ned that those\nof V AEs, but training is very unstable and compu-\ntationally intense.",
                "[21] and Musegan",
                "Both models are based on CNNs,\nbut while MidiNet uses a fairly classical CNN\narchitecture, MuseGan uses an unprecedented model\nwith a strong hierarchy that is based not on the\nsingle not but on the bar as a unit, featuring a bar\ngenerator controlled by a phrase generator.",
                "MuseGan is especially good for its ability to gener-\nate 5 instrument arrangements that sound rough, but\nshow a degree of coherence.",
                "It has to be noted that\nGANs are still affected by the issues of LSTM and\nCNN if those architecture are part of the generator\nand discriminator networks.",
                "Yang,\n\u201cMuseGAN: Symbolic-domain music generation and ac-\ncompaniment with multi-track sequential generative adver-\nsarial networks,\u201d arXiv preprint arXiv:1709.06298, 2017."
            ],
            "lstm": [
                "Long Short Time\nMemory(LSTM) cells solve the vanishing/exploding\ngradient problem of simple RNN introducing three\ngates (input, output and forget gates) to regulate the\n\ufb02ow in each cell [7].",
                "As anticipated, given the similarities between\nnatural languages and music, LSTMs have been\nused for music generation [12]",
                "One advantage of LSTM is that, since they pro-\ncess data sequentially, they don\u2019t need quantization\nand are therefore able to encode the subtle timing\nvariations of a human performance.",
                "The results are certainly better than\nLSTM, although long term cohesion is still dif\ufb01cult\nand for this reason the authors limited themselves to\na maximum of 16 bars.",
                "It has to be noted that\nGANs are still affected by the issues of LSTM and\nCNN if those architecture are part of the generator\nand discriminator networks.",
                "[12] D. Eck and J. Schmidhuber, \u201cFinding temporal structure\nin music: Blues improvisation with LSTM recurrent net-\nworks,\u201d in Neural Networks for Signal Processing, 2002."
            ]
        },
        {
            "title": "The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation",
            "gan": [],
            "lstm": [
                "In particular, we explore the effect of explicit\narchitectural encoding of musical structure via comparing two\nsequential generative models: LSTM (a type of RNN) and\nWaveNet (dilated temporal-CNN).",
                "We did a systematic\ncomparison between two main-stream approaches of handling\nmusic structure representation using two sequential representa-\ntion generative models: LSTM (a type of RNN) and WaveNet\n(dilated temporal-CNN).",
                "In terms of the dependencies\nbetween hidden variables, the relationship between LSTM\nand WaveNet is analogous to the one between a \ufb01rst-order\nautoregressive moving average (ARMA) model and a higher-\norder moving average (MA) model.",
                "From a signal processing\nperspective, the output signals of LSTM and ARMA models\ndepend on both history input and output signals, while the\noutput signals of WaveNet and MA models solely depend on\nhistory inputs.",
                "B. LSTM for Music Generation\nMany music generation works by deep neural networks start\nwith unconditional (monophonic) symbolic melody genera-\ntion.",
                "[27] and its ad-\nvanced versions (LSTM and GRU)",
                "They\ntested the Blues improvisation performance of LSTM by\ninputting note slices in real time.",
                "The unit selection method [32] took a series\nof measures in music as a unit and used a deep structured\nsemantic model (DSSM) with LSTM to predict future units,\ninstead of directly generate essential elements like notes.",
                "This work inspired us to use Bi-LSTM\nfor conditioned melody generation.\nIII.",
                "Data representation of LSTM models.",
                "B. Data Representation\nFor LSTM",
                "The bidirectional LSTM model.",
                "C. LSTM Architecture",
                "In our LSTM model, we start with a one-way model and\nfurther develop it into a bidirectional one.",
                "LSTM is consists of four gates: a cell gate c, an input gate\ni, an output gate oand a forget gate f:\nft=\u03c3g(Wfxt+Ufht\u22121+bf) (3)",
                "=\u03c3g(Woxt+Uoht\u22121+bo), (5)\nct=ft\u2299ct\u22121+it\u2299\u03c3c(Wcxt+Ucht\u22121+bc), (6)\nht=ot\u2299\u03c3h(ct) (7)\nThe detailed structure of our bidirectional LSTM model is\nillustrated in Fig.",
                "The left is the input layer followed by 7-\nlayer bidirectional LSTM layers and the fully connected layer.",
                "One thing must be noticed is that the difference between\nthe unidirectional and the bidirectional LSTM model lies in\nthe amount of chord information.",
                "In that, the conditional probability\nfor bidirectional LSTM model should be revised as:\np(MT\u2212t+1:",
                "B. Survey\nWe conducted a survey on audiences to compare the per-\nformance of the three proposed models (LSTM, Bi-LSTM,\nand WaveNet).",
                "WaveNet performs better than the unidirectional LSTM\nmodel.",
                "This result implies that the explicit dependency in\ndilated convolutions performs better than the implicit de-\npendency in LSTM.",
                "The bidirectional LSTM is even better\nthan WaveNet, and in our tests it is the best model.",
                "From the top graph to\nthe bottom one, each shows the original sample, bidirectional\nLSTM sample, unidirectional LSTM sample and the WaveNet\nsample.",
                "B. LSTM",
                "We found that music generated by LSTM model have\ngreat potential in repeating patterns.",
                "Fig. 10 shows that the\nunidirectional LSTM model appears a few repetitions (blue\nboxes), while the bidirectional LSTM model has more pattern\nrepetitions within the time frames (red boxes and yellow boxes\nindicate that).",
                "Bene\ufb01t from the short-term memory structure\nand the explicit input, it is natural for the LSTM model to\ncapture innate structures in the dataset.",
                "Moreover, we noticed that music generated by bidirectional\nLSTM is more stable and sensitive to chord changes compared\nto unidirectional LSTM.",
                "11 shows an example, where\nthe top system represents bidirectional LSTM model and the\nbottom system represents unidirectional LSTM model.",
                "The generated note\nsequence tends to be more unstable, smooth, and musical for\nbidirectional LSTM.",
                "The computational complexity of the LSTM model per layer\nisO(nd2), wherenis the music sequence length and dis\nthe dimension of the input.",
                "Comparison of results from one-way and bidirectional LSTM.",
                "our LSTM models is re\ufb02ected by the increment of dimension\nd.",
                "Similarly to the analysis of LSTM models, the increment\nof dimension dby the implementation of chord progression\ncondition will not affect the training ef\ufb01ciency.",
                "Then, we proposed bidirectional structures in the LSTM\nmodel and further improved the performance."
            ]
        },
        {
            "title": "PopMNet: Generating structured pop music melodies using neural networks",
            "gan": [
                "However,  generating  pop music melodies  with well organized  structures  remains  to be \nchallenging.",
                "First, structures  are generated  by the Structure  Generation  Net (SGN),  which  is a convolutional  generative  \nadversarial  network  (GAN)  (see Section 3for details).",
                "RNN-based  and CNN-based  GANs are also proposed  to learn the \ndistribution  of melody  clips",
                "To generate  melodies  with well-organized  structures,  StructureNet  imposed  structural  restrictions  on gener-\nated melodies [ 19].",
                "Similar  to GraphVAE,  we represent  melody  structures  \nas adjacency  matrices  and utilize  GAN to generate  adjacency  matrices.",
                "To generate  images  with high \ndiversity,  FaceFeat-GAN  generates  diverse  features  \ufb01rst and then renders  features  to images [ 38].",
                "As illustrated  in Fig.3, a convolutional  GAN is designed  to generate  the adjacency  matrix  of the melody  structure  graph.",
                "The generator  Gand critic Dare trained  jointly  under  the Wasserstein  GAN with gradient  penalty  framework [ 39].",
                "The choice  of GAN for generating  the adjacency  matrix  is somehow  arbitrary.",
                "Other  methods  may perform  as well as GAN, or even better,  but that \nis not the focus of this paper.",
                "Structure  generation  net\nThe generator  Gand critic Dwere trained  with the Adam  optimizer [ 42].",
                "\u2022MidiNet: a CNN-based  GAN",
                "[7]H. Dong, W. Hsiao, L. Yang, Y. Yang Musegan,  Multi-track  sequential  generative  adversarial  networks  for symbolic  music generation  and accompaniment,  \nin: Proceedings  of the 32nd AAAI Conference  on Arti\ufb01cial  Intelligence,  New Orleans,  Louisiana,  USA, 2018, pp.",
                "Arti\ufb01cial Intelligence 286 (2020) 103303 15\n[17]O. Mogren,  C-rnn-gan:  a continuous  recurrent  neural network  with adversarial  training,  in: International  Conference  on Neural Information  Processing  \nSystems,  Constructive  Machine  Learning  Workshop,  2016.",
                "[29]N. De Cao, T. Kipf, MolGAN:  an implicit  generative  model for small molecular  graphs,  in: International  Conference  on Machine  Learning,  Workshop  on \nTheoretical  Foundations  and Applications  of Deep Generative  Models,  2018.",
                "[38]Y. Shen, B. Zhou, P. Luo, X. Tang, Facefeat-gan:  a two-stage  approach  for identity-preserving  face synthesis,  arXiv preprint,  arXiv:1812  .01288,  2018.",
                "[39]I. Gulrajani,  F. Ahmed,  M. Arjovsky,  V. Dumoulin,  A.C. Courville,  Improved  training  of wasserstein  gans, in: Advances  in Neural Information  Processing  \nSystems,  vol."
            ],
            "lstm": [
                "a b s t r a c t\nArticle history:\nReceived  19 August 2019\nReceived  in revised  form 12 May 2020\nAccepted  15 May 2020\nAvailable  online 26 May 2020\nKeywords:\nMelody  generation\nMelody  structure\nArti\ufb01cial  neural network\nGenerative  adversarial  network\nLSTMRecently,  many deep learning  models  have been proposed  to generate  symbolic  melodies.",
                "Long Short Term \nMemory (LSTM)  is a widely  used model  in this \ufb01eld.",
                "Two more advanced  versions  were proposed  recently [ 4]\u2014 the Lookback  RNN, where  a lookback  feature  is employed  to \nmodel  the repetition  in melodies,  and the Attention  RNN, where  an attention  mechanism  is added  to LSTM.",
                "A task-aware  neural  language  \nmodel  termed  LM-LSTM-CRF  is proposed  to extract  character-level  embedding  under  a multi-task  framework [ 28].",
                "Li et al. propose  an \nLSTM model  that hierarchically  builds  embedding  of a paragraph  from the embedding  of sentences  and words  to reconstruct  \nthe original  paragraph [ 36].",
                "[14]D. Eck, J. Schmidhuber,  A \ufb01rst look at music composition  using LSTM recurrent  neural networks,  in: Istituto  Dalle Molle Di Studi Sull Intelligenza  \nArti\ufb01ciale,  vol."
            ]
        },
        {
            "title": "Singability-enhanced lyric generator with music style transfer",
            "gan": [
                "The remainder of the paper is organized as follows.",
                "Many proposed methods borrow concepts from generative adver-\nsarial network (GAN) frameworks for generative adversarial training\ndiscriminators.",
                "[14] H. Jhamtani, V. Gangal, E.H. Hovy, E. Nyberg, Shakespearizing modern language\nusing copy-enriched sequence-to-sequence models, in: Proceedings of the Confer-\nence on Empirical Methods in Natural Language Processing, EMNLP 2017, 2017,\nSeptember."
            ],
            "lstm": [
                "[15] proposed a simple AWD-\nLSTM neural language model architecture and pretrained weights for\nstyle-specific text generation.",
                "Several studies\nhave also shown that Transformer outperforms traditional models such\nas LSTM in feature extraction because GPT-2 uses a large amount of\nhigh quality pre-trained data, allowing the model to be applied to\ntext generation with good results.",
                "[2] A. Prakash, S.A. Hasan, K. Lee, V. Datla, A. Qadir, J. Liu, O. Farri, Neural\nparaphrase generation with stacked residual LSTM networks, in: Proceedings of\nCOLING 2016, the 26th International Conference on Computational Linguistics:\nTechnical Papers, 2016, pp."
            ]
        },
        {
            "title": "Human, I wrote a song for you: An experiment testing the influence of machines\u2019 attributes on the AI-composed music evaluation",
            "gan": [
                "Generative Adversarial Networks \n(GANs) have been a popular structure type for developing creative machines.",
                "In music generation, \nMidiNet and MuseGAN are both based on GANs (Dong et al., 2018 ; Yang \net al., 2017 ).",
                "While using GANs is a well-known method to develop \nmusic-composing machines, it is not the only one.",
                "Apart from GANs and Transformer, a hierarchical recurrent \nneural network (HRNN), which Deep Bach uses, is based on \npseudo-Gibbs sampling in order to produce notes in the style of Bach \nchorales, providing more techniques to create music (Hadjeres et al., \n2017 ; Wu et al., 2020 ).",
                "While \nrole theory is somewhat new to the HCI field, the theory is expected to be \nin more demand as machine agents permeate social groups, particularly \nwhere HCI and organizational communication intersect.",
                "Social role has \nbeen a key to manage social organizations and communities efficiently \n(Gleave et al., 2009 ; Takagi et al., 2013 ).",
                "This AI composed music appeared well-organized, not random.",
                "Organised Sound, 23(2), \n167\u2013180.",
                "Musegan: Multi-track \nsequential generative adversarial networks for symbolic music generation and \naccompaniment."
            ],
            "lstm": []
        },
        {
            "title": "Rethinking musicality in dementia as embodied and relational",
            "gan": [
                "Given that embodied selfhood and relationality are fundamental to\nthe human condition, it is essential that they be supported through\nsocio-political institutions and organizational practices at the local level\nof citizenship.",
                "This pre-re \ufb02ective notion of perception is particularly\npertinent to our analysis of musicality in dementia given Merleau-\nPonty's critique of the reduction of experience and behaviour of an\norganism to simple causal and physiological mechanisms, and also his\ncritique that the mind is what gives meaning to the world."
            ],
            "lstm": []
        },
        {
            "title": "deepsing: Generating sentiment-aware visual stories using cross-modal music translation",
            "gan": [
                "For example, Generative Adversarial Networks (GANs)\n(Brock, Donahue, & Simonyan , 2018 ; Goodfellow, Pouget-Abadie, Mir-\nza, Xu, Warde-Farley, Ozair, et al. , 2014 ; Karras, Aila, Laine, & Lehti-\nnen, 2017 ) are capable of synthesizing highly realistic visual content\nthat has not been encountered during the training process, neural style\ntransfer methods can re-paint images to match the style of reference\nimages ( Luan, Paris, Shechtman, & Bala , 2017 ), or even to follow\nthe style of well-known artists ( Gatys, Ecker, & Bethge , 2015 ; Wang,\nOxholm, Zhang, & Wang , 2017 ), while deep dreaming methods have\ndemonstrated that neural networks can exhibit a behavior known as\npareidolia in humans, i.e., recognize and synthesize patterns on seem-\ningly random data ( Mordvintsev, Olah, & Tyka , 2015 ).",
                "Finally, a generator model, e.g., a GAN, is employed to\ngenerate the final content.",
                "Our work go beyond\nexisting approaches by exploiting the power of Generative Adversarial\nNetworks (GANs) to generate unconstrained visualizations (Goodfellow\net al., 2014).",
                "Instead of directly training GANs for music visualization,\nwhich would be very challenging given the lack of appropriate datasets,\nwe propose employing an efficient cross-modal translation approach\nthat allows for translating the audio sentiment space into the visual\nsentiment space, through any pre-trained GAN model.",
                "To the best of\nour knowledge, this is the first work that proposed and evaluated an\nefficient cross-modal translation approach for sentiment-aware music\nvisualization using GANs.",
                "In this work, we employ a GAN for generating the\nfinal images from the intermediate generator vectors.",
                "(4)\nIt is worth noting that for the case of GANs, sampling the generator\nspace is easy, since GANs are typical trained to generate images from a\nGaussian distribution (Brock et al., 2018).",
                "However, in practice we observed that it was very difficult to fit\nsuch translation models, especially if GANs with very complex genera-\ntor spaces are used, e.g., GANs capable generating 1000 classes ( Brock\net al. , 2018 ).",
                "To understand why this happens, we have to consider\nthe mapping between the (sub)-classes produced through the GAN and\nthe attribute space.",
                "This behavior was also experimentally confirmed for the GAN\nused in the experiments conducted in this paper, as shown in Fig. 3(c).",
                "The plot shown in Fig. 3(c) was generated by clustering the sound\nattribute space and then measuring the number of GAN classes mapped\nin each cluster.",
                "[]\n5: foreach cluster do\n6: Sample one GAN category \ud835\udc50from each cluster according to\nthe probability of observing each class in the generator space\n7: Add every instance (\ud835\udc2d(\ud835\udc61)\n\ud835\udc56,\u0303\ud835\udc2d(\ud835\udc4e)",
                "In this work, we assume that a class-based\nGAN is employed.",
                "However, the proposed method can be also extended\nto handle any kind of GANs.",
                "For each cluster, we\nsample a GAN category with probability proportional to its cardinality\nin the cluster.",
                "This process allows to effectively\nkeep only one GAN category per cluster, leading to a smoother and\nmuch stabler matching, since every remaining attribute vector in each\ncluster is mapped to the same class.",
                "Note that, as demonstrated in\nFig. 3(c), for a GAN capable of generating images belonging to 1,000\ndifferent categories, each initial cluster could be mapped to more than\n500 different categories.",
                "Proposed method for fitting the translation model (a), an unstable mapping can occur between the attribute and generator spaces (b), and a toy example demonstrating\nthe unstable mapping for a GAN with 1000 classes (c).",
                "Hyper-stylization\nEven though GANs can offer a satisfactory degree of variation for\nthe generated content, they currently mostly fail to also simultaneously\nstylize the generated images according to the requirements of the\nusers.",
                "For developing the audio attribute estimator we extracted (a) 40 mel\nfrequency cepstral coefficients (MFCC) ( Logan et al. , 2000 ), (b) chroma\nenergy normalized (CENS) features ( M\u00fcller & Ewert , 2011 ), and (c)\ntempogram features ( Grosche, M\u00fcller, & Kurth , 2010 ).",
                "A pretrained BigGAN ( Brock et al. , 2018 ) model was used for\ngenerating images of 512 \u00d7512 pixels.",
                "The translation model consists\nof two hidden layers with 64 and 256 neurons respectively which\nbranches out into two streams: (a) a 1000 classification (softmax) layer\nused for predicting the category that will be used by the GAN, and\n(b) a 128 fully connected layer with no activation function used for\npredicting the latent vector to be fed to the GAN.",
                "First, note that even though the employed GAN wasExpert Systems With Applications 164 (2021) 114059\n7N. Passalis and S. Doropoulos\nFig.",
                "The proposed method works by\nfirst extracting the sentiment of a music track, which is then appro-\npriately translated into a space, from which a GAN can be employed\nfor generating the frames of the visual story.",
                "Large scale gan training for\nhigh fidelity natural image synthesis.",
                "Progressive growing\nof gans for improved quality, stability, and variation.",
                "Logan, Beth, et al."
            ],
            "lstm": []
        },
        {
            "title": "ComposeInStyle: Music composition with and without Style Transfer",
            "gan": [
                "Contents lists available at ScienceDirect\nExpert Systems With Applications\njournal homepage: www.elsevier.com/locate/eswa\nComposeInStyle: Music composition with and without Style Transfer\nSreetama Mukherjeea, Manjunath Mulimanib,\u2217\naMicrosoft R&D Pvt. Ltd., Hyderabad, 500 033, India\nbDepartment of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576 104, India\nA R T I C L E I N F O\nKeywords:\nMusic composer classification\nStyle transfer\nGenerative Adversarial Networks (GAN)\nHybrid model\nMusical Instrument Digital Interface (MIDI)",
                "The GAN architecture of the style transfer step\nis built out of the GAN architecture of the second step.",
                "The paper is organized in a very systematic manner adopting a\nstep by step approach towards accomplishing the desired research\nobjectives.",
                "Then, the compositions\nspecific to a particular composer of choice is generated using vanilla\nGenerative Adversarial Network (GAN) (Goodfellow, 2016).",
                "The rest of the paper is organized as follows, Section 2 reviewsthe existing works on music composition.",
                "Although various attempts have\nbeen recorded in the literature to compose music using vanilla GAN\nsuch as (Abdulatif, Armanious, Guirguis, Sajeev, & Yang, 2019; Dong,\nHsiao, & Yang, 2018; Dong & Yang, 2018; Kaneko, Takaki, Kameoka,\n& Yamagishi, 2017; Marafioti, Perraudin, Holighaus, & Majdak, 2019),\nthey are not specifically for composers.",
                "Among many attempts at generating music at par with paintings,\nresearchers have made several attempts starting from Variational Auto\nEncoder (VAE) (Brunner, Konrad, Wang, & Wattenhofer, 2018a; Luo,\nYang, Ji, & Li, 2020) to modern GANs (Abdulatif et al., 2019; Dong\net al., 2018; Dong & Yang, 2018; Kaneko et al., 2017; Marafioti et al.,\n2019).",
                "In this section, the most recent development in music compo-\nsition using GAN in the (2016\u20132020) timeframe has been discussed\nin detail.",
                "GAN was introduced by Goodfellow (2016).",
                "It describes the\nadvantages of using GAN over other generative networks.",
                "Another\npaper (Zhu, Park, Isola, & Efros, 2017) introduces the architecture\nfor cycle GAN which is also a significant advancement in the field\nof generative networks.",
                "Cycle GANs were made keeping in mind the\nuse case in which paired data is unavailable for training GANs.",
                "It\nintroduces a cycle consistency loss in addition to the adversarial loss\nin order to evaluate the GAN performance in training with unpaired\ndata.",
                "Now beginning with\nthe various applications of GAN in practical use cases.",
                "The next model GNACM uses\na GAN based framework with the output of NACM as input to the GAN\nin Zhang et al.",
                "Other various applications of GANs have been\nexplained in the survey paper by Yu (2020).",
                "Here, they give a detailed\ndescription of the pros and cons of using GANs over other generative\nnetworks.",
                "In the field of music, GAN is found to be used only in the very\nrecent works (Abdulatif et al., 2019; Dong et al., 2018; Dong & Yang,\n2018; Kaneko et al., 2017; Marafioti et al., 2019).",
                "Although (Hantrakul,\nEngel, Roberts, & Gu, 2019) applies WaveNet (WaveRNN) for mu-\nsical audio synthesis, (Engel, Agrawal, Chen, Gulrajani, Donahue, &\nRoberts, 2019) demonstrate the practical advantages of using GANs\nover WaveNets (Oord et al., 2016) by generating log magnitude spec-\ntrograms of musical data.",
                "They have used a non-autoregressive CNN based GAN for this purpose.",
                "(Marafioti et al., 2019) proposes GAN models to generate time\nfrequency based raw audio waveforms.",
                "The GAN model is trained on\ntime-frequency features which produce reliable and invertible Short\nTerm Fourier Transform (STFT) representations which can be converted\nto raw audio waveforms.",
                "(Abdulatif et al.,\n2019) have explored the domain of denoising audio tracks using GANs.",
                "(Dong & Yang, 2018) aims to solve this process of\nbinarizing generated piano rolls using GANs.",
                "In another work by Dong et al. (2018), 3 GAN models\n(jamming, composer and the hybrid model) are trained for music\ncomposition generation.",
                "They have proposed a GAN based\npost filter to reconstruct high fidelity STFT spectrograms.",
                "The GAN is\ntrained by divide and conquer by dividing the dataset with overlap and\nthen concatenating the generated output again with overlap in order\nto minimize information loss.",
                "In this, the authors attempt to solve\nthe problem using a sequential framework called SeqGAN.",
                "The corresponding intermediate state\u2013action steps get the rewards\nusing Monte-Carlo search after the GAN discriminator judges on a\ncomplete generated sequence.",
                "Another work by Yang, Chou, and Yang\n(2017) demonstrates the generation of symbolic music using GAN\neither from scratch, followed by a chord sequence or a priming melody.",
                "(Mogren,\n2016) proposes a Convolutional-RNN-GAN (C-RNN-GAN) to generate\nclassical music trained n sequential data.",
                "Pitch and rhythm organization is captured using\nunsupervised models.",
                "Yet another work on genre style transfer\nby Brunner et al. (2018a) is performed using cycle GAN.",
                "Unsupervised multi-\nmodal music style transfer for one-to-one generation is done using\nRelativistic average Generative Adversarial Networks (RaGAN).",
                "Moreover, in our approach we propose a cycle GAN way of training\nin presence of unpaired data.",
                "The\nentire procedure is performed in a step by step manner by training\nthe classifiers first, followed by the training of the GAN models for\ngenerating random compositions in the style of a composer A from\nnoise and then performing style transfer using the final style transfer\nmodel to generate composer A\u2019s compositions in the style of the target\ncomposer B.",
                "There are 2 GANs (each with 2\ngenerators) along with 2 common discriminators.",
                "The 2 GANs have\ndifferent functionalities.",
                "The vanilla GAN (in step 2) as shown in Fig.",
                "Given noise as input, the vanilla\nGAN can generate compositions in the style of the preferred composer\nof choice.",
                "The style transfer GAN architecture focuses on keeping the\nFig.",
                "Architecture of using Vanilla GAN for generating compositions from noise in\nStep 2.\ncontent of composition by composer A and outputs the melody in the\nstyle of the preferred composer B of choice.",
                "Architecture of the vanilla GAN\nThe audio melodies thus generated are in MIDI format which is then\nconverted into raw audio format (wav).",
                "Description of GAN architectures \ud835\udc3a\ud835\udc34,\ud835\udc3a\ud835\udc35,\ud835\udc37\ud835\udc34and\ud835\udc37\ud835\udc35\nIn step 2, paired vanilla GAN has been trained to generate multi-\ntrack polyphonic music.",
                "The overall architecture of a single GAN consists\nmainly of 2 parts: Generator and Discriminator 4.",
                "They are the 2 players\nin the minimax game of the GAN.",
                "Individual Architecture of a single GAN:\n1.Generator: The generator which is used here for step 2 com-\nprises of the following high-level components:",
                "The style transfer composite model consists of 4 generators and 2\ndiscriminators forming a cycleGAN.",
                "Step 2: Generate composer specific melody using a single composite\nGAN model.",
                "Steps of preprocessing the data\nBefore the data can be used to train the vanilla GAN and subse-\nquently the hybrid style transfer model, the data must be preprocessed\nin the appropriate format.",
                "These piano rolls are used in the next step for training the vanilla\nGAN and the hybrid style transfer GAN described in the next sections.",
                "The GAN generates piano rolls as the output.",
                "However, for training the\nhybrid style transferred GANs, the data type used is piano rolls.",
                "As a result, MIDI\nformat is most suitable for structured music composition generation\nby GANs as described in the later sections.",
                "Description of algorithm\nFor each epoch, real training audio data is taken for composer A\nand B. From noise, fake audio data in the style of composer A and B\nare then generated after training the GAN.",
                "In the comparison paper, the discriminators\nare 4 in number which are trained as a part of the cycle GANs.",
                "Metric Proposed Hybrid Style Transfer Model Comparison model\nGenerators Paired Vanilla GAN (2 GANs) and\nCycle GAN (2 GANs)Cycle GAN (only 2 GANs)\nDiscriminators Discriminator for composer A,\nDiscriminator for composer BDiscriminator for composer A,\nDiscriminator for composer B,\nDiscriminator for composer A\n(mixed) and\nDiscriminator for composer B\n(mixed)",
                "The accuracies for the respective bagging, boostingand stacking models are noted.",
                "Does the composition show coherent and organized musical\nideas?",
                "Aegan:\nTime-frequency speech denoising via generative adversarial networks.",
                "Symbolic music genre\ntransfer with cyclegan.",
                "Gansynth:",
                "Melgan: Generative adversarial networks for conditional waveform synthesis.",
                "C-RNN-GAN: Continuous recurrent neural networks with adversarial\ntraining.",
                "Seqgan: Sequence generative adversarial\nnets with policy gradient."
            ],
            "lstm": [
                "The\nfirst model uses Gated Recurrent Unit (GRU) and Long Short Term\nMemory (LSTM) to discover the mappings from the audio features and\nthe corresponding text features (captions).",
                "2 models namely, Tied Parallel - Long Short Term Memory\n- Neural Autoregressive Distribution Estimator (TP-LSTM- NADE) and\nBiaxial Long Short Term Memory (BALSTM) are used for this purpose.",
                "Colombo and Gerstner (2018)\nfocuses on capturing the note transition probabilities of songs using\na deep LSTM network.",
                "Mao, Shin, and Cottrell (2018) generates music\nby improved version of biaxial LSTM which presents an augmented\nrepresentation of music dynamics and beats."
            ]
        },
        {
            "title": "The algorithmic composition for music copyright protection under deep learning and blockchain",
            "gan": [
                "AppliedSoftComputing112(2021)107763\nContents lists available at ScienceDirect\nAppliedSoftComputing\njournal homepage: www.elsevier.com/locate/asoc\nThealgorithmiccompositionformusiccopyrightprotectionunder\ndeeplearningandblockchain\nNanaWanga,HuiXub,FengXuc,\u2217,LeiChengd\naDepartment of Physics, Harbin University, Harbin, 150080, China\nbCollege of Education, University of Perpetual Help System DALTA, Manila, Philippines\ncAcademy of Music, Chuzhou University, Chuzhou, 239000, China\ndConservatory of Music, Moscow State Normal University, Moscow 101-135, Russia\na r t",
                "The\ndevelopmentoflogisticsinformationsearchtechnologysuchas\nshipautomaticidentificationsystem,blockchain,andmodern\ninformationprocessingtechnologysuchasbigdataandcloud\ncomputinghasprovidedinformationtechnologysupportforthe\nsharingandallocationoflogisticsresourcesinportgroups.",
                "In\naddition,theupsizingofships,theintensificationandallianceofshippingenterprises,andothernewtransportationformsalso\ncontributetotherealizationofthesharingandoptimizationof\nport group logisticsresources.",
                "Intherelatedterminology,thedeeplearningandblockchainare\nproposed,andthereasonsforthemethodadoptedareexplained.",
                "I.Anewal-\ngorithmiccompositionnetworkisproposedfromtheperspective\nofmachinelearningandmanualcreation.",
                "Theoriginandessenceofblockchainand\nthe overall data structure of blockchain were introduced, the\nissuesinadoptionofblockchainindigitalcopyrightprotection\nwereintroduced,andabriefoverviewofadoptionexampleswas\ngiven[24].FengandWei(2017)pointedoutthatdigitalcopyright\nwasnowfacingnumerousdilemmas\u2014thepublic\u2019sawarenessof\ncopyrightprotectionwasweak,infringementproblemsoccurred\nrepeatedly,andthefoundationofbasicvaluewasalsoweak.",
                "Summary of relative works\nTosumup,deeplearningcanrealizethelearningandanalysis\nofrelevantdata,especiallyinmusiccreation,whichhasbeenre-\nportedbymanyscholars,buttheydidnothaveacompletesystem\nforcomposingmusic.",
                "Moreover,thelatestcompositionsystem\ncannotdisplaythelearningandcreationfunctionofmultipledata\nsets,whichgreatlylimitsthedevelopmentofdeeplearninginthe\nfieldofmusiccreation.",
                "More-\nover,theorganizationofrulesbetweendifferentelementscan\ncreatedifferentstylesofmusic,suchasclassical,rock,jazz,rap,\nandhip-hop.",
                "In view of the existing problems of the\ncurrentalgorithmiccompositionsystem,anewmusiccomposi-\ntionneuralnetworkstructureMCNNisproposedfromthetwo\nperspectivesofmachinelearningandmusiccreation.",
                "Fromforeigncompanymanage-\nmentmodels,manywaysandpathsof\u2018\u2018blockchain+copyright\u2019\u2019\narefoundintherightusingsystem,whichisadoptedinadvertis-\ningandmusicindustrychainIPincubation,aswellasothertypes\nofprojects,tofullyexpandthesinglerightapplicationsystem.",
                "Algorithm performance test\nToverifythesuperiorityoftheMCNNalgorithm,experiments\nareconductedbasedontheCPMGdataset,andtheresultsare\ncompared with three music generation algorithms: VAE\u2013GAN,\nSeqGAN,andRL-RNN.VAE\u2013GANaddstheencodinganddecoding\nprocesstotheGANnetwork.",
                "SeqGANintroducesareinforcement\nlearningalgorithmPolicyGradient.",
                "Thegreaterthenumberofiterations,\nthegreaterthenumberofweightparameterlearningandad-\njustments,whichcanimprovetheaccuracyofthemodeltoa\ncertainextent.",
                "Figs.20and21showthatthesamplecoincidencerateofthe\nGANalgorithmisonly0.62,andthesamplecoincidencerateof\nFig.",
                "theVAE\u2013GANalgorithmreaches0.81,whichishigherthanthat\noftheGANalgorithm.",
                "ThesamplecoincidencerateoftheRL-\nRNNalgorithmreaches0.946,whilethesamplecoincidencerate\noftheMCNNalgorithmreaches0.95,whichisthehighest,far\nexceedingGANandVAE\u2013GAN,andslightlybetterthanRL-RNN.",
                "IntheGAN\nnetwork,therelationshipbetweenthegeneratorandthediscrim-\ninatormakesthegeneratedmusicsequenceverysimilartothe\ntrainingset,andthediversityislacking.\nFig. 21.Qualifiedrateoffourkindsofalgorithminmusicgeneration.\nFig.",
                "Comparison results of performance of different algorithms\nInFig.26,theresultsofthisalgorithmarecomparedwith\nVAE\u2013GAN,GAN,andRL-RNN.Fromtheexperimentalresults,the\nmusic samples generated by MCNN network have the highestTable 3\nMusiccopyrightsecurityanalysisresults.",
                "Technology Safety Protectionrate\nDeeplearning 65.36% 65.23%\nBigdata 66.14% 69.43%\nBlockchain 65.12% 70.05%\nDeeplearning+blockchain 72.14% 82.33%\ncompliance rate, which is far better than GAN and VAE\u2013GAN,\nandslightlybetterthanRL-RNN.Themusicgenerationalgorithm\nbasedonMCNNnetworkcanbasicallyguaranteetheintegrityof\nthegeneratedmusic.",
                "Fromtheprincipleofthealgorithm,\nit is not difficult to find that the game relationship between\ngeneratoranddiscriminatorintheGANnetworkmakesthemusic\nsequencegeneratedandthetrainingsetextremelysimilar,and\nthediversityissomewhatmissing.",
                "ThesupportrateoftheGANs1\nmodelproposedbyYe(2019)wasabout33.3%,thesupportrate\nofDeepBachwasabout38.3%,thesupportrateofrealsongswas\nabout38.3%,andthesupportrateofreconfigurationharmony\nwasabout33.3%.ThesupportrateoftheGANs2modelproposed\nisabout46.7%,whichiscloseto50%andfarmorethanthatof\nYe\u2019s[35].ThemusicgeneratedbytheZhang(2020)modelisless\nthan50%ofthesatisfactionrateofthemusicgeneratedbythe\nBiLSTM-GANsunderthecategoryofpianomusicians.",
                "[35] W.H.Ye,AutomaticcompositionbasedonBiLSTMandGANsalgorithm,J.\nJilinUniv.21(2)(2019)193\u2013194."
            ],
            "lstm": [
                "Theprobability distributionofLSTMgeneration\nisadjustedbyconstructingareasonablerewardfunction.",
                "In this work, the music audio\nisdeemedastheresearchobject,andanewautomaticmusic\nsynthesisalgorithmisproposedbasedonLSTMRNN.Theriseof\ndigitalcontentoperationshasdemonstratedstrongdevelopment\npotentialandacceleratedthediversifieddevelopmentofthecom-\nmerciallayoutofthecontentindustry.",
                "Moreover,theRewardfunctionvalueisfedback\ntotheLSTMgenerationnetwork,andthenetworkparameters\nareadjustedinrealtimetorealizetheintelligentgenerationof\nspecificstylemusic.",
                "Thevalueofthe\nReward function isfed back to the LSTMgeneration network,\nand the network parameters are adjusted in real time to re-\nalizetheintelligentgenerationofspecificstylemusic.",
                "Kim et al. (2019) built an\nLSTMspeechrecognitionmodelandobtainedfavorableresults\nthrough large vocabulary speech recognition tasks [18].",
                "Tosolvethis\nproblem,Ooreetal.(2020)usedtwoLSTMmodelstotrainand\ncreatebluesmusic,whichwereusedtolearnchordsandmelody,\nrespectively.",
                "LSTM\nLSTMisatypeofRNNnetwork,whichisoftenusedtoprocess\nandpredictimportanteventswithverylongintervalsanddelays\nintimeseries.",
                "ALSTMunitcontainsinputgates,outputgates,\nand forgetting gates.",
                "The input gate controls the input of the\nmodel,theoutputgatecontrolstheoutputofthemodel,andthe\nforgettinggatecalculatesthedegreeofforgettingofthememory\nmoduleatthepreviousmoment[30].ThestructureoftheLSTM\nmodelisshowninFig.2,andthespecificcalculationisasfollows.",
                "2.LSTMneuralnetworkstructurediagram.",
                "ThisworkintroducesLSTMnetwork,which\ncaneffectivelysolvetheproblemoflong-termdependenceon\ndata,therebyeffectivelyprocessingscoredata.",
                "ThecoreofLSTM\nliesinthestateofthecelllayer.",
                "Thespecific\nstructureisshowninFig.8.TheLSTMgenerationnetworkispre-\ntrainedbyinputtingtheclassicalmusicdatabaseinMIDIformat,\nandtheinitialnotesaresettorandomlygeneratemusic.",
                "Algorithmic composition\nFig.9showsthemusicgenerationmodel.LSTMnetworkis\nadoptedtogeneratemusicsequencesbysettingtheinitialnotes,\nandtheRewardfunctionisformedthroughweightedcalculation\nFig. 9.Generationmodel.\nFig.",
                "Moreover,theLSTMnetworkparameters\nareupdatedandadjustedthroughtheRewardfunctioncalcula-\ntion.",
                "Algorithm flow\nIntheMCNNnetwork,theupdateprocessoftheLSTMnet-\nworkandtheRewardnetworkispresentedinFig.10.Thestrategy\n\u03c0/S\u25a1Aisthemappingfromstatespacetoactionspace.",
                "Inaddition,theLSTMnetworkparametersareupdated\nandthesequencegenerationstrategy \u03c0isadjustedtoobtaina\nlargerewardvalue.",
                "Pre-train LSTM network weights is \u03b8, Pre-train CNN to\ndiscriminatenetworkweightsis \u03d5.\n2.ExampleInitializetheRewardnetworkweightis \u03b8v=\u03b8.\n3.Initializesarandomaction a0.",
                "16 shows the influence of different LSTM cells on the\nexperimentalresults.",
                "ThemoreLSTMcellsinthehiddenlayer,\nthestrongertheabilityoftheLSTMnetworktolearnabstract\ndatafeatures,andthemoreeffectiveitistoreducetheerror\nbetweenthepredictedvalueandthetargetvalue.",
                "16.TheinfluenceofdifferentLSTMcellsonexperimentalresults.\nFig. 17.Theeffectofdifferentiterationtimesontheresults.",
                "ThesupportrateoftheGANs1\nmodelproposedbyYe(2019)wasabout33.3%,thesupportrate\nofDeepBachwasabout38.3%,thesupportrateofrealsongswas\nabout38.3%,andthesupportrateofreconfigurationharmony\nwasabout33.3%.ThesupportrateoftheGANs2modelproposed\nisabout46.7%,whichiscloseto50%andfarmorethanthatof\nYe\u2019s[35].ThemusicgeneratedbytheZhang(2020)modelisless\nthan50%ofthesatisfactionrateofthemusicgeneratedbythe\nBiLSTM-GANsunderthecategoryofpianomusicians.",
                ",Speaker-independentsilentspeechrecognitionfromflesh-\npoint articulatory movements using an LSTM neural network, IEEE/ACM\nTrans.AudioSpeechLanguageProcess.25(12)(2017)2323\u20132336.",
                "[30] \u00d6.Yildirim,AnovelwaveletsequencebasedondeepbidirectionalLSTM\nnetworkmodelforECGsignalclassification,Comput.",
                "[35] W.H.Ye,AutomaticcompositionbasedonBiLSTMandGANsalgorithm,J.\nJilinUniv.21(2)(2019)193\u2013194."
            ]
        },
        {
            "title": "Towards a Deep Improviser: a prototype deep learning post-tonal free music generator",
            "gan": [
                "We organise the pitches of the\nnotes to descend from p1 towards p10.",
                "We undertook the statistical\nanalyses in the R. For this purpose (and for playback), wereorganised the pitches of each chord into MIDI sequences\nof individual notes (for this assessment we are not con-\nsidering time series sequential relationships).",
                "Separate\nAnderson\u2013Darling analyses (not shown) of the outputs\nfrom the CNN/RNN model of the Algorithmic Corpus (and\nthe parallel analyses with the CNN\u2013RNN ImprovisedCorpus model) support a general conclusion: that our\napproach permits the generation of sequences statistically\ndistinct from either the learned corpus or the input seeddistribution which are yet organised rather than random.\nGoing from statistical distinction to substantive human\nevaluation of computational artistic generativity is a hugelydif\ufb01cult task [ 38\u201341] as for that matter is evaluation of\n(manually) composed work, and there is also an argument\nthat computational creativity (to which this paper poten-tially contributes) should be assessed in relation to its own\nspeci\ufb01ed objectives, partly or even solely by an internal\nmechanism [ 42]."
            ],
            "lstm": [
                "Because of the apparently greater\ncapacity of recurrent neural nets (RNN) and in particularthose using the LSTM (long short-term memory) nodes, we\nthen considered RNNs receiving outputs from an initial\nCNN layer.",
                "We largely overcame over\ufb01tting by a combination of\nstringent dropout (parameter 0.5) at each layer, including\nboth dropout and recurrent dropout in LSTM layers,\ntogether with L2 kernel regularisation at each layer (pa-rameter 0.01), and by application of the stopping criterion\nde\ufb01ned above.",
                "CNN/RNN: CNN 32 \ufb01lters, kernel of 4, dilation 8, RNN LSTM 32.",
                "Note that the purpose of adding the RNN with LSTM\nnodes (with only a c.50% increase in model parameters) isnot solely to enhance the model precision, but also in the\nhope of enhancing model \u2018memory\u2019 (autoregressive and\ncross-parameter temporal relationships), such that it mightpredict longer sequences.",
                "We expected this regression might\noccur in both models, particularly the CNN-only, both\nbecause of the limited capacity of CNN for temporallyordered sequences in comparison with LSTM (or gated\nresponse units, which we found to behave similarly in our\nsystem) units, and also the relatively small size of thelearned corpora."
            ]
        },
        {
            "title": "From artificial neural networks to deep learning for music generation: history, concepts and trends",
            "gan": [
                "1.1 Related work and organization\nThis article takes some inspiration from the comprehensive\nsurvey and analysis proposed by the recent book [ 3], but\nwith a different organization and material, and it also\nincludes an original historical retrospective analysis.",
                "This article is organized as follows:",
                "Hierarchically organized and con-\nnected sets of sequential networks hold promise foraddressing these dif\ufb01culties.",
                "Before that, we will introduce a conceptual\nframework in order to help at organizing, analyzing and\nclassifying various types of architectures, as well as varioususages of arti\ufb01cial neural networks for music generation.",
                "We will see that, from an\narchitectural point of view, various types of combination42\nmay be used:\n8.1 Composition\nSeveral architectures, of the same type or of different types,\nare composed, e.g.:\n\u2013 A bidirectional RNN, composing two RNNs, forward\nand backward in time, e.g., as used in the C-RNN-GAN\n[44] (see Fig. 16) and the MusicVAE [ 53] (see Fig. 7\nand Sect. 9.3) architectures; and\n\u2013 The RNN-RBM architecture [ 1], composing an RNN\narchitecture and an RBM architecture.",
                "8.4 Pattern\nAn architectural pattern is instantiated onto a given archi-\ntecture(s),45e.g.:\n\u2013 The anticipation-RNN architecture [ 19] that instantiates\ntheconditioning pattern46onto an RNN with the output\nof another RNN as the conditioning input; and\n\u2013 The C-RNN-GAN architecture [ 44], where the GAN\n(Generative Adversarial Networks) pattern (to be\nintroduced in Sect. 9.4) is instantiated onto two RNN\narchitectures, the second one (discriminator) beingbidirectional (see Fig. 16); and\n\u2013 The MidiNet architecture [ 67] (see Sect. 9.4), where\ntheGAN pattern is instantiated onto two convolu-\ntional\n47feedforward architectures, on which a condi-\ntional pattern is instantiated.",
                "16 C-RNN-GAN architecture with the D(iscriminator)",
                "GAN\ncomponent being a bidirectional RNN (LSTM).",
                "9.4 Generative adversarial networks (GAN)\narchitecture\nAn interesting example of architectural pattern is the\nconcept of Generative Adversarial Networks (GAN) [ 16],\nas illustrated in Fig.",
                "An example of the use of GAN for generating music is\nthe MidiNet system [ 67], aimed at the generation of single\nor multitrack pop music melodies.",
                "The architecture, illus-\ntrated in Fig. 23, follows two patterns: adversarial (GAN)\nand conditional (on history and on chords to condition\nmelody generation).",
                "22 Generative adversarial networks (GAN) architecture.",
                "A notable attempt has been proposed for creating\npaintings in [ 9], by extending a GAN architecture to favor\nthe generation of content dif\ufb01cult to classify within exist-ing styles and therefore favoring the emergence of new\nstyles.",
                "Architecture An (arti\ufb01cial neural network) architecture\nis the structure of the organization of computational units\n(neurons), usually grouped in layers, and their weightedconnexions.",
                "Examples of types of architecture are:\nfeedforward (aka multilayer perceptron), recurrent\n(RNN), autoencoder and generative adversarial networks(GAN).",
                "Discriminator The discriminative model component of\ngenerative adversarial networks (GAN) which estimates\nthe probability that a sample came from the real data\nrather than from the generator.",
                "Generative adversarial networks (GAN) A compound\narchitecture composed of two component architectures,the generator and the discriminator, who are trained\nsimultaneously with opposed objectives.",
                "The generative model component of gener-\native adversarial networks (GAN) whose objective is to\ntransform a random noise vector into a synthetic (faked)\nsample which resembles real samples drawn from adistribution of real data.",
                "Harmony In musical theory, a system for organizing\nsimultaneous notes.",
                "Mogren O (2016) C-RNN-GAN: continuous recurrent neural\nnetworks with adversarial training."
            ],
            "lstm": [
                "7.2.1 Recursive strategy\nThe \ufb01rst music generation experiment using current state of\nthe art of recurrent architectures, the LSTM (Long Short-\nTerm Memory [ 25]) architecture, is the generation of blues\nchord (and melody) sequences by Eck and Schmidhuber in[8].",
                "GAN\ncomponent being a bidirectional RNN (LSTM).",
                "Therefore, two recurrent networks (more\nprecisely, LSTM) are used, one summing up past infor-\nmation and another summing up information coming from\nthe future, together with a nonrecurrent network for notesoccurring at the same time.",
                "Long short-term memory (LSTM)",
                "The long short-term memory (LSTM) architecture solved the problem.",
                "A \ufb01rst look at music composition\nusing LSTM recurrent neural networks."
            ]
        },
        {
            "title": "Conditional hybrid GAN for melody generation from lyrics",
            "gan": [
                "ORIGINAL ARTICLE\nConditional hybrid GAN for melody generation from lyrics",
                "Inthis paper, we propose a novel conditional hybrid GAN (C-Hybrid-GAN) for melody generation from lyrics.",
                "Through extensive experiments using evaluation metrics, e.g., maximum mean\ndiscrepancy, average rest value, and MIDI number transition, we demonstrate that the proposed C-Hybrid-GAN outper-forms the existing methods in melody generation from lyrics.",
                "Keywords Melody generation from lyrics /C1GAN /C1AI music /C1Conditional sequence generation\n1 Introduction\nGenerating melody from lyrics to compose a song has been\na challenging research task in the \ufb01eld of arti\ufb01cial intelli-\ngence and music, which falls under the \ufb01eld of conditional\ndiscrete-valued sequence generation.",
                "An earlier study by [ 1] has shown the feasibility\nof exploiting conditional long short-term memory\u2014gen-\nerative adversarial network (LSTM-GAN) for melodygeneration from lyrics.",
                "On the one\nhand, the continuous-valued sequence, as the output of thegenerator in the GAN, is not in accordance with the dis-\ncrete-valued music attributes.",
                "On this basis,a novel conditional hybrid generative adversarial network\n(C-Hybrid-GAN) is suggested to generate melodies from\nlyrics, where three discrete sequences of music attributesare separately generated by the melody generation model\nconditioned on the same lyrics.",
                "Through the extensive\nexperiments, we show that the proposed C-Hybrid-GANoutperforms the existing melody generation methods and\nhas the capability of generating more natural and plausible\nmelodies.",
                "Consequently, in this work, we mainly discuss\nmelody generation from lyrics and GAN-based discretesequence generation.",
                "2.2 GAN-based discrete sequence generation\nGenerative adversarial networks (GANs) in [ 11] were\noriginally developed to generate continuous data [ 12],\nwhich have been applied successfully in the conditional\nsequence generation such as dialogue in [ 13], text-to-video\nin [14], and lyrics-to-melody in [ 1] generation.",
                "However,\nGANs have the limitation in generating discrete sequence\ndue to the non-differentiable problem of the discrete-val-\nued outputs from the generator.",
                "SeqGAN by [ 15] models the generator as a\nstochastic policy in reinforcement learning, which\navoids the generator differentiation problem by\ndirectly performing policy-gradient updates.",
                "Rank-GAN by [ 16] uses the ranking score as the rewards\nto learn the generator, which is optimized through\nthe policy gradient method.",
                "LeakGAN by [ 17]\naddresses a mechanism of providing richer infor-\nmation from the discriminator to the generator by\nexploiting hierarchical reinforcement learning.",
                "InMaskGAN, [ 18] proposes the actor-critic GAN\narchitecture that uses reinforcement learning to\ntrain the generator, where the in-\ufb01lling techniquemay alleviate mode collapse.",
                "In TextGAN, [ 19] utilizes a kernelized\ndiscrepancy metric to map high-dimensional latent\nfeature distributions of real and synthetic sentences,\nwith the aim of mitigating the model collapse.",
                "Instead of applying standard GAN objective, FM-\nGAN by [ 20] suggests to match the latent feature\ndistributions of real and synthetic sentences exploit-ing the feature-movers distance.",
                "In ARAE, [ 21]\nutilizes the adversarial autoencoder to transform the3192 Neural Computing and Applications (2023) 35:3191\u20133202\n123discrete data into a continuous latent space for\nGAN training.",
                "In GAN for sequences of discreteelements by [ 2] and RelGAN by [ 22], Gumbel-\nSoftmax approaches are suggested to approximate\nthe discrete-valued distribution for continuous-valued distribution.",
                "Here, we propose a hybrid GAN structure for learning\nmultiple melody attributes which contains two noveltechniques to improve the quality of lyrics-conditioned\nmelody generation: (i) Relational reasoning technique is\napplied to modeling not only dependency inside eachsequence of music attributes, but also consistency among\nthree sequences of music attributes during the training\nstage.",
                "(ii) Gumbel-Softmax technique is utilized toapproximate the discrete-valued distribution of music\nattributes in a conditional hybrid GAN.",
                "Conditional GAN\nWe propose an end-to-end deep generative model for\ngenerating melodies conditioned on lyrics.",
                "The proposedC-Hybrid-GAN model is trained by considering the\nalignment relationship between sequences of music attri-\nbutes and their corresponding lyrics.",
                "The Gumbel-\nSoftmax relaxation technique is exploited to train GAN fordirectly generating discrete-valued sequences.",
                "The RMC layer followingthe fully connected layer uses a single memory slot with\nthe head size set to 16, the number of heads set to 2, and the\nnumber of blocks set to 2.\n3.3 Gumbel-Softmax\nTraining GANs for the generation of discrete data faces a\nnon-differentiable problem due to discrete-valued output\nfrom the generator.",
                "1 Architecture of\nconditional hybrid GAN3194 Neural Computing and Applications (2023) 35:3191\u20133202\n123yp\nt\u00fe1/C24softmax \u00f0ot\u00de: \u00f03\u00de\nHere, softmax \u00f0ot\u00derepresents the multinomial distribution\non the set of all possible MIDI numbers.",
                "Then, the discriminator loss\nis given by\nloss\nD\u00bclogsigmoid1\nTXT\nt\u00bc1ot/C01\nTXT\nt\u00bc1^ot !\n: \u00f05\u00de\nHere, we use the relativistic standard GAN (RSGAN) loss\nfunction in [ 29].",
                "4 Experiments\nIn this section, we discuss the experimental setup andresults to demonstrate the feasibility of our proposed C-\nHybrid-GAN.",
                "Moreover, [ 22] showed that GAN\nwith RMC and Gumbel-Softmax outperforms other exist-\ning state-of-the-art generative models in terms of samplequality and diversity in text generation.",
                "In these research\nreports, we have seen that GAN with RMC and Gumbel-\nSoftmax performs best.",
                "This work aims to discuss theeffectiveness of melody generation from lyrics where three\ndiscrete sequences corresponding to music attributes,\nnamely pitch, duration, and rest, are separately generatedby GAN with RMC and Gumbel-Softmax when given\nlyrics.",
                "In addition, four competitive methods are implemented\nto compare with the proposed C-Hybrid-GAN as follows:\nTBC-LSTM-MLE:",
                "TBC-LSTM-GAN: It is similar to TBC-LSTM-MLE, except that GAN is used and the Gumbel-Softmax\ntechnique is exploited to train a GAN for discrete-valued\nsequence generation.",
                "C-Hybrid-GAN: The proposed\nmethod uses both RMC and Gumbel-Softmax.",
                "C-LSTM-GAN in [ 1]: It contains a deep LSTM generator and a deep\nLSTM discriminator both conditioned on lyrics, without\nusing RMC and Gumbel-Softmax.",
                "As Gumbel-Softmaxand RMC are mainly involved in the proposed C-Hybrid-\nGAN, their impacts are further investigated as the ablation\nstudy, and the results of TBC-LSTM-MLE and TBC-LSTM-GAN are shown in Table 2.\n4.1 Experimental setup\nWe use the Adam [ 32] optimizer with b1\u00bc0:9 and b2\u00bc\n0:99 and perform gradient clipping if the norm of the\ngradients exceeds 5.",
                "The value of the Self-BLEU score ranges between 0 and 1with a smaller value of Self-BLEU implying a higher\nsample diversity hence a less chance of mode collapse in\nthe GAN model.",
                "During the adversarial training,\nSelf-BLEU values of our C-Hybrid-GAN architecturereach the peak around 45 epochs, decrease until 100\nepochs, and then approach to the stability.",
                "4.5 Comparison with state-of-the-art methods\nTo study if C-Hybrid-GAN can generate sequences that\nresemble the same distribution as training samples, quan-titative evaluation is performed to compare existing state-\nof-the-art approaches following the previous quantitative\nmeasurements in [ 1], for example, 2-MIDI numbers repe-\ntitions, 3-MIDI numbers repetitions, MIDI numbers span,\nthe number of unique MIDI, the number of notes without\nrest, average rest value in a song, and song length.",
                "It is very obvious that the\noverall performance of the proposed C-Hybrid-GAN out-\nperforms other competitive methods in most aspects.",
                "Forpitch-related attributes such as MIDI number span and the\nnumber of unique MIDI numbers, the proposed C-Hybrid-\nGAN method is closest to the ground truth.",
                "In addition, for\nmetrics on temporal attributes such as average rest value\nand the number of notes without rest, C-Hybrid-GAN isalso closest to the ground truth.",
                "4 Training curves of MMD\nscores on testing dataset3198 Neural Computing and Applications (2023) 35:3191\u20133202\n123melodies generated by our model C-Hybrid-GAN,\nC-Hybrid-MLE, and C-LSTM-GAN.",
                "According to theoccurrence of MIDI number transition in the \ufb01gures, it is\nvery clear that the proposed C-Hybrid-GAN model can\nbetter capture the distribution of MIDI number transition.",
                "Mean values\narelrs\u00bc1:3666, lrn\u00bc1:3692,\nandlrns\u00bc1:3679, respectively\nTable 2 Metrics evaluation of attributes\nGround truth C-LSTM-GAN C-Hybrid-MLE C-Hybrid-GAN TBC-LSTM-MLE TBC-LSTM-GAN\n2-MIDI repetitions 7.4 9.7 6.8 6.5 9.1 10.6\n3-MIDI repetitions 3.8 2.2 2.8 2.7 2.1 3.0MIDI span 10.8 7.7 12.7 12.0 13.7 12.1Unique MIDI number 5.9 5.1 6.0 6.1 6.2 6.1Average rest value 0.8 0.6 1.4 0.7 1.1 0.8Non-rest note number 15.6 16.7 12.7 16.1 12.7 15.9Song length 43.3 39.2 60.9 39.1 51.0 41.4\nFig.",
                "Generally, it can be seen that the overall result of the\nproposed method C-Hybrid-GAN is the closest to that ofthe ground truth, especially for melody and rhythm scores.",
                "To avoid the problem ofnon-differentiability in GANs for discrete data generation,\nwe exploit the Gumbel-Softmax to approximate the dis-\ntribution of discrete-valued sequences.",
                "Through extensiveexperiments of melody generation from lyrics including the\ndiversity and quality of generated melody samples, the\neffect of lyrics-based context conditioning, and the com-parison with existing works, we indicate that the proposed\nC-Hybrid-GAN outperforms the existing cutting-edge\nmethods in lyrics-conditioned melody generation withmultiple music attributes.",
                "Conditional lstm-gan for\nmelody generation from lyrics.",
                "Kusner MJ, Herna \u00b4ndez-Lobato JM (2016) Gans for sequences of\ndiscrete elements with the gumbel-softmax distribution.",
                "In: IEEE conference on multimedia information processingand retrieval, pp 388\u2013391\n10.",
                "Zhou D, Zhang H, Li Q, Ma J, Xu X (2022) Cout\ufb01tgan: learning\nto synthesize compatible out\ufb01ts supervised by silhouette masks\nand fashion styles.",
                "Deng K, Fei T, Huang X, Peng Y (2019) Irc-gan: introspective\nrecurrent convolutional gan for text-to-video generation.",
                "Seqgan: sequence gen-\nerative adversarial nets with policy gradient.",
                "Fedus W, Goodfellow I, Dai AM (2018) Maskgan: better text\ngeneration via \ufb01lling in the______.",
                "Zhang Y, Gan Z, Fan K, Chen Z, Henao R, Shen D, Carin L\n(2017)",
                "Chen L, Dai S, Tao C, Zhang H, Gan Z, Shen D, Zhang Y, Wang\nG, Zhang R, Carin L (2018)",
                "Nie W, Narodytska N, Patel A (2018) Relgan: relational gener-\native adversarial networks for text generation.",
                "Symbolic music gener-ation with transformer-gans.",
                "The relativistic discriminator: a\nkey element missing from standard gan."
            ],
            "lstm": [
                "An earlier study by [ 1] has shown the feasibility\nof exploiting conditional long short-term memory\u2014gen-\nerative adversarial network (LSTM-GAN) for melodygeneration from lyrics.",
                "In our initial work by [ 1], we not only built a large dataset\nconsisting of 12,197 MIDI songs each with paired lyricsand melody, but also have veri\ufb01ed the feasibility of melody\ngeneration from lyrics by LSTM-based deep generative\nmodel [ 8].",
                "[ 25] empirically shows that RMC is better-suited for\ntasks such as language modeling that bene\ufb01ts from rela-\ntional reasoning across the sequential information ascompared to LSTM.",
                "[25] empirically showed that RMC is better suited for\ntasks such as language modeling that bene\ufb01ts from rela-\ntional reasoning across the sequential information ascompared to LSTM.",
                "In addition, four competitive methods are implemented\nto compare with the proposed C-Hybrid-GAN as follows:\nTBC-LSTM-MLE:",
                "It contains a lyrics-conditioned LSTM-based generator which is composed of three branches of\nidentical and independent lyrics-conditioned LSTM-basedNeural Computing and Applications (2023) 35:3191\u20133202 3195\n123sub-networks, each responsible for generating an attribute\nof a melody and trained with the MLE objective.",
                "TBC-LSTM-GAN: It is similar to TBC-LSTM-MLE, except that GAN is used and the Gumbel-Softmax\ntechnique is exploited to train a GAN for discrete-valued\nsequence generation.",
                "C-Hybrid-MLE: It is\nsimilar to TBC-LSTM-MLE except that RMC-based gen-\nerator is used.",
                "C-LSTM-GAN in [ 1]: It contains a deep LSTM generator and a deep\nLSTM discriminator both conditioned on lyrics, without\nusing RMC and Gumbel-Softmax.",
                "As Gumbel-Softmaxand RMC are mainly involved in the proposed C-Hybrid-\nGAN, their impacts are further investigated as the ablation\nstudy, and the results of TBC-LSTM-MLE and TBC-LSTM-GAN are shown in Table 2.\n4.1 Experimental setup\nWe use the Adam [ 32] optimizer with b1\u00bc0:9 and b2\u00bc\n0:99 and perform gradient clipping if the norm of the\ngradients exceeds 5.",
                "4 Training curves of MMD\nscores on testing dataset3198 Neural Computing and Applications (2023) 35:3191\u20133202\n123melodies generated by our model C-Hybrid-GAN,\nC-Hybrid-MLE, and C-LSTM-GAN.",
                "Mean values\narelrs\u00bc1:3666, lrn\u00bc1:3692,\nandlrns\u00bc1:3679, respectively\nTable 2 Metrics evaluation of attributes\nGround truth C-LSTM-GAN C-Hybrid-MLE C-Hybrid-GAN TBC-LSTM-MLE TBC-LSTM-GAN\n2-MIDI repetitions 7.4 9.7 6.8 6.5 9.1 10.6\n3-MIDI repetitions 3.8 2.2 2.8 2.7 2.1 3.0MIDI span 10.8 7.7 12.7 12.0 13.7 12.1Unique MIDI number 5.9 5.1 6.0 6.1 6.2 6.1Average rest value 0.8 0.6 1.4 0.7 1.1 0.8Non-rest note number 15.6 16.7 12.7 16.1 12.7 15.9Song length 43.3 39.2 60.9 39.1 51.0 41.4\nFig.",
                "Conditional lstm-gan for\nmelody generation from lyrics."
            ]
        },
        {
            "title": "Scene2Wav: a deep convolutional sequence-to-conditional SampleRNN for emotional scene musicalization",
            "gan": [
                "The remainder of this paper is organized as follows.",
                "This model mimics the organization of the human visual cortex\nsystem by having a structure of stacked convolutions and non-linear functions, thus being\nable to effectively extract feature from raw images.",
                "Zhou C, Horgan M, Kumar V, Vasco C, Darcy D (2018)"
            ],
            "lstm": []
        },
        {
            "title": "Attentional networks for music generation",
            "gan": [
                "More recent generative models such as gener-ative adversarial networks (GANs) or variational auto-encoders (V AEs) can even generatenovel timbral spaces as well as render novel songs while directly working in the waveformdomain.",
                "Generative AdversarialNetworks (GANs)",
                "MuseGAN",
                "Dong HW, Hsiao WY, Yang LC, Yang YH (2018) Musegan: Multi-track sequential generative adversar-\nial networks for symbolic music generation and accompaniment."
            ],
            "lstm": [
                "In this work, we propose a deep learning based music generation\nmethod in order to produce old style music particularly JAZZ with rehashed melodic struc-\ntures utilizing a Bi-directional Long Short Term Memory (Bi-LSTM) Neural Network with\nattention.",
                "Owing to the success in modelling long-term temporal dependencies in sequen-\ntial data and its success in case of videos, Bi-LSTMs with attention serves as a natural\nchoice and early utilization in music generation.",
                "We validate in our experiments that Bi-\nLSTMs with attention are able to preserve the richness and technical nuances of the music\nperformed.",
                "Keywords Recurrent neural network (RNN) \u00b7Long short term memory (LSTM) \u00b7\nAttention \u00b7Bidirectional LSTM \u00b7MIDI format\n/envelopebackPrerana",
                "Their work first utilized LSTMs in music generation.",
                "In [ 15], authors stated that\nLSTMs are able to capture the medium-scale melodic structure in music pretty well.",
                "In this work, we utilize an end to end pipeline based on Bi-LSTM network with an atten-\ntion module to produce old style Jazz music with rehashed melodic structures automaticallywithout any human intervention.",
                "This is then fed to a Bi-LSTM network with 512 hidden units with atten-tion which is again followed by an LSTM layer and flattened to generate the consecutivemusical notes.",
                "Global (soft) attention aims to derive a context vector based on all hid-\nden states of the BiLSTM network.",
                "[10] utilized two distinctive LSTM based networks \u2013 i) to learn harmony struc-ture and nearby note structure, ii) to realize longer dependency conditions so as to attemptto become familiar with a song and hold it all through the piece.",
                "2.2 LSTM\nLong Short Term Memory networks \u2013 generally called \u201cLSTMs\u201d \u2013 are an extraordinary sortof RNN, equipped for adapting to long term conditions.",
                "Regularly used architecture of LSTM units have a cell andthree regulators.",
                "Cell is the memory part of the LSTM unit.",
                "The sigmoid layer (activation function of LSTM) yields numbers somewhere inthe range of zero and one, depicting the amount of every part ought to be let through.",
                "2.3 AttentionbasedLSTM\nAttention is a later advancement that really takes care of our center issue.",
                "This enables totake care of specific segments of the contribution at some random moment and utilize thosesegments to help produce portions of the yield as opposed to simply the last output of theLSTM layer.",
                "2.4 BidirectionalLSTM\nBidirectional LSTMs are an expansion of conventional LSTMs that can improve model\nexecution on arrangement order issues.",
                "In issues where all timesteps of the informationarrangement are accessible, Bidirectional LSTMs train two rather than one LSTM on theinformation grouping.",
                "Since, theinput feed to the BiLSTM with Attention network should be in real values, we provide anindex representation (integer values) to each unique pitch in the notes list which are origi-nally in string format.",
                "Each BiLSTM cell isfed the sampled index.",
                "3.1 Preprocessing\nTo prepare the model, the MIDI records should have to be changed over into a structurethat can be encoded as numeric information to feed the Bidirectional LSTM network.",
                "Those subdivided samples are then fed intothe Bidirectional LSTM for training in order to generate the consecutive notes.",
                "Bi-LSTMs networks can mimic that properly.",
                "3.2 MusicgenerationusingattentionbasedLSTM\nRecurrent neural networks (RNNs) are quite widely used to process sequential data infor-mation.",
                "In order to addressthis and incorporate long-term dependencies, Hochreiter and Schmidhuber [ 14] proposed\nLong-Short Term Memory networks (LSTMs).",
                "As LSTMs only account for the forwardinformation flow, Bidirectional Long Short-Term Memory networks were thus proposed byGraves and Schmidhuber [ 12] a variant of LSTM networks, which is composed of a for-\nward LSTM network and a reverse LSTM network, capturing the context knowledge oftime series.",
                "To achieve our objective of generating old style music with rehashed melodicstructure, we have utilized Bi-LSTM network with attention layer [ 22].",
                "81:5179\u201351891x100 dimensional input notes sample size into a Bidirectional LSTM with 512 cells, fol-\nlowed by an attention layer, subsequent LSTM layer with 512 nodes.",
                "The second LSTM enables further learning these interdependenciesbetween the notes and harmonies.",
                "As seen in Table 2, the cross entropy loss and mean square error (MSE)\nbetween the subsequent notes is compared for three variants: LSTM, LSTM with Attention,Bidirectional-LSTM with Attention and LSTM.",
                "LSTM performs the least than other two as it giveshigh error rates.",
                "In order to improve the learning capability, attention layer is added to theLSTM and it is found that it perfoms better than vanilla LSTM network.",
                "Finally, we observe that in all cases Bidirectional-LSTMwith Attention and followed by stacked LSTM gives the best results.",
                "Figure 3shows the\ncategorical cross entropy loss for the three variants of LSTMs considered in this work.",
                "[24] 0.2317 0.9904 0.9808\nLSTM 0.5097 1.0919 1.1924\nLSTM + Attention 0.2286 0.8924 0.7864\nBi-LSTM + Attention + LSTM 0.1069 0.6694 0.4481\nFig.3 Graph: Categorical Cross Entropy Loss\nFig. 4 Performance Evaluation Graph: (a)-(c) shows Mean Square Error for LSTM, LSTM+attention and\nBi-LSTM+attention respectively5187 Multimedia Tools and Applications (2022) 81:5179\u20135189Fig.5 a) Input music sheet b) Output music sheet generated by the proposed framework for songs Chameleon\n(Top Row) and Last farewell (Bottom row)\n4.3 Comparisonwithrelatedworks\nIn [24], the author discusses about polyphonic midi sequences using LSTM networks.",
                "On\ncomparing with the LSTM and LSTM+Attention it showed better results, but it showed\na relatively high error rate when compared with Bi-directional LSTM with Attention andLSTM.",
                "In [ 23], the error rate is\nrelatively lower than [24] and when compared with Bidirectional LSTM with Attention andLSTM the error rates are almost similar.",
                "5 Conclusion\nIn this paper, we presented bidirectional LSTM with Attention and LSTM with the objectiveof producing music that is coherent and good to hear.",
                "Blues improvisation with lstm recur-\nrent networks.",
                "A first look at music composition using lstm recurrent neural networks.",
                "Framewise phoneme classification with bidirectional lstm and other\nneural network architectures.",
                "A study on lstm networks for polyphonic music sequence modelling."
            ]
        },
        {
            "title": "Monophonic music composition using genetic algorithm and Bresenham\u2019s line algorithm",
            "gan": [
                "It is an organization of rhythm, melody and harmony.",
                "When organized with single\nmelody and rhythm, the music has a monophonic texture.",
                "The bottom number indicates the typeof note, namely quarter, eighth or sixteenth etc.\n\u2013 Scale: Scale, in music theory, is an organised set of notes within an octave."
            ],
            "lstm": [
                "A first look at music composition using lstm recurrent neural networks."
            ]
        },
        {
            "title": "Polyphonic music generation generative adversarial network with Markov decision process",
            "gan": [
                "Therefore, this paper proposes a novel polyphonic music creation\nmodel, combining the ideas of the Markov decision process (MDP) and Monte Carlo tree\nsearch (MCTS) and improving the Wasserstein Generative Adversarial Network\n(WGAN) theory.",
                "Generative Adversarial Network (WGAN)\n1 Introduction\nProducing various independent melodies and combining them harmoniously through technical\nprocessing are key to creating polyphonic music.",
                "i n a#The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022classic Generative Adversarial Network (GAN) [ 7,13] is limited for polyphonic music\ncreation.",
                "For example, it is difficult for a classic GAN to create a new melody outside thetraining dataset, and it is difficult to break through the shackles of melody and tone in that\ndataset.",
                "Therefore, the present research team designed a GAN model based on the Markov decisionprocess (MDP) [ 21] and Monte Carlo tree search (MCTS) [ 4] to generate polyphonic music.",
                "Based on the above research, Goodfellow et al. have drawn lessons from a novel pair theory\n(GAN) for the network construction of image generation and learning.",
                "[ 9] proposed a GAN model which can have a convolution, which needs to add an\nadditional refiner network to the generator.",
                "The team has also proposed two versions of music generation models MuseGAN V1[10] and MuseGAN V2 [ 11].",
                "MuseGAN V1 uses convolution in the generator and discrim-\ninator, which can generate multi-channel pop/rock music from scratch or can accompany a\ntrack provided by the user.",
                "MuseGAN V2 uses three symbolic, multi-channel, music gener-\nation models in a GAN framework: the interference model, the composer model, and thehybrid model.",
                "Experiments show that, given a specific track composed by humans, MuseGANV2 can generate four additional accompaniment tracks.",
                "However, since GAN was proposed,29866 Multimedia Tools and Applications (2022)",
                "Many studies, including those concerning MuseGAN V1 and MuseGAN\nV2, are trying to solve this problem, but the effect is not yet satisfactory.",
                "For example, some\nresearchers [ 22,32\u201334] have improved algorithms in the original GAN.",
                "[ 2] propose using the Wasserstein Generative Adversarial Network (WGAN),\nbased on the work of Goodfellow et al.",
                "[ 13], to remedy the instability of GAN training and\ncollapse mode.",
                "However, in polyphonic musicgeneration, as the length of the music sequence generated by WGAN increases, sequencecoherence is broken, and the discriminator in the WGAN model finds it difficult to evaluatethe incomplete sequence.",
                "Therefore, aiming atthe problems of WGAN polyphonic music generation, this paper improves the structure of the\ngenerator and discriminator in the WGAN model and adds a policy gradient algorithm [ 29]t o\nupdate the generator parameters.",
                "Forpolyphonic music generation, the generator in the GAN model supervises learning needs to beinformed of the appropriate notes required for each small segment sequence.",
                "However, thereare two problems in the existing GAN model; (1) the generator finds it difficult to transfer\ngradient update, and (2) the discriminator finds it difficult to evaluate the incomplete sequence.",
                "Therefore, the present research team has the introduced MDP mechanism into the GAN modelto generate polyphonic music.",
                "This effectively solves the problem of the GAN generator finding it difficult totransfer the gradient update, and MDP is suitable for the generation model construction in thispaper.",
                "This paper combines WGAN and MDP theory, integrates MCTS, and proposes a poly-\nphonic music generation method based on MDP and WGAN, as shown in Fig.",
                "GAN model is unable to evaluate incomplete music sequences.",
                "(1) The model is built based on MDP and\nWGAN.",
                "(2) The construction of this model draws\nlessons from WGAN principles and adds Wasserstein distance [ 2], which stabilizes model\ntraining by resolving the issue in which, the better the discriminator is trained in the existing\nGAN, the more seriously the generator gradient disappears.",
                "(3) After training, the generator in\nthe GAN model can be used independently of the discriminator.",
                "(4) This paper is the first to integrateMCTS into WGAN for polyphonic music generation, removing the issues in which it isdifficult for the discriminator, in the existing GAN model, to evaluate incomplete sequences.",
                "2 Technical details\n2.1 Network construction\nIn this paper, the researchers have built a neural network based on WGAN and MDP to learnand generate polyphonic music.",
                "This model was constructed by referring to the WGAN model presented by Martinet al.",
                "This study combines\nWGAN and MDP to generate polyphonic music.",
                "Discrete music sequences differ fromcontinuous image processing, which is not conducive for training typical GAN.",
                "Moreover,GAN can only evaluate the whole sequence, rather than the local sequence, which leads to thegenerator producing a discrete output and makes it difficult for the discriminator to return agradient to update the generator.",
                "As mentioned above, the generator network has been built with reference to the WGAN\nstructure proposed by Martin et al.",
                "2.2 Network strategy gradient\nWhen designing the strategy gradient for the polyphonic music generation model, it wasnecessary for the authors to introduce the relevant GAN theory proposed by Goodfellow et al.[13].",
                "To learn the target generator distribution from standard data, the GAN input is defined as\nP\nzz\u00f0\u00de, and the generation model is defined as G\u03b8yt\u00f0jY1:t/C01\u00de.G\u03b8is the model parameter; ytis the\noutput at time t;a",
                "min\nGmax\nDVD;G\u00f0\u00de \u00bc E\nx~Pdatax\u00f0\u00delog D x\u00f0\u00de\u00f0\u00de\u00bd/C138 \u00fe Ez~Pzz\u00f0\u00delog1/C0DGz\u00f0\u00de\u00f0\u00de\u00f0\u00de\u00bd/C138 \u00f0 1\u00de\nHowever, classic GAN presents a large problem.",
                "[ 2] propose WGAN,\nbased on Goodfellow et al.",
                "[ 13], and replace Jensen-Shannon divergence with Wasserstein\ndistance, which effectively solves the problem of instability in the classic GAN trainingprocess.",
                "Therefore, the team introduced WGAN into the polyphonic music generation model.",
                "The discriminator in the original GAN accom-\nplishes the task of true and false dichotomy, so the sigmoid function is used in the last layer.",
                "However, the discriminator, f\n!, in WGAN does approximate fitting Wasserstein distance,\nwhich belongs to the regression task, so it is necessary to remove the sigmoid function of thelast layer.",
                "Considering that the first term of Lis independent of the generator, two\nWGAN losses can be obtained, as shown in Eqs.",
                "Because this study introduces MDP into the GAN model, the goal of the generator is to\ngenerate sequences to maximize the expectation of return, as shown in Eq.",
                "The experiment has employed three models to generate music results: the WGAN model, the\nWGAN and MDP model, and the proposed model for comparative training (Fig. 6).",
                "At the\nsame time, the research team has compared the effects of the model in this study with theMuseGAN v1 model [ 10], the MuseGAN v2 model [ 11], and the LSTM music generation\nmodel [ 23] for polyphonic music generation.",
                "In the training process, whenusing WGAN for music generation, the research team has employed a convolution network as\nthe generator.",
                "Then, theteam used WGAN + MDP to generate music, and this has improved the original WGAN byreplacing the convolution layer of the generator with an LSTM layer to better extract thecharacteristics of the original music sequence.",
                "The MDP algorithm also makes the generatedmusic more fluent, and, as the generated music sequence grows, the model will not alwaysgenerate single melody music, thereby giving the GAN model a certain creative ability.",
                "81:29865 \u201329885 29875Figure 7shows the change of loss value with iteration times for the WGAN model, the\nWGAN and MDP model, and the proposed model.",
                "WGAN WGAN+MDPWGAN+\nMDP+MCTS Original Music\nFig.",
                "The music generated by the proposed model, the music generated by the WGAN model, and the music\ncontrast map generated by the WGAN and MDP model\n0102030405060Loss\nEpochLoss curves of generator\nWGAN\nWGAN+MDP\nOur model\n0510152025303540\n0 100 200 300 400 500 600 700 800 900 1000Loss\nEpochLoss curves of discriminator\nWGAN\nWGAN+MDP\nOur model\nFig. 7 Curve of functions changing with iterations29876 Multimedia Tools and Applications (2022)",
                "During the training process, it is difficult for the convolutionlayer of the generator in the WGAN model to learn complex music sequence features.",
                "When comparing the WGAN and MDP model with the proposed model, it is evident thatthe generator with the MCTS mechanism has the fastest convergence effect.",
                "3.3 Experimental result\nBy studying the existing music generation methods and analyzing the WGAN model, through\na large number of experiments, it has been proven that the model proposed in this paper is\nmore suitable for polyphonic music generation than both the WGAN model and the WGANand MDP model.",
                "It should be noted that, with the change of test range, the value obtained formacro/C0F1 is always higher than that obtained for macro/C0F1\nD. Thus, it is proven that the\npolyphonic music generation model based on WGAN theory, which integrates the ideas of\nMDP and MCTS, is better than the MuseGAN model proposed by Dong et al.",
                "To more effectively verify the effectiveness of the model proposed in this paper, the\nresearch team has compared this model with the latest music generation models, includingthe MuseGAN v1 model [ 10], the MuseGAN v2 model [ 11], and the LSTM model [ 23].",
                "To\navoid generator collapse in the MuseGAN model and to optimize the training speed, theWGAN gradient penalty (WGAN-GP) network is used.",
                "Compared with the above research, the model in this paper combines MDP theory and MCTS,\nbased on WGAN.",
                "Therefore, the research team did notadd a gradient penalty term to the original WGAN.",
                "In this paper, GAN is used instead of RNNto generate music, and LSTM units are used in the generator.",
                "The music visualization diagrams generated by the proposed model, theMuseGAN v1 model, the MuseGAN v2 model, and the LSTM models are shown in Fig. 12.",
                "Combining GAN and MDP theory can also effectively enhance\nFig.",
                "Afterlistening to music, subjects rated them on a 10-point scale ranging from 1 (very low) to 10\nMuseGAN v1",
                "[8] MuseGAN v2",
                "i o n s\nBased on the WGAN model, a novel polyphonic music generation method using MDP",
                "Wasserstein GAN. arXiv, 1701.07875.",
                "Dong HW, Hsiao WY, Yang LC (2017) MuseGAN: demonstration of a convolutional GAN based model\nfor generating multi-track piano-rolls.",
                "Dong HW, Hsiao WY, Yang LC (2017) MuseGAN: multi-track sequential Generative Adversarial\nNetworks for symbolic music generation and accompaniment.",
                "FusionGAN:",
                "Xie Y, Franz E, Chu MY, Thuerey N (2018) tempoGAN:",
                "A temporally coherent, volumetric GAN for\nsuper-resolution fluid flow.",
                "StackGAN plus plus: Realistic image synthesis with stacked generative adversarial\nnetworks."
            ],
            "lstm": [
                "[ 1] introduced some\neffective data set preprocessing and reconstruction techniques, and used LSTM to generategrammatically correct music scores.",
                "The\ngenerator in the proposed network is a long short-term memory (LSTM) unit, while the\ndiscriminator is a convolution unit.",
                "During the generation process, the original learning sequence and the polyphony of\nthe LSTM are reconstructed.",
                "However, the current team has applied an LSTM layer\ninstead of the standard convolution layer.",
                "The main feature of LSTM is that it is helpful forlearning the internal memory of sequence information, receiving important input informationand accurately predicting the next input based on it.",
                "Therefore,\nthe generator network designed by the present team is composed of LSTM units.",
                "When inputsamples are input into an LSTM unit, the unit will not hide the activation from the beginning toreplace the whole activation but will, instead, keep the state unaffected by the previousactivation.",
                "To improve the overall stability of the network, the team has added a batchnormalization layer to all LSTM layers.",
                "In addition, to increase the processing capacity ofthe generator network for more complex and longer music sequences, the team has increasedthe number of LSTM units.",
                "There are 512 LSTM units in the network, and each LSTM unit\nhas 400 feature maps.",
                "The generator network is composed of LSTM units, while the discriminator network iscomposed of a convolution layer and a fully-connected layer.",
                "Additionally, a Rectified Linear Unit\n\u0011\u0011\u0011\nLSTM LSTM LSTM LSTM \u0011\u0011\u0011LSTM\ncellReal music \nsequenceGenerated \nmusic \nsequence\nReal music \nsequence Fully-connected \nlayerGenerator DiscriminatorRewardPolicy Gradient\nFig.",
                "At the\nsame time, the research team has compared the effects of the model in this study with theMuseGAN v1 model [ 10], the MuseGAN v2 model [ 11], and the LSTM music generation\nmodel [ 23] for polyphonic music generation.",
                "Then, theteam used WGAN + MDP to generate music, and this has improved the original WGAN byreplacing the convolution layer of the generator with an LSTM layer to better extract thecharacteristics of the original music sequence.",
                "To more effectively verify the effectiveness of the model proposed in this paper, the\nresearch team has compared this model with the latest music generation models, includingthe MuseGAN v1 model [ 10], the MuseGAN v2 model [ 11], and the LSTM model [ 23].",
                "[ 23] propose an\nalgorithm to generate notes using RNN, mainly via an LSTM network.",
                "The model proposedin this study seeks to learn polyphonic note sequences on a single-layer LSTM network.",
                "In this paper, GAN is used instead of RNNto generate music, and LSTM units are used in the generator.",
                "Compared with convolutionunits, LSTM units have better music sequence generation ability and produce higher quality,more detailed music.",
                "The music visualization diagrams generated by the proposed model, theMuseGAN v1 model, the MuseGAN v2 model, and the LSTM models are shown in Fig. 12.",
                "[9]\nLSTM [26] Proposed Model\nFig.",
                "LSTM based music generation with dataset prepro-\ncessing and reconstruction techniques\n2.",
                "Mangal S, Modak R, Joshi P (2019) LSTM based music generation system."
            ]
        },
        {
            "title": "A combination of multi-objective genetic algorithm and deep learning for music harmony generation",
            "gan": [
                "The rest of the paper is organized as follows:\nSection 2provides the state-of-the-arts in AMG.",
                "One of the recent AMG approaches is the use of Generative Adversarial Networks (GANs)\nas well as reinforcement learning methods.",
                "In 2017, Yang et al. proposed a system called\nMidiNet, which in two separate experiments generated melody and melody with harmony bycombining the GAN and Convolutional Neural Networks (CNN)"
            ],
            "lstm": [
                "The scoring of experts and listeners separately are modeled using Bi-directional Long Short-\nTerm Memory (Bi-LSTM) neural networks.",
                ".Multi-objective\ngenetic algorithm .Bi-LSTM\n1 Introduction\nBy growing the multimedia contents, attracting a udiences is becoming more difficult day by day.",
                "In order to remove the listeners in the AMG\nprocess, and completely automizing the system, we have modeled the human scoring using\nt\nwo Bi-directional Long Short-Term Memory (Bi-LSTM) neural networks (one for regular\nlisteners and another for expert listeners).",
                "&Considering both expert and ordinary listeners as separate objective functions.\n&Introducing a polyphonic music generation system, including the composition of melody,\nharmony, and rhythm in desired styles and lengths.\n&Modeling audience behavior in understanding the beauty of music by a Bi-LSTM model,and using it as an evaluation function.",
                "Fra nklin created a recurrent neural network with Long\nShort-Term Memory (LSTM) that generates melody or monophonic music pieces [ 13].\nHarmony or polyphonic music is a process of combining individual voices that are\nanalyzed by hearing them simultaneously.",
                "The scoring of each group is then modeled by a Bi-LSTM neural network.",
                "Objective FunctionStart\nGenerating Initial \nPopulation randomly\nCalculating the Fitness \nof each music piece\nSelection\nCrossover\nMutation\nReplacementMusic Grammar\nBi-LSTM 1\nBi-LSTM 2\nEnd+\nConver ged?Music\nScore\nFig. 1",
                "4.3 Bi-LSTM-based evaluating models\nEach generated music piece from the previous stage is heard by two groups of listeners:\nexperts and regular listeners.",
                "In this research, a sequence of generatedmusic in a time frame is considered as the input of a Long Short-term Memory (LSTM)network.",
                "The LSTM model is a type of RNNs that uses a memory cell built to show long-termdependencies on time-series data.",
                "To simulate the way of scoring for each grou p of listeners, a Bi-LSTM neural network\nis used.",
                "Since all the musical notes are interconnected and their connection is maintainedto the end, the Bi-directional LSTM recursive neural network is used, which has amemory and examines the logical connection of the notes from the beginning to the2425 Multimedia Tools and Applications (2023) 82:2419\u20132435end, and from the end to the beginning.",
                "The architecture of theLSTM network is shown in Fig.",
                "As can be seen in Fig. 3, GA2 for the AMG system has the same process as the GA1, except\nthat, the fitness function, by adding human, has several objectives: Music rules, Bi-LSTM\nFig.",
                "The architecture of the Bi-LSTM network2426 Multimedia Tools and Applications (2023) 82:2419\u20132435trained neural network model with opinions of music experts, and the trained Bi-LSTM neural\nnetwork model with opinions of regular listeners.",
                "In Bi-LSTM neural networks, four layers are used, and:\n&The first layer in networks is actually the input layer that takes the matrix of notes (pianomatrix).",
                "82:2419\u20132435&The second layer is the Bi-LSTM layer, which has 50 neurons in the hidden layer.\n&The third layer is a Fully Connected layer with one output.\n&The last layer is a regression layer with a cost function of the Root Mean Square Error\n(RMSE).",
                "We\nthen stored the range of different outputs generated in this section for scoring expert and\nnormal listeners and trained the two Bi-LSTM artificial neural networks using the opinions ofthese two groups of listeners.",
                "Lstm based music generation with dataset preprocess-\ning and reconstruction techniques.",
                "A first look at music composition using lstm recurrent neural networks.",
                "Combining LSTM and feed\nforward neural networks for conditional rhythm composition."
            ]
        },
        {
            "title": "A Style-Specific Music Composition Neural Network",
            "gan": [
                "At present, most of the researchon intelligent composition relying on neural network is focusing on how to improve thestructure of neural network and how to preprocess the training data from signal processingangles.",
                "The remaining sections are organized as follows:",
                "[ 18] proposes a sequence gen-\neration algorithm and a combined network based on V AE\u2013GAN structure, in which eachgenerated continuous frame is encoded, then the generator predicts the generated content ofthe next frame according to the coding sequence of the previous frame, \ufb01nally the discrimi-nator determines between the real data and the generated data, but this model is dif\ufb01cult tomodel long sequence.",
                "Nowadays,generative adversarial network (GAN) has achieved remarkable results in image generationand processing, and Yang et al.",
                "[ 21] introduces MidiNet, which combines GAN network\nand CNN network to generate popular melody.",
                "In addition to this, MuseGAN proposed by Dong et al.",
                "Based on above, SeqGAN\nproposed by Y u et al.",
                "GAN network is dif\ufb01cult to update the\ngeneration model of discrete sequence, which can be used to model the data generator as arandom policy in reinforcement learning, and to bypass the difference of the generator byimplementing updates to the gradient strategy.",
                "[24] proposes that the CycleGAN model can be applied to the style migration of symbolic\nmusic, adding another discriminator to keep the generator\u2019s structure features of the originalmusic, but the generated style is not rich enough.",
                "(2)SeqGAN [23] The novel collision of adversarial network and reinforcement learning,\nwhich is devoted to generating discontinuous sequence, such as text sequence generation.",
                "While we add musicrules as the reward function in order to generate music with speci\ufb01c styles.\n(3)VAE\u2013GAN [18] The variational auto-encoder and generative adversarial networks are\nboth the generative models based on the unsupervised learning method, which canintegrate the adversarial ideas into the variational auto-encoder.",
                "Thereby, we can trainencoder, generator and discriminator synchronously to generate images on the basisof mutual compensation between GAN and V AE.",
                "Different time steps have beenassessed: RNN +DQN, SeqGAN, V AE\u2013GAN and MCNN, and considered in the truncated\nback propagation.",
                "Execution are shownfor RNN +DQN, SeqGAN, V AE\u2013GAN and MCNN with 100 timesteps.",
                "SeqGAN and V AE\u2013GAN models shown in Fig.",
                "123A Style-Speci\ufb01c Music Composition Neural Network 1907\nTable 1 Compliance rate of generation music of 4 algorithms\nAlgorithm SeqGAN V AE\u2013GAN RNN\u2013DQN MCNN\nCompliance rate 0.6234 0.8152 0.9489 0.951\n1.",
                "In the experiment, the algorithm in this paper is compared with the results of three music\ngeneration algorithms of V AE\u2013GAN, SeqGAN and RNN\u2013DQN, as shown in Table 1.",
                "According to the experimental results, music sample generated by MCNN network has the\nhighest compliance rate, far exceeding SeqGAN and V AE\u2013GAN, slightly exceeding RNN\u2013DQN.",
                "In the experiment, as shown in Fig. 8, the ROC curves\nof V AE\u2013GAN and SeqGAN are more twisted and sharp, and relatively speaking, RNN\u2013DQNand MCNN are more smooth.",
                "On the one hand, using GAN individually on generating music creates much similarities\non the music sequence and the training data, causing a lack of diversity.",
                "On the other hand,if we use reinforcement learning individually, considering there is a bottleneck to construct\n123A Style-Speci\ufb01c Music Composition Neural Network 1909\nTable 2 Score sheet of subjective validation\nModel Melody Rhythm Harmony of chord Musical texture Emotion\nRNN\u2013DQN 6.14 5.13 6.21 6.36 6.00\nSeqGAN 6.27 7.07 7.36 6.29 6.07V AE\u2013GAN 5.71 5.93 5.60 6.14 5.27MCNN 7.86 7.73 8.00 7.93 7.00\na suitable reward function, it is hard to put it into use to generate music based musical theory\nrule.",
                "This network makes use ofthe advantages of AC reinforcement learning algorithm to select the diversity of actions, aswell as avoids the drawback of GAN network generating samples, which are too similar, andimproves the creativity of style music generation.",
                "Semi-recurrent CNN-based V AE\u2013GAN for sequential data generation.",
                "Dong HW, Hsiao WY , Yang LC, Yang YH (2018) MuseGAN: symbolic-domain music generation and\naccompaniment with multitrack sequential generative adversarial networks.",
                "SeqGAN: sequence generative adversarial nets with policy gradient.",
                "Symbolic music genre transfer with CycleGAN.",
                "Generalizing GANs: a turing perspective."
            ],
            "lstm": [
                "GoogleBrain\u2019s Magenta project [ 15] presents further optimizations based on LSTM to enhance long-\nterm associations between multiple measures; Attention RNN adds mask vectors to loop joinsto control model weights for different historical states for the effect of music generation, butthe resulting fragments are short and have a relative poor structure.",
                "[ 16]\ndesigns the LSTM network called \u201cDeepBach\u201d to solve this problem, which combines twofeed forward networks and two LSTM networks to generate music.",
                "This method adopts LSTM to learn music with different melodystructure styles, and then takes this unique and symbolic result as a conditional input fororiginal audio generators to generate a model that automatically produces new music.",
                "1.T h e\nnovelty of MCNN is making full use of the superiority of LSTM model in the aspect ofsequence prediction and actor\u2013critic (AC) network in constraints of music theory to generatehigh-quality music, with probability network [ 28,29] integrated into AC network [ 30,31].",
                "We \ufb01rst represent a generator combined actor\u2013critic (AC) network with LSTM network as aresult of music generation model with the construction and parameters.",
                "Then we formulatethe process of sequence prediction as inference with LSTM hidden layers following thegiven weight matrix in Sect. 3.1.",
                "Finally, the probability network is composed\nof CNN discriminator network and probability output, and it can return P\nDwhen generated\nsequence is true data.\n3.1 Generator\nIn this model, LSTM network can be regarded as a generator, and CNN network is regardedas a discriminator.",
                "Music sequence is generated by LSTM based on a speci\ufb01c probabilitydistribution, while CNN network as a probability discriminator is dif\ufb01cult to judge a single or\n123A Style-Speci\ufb01c Music Composition Neural Network 1897\nFig.",
                "(In this mixed network, LSTM works as a generator,\nmusic rules as reward function are added to the control network and CNN performs as a discriminator topredict the probability output called probability network.)",
                "In this paper, the generation of music sequence is realized by LSTM network [ 32,33].",
                "The LSTM network is pre-trained by inputting the\nclassical piano database in MIDI format.",
                "By\nsetting initial notes, music sequence is generated by LSTM network and labeled as 1. AfterCNN network, the output binary discriminant probability is output.",
                "The output of music rulesadjust LSTM network through weighted calculation.",
                "Generate music sequence using LSTM\nLSTM has special temporal Memory function.",
                "LSTMnetwork structure conforms to the acoustic timing [ 34\u201336], and its sequence generation and\nprediction process is as Fig.",
                "The algorithm \ufb02ow of LSTM sequence generation is as follows.",
                "Algorithm 1 Training Process of LSTM Generator\nInput: preprocessed training data are fed into the network\nOutput: model parameters of neural network\n1: Input training data, set iteration times, batch size, the number of hidden layer\u2019s cells and network layers.2:",
                "3: LOOP step 2 until the iteration is completed, the loss is converged.4: Output weight parameters of LSTM Generator.\n3.2 Control Network\n1.",
                "Algorithm 2 Actor\u2013critic network updating process\nRequire: An actor policy \u03c0(a/s,\u03b8a)and a critic Q(s,a,\u03b8v)with weights \u03b8aand\u03b8vrespectively; Pre-train\nLSTM with weight \u03b8and pre-train CNN discriminator with weight \u03d5.\nOutput: The loss function of the reward network L(\u03b8v).",
                "Actor\u2013critic rein-forcement learning algorithm is integrated into generating network to generate music withconsistent styles but diverse tunes, where, LSTM network is the generator and CNN networkis the discriminator.",
                "As illustrated in Fig. 5, the CNN discriminator [ 45,46] is trained by pos-\n123A Style-Speci\ufb01c Music Composition Neural Network 1903\nitive samples from the true data and negative samples from the fake data which is generated\nby the LSTM generator, labeled as 1 and 0 respectively.",
                "MCNN network is mainly composed of LSTM, reward functionand CNN network.",
                "LSTM network is mainly divided into three layers: input layer, hidden layer and output\nlayer, each of which contains 512 LSTM cells.",
                "In the experiment, reward function will generateaQvalue for each pair of state action (s,a), which will be applied to return the music score\nto LSTM, adjust network parameters, and optimize the probability of distribution generatedby the sequence.",
                "Experimental results show that the more cells inthe hidden layer, the stronger the ability of LSTM network to learn and extract characteristics\n123A Style-Speci\ufb01c Music Composition Neural Network 1905\nof data, and the more effectively reduce the loss between predicted value and target value.",
                "The relationship between LSTM generator andCNN discriminator enables the network to produce more appropriate samples, which is con-ducive to generate music with a speci\ufb01c topic under the multi-style topics.",
                "In MCNN model, we treat the probability\noutput of CNN discriminator as a part of the reward of RNN, which takes part in the feedbackof LSTM and contributes to the speci\ufb01c music style element in the music.",
                "Fan Y , Qian Y , Xie F, Soong FK (2014) TTS synthesis with bidirectional LSTM based recurrent neural\nnetworks."
            ]
        },
        {
            "title": "Self-Supervised Music Motion Synchronization Learning for Music-Driven Conducting Motion Generation",
            "gan": [
                "Therefore, we propose a novel Music Motion Synchronized Generative\nAdversarial Network (M2S-GAN), which generates motions according to the automatically learned music representations.",
                "More speci\fcally, M2S-GAN is a cross-modal generative network comprising four components: 1) a music encoder that\nencodes the music signal; 2) a generator that generates conducting motion from the music codes; 3) a motion encoder\nthat encodes the motion; 4) a discriminator that di\u000berentiates the real and generated motions.",
                "These four components\nrespectively imitate four key aspects of human conductors: understanding music, interpreting music, precision and elegance.",
                "Extensive experiments on ConductorMotion100 demonstrate the e\u000bectiveness of M2S-GAN.",
                "Conductors, the soul of an orchestra, perform el-\negantly and charmingly in every concert.",
                "Finally, the proposed Music\nMotion Synchronization-Based Generative Adversarial\nNetwork (M2S-GAN) is trained jointly with sync loss\nand Wasserstein distance based adversarial loss[18,19],\nsuch that both music motion synchronization and mo-\ntion realism are guaranteed.",
                "The model \frst\nlearns music and motion feature representations in a\nReal\nMotionE\nIn-Sync/\nOut-of-Sync\nEE\nDSync \nLoss\nAdversarial \nLoss\nE GGenerative Learning Stage\n(M2S-GAN)Contrastive Learning Stage\n(M2S-Net)Real\nMotion\nMusicMusic Generated\nMotion\nMotion\nz\nMotion\nGenerated\nNoise\nFig.1.",
                "Therefore, an increasing number of researchers have\nbegun to apply advanced deep generative models, par-\nticularly generative adversarial nets (GAN)[42].",
                "We\nbelieve that the objective of regression loss is in con\rict\nwith the adversarial loss: the optimal output of the re-\ngression loss is over-smoothed, while the discriminator\nof GAN can easily learn the over-smooth pattern and\nprovide con\rict gradients to the generator.",
                "The resulting conducting motion should\nbe elegant, which means it must avoid looking unnatu-\nral.",
                "3)Elegance .",
                "Subsequently, the trained Emusic andEmotion are\ntransferred to the generative learning stage, where these\ntwo encoders, together with the generator and discrim-\ninator, form the Music Motion Synchronized Genera-\ntive Adversarial Network (M2S-GAN).",
                "We use the word\n\\synchronized\" here to imply that M2S-GAN requires\na preparation stage and that the weights of the trans-\nferred encoders are frozen.",
                "In the\nfollowing generative learning stage, all four of these net-\nworks form M2S-GAN.Ggenerates a motion sequence\naccording to zand the output of Emusic.",
                "Training Procedure of M2S-Net and M2S-GAN\nInput : datasetD=f(Xi;Yi)gN\ni=1; loss function weights \u0015adv,\u0015sync,wGP;\nOutput : trained music encoder Emusic , generator G;\n/",
                "Network structure of M2S-Net and M2S-GAN.",
                "(b) M2S-GAN. Conv: convolutional layer.",
                "Note that several GAN-based\naudio-to-motion translation methods also feed the au-\ndio into the discriminator[1,37,43].",
                "Negative pairs\nare mismatched sequences, of which the details will be\nintroduced in Subsection 4.5.\nLM2S-Net\n=MX\ni;j=1cijlog2\u0010\nf[Emusic(Xi)\bEmotion (Yj)]\u0011\n+\n(1\u0000cij)log2\u0010\n1\u0000f[Emusic(Xi)\bEmotion (Yj)]\u0011\n;(1)wherecijis de\fned by\ncij=(\n1;ifi=j;\n0;otherwise:\n4.4.2 Loss Function for M2S-GAN\nIn the generative learning stage, the generator gene-\nrates the conducting motion by ^Y=G(Emusic(X);z).",
                "The third term is the gradient penalty\nterm of the Wasserstein GAN[19], where ~Yis obtained\nvia the random linear interpolation between ^YandY.\nNote that, here, GandDminimizeLGandLDre-\nspectively, but Emusic andEmotion do not participate in\nthe optimization; their weights are directly transferred\nfrom the trained M2S-Net and frozen in M2S-GAN.",
                "Chen et al.[57]added a self-supervised loss to the dis-\ncriminator of a conditional generative adversarial net-\nwork (CGAN); afterwards Hao et al.[58]extended the\nmodel to a cross-modal cycle generative adversarial net-\nwork (CMCGAN).",
                "Choi et al.[60]\frst de-\nployed a cross-modal identity matching task, and then\ntransferred the learned features to a CGAN.",
                "For its part, our proposed approach has two\nlearning stages: we \frst obtain an optimal M2S-Net,\nand then apply it to M2S-GAN.\n5.2 Sync Loss vs Perceptual Loss\nPerceptual loss[61]is a popular choice in many ill-\nposed image manipulation tasks.",
                "The third type involves using the\nfeature of a GAN's discriminator to calculate percep-\ntual loss[63].",
                "As suggested in [19], our\nM2S-GAN is trained by the RMSprop optimizer[64]with a learning rate of 0.000 5.",
                "Overall, the training of our approach (M2S-\nNet + M2S-GAN) takes approximately 48 hours.",
                "Following WGAN-\nGP[19], in each step, we train the discriminator \fve\ntimes and train the generator once.",
                "How-\never, it would not be suitable to use the sync loss as\nan evaluation metric, since our proposed M2S-GAN\ndirectly minimizes it.",
                "Note that this\napproach does not result in any data leakage, since\nEmotion trained on the testing set is not involved in the\ntraining of M2S-GAN.",
                "parallel with the generator,\neven if the generator is not trained with the GAN loss.",
                "Fixing\u0015adv= 1, we\ntest\u0015sync=f0:001, 0.01, 0.02, 0.05, 0.1, 1 gand com-\npare the performance on RDE and SCE.",
                "(GAN )[43].",
                "The beat data is modi\fed\nto sawtooth the wave-like data, as in [ 12].The performances of the above comparison meth-\nods and our proposed M2S-GAN are listed in Table 2 .",
                "As the ta-\nble shows, our M2S-GAN outperforms all comparison\nmethods on SE, RDE, SCE, and W-dis.",
                "The superior\nresults on SE, RDE, and SCE indicate that M2S-GAN\ncan model the music-motion relationships the most ac-\ncurately.",
                "M2S-GAN's advantages on SDP and W-dis\nindicate that it generates the most realistic conducting\nmotion.",
                "Notably, M2S-GAN does not achieve the low-\nest MSE.",
                "Comparatively, the motion distri-\nbutions generated by GAN-based approaches (GAN[43]\nand Our M2S-GAN) look much closer to the real mo-\ntion.",
                "Com-\nparing these two GAN-based approaches, we \fnd that\nthe motion generated by our M2S-GAN conforms far\nmore closely with the music.",
                "Performances of Music-Driven Conducting Motion Generation\nModel MSE ( \u0002103) SE W-dis ( \u0002103) RDE SCE SDP (%)\nShlizerman et al. , LSTM[32]3.50 1.301 87.470 0 0.973 9 2.511 38.98\nYelta et al. , CNN-LSTM[28]3.08 0.911 50.850 0 0.991 1 2.482 27.11\nGinosar et al. , GAN[43]6.60 1.371 29.980 0 0.943 7 2.864 97.93\nOurs, M2S-GAN 5.40 0.883 1.426 4 0.049 0 2.046 99.62\n(b) (a) (c) (d) (e)\nFig.8.",
                "(c) GAN[43], SD = 0.043 51, SDP = 97.93%.",
                "(d) M2S-GAN, SD = 0.043 51, SDP = 99.62%.",
                "In our experiment, although\nthe super-hard negatives under-perform hard negatives\nin M2S learning, the training process under super-hard\nnegatives is reasonably smooth, as shown in Fig.9.6.6 Impact of Training Set Scale\nWe next conduct another experiment to demon-\nstrate the necessity of discarding the MSE loss: we\ntrain the MSE model and our proposed M2S-GAN us-\ning di\u000berent scales of the training set.",
                "Comparatively, the SDP of our M2S-GAN (red\nlines) does not change with the training set scale, re-\nmaining stable at around 100%.",
                "Since M2S-GAN does\nnot seek to regress the ground truth motion, it has a\nmuch higher MSE than the MSE model.",
                "7\u25cbhttps://github.com/ChenDelong1999/VirtualConductor, Mar. 2022.Fan Liu et al. : Self-Supervised Music Motion Synchronization Learning 555\n0 20 120\n100\n80 \n60 \n40 \n20 \n0\n         40      60 80 100 0 20         40      60 80 100\nSDP (%) \nTraining Set Scale (h) MSE \nTraining Set Scale (h) \u03a410 -3 \n7\n6\n5\n4\n3\n2\n1\n0Ours, MSE (Testing Set)\nOurs, M2S-GAN (Testing Set)\nOurs, MSE (Training Set)\nM2S-GAN (Training Set)Ours, MSE (Testing Set)\nOurs, M2S-GAN (Testing Set)\nOurs, MSE (Training Set)\nM2S-GAN (Training Set)\nFig.10.",
                "The ConductorMotion100\ndataset enables M2S-GAN to learn rich music seman-\ntics.",
                "An M2S-Net is \frst trained\nwith a self-supervised loss, and then an M2S-GAN\nis trained with adversarial loss and a proposed sync\nloss, which measure the realism and the perceptual\nsimilarity between the real motion and the generated\nmotion respectively.",
                "the Annual Conference on Neural Informa-\ntion Processing Systems , December 2018, pp.7774-7785.[18] Arjovsky M, Chintala S, Bottou L. Wasserstein GAN.\narXiv:1701.07875, 2017.",
                "[19] Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville\nA C. Improved training of Wasserstein GANs.",
                "[58] Hao W, Zhang Z, Guan H. CMCGAN:"
            ],
            "lstm": [
                "In [26], Yelta used the convolutional neural network\n(CNN)[27]to extract audio features from the spec-\ntrums, and used a long sort-term memory (LSTM) net-\nwork to learn the temporal relationship.",
                "Models following a similar CNN-LSTM archi-\ntecture design are proposed in [3]; however, these ap-\nproaches use MIDI as input, leading to restricted appli-\ncation scenarios.",
                "Many researchers have also tried using\nfully LSTM models to map music features directly to\nmotion[29{35], but due to the transition of the adopted\nmusic features being non-smooth over time, the jitter-\ning e\u000bect of these LSTM-based models is di\u000ecult to\neliminate.",
                "Moreover, due to error accumulation, LSTM\noften outputs frozen motion during test time[2,36,37].",
                "Recently, Huang et al.[36]\nintegrated a transformer[40]-based music encoder with\na LSTM motion decoder.",
                "G\nis based on a temporal convolution network (TCN)[54],\nwhich achieves similar performance to standard LSTM\nwith a much lower computation time.",
                "(LSTM )",
                "This is an LSTM-\nbased model originally designed for translating music\nto instrument-playing motions (violin and piano).",
                "Fol-\nlowing the design in [ 32], this model consists of a single-\nlayer uni-directional LSTM with 200 hidden units and\nseveral fully-connected layers.",
                "(CNN-LSTM )",
                "It uses\na CNN to extract music features from the Mel spec-\ntrogram, and then uses an LSTM encoder-decoder to\nlearn temporal dependencies and predict motions.",
                "A\ncontrastive loss is applied to enforce the feature from\nthe LSTM encoder to be aligned with the motion.",
                "The low MSE of MSE loss-based com-\nparisons LSTM[32]and CNN-LSTM[28]is caused by low\nSDP rather than high motion realism or consistency.\nInFig.8 , we present the motion distribution gene-\nrated from the \frst movement of Beethoven's Sym-\nphony No. 5 in C minor, Op. 67 for qualitative compa-\nrison.",
                "Performances of Music-Driven Conducting Motion Generation\nModel MSE ( \u0002103) SE W-dis ( \u0002103) RDE SCE SDP (%)\nShlizerman et al. , LSTM[32]3.50 1.301 87.470 0 0.973 9 2.511 38.98\nYelta et al. , CNN-LSTM[28]3.08 0.911 50.850 0 0.991 1 2.482 27.11\nGinosar et al. , GAN[43]6.60 1.371 29.980 0 0.943 7 2.864 97.93\nOurs, M2S-GAN 5.40 0.883 1.426 4 0.049 0 2.046 99.62\n(b) (a) (c) (d) (e)\nFig.8.",
                "(a) LSTM[32], SD = 0.017 32, SDP = 36.98%.",
                "(b) CNN-\nLSTM[28], SD = 0.011 06, SDP = 27.11%.",
                "[29] Tang T, Jia J, Mao H. Dance with melody: An\nLSTM-autoencoder approach to music-oriented dance\nsynthesis.",
                "[33] Haag K, Shimodaira H. Bidirectional LSTM networks em-\nploying stacked bottleneck features for expressive speech-\ndriven head motion synthesis."
            ]
        },
        {
            "title": "Integration of a music generator and a song lyrics generator to create Spanish popular songs",
            "gan": [
                "4.1.1  Phrase breakerIn PoeTryMe, a generation strategy selects and organizes \nlines, produced by the Line Generator module, according to a given form.",
                "Michigan Publishing\nCollins T, Laney R, Willis A, Garthwaite PH (2016)"
            ],
            "lstm": [
                "There are some works that make use of neural networks for generat-ing melodies from lyrics in Chinese Bao et\u00a0al. (2019), learn higher-level rhyming constraints with generative adversarial networks Jhamtani et\u00a0al. (2019) or generate song lyrics in Korean, with a neural network with a LSTM for maintain-ing context Son et\u00a0al."
            ]
        },
        {
            "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
            "gan": [
                "Awavegenerative\nadversarialnetwork(WaveGAN)architectureisalsoemployedtogenerateaudiofilesfordataaugmentation.",
                "3 Weexplorethetime-domainstrategyofsynthetic\nmusicaudiogenerationfordataaugmentationusing\nWaveGAN.Theproposedtaskisaddressedwithand\nwithoutdataaugmentation.",
                "Asapartofdataaug-\nmentation, additional training files are generated using\nWaveGAN (Fig.",
                "Fig.1Visualrepresentationofanaudioexcerptwithacousticguitarasleading,UpperpanerepresentstheMel-spectrogram,modgdgram,and\ntempogramoftheoriginalaudiofileandlowerpanerepresentstheWaveGANgeneratedfilesReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page4of14\n3.1.2 Modifiedgroupdelayfunctionsandmodgdgram\nGroup delay features are being employed in numerous\nspeech and music processing applications [ 18],[33].",
                "Patch merging works in a similarway to CNN\u2019s pooling layer by concatenating the features\nof each group of neighboring patches and applying a lin-\near embedding layer to change the output dimension to\n2C. Hence the output of patch merging layer is (H\n8xW\n8x\n2C)andisfollowedbyglobalaveragepoolingandadense\nlayer with 11 nodes and a softmax activation function.",
                "The classes include\ncello(Cel),clarinet(Cla),flute(Flu),acousticguitar(Gac),\nelectric guitar (Gel), organ (Org), piano (Pia), saxophone\n(Sax), trumpet (Tru), violin (Vio), and human singing\nvoice (Voice).",
                "5.2 DataaugmentationusingWaveGAN\nGenerative adversarial networks (GAN) have been suc-\ncessfully applied to a variety of problems in image gen-\neration [41] and style transfer [ 42].",
                "WaveGAN architec-\nture is similar to deep convolutional GAN (DCGAN),\nwhich is used for Mel-spectrogram generation in vari-\nous music processing applications.",
                "The DCGAN gener-\nator uses transposed convolution to iteratively upsample\nlow-resolutionfeaturemapsintoahigh-resolutionimage.",
                "In WaveGAN architecture, the transposed convolution\noperation is modified to widen its receptive field.",
                "Thediscriminatorisalsomodifiedsimilarly,usinglength-\n25 filters in one dimension and increasing stride from\ntwo to four which results in WaveGAN architecture [ 43].The transposed convolution in the generator produces\ncheckerboard artifacts [ 43].",
                "Finally,thesystemistrainedusingtheWasser-\nstein GAN with gradient penalty (WGAN-GP) strategy",
                "For training, the WaveGAN optimizes\nWGAN-GP using Adam for both generator and discrim-\ninator.",
                "WaveGAN is trained for 2000 epochs on the three-sec\naudio files of each class to generate similar audio files\nbased on a similarity metric ( s)[45] with an acceptance\ncriterion of s>0.1.",
                "The values of parameters and hyper-\nparameters associated with WaveGAN for our experi-\nments are listed in Table 2.",
                "A total of 6585 audio files\nwith cello (625),clarinet (482),flute (433),acoustic guitar\n(594), electric guitar (732), organ (657), piano (698), sax-\nophone(597),trumpet(521),violin(526),andvoice(720)\nare generated.",
                "This value is comparable to the MOS\nscoreobtainedin[ 43]and[46]usingW aveGAN.\n5.3 Experimentalset-up\nTheexperimentisprogressedinfourphases,namelyMel-\nspectrogram-based, modgdgram-based, and tempogram-\nTable2VarioushyperparameterschosenforWaveGAN\nName Value\nWavGANLatentdimension 100\nNumberofchannels 1\nWavGANdimension 32\nTrainingbatchsize 64\nKernellength 25\nGenerationlength 65,536samples\nLoss WGAN-GP( \u03bb=10)\nDupdatesperGupdates 5ReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page8of14\nbased, followed by soft voting.",
                "Whilethe\nHanmodelreportsamacro-F1scoreof0.55,ourproposed\nModgdgram-Swin-Tgives0.51.Inthecaseofmodgdgram\nprocessing, instruments like the electric guitar, organ,\nsaxophone, trumpet, and violin show enhanced perfor-\nmance over the Mel-spectrogram-Swin-T. They showed\nimproved performance for four instruments than Han\u2019s\nmodel.",
                "6.4 Effectofvotingandablationstudyofensemble\nSeveral studies [ 48,49] have demonstrated that by con-\nsolidating information from multiple sources, better per-\nformance can be achieved compared to uni-modal sys-\ntems which motivated us to perform the ensemble voting\nmethod.",
                "The time-domain strat-\negy of synthetic music generation for data augmentation\nusing WaveGAN is explored.",
                "WaveGAN data augmenta-\ntion for instrument detection is probably a new attempt\nin predominant instrument recognition.",
                "K.Racharla,V.Kumar,C.B.Jayant,A.Khairkar,P.Harish,in Proc.of7th\nInternationalConferenceonSignalProcessingandIntegratedNetworks\n(SPIN),Noida,India .Predominantmusicalinstrumentclassificationbased\nonspectralfeatures,(2020),pp.617\u2013622.",
                "M.D.Zeiler,R.Fergus,in Proc.ofEuropeanconferenceoncomputervision\n(ECCV).Tvisualizingandunderstandingconvolutionalnetworks(Springer\nInternationalPublishing,Switzerland,2014),pp.818\u20138331\n40.",
                "I.Gulrajani,F.Ahmed,M.Arjovsky,V.Dumoulin,A.Courville,in Proc.ofthe\n31stInternationalConferenceonNeuralInformationProcessingSystems,\nLongBeachCaliforniaUSADecember4-9,2017 .Improvedtrainingof\nwassersteinGANs(CurranAssociatesInc.,MorehouseLaneRedHookNY,\n2017)\n45.",
                "J.Kong,J.Kim,J.Bae,in Proc.of34thConferenceonNeuralInformation\nProcessingSystems(NeurIPS2020),Vancouver,Canada .HiFi-GAN:\nGenerativeAdversarialNetworksforEfficientandHighFidelitySpeech\nSynthesis,vol.33(CurranAssociates,Inc,2020),pp.17022\u201317033\nPublisher\u2019sNote\nSpringerNatureremainsneutralwithregardtojurisdictionalclaimsin\npublishedmapsandinstitutionalaffiliations."
            ],
            "lstm": []
        },
        {
            "title": "Genre Recognition from Symbolic Music with CNNs: Performance and Explainability",
            "gan": [
                "The abundant availability of MIDI files online, from mul-\ntiple sources, has given rise to the challenge of automati-\ncally organizing such large collections of MIDI files.",
                "One \ncriterion for organization is music genre, among others such \nas music style, similarity, and emotion.",
                "Musegan: Multi-track \nsequential generative adversarial networks for symbolic music \ngeneration and accompaniment."
            ],
            "lstm": [
                "Wu Y, Li W. Automatic audio chord recognition with midi-trained \ndeep feature and blstm-crf sequence decoding model."
            ]
        },
        {
            "title": "CycleDRUMS: automatic drum arrangement for bass lines using CycleGAN",
            "gan": [
                "Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7\n1 3Discover Artificial IntelligenceResearch\nCycleDRUMS: automatic drum arrangement for\u00a0bass lines using \nCycleGAN\nGiorgio\u00a0Barnab\u00f21\u00a0\u00b7 Giovanni\u00a0Trappolini1\u00a0\u00b7 Lorenzo\u00a0Lastilla1\u00a0\u00b7 Cesare\u00a0Campagnano2\u00a0\u00b7 Angela\u00a0Fan3\u00a0\u00b7 Fabio\u00a0Petroni3\u00a0\u00b7 \nFabrizio\u00a0Silvestri1\nReceived: 12 September 2022 / Accepted: 4 January 2023",
                "We formulated this task as an unpaired \nimage-to-image translation problem, and we addressed it with CycleGAN, a well-established unsupervised style transfer \nframework designed initially for treating images.",
                "Keywords Automatic music arrangement\u00a0\u00b7 Cycle-GAN\u00a0\u00b7 Deep learning\u00a0\u00b7 Source separation\u00a0\u00b7 Audio and speech \nprocessing\n1 Introduction\nThe development of home music production has brought significant innovations into the process of pop music composi-\ntion.",
                "To solve this task, we tested an unpaired image-to-image translation strategy known as CycleGAN [7 ].",
                "In particular, we \ntrained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset [8 ] and the musdb18 dataset [9 ].",
                "To sum up, our main contributions are the following:\n\u2022 we trained a CycleGAN architecture on bass and drum mel-spectrograms in order to automatically generate drums \nthat follow the beat and sound credible for any given bass line;\n\u2022 our approach can generate drum arrangements with low computational resources and limited inference time, if \ncompared to other popular solutions for automatic music generation [12];\n\u2022 we developed a metric\u2014partially based on or correlated to human (and expert) judgment\u2014to automatically evaluate \nthe obtained results and the creativity of the proposed system, given the challenges of a quantitative assessment of \nmusic;\n\u2022 we compared our method to Pix2Pix, another popular image transfer network, showing that the music arrangement \nproblem can be better tackled with an unpaired approach and adding a cycle-consistency loss.",
                "It is worth recalling that the application of GANs to music generation tasks is not \nnew: in [45], GANs are applied to symbolic music to perform music genre transfer, while in [46, 47], authors construct \nand deploy an adversary of deep learning systems applied to music content analysis; however, to the best of our knowl-\nedge, GANs have never been applied to raw audio in the mel-frequency domain for music generation purposes.",
                "[48], and (ii) the definition of an objective \nmetric and loss is a common problem to generative models such as GANs: as of now, generative models in the music \ndomain are evaluated based on the subjective response of a pool of listeners, because an objective metric for the raw \naudio representation has never been proposed so far.",
                "3.3  Image to\u00a0image translation\u2014CycleGAN\nWe cast the automatic drum arrangement generation task as an unpaired image-to-image translation task, and we \nsolved it by adapting the CycleGAN model to our purpose.",
                "CycleGAN is a framework designed to translate between \ndomains with unpaired input\u2013output examples.",
                "This property is achieved by training both the mapping G  and F  simultaneously \nwith a \u201cstandard\u201d GAN loss of the form\nFig. 1  To solve the automatic drum arrangement task, we tested an unpaired image-to-image translation strategy known as CycleGAN [7].",
                "In particular, we trained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset",
                "For the discriminator networks, we use PathcGANs [ 11, 57, 58], which aim to classify \nwhether overlapping image patches are real or fakes.",
                "Since the CycleGAN model takes 256\u00d7256 images as input, each mel-spectrogram is chunked into smaller pieces \nwith an overlapping window of 50 time frames, obtaining multiple samples from each song (each equivalent to 5 s of \nmusic); finally, in order to obtain one channel images from the original spectrograms, we performed a discretization \nstep in the range [0\u2013255].",
                "In the final stage of our pipeline, we fed CycleGAN architecture with the obtained dataset.",
                "As previously anticipated, this task is an appropriate first step toward fully automated LGAN(G,DY,X,Y)=/u1D53Cy\u223cpdata(y)[logDY(y)]\n+/u1D53Cx\u223cpdata(x)[log(1 \u2212DY(G(x)))],\nLcyc(G,F)=/u1D53Cx\u223cpdata(x)[\u2016F(G(x))",
                "L(G,F,DX,DY)=LGAN(G,DY,X,Y)",
                "+LGAN(F ,DX,Y,X)+/u1D706Lcyc(G,F).",
                "Fig. 2  CycleGAN is a framework designed to translate between domains with unpaired input\u2013output examples.",
                "The \nhop size, 256, was chosen according to recommendations from [35].\n4.2  Training of\u00a0the\u00a0CycleGAN model",
                "As to the CycleGAN model used for training, we relied on the default network5.",
                "As a result, the model uses a resnet_9blocks ResNet generator and a basic 70 \u00d7 70 PatchGAN as a discriminator.",
                "Cycle GAN- and-",
                "FID-based features: in the context of GANs result evaluation, the Fr\u00e9chet Inception distance (FID) is supposed to \nimprove on the Inception Score by actually comparing the statistics of generated samples to authentic samples [63].",
                "Unlike CycleGAN, Pix2Pix learns to translate between domains when \nfed with paired input\u2013output examples.",
                "We then asked the same four evaluators to grade the new drum samples according to the \nprinciples presented in Sect.\u00a0 4.4.\n4.6  Experimental results\nFigure\u00a0 3 shows the distribution of grades for the 400 test drums for both CycleGAN and Pix2Pix\u2014averaged among all \nfour independent evaluators and over all four dimensions.",
                "Given this pretty good result, we could then use this trained logistic model to label 14,000 different 5s fake drum clips \nproduced from as many real bass lines using both CycleGAN and Pix2Pix.",
                "Cycle GAN- and-",
                "We applied \nCycleGAN to real bass lines, treated as gray-scale images (mel-spectrograms), obtaining good ratings, especially com-\npared to another image-to-image translation approach (Pix2pix).",
                "It is not unlikely that in the future, artists and composers will \nstart creating their music almost like they were drawing.\nFig. 3  (left) The distribution of grades for the 400 test drums for both CycleGAN and Pix2Pix (baseline)\u2014averaged among all four independ-\nent evaluators and over all four dimensions.",
                "Given this pretty good result, we could then use this trained logistic \nmodel to label 14,000 different 5s fake drum clips produced from as many real bass lines using both CycleGAN and Pix2Pix (baseline).",
                "The patch size refers to the pixel dimensions of the patches considered for the Patch-\nGAN, that is used as the building block for the discriminator.",
                "Mogren O. C-RNN-GAN: continuous recurrent neural networks with adversarial training.",
                "Brunner G, Wang Y, Wattenhofer R, Zhao S. Symbolic music genre transfer with cyclegan.",
                "Nistal J, Lattner S, Richard G. Drumgan: synthesis of drum sounds with timbral feature conditioning using generative adversarial networks.",
                "San Mateo: Morgan Kaufmann Publishers; 2018.",
                "53. Logan B, Robinson T. Adaptive model-based speech enhancement.",
                "Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S. Gans trained by a two time-scale update rule converge to a local nash \nequilibrium.",
                "San Mateo: Morgan Kaufmann Publishers; 2017.",
                "Spa-gan: spatial attention gan for image-to-image translation."
            ],
            "lstm": [
                "In [13], CNNs are used for generating melody as a series of MIDI notes either from scratch, by following a chord sequence, \nor by conditioning on the melody of previous bars, whereas in [14\u2013 17] LSTMs are used to generate musical notes, melo -\ndies, polyphonic music pieces, and long drum sequences under constraints imposed by metrical rhythm information and Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7 \n Research\n1 3\na given bass sequence.",
                "[38] tested a model for unconditional audio synthesis based on generating one audio sample at a time, and \n[39] applied Restricted Boltzmann Machine and LSTM architectures to raw audio files in the frequency domain in order \nto generate music.",
                "It features a U-NET encoder\u2013decoder architecture with a bidirectional LSTM as hidden layer.",
                "LSTM based music generation system.",
                "Combining LSTM and feed forward neural networks for conditional rhythm \ncomposition."
            ]
        }
    ]
}