{
    "data_collection": [
        {
            "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
            "RNN": [
                "MetNet combines an RNN-\nbased autoregressive model with a multi-scale generation\nprocedure to generate STFT amplitude spectrograms which\ncapture both the local as well as more long-range structures\nof spoken language and music."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [
                "Us-\ning Variational Autoencoder (V AE) based model, Jukebox\n(Dhariwal et al., 2020) produces impressive quasi-realistic\nmulti-minute songs primed on music genre, artist and lyrics."
            ],
            "GAN": [
                "MP3net: coherent, minute-long music generation from raw audio\nwith a simple convolutional GAN\nKorneel van den Broek1\nAbstract\nWe present a deep convolutional GAN which\nleverages techniques from MP3/V orbis audio com-\npression to produce long, high-quality audio sam-\nples with long-range coherence.",
                "[cs.SD]  12 Jan 2021MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nFigure 1.",
                "In this work, we use the MDCT amplitude as the data rep-\nresentation for raw audio in a deep 2D convolutional Gen-\nerative Adverserial Network (GAN).",
                "The architecture of our network is in-\nspired by the ProGAN model (Karras et al., 2018) with the\nnoticeable difference that we don\u2019t increase/decrease the\npixel density along the frequency axis with each successive\nmodel-block in the generator/discriminator.",
                "The discretized window function wnsatis\ufb01es\nwn=w2N\u00001\u00001 (4)\nw2\nn+w2\nn+N= 1 (5)MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nThese latter conditions ensure that the MDCT transforma-\ntion is invertible.",
                "Experimentally, these frequency ranges have been estab-\nlished as critical bands or Bark bands, with fjthe mid pointMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nof each band.",
                "As studied by Arjovsky & Bottou (2017) and Jenni\n& Favaro (2019), this process improves the stability of the\nGAN training process, since it smooths the distribution of\nboth the true and generated distributions, extending the sup-\nport of both distributions.",
                "2D convolutions to up- and downscale the MDCT\namplitude representation\nOur network architecture is based on the architecture of\nthe ProGAN network (Karras et al., 2018) with successive\nmodel blocks which scale up/down the image using strided\n2D convolutions in the generator and discriminator respec-\ntively.",
                "We\nalso note that while the number of \ufb01lter bands Nhas notMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nchanged, the corresponding frequencies of the \ufb01lter bands\nhave doubled to:\n~f0\nk=(2fs)\n2Nkwithk= 0;:::;N\u00001 (19)",
                "The\nmodel is a WGAN with gradient penalty.",
                "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nFigure 4.",
                "This model con-\n\ufb01guration has 64 million parameter for both the generator\n1Depending on the model type and implementation details 1\nto 3 NVIDIA V100s are roughly equivalent to 1 Cloud TPUv2,\nwhich has 8 cores (Wang et al., 2019)MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nFigure 5.",
                "This latter isMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nto be expected since \u00184%of the training dataset contains\natonal pieces (mostly from Alexander Scriabin).",
                "One of these techniques, Generative\nAdversarial Networks (GAN), uses two competing neural\nnetworks to generate a distribution of generated samples\nwhich closely approximates the true distribution of real sam-\nples.",
                "A lot of research in recent years has contributed to improv-\ning the stability of the training process of GANs.",
                "Arjovsky\net al. (2017) introduced the WGAN model with a loss func-\ntion based on the Wasserstein distance between distributions.",
                "Gulrajani et al. (2017) introduced a further key im-\nprovement to the WGAN technique by replacing the weight\nclipping with a gradient penalty in the loss function.",
                "Multi-\nple other GAN \ufb02avors exist with different loss functions and\nother features to stabilize the training process (Roth et al.,\n2017; Lim & Ye, 2017; Mescheder et al., 2018).",
                "Recent\nresearch on GAN stability has identi\ufb01ed the key role played\nby the singular values of the network kernels (Sedghi et al.,\n2019).",
                "GAN-based models have produced impressive results in the\n\ufb01eld of image generation.",
                "Karras et al. (2018) introduced\nthe ProGAN model.",
                "The StyleGAN model (Karras et al., 2019), is a\nfurther modi\ufb01cation to the ProGAN model allowing one to\ntune the style of the generated image at each level of detail\ngoing from \ufb01ne-grained features over middle-level styles,\nsuch as eyes, hair and lighting of the picture to high-level\nstyles like hair style and face shape.",
                "The SAGAN model\n(Zhang et al., 2019) uses a self-attention layer to increase the\nlong-range coherence of convolutional neural nets and boost\nmodel performance for images which contain geometric\nstructures.",
                "Brock et al. (2019)\nexplore these issues and give an extensive hyper-parameter\nscan for hinge-loss based model such as SAGAN.\nPart of the research into audio generation has focus on sym-\nbolic music generation such as in Hadjeres et al.",
                "GAN-based models on raw audio were introduced in Wave-\nGAN (Donahue et al., 2019).",
                "Most closely related to the work presented here is GAN-\nsynth (Engel et al., 2019) and MelNet (Vasquez & Lewis,\n2019).",
                "GANsynth produces the STFT spectrograms for 4s\naudio samples.",
                "The GANsynth architecture is based on Pro-\nGAN.",
                "The resulting GANsynth audio samples of musical\nnotes from different instruments are consistently judged by\nhuman evaluators of better \ufb01delity compared to the similar\nsamples generated by WaveNet.",
                "This training process is much faster\nsince the memory footprint of the features in the deeperMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nFigure 6.",
                "In the\ncase of images, the authors of ProGAN take blurred versions\nof the real images and present these when training the deep\nlayers.",
                "While the MP3net architecture is similar in many\nrespects to ProGAN, the characteristics of the data represen-\ntation is rather different.",
                "Another approach to controlling the gener-\nated samples is to upgrade the network structure in line with\nStyleGAN (Karras et al., 2019).",
                "7. Conclusion\nIn this paper, we introduce MP3net, a 2D convolutional\nGAN with an architecture similar to some of the most suc-\ncessful image generation GANs (Karras et al., 2018; Zhang\net al., 2019; Karras et al., 2019).",
                "Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein gan,\n2017.MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nBrandenburg, K. Mp3 and aac explained.",
                "Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A.\nSelf-attention generative adversarial networks, 2019.MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nZ\u00f6lzer, U. Digital audio signal processing."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "The dimensions of our model was\nmemory-bound by the 8GiB HBM of each TPUv2 core."
            ],
            "Hopfield Network": []
        },
        {
            "title": "Hierarchical Recurrent Neural Networks for Conditional Melody Generation with Long-term Structure",
            "RNN": [
                "In this work,\nwe propose CM-HRNN: a conditional melody generation model\nbased on a hierarchical recurrent neural network.",
                "We also\ncompared the system with the state-of-the-art AttentionRNN",
                "The comparison shows that melodies generated by CM-HRNN\ncontain more repeated patterns (i.e., higher compression ratio)\nand a lower tonal tension (i.e., more tonally concise).",
                "Results\nfrom our listening test indicate that CM-HRNN outperforms\nAttentionRNN in terms of long-term structure and overall rating.",
                "Index Terms \u2014Hierarchical RNN, Recurrent neural network,\nGenerative model, Conditional model, Music generation, Event-\nbased representation, Structure\nI. I NTRODUCTION",
                "Since music data is sequential, researchers have recently\nfocused on developing recurrent neural networks (RNN) and\ntheir variants (e.g., long-short term memory (LSTM) / gated\nrecurrent units (GRU)) for music generation due to theirarXiv:2102.09794v2",
                "By\ninjecting a hierarchical structure between the RNN layers of\nour proposed model, we allow the model to capture musical\npatterns on different time scales.",
                "In this work, we propose\nCM-HRNN, a conditional melody generation model based on\na hierarchical recurrent neural network.",
                "In Section III and\nIV, we describe the proposed event representation and CM-\nHRNN architecture in detail.",
                "We then thoroughly analyze the\nmusic generated by CM-HRNN in terms of musical quality\nand the presence of repeated patterns in Section V.",
                "Two\nrecent RNN-based systems, LookbackRNN and AttentionRNN\n[1], aim to generate monophonic melodies with long term\nstructure in an autoagressive way.",
                "LookbackRNN",
                "Similar to LookbackRNN,\nAttentionRNN",
                "The input music data is \ufb01rst encoded by\na bidirectional RNN encoder which generates a latent vector\nz.",
                "The latent vector zwill then be fed as a initial state to an\nRNN which generates a series of \u201cconductors\u201d which will then\n\u201cconduct\u201d the hierarchical RNN decoder to generate music\nsequences.",
                "Hierarchical RNN (HRNN)",
                "Beat\npro\ufb01le generation is directly conditioned by the latent output\nfrom the upper RNN layer which generates the bar pro\ufb01les.",
                "By combining the generated pitch and rhythmic pattern, the\ngenerated monophonic melodies are qualitatively evaluated\nthrough multiple listening tests and are shown to outperform\nthe LookbackRNN",
                "In this paper, we propose an architecture similar to HRNN\nbut with different design motivations.",
                "Hence, instead of\nconditioning the pitch generation with the generated rhythmic\npatterns, we apply the hierarchical RNN structure to both\nrhythmic and pitch latent spaces simultaneously.",
                "Compared\nto HRNN, our proposed CM-HRNN is able to condition the\nmelody generation with the provided chords.",
                "Moreover, the\nmusical quality of HRNN-generated music is only evaluated\nthrough a subjective listening test.",
                "In contrast, we evaluate\nour proposed CM-HRNN with extensive analytical measures\nas well as a user study.",
                "B. Hierarchical architectures for long term dependencies\nDrawing inspiration from other \ufb01elds in which long-term\ndependencies of sequential data have been modelled by RNNs,\nwe \ufb01nd that multiple hierarchical architectures have been\nproposed for this challenge [20], [21], [26]\u2013[28].",
                "These models\ntry to control the weight update rate within the RNN cells.",
                "RNN weights matrices are separated into different chunks and\nare updated using different rates: high update rates capture\nthe short-term dependencies whereas low update rates capture\nlong-term dependencies.",
                "For instance, SampleRNN",
                "[7] is able to synthesise\nrealistic sounding audio with a hierarchical RNN structure.",
                "Instead of manipulating the RNN weights update rate to\ncapture the temporal and long-term structure of sequences of\naudio samples, it uses multiple stacks of RNN with upper tier\nRNN layers operating on more grouped audio samples per\nstep and lower tier RNN layers operating on fewer grouped\nsamples per step.",
                "Inspired by SampleRNN, we group our\ndata (i.e., music events) in different resolutions and process\nthese groups of data separately to obtain the coarse-to-\ufb01ne\nhierarchical features of the data.",
                "Hence, using multi-hot encoded\ndata enables RNNs to learn different high-level features and\npatterns separately.",
                "P ROPOSED MODEL : CM-HRNN\nA. Model Architecture\nOur proposed CM-HRNN architecture consists of multiple\nhierarchical tiers (see Fig. 3).",
                "Inspired by [7], the CM-HRNN\u2019s bottom\ntier uses a conv1d operation to process overlapping sliding\nwindows of ntier 1events ( 1indicates the bottom tier) and\ngenerate the next event.",
                "In this paper, we\nfocus on two variants of the proposed architecture: a 2-tier\nCM-HRNN and a 3-tier CM-HRNN.",
                "The architecture of the\nproposed 3-tier CM-HRNN is shown in Fig.",
                "By removing\nthe top tier of the 3-tier CM-HRNN, we obtain the architecture\nof the 2-tier CM-HRNN.",
                "In the 3-tier CM-HRNN, residual connections exist between\nthe top tier and the bottom tier to alleviate the effect of the\nvanishing gradient.",
                "3: Our proposed CM-HRNN architecture.",
                "Fig. 4: Details of the predictive network within CM-HRNN.\nFC(pitch) represents 2 fully connected layers (130 nodes each).",
                "V. E XPERIMENTS\nA. Experimental setup\nWe set up several experiments to determine the optimal\nnetwork architecture; evaluate the model\u2019s ability to generate\nhigh-quality music with structure, and to compare it with a\nstate-of-the-art system, AttentionRNN.",
                "Next, we tested the in\ufb02uence of the residual\nconnection and the frame size FS2in the 3-tier CM-HRNN,\non the model\u2019s ability to include repeated patterns in the\ngenerated music, as well as keep track of the musical meter.",
                "Finally, the best 2- and 3-tier CM-HRNN are compared to\nAttentionRNN, both analytically, as well as through a listening\ntest.",
                "While it is not always easy to compare different music\ngeneration systems, given that they often have slightly different\nfunctional tasks or input representations, a comparison with\nAttentionRNN was facilitated through their use of multi-hot\nencoded input data which is similar to our data representa-\ntion.",
                "Each participant was asked to\nlisten to 12 musical fragments ranging from 13 to 44 seconds\ngenerated by 3 models: 2-tier CM-HRNN, 3-tier CM-HRNN,\nand AttentionRNN (with an attention size of 32).",
                "Model FS2FS3acc t SBR CPR\n2-tier CM-HRNN 16 n.a.",
                "no 91.0% 1.61\n2-tier CM-HRNN 16 n.a.",
                "yes 96.1% 1.69\n3-tier CM-HRNN 2 16 no 66.2% 1.62\n3-tier CM-HRNN 2 16",
                "time information; SBR: successful bar ratio; CPR: compression ratio\nB. Residual connections and FS2in 3-tier CM-HRNN\nTo validate the effectiveness of adding residual connections\nin the 3-tier CM-HRNN, we implemented six model variants,\nsome with and without residual connections.",
                "FS2FS3Residual connections SBR CPR\n2 16 yes 100.0% 1.68\n2 16 no 100.0% 1.67\n4 16 yes 100.0% 1.64\n4 16 no 100.0% 1.63\n8 16 yes 99.2% 1.65\n8 16 no 98.7% 1.63\nSBR: successful bar ratio; CPR: compression ratio\nC. Comparison with AttentionRNN",
                "We compared our best performing model with a state-of-the-\nart model AttentionRNN",
                "We implemented two AttentionRNN models with the same\nLSTM setting: 2 layers (each with 256 nodes).",
                "A comparison between the calculated measures for gen-\nerated pieces by both our proposed CM-HRNN and At-\ntentionRNN is shown in Table IV.",
                "From these results, we\nsee that music generated by CM-HRNN has a much higher\ncompression ratio compared to AttentionRNN.",
                "The tension\nvalues indicate that music generated by AttentionRNN sounds\nmore tense and dissonant, with less tonality movement.",
                "TABLE IV: Analytical measures for both CM-HRNN and\nAttentionRNN.",
                "Meter Structure Tension\nModel SBR CPR CD TS CM\nCM-HRNN(3t) 100.0% 1.68 2.16 \u00060.47 0.54\u00060.20",
                "0.69 \u00060.21\nCM-HRNN(2t) 96.1% 1.69 2.17\u00060.47 0.53\u00060.20",
                "0.70 \u00060.21\nAttentionRNN(16) 88.2% 1.58 2.22 \u00060.47 0.68 \u00060.20 0.52",
                "\u00060.20\nAttentionRNN(32) 90.0% 1.58 2.21 \u00060.47 0.68 \u00060.20",
                "cloud momentum\n2https://github.com/guozixunnicolas/CM-HRNND.",
                "The resulting ratings in Table V show that our proposed\nCM-HRNN outperforms AttentionRNN in terms of overall\nenjoyment rating and long-term coherence, which again proves\nthe effectiveness of our proposed model in capturing the long-\nterm structure of music data.",
                "Listening test rating results on a scale from 1 to 5.\nOverall Pitch Duration\nModel Rating Coherence Consonance naturalness naturalness\nCM-HRNN (3t) 3.42 3.32 3.52 3.55 3.39\nCM-HRNN (2t) 3.35 3.24 3.49 3.52 3.40\nAttentionRNN (32) 2.80 2.85 2.95 2.98 2.79\nVII.",
                "We propose a novel conditional hierarchical RNN network,\nCM-HRNN, to generate melodies conditioned with chords.",
                "In\naddition to using a novel, effective event-based representation\nthat explicitly encodes bar information, CM-HRNN generates\nmusically sound melodies that contain long-term structure.",
                "The TensorFlow implementation of the CM-HRNN imple-\nmentation is available online2In extensive experiments, both\nby using calculated features as well as a listening test, we\nshow that pieces generated by CM-HRNN have greater tonal\nstability and more repeated patterns than those generated by\nAttenionRNN.",
                "[28] M. Huzaifah and L. Wyse, \u201cMTCRNN: A multi-scale rnn for directed\naudio texture synthesis,\u201d arXiv:2011.12596 , 2020."
            ],
            "Recurrent Neural Network": [
                "Hierarchical Recurrent Neural Networks for\nConditional Melody Generation with Long-term\nStructure\nGuo Zixun\nInformation Systems,\nTechnology, and Design\nSingapore University\nof Technology and Design\nSingapore\nnicolas guo@sutd.edu.sgDimos Makris\nInformation Systems,\nTechnology, and Design\nSingapore University\nof Technology and Design\nSingapore\ndimosthenis makris@sutd.edu.sgDorien Herremans\nInformation Systems,\nTechnology, and Design\nSingapore University\nof Technology and Design\nSingapore\ndorien herremans@sutd.edu.sg\nAbstract \u2014The rise of deep learning technologies has quickly\nadvanced many \ufb01elds, including generative music systems."
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "Similarly, [25] train a\nconvolutional restricted Boltzmann machine (C-RBM), but use\nsimulated annealing as a sampling technique."
            ],
            "Hopfield Network": []
        },
        {
            "title": "LEARNING TO GENERATE MUSIC WITH SENTIMENT",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [
                "Hierarchical Variational Autoencoders for Music ,\n2017."
            ],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Personalized Popular Music Generation Using Imitation and Structure",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [
                "Recently, representation\nlearning has motivated the practice of music style representation and disentanglement using\nVariational Auto-Encoders (VAEs) (Kawai, Esling, & Harada, 2020; Roberts, Engel, Ra\u000bel,\nHawthorne, & Eck, 2018; Yang et al., 2019) and Generative Adversarial Networks (GANs)\n(Yu, Zhang, Wang, & Yu, 2017).",
                "These models are more controllable than previous deep\nlearning approaches, e.g. VAEs can be used to interpolate between phrases or explore near\nneighbors along di\u000berent learned dimensions."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "Recently, representation\nlearning has motivated the practice of music style representation and disentanglement using\nVariational Auto-Encoders (VAEs) (Kawai, Esling, & Harada, 2020; Roberts, Engel, Ra\u000bel,\nHawthorne, & Eck, 2018; Yang et al., 2019) and Generative Adversarial Networks (GANs)\n(Yu, Zhang, Wang, & Yu, 2017)."
            ],
            "Generative Adversarial Network": [
                "Recently, representation\nlearning has motivated the practice of music style representation and disentanglement using\nVariational Auto-Encoders (VAEs) (Kawai, Esling, & Harada, 2020; Roberts, Engel, Ra\u000bel,\nHawthorne, & Eck, 2018; Yang et al., 2019) and Generative Adversarial Networks (GANs)\n(Yu, Zhang, Wang, & Yu, 2017)."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network",
            "RNN": [
                "Mogren [42] proposed an RNN GAN for generating single voice polyphonic music.",
                "[39] Generate melodies based on lyrics Conditional GAN Cross entropy LSTM generators and discriminators No evaluation was provided  [41] Generate melodies based on lyrics Conditional LSTM GAN Cross entropy Dense layer followed by 2 LSTM followed by a dense layer for generator, 2 LSTM followed by dense for discriminator BLEU-2 score of 0.735, scores of about 3.8, 3.5, 4.1 respectively out of 5 for lyrics, rhythm, and melody by evaluators [42] Generate single voice polyphonic music RNN GAN Cross entropy and Squared error loss 2 LSTM layers for generator, 2 Bi-directional LSTM layers followed by a dense for discriminator No evaluation was provided  [43] Generate multi-track, polyphonic music Conditional GAN Wasserstein Generators contain 1D transposed CONV, discriminators 5 contain 1D Conv layers followed by one dense layer The highest score for conditional generation was 3.1 and non-conditional was 3.16 out of 5 by \u2018non-pro\u2019 evaluators.",
                "For intra-track metrics, jamming model performed best [44] Generate folk music RL GAN Cross entropy and policy gradient RNN generators, CNN discriminators BLEU score of 0.94 and MSE of 20.6 outperformed baseline maximum likelihood estimation D. Poetry and Literary Text Generation using GANs Researchers have also focused on literary text generation using GANs.",
                "The generative model is a CNN-RNN architecture, acting as an agent.",
                "This architecture of the CNN-RNN model as an agent and two discriminators is similar to that of [45].",
                "[44] Chinese Poetry generation RL GAN Cross entropy and policy gradient RNN Generators and CNN discriminators 16394 Chinese quatrains BLEU-2 score of 0.74, overall score of 0.54 by human evaluators [45] Generate poetry from an image Multiadversarial GAN with an embedding model Cross entropy and policy gradient RNN Generators, GRU-based discriminator, CNN image encoder and RNN poem decoder Novel dataset with paired image and poetry Overall BLEU score of 0.77, 7.18 out of 10 overall score by human evaluators [46] Generate Shakespearean prose from a painting Multiadversarial GAN with encoder and decoder Cross entropy and policy gradient RNN Generator, CNN-RNN agent for encoding and decoding painting, LSTM encoder and decoder for generating prose Two datasets for generating English poem from an image, and Shakespeare plays and their English translations for text style transfer Average scores of 3.7, 3.9, and 3.9 out of 5 by evaluators for content, creativity, and similarity to Shakespearean style respectively [47] Chinese Poetry generation RL GAN Maximum-likelihood A single layer LSTM generator, two-layer Bi-directional LSTMs discriminator Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.76 and 0.55 for the two datasets respectively",
                "[48] Chinese Poetry generation RNN GAN Wasserstein distance LSTM generator and discriminator with WGAN-GP training Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.88 and 0.67 for the two datasets respectively",
                "[42] O. Mogren, \u201cC-RNN-GAN: Continuous recurrent neural networks with adversarial training,\u201d arXiv preprint arXiv:1611.09904, 2016."
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "[34] used Musical Instrument Digital Interface (MIDI) files, which contain data to specify the musical instruction such as note\u2019s notation, pitch, and vibrato, to train a hybrid variational autoencoder (VAE) and GAN to generate musical melody for a specific genre.",
                "[34] Generate melody for a specific genre Hybrid VAE and GAN KL Divergence Four DeCONV layers for both generator & discriminator, VAE encoder used three Conv2D No quantitative evaluation, concluded that consistency of generated melodies was not up to the same level as human composition"
            ],
            "Variational Autoencoder": [
                "A Hybrid Approach to Intelligent Musical Composition Based on Generative Adversarial Networks with a Variational Autoencoder,\u201d in Proceedings of the Future Technologies Conference, 2020, pp."
            ],
            "GAN": [
                "IEEE GAN Computers Generate Arts?",
                "Using generative adversarial networks (GANs), applications such as synthesizing photorealistic human faces and creating captions automatically from images were realized.",
                "This survey takes a comprehensive look at the recent works using GANs for generating visual arts, music, and literary text.",
                "A performance comparison and description of the various GAN architecture are also presented.",
                "Finally, some of the key challenges in art generation using GANs are highlighted along with recommendations for future work.",
                "Keywords\u2014GAN, deep learning, arts, generative learning, computer vision, computer art I. INTRODUCTION  Art, a manifestation of human creativity, has always been an essential component of human culture.",
                "This is where deep learning and more \nspecifically generative adversarial networks (GANs) can be effective.",
                "Generative Adversarial Networks (GANs) use deep learning architectures to facilitate generative modeling.",
                "GANs primarily consist of two deep learning models, namely the generator and the discriminator.",
                "GANs have already achieved remarkable results in various applications including the generation of photorealistic images, scenes, and people which are non-recognizable as fakes even to humans.",
                "The idea of computers generating art without human supervision can be realized using GANs, unlike previous approaches.",
                "The fact that GANs can automatically learn to generate new examples from a given set of data proves to be a very useful component in computer art generation.",
                "Also, due to the advancement of technology and digitization, the two main requirements of GANs, datasets and computing power, have become widely available.",
                "Therefore, this makes GANs, a natural candidate to solve the difficult problem of computer-generated arts.",
                "GANs, on the other hand, are not restricted to any such conditions, and therefore has the potential to create more realistic arts.",
                "At the same time, we keep the paper focused by reviewing art works generated using a specific deep learning model, i.e., GANs.",
                "We hope this work will inspire researchers and artists to collaborate on advancing the field of computer-generated arts using GANs.",
                "Following are the key contributions of this paper: \u2022 It provides an overview of the primary GAN architectures for generative arts.",
                "In this Section, the necessary background information including relevant GAN architectures will be discussed.",
                "A. Generative Adversarial Netowrks GANs, first introduced by [14], can generate new content based on a min-max game between two networks, the generator and the discriminator.",
                "Figure 2 depicts a basic GAN architecture consisting of a single generator and discriminator.",
                "Meanwhile, the discriminator has access to the ground truths, whether the data came from the generator or real dataset, which it can use to minimize its error.  \n Figure 2 A Basic GAN Architecture More sophisticated GAN architectures can make use of labels to generate data points for a specific category.",
                "This is useful because in the standard GAN, data points are generated only based on the input noise.",
                "Next, various GAN architectures relevant to art generation is discussed.",
                "1) Conditional GAN: Extended from the regular GAN, it is a conditional architecture if the generator and discriminator are conditioned on auxiliary information such as class labels [15].",
                "The auxiliary information is combined in a joint representation with the noise in the basic GAN, allowing the generator to control the generation of data points based on the \ninput condition.",
                "For instance, conditional GANs can be used to generate various genres of music including jazz, rock, and classic.   2) Deep Convolutional GAN: Commonly known as DCGAN",
                "Given the success of convolutional neural network in image and video classification in recent years, DCGAN remains a suitable architecture for image generation applications.   3) Recurrent Adversarial Netwokrs: In this architecture, a recurrent computation is obtained by unrolling the gradient descent optimization [18].",
                "Other common GAN architectures include InfoGAN",
                "Laplacian GAN [20], and",
                "Wasserstein GAN",
                "For a comprehensive comparison of GAN architectures, the readers are encouraged to refer to [17] and [22].",
                "B. Common Loss Functions GAN training in a broader sense is based on a min-max game between two networks, the generator, and the discriminator.",
                "This loss function is typically used in the least square GAN (LSGAN).",
                "More recently, Wasserstein distance has gained interest as a loss function, especially in GAN training.",
                "RECENT ADVANCES IN ARTS GENERATION USING GANS In this section we provide a discussion and comparison of the recent works using GANs for generating various art forms including visual arts, music, and literary texts.",
                "Furthermore, the loss function used, the GAN type, and architectural details will also be highlighted.",
                "Figure 3 Framework of the Survey B. Visual Arts Generation using GANs This section presents a comparison of GAN approaches for visual arts generation.",
                "[24] presented a conditional GAN framework that can automatically generate painted cartoon images from a given sketch.",
                "A similar conditional GAN architecture using a U-Net generator for generating shoe image from a given shoe sketch is presented in [25].",
                "Similarly, the authors in [26] implemented a GAN-based solution for generating synthetic images that are fully colored and textured from a given sketch.",
                "This led the author to experiment with GANs for generating more realistic brushstrokes.",
                "Instead of giving the generator random noise as input as it is in typical GAN, the action space was provided in this approach.",
                "The brushstrokes generated by the GAN architecture were rougher and more realistic.",
                "In [28], the authors proposed a modified version of DCGAN for generating arts without any condition.",
                "When compared to the baseline DCGAN, the proposed work had better performance with 53% respondents believing that the works were generated by an artist as opposed to 35% for DCGAN.",
                "The authors then explored with an existing GAN architecture known as styleGAN",
                "Unlike regular GAN, styleGAN uses a mapping network to map points in the latent space to an intermediatory latent space to control the style of the data in the generator.",
                "Figure 6 Samples from Japanese Art Facial Expression Generated using StyleGAN",
                "A framework for generating a face photo from a given sketch as well as a sketch from a given face photo is implemented using GANs in [31].",
                "The authors trained a simple conditional GAN for both applications.",
                "[32] introduced StrokeNET, a GAN-based architecture to generate digits and character strokes.",
                "This section focused on the recent applications of GAN with regards to the generation of visual arts.",
                "Furthermore, the table also summarizes the GAN type used, the loss function as well as the generator and discriminator architectures.",
                "Recent Advances in Visual Arts Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Result",
                "[24] Generate cartoon image from sketch Conditional GAN Cross entropy, L1 distance for pixel-level loss U-Net Generator, CNN-FC discriminator Qualitative only, outperformed existing works",
                "[25] Generate shoe image from sketch Conditional GAN Binary cross-entropy loss for discriminator U-Net Generator, Deep CNN discriminator No evaluation",
                "[26] Generate fully colored synthetic images from sketch Vanilla GAN with Encoder for image style recognition Auxiliary Losses, discriminator is trained on style loss & content loss.",
                "[27] Generate painting by brushstrokes Conditional GAN Wasserstein Loss Configurations not provided No evaluation [28] Unconditional art generation DCGAN Cross entropy loss and added classification, style ambiguity losses Deep CNNs, CONV followed by LeakyReLU 53% of evaluators believe that synthesized images were by an artist.",
                "[29] Generate pre-modern Japanese art facial expression StyleGAN WGAN-GP Configurations not provided No evaluation [31] Generate face photo from a sketch and vice versa Conditional GAN Cross entropy Configurations not provided No evaluation, the loss values were reported [32] Generate digits and character strokes Modified DCGAN + agent MSE Generator contains CONV+LeakyReLU, Agent is VGG Generated images not evaluated, classification accuracy 91% on MNIST [33] Generate calligraphy and handwritten digits Modified conditional GAN with multiple encoders for style & content-encoding Binary Cross entropy discriminator, cross-entropy style loss & Kullback-Leibler content loss Two residual blocks then 4 CONV modules generator, 1 CONV layer then 6 residual blocks discriminator FID 120.1, 49.3% of the images were identified to be synthesized C. Music and Melody Generation using GANs  Next, GAN approaches for music and melody generation are presented.",
                "[34] used Musical Instrument Digital Interface (MIDI) files, which contain data to specify the musical instruction such as note\u2019s notation, pitch, and vibrato, to train a hybrid variational autoencoder (VAE) and GAN to generate musical melody for a specific genre.",
                "Similarly, [36] proposed a melody generation framework using GAN, consisting of a Bi-directional LSTM generator and an LSTM discriminator.",
                "[39] used conditional GANs for melody generation.",
                "The output of the layer is fed into the conditioned-lyrics GAN.",
                "A similar study was conducted in [41] using the exact dataset and conditional-LSTM GANs.",
                "Mogren [42] proposed an RNN GAN for generating single voice polyphonic music.",
                "[43] introduced a novel multi-track polyphonic symbolic music generator using GANs.",
                "The proposed \u2018MuseGAN\u2019 architecture utilized three different GANs namely the jamming model, the composer model, and the hybrid model.",
                "To implement the GANs, the Lakh MIDI dataset was transformed into a multi-track piano-rolls representation.",
                "This section focused on the recent applications of GAN with regards to music and melody generation.",
                "Moreover, the table also summarizes the GAN type used, the loss function as well as the generator and discriminator architectures.",
                "Recent Advances in Music and Melody Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Result",
                "[34] Generate melody for a specific genre Hybrid VAE and GAN KL Divergence Four DeCONV layers for both generator & discriminator, VAE encoder used three Conv2D No quantitative evaluation, concluded that consistency of generated melodies was not up to the same level as human composition",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds",
                "[39] Generate melodies based on lyrics Conditional GAN Cross entropy LSTM generators and discriminators No evaluation was provided  [41] Generate melodies based on lyrics Conditional LSTM GAN Cross entropy Dense layer followed by 2 LSTM followed by a dense layer for generator, 2 LSTM followed by dense for discriminator BLEU-2 score of 0.735, scores of about 3.8, 3.5, 4.1 respectively out of 5 for lyrics, rhythm, and melody by evaluators [42] Generate single voice polyphonic music RNN GAN Cross entropy and Squared error loss 2 LSTM layers for generator, 2 Bi-directional LSTM layers followed by a dense for discriminator No evaluation was provided  [43] Generate multi-track, polyphonic music Conditional GAN Wasserstein Generators contain 1D transposed CONV, discriminators 5 contain 1D Conv layers followed by one dense layer The highest score for conditional generation was 3.1 and non-conditional was 3.16 out of 5 by \u2018non-pro\u2019 evaluators.",
                "For intra-track metrics, jamming model performed best [44] Generate folk music RL GAN Cross entropy and policy gradient RNN generators, CNN discriminators BLEU score of 0.94 and MSE of 20.6 outperformed baseline maximum likelihood estimation D. Poetry and Literary Text Generation using GANs Researchers have also focused on literary text generation using GANs.",
                "This section presents a comparison of GANs in poetry and literary text generation.",
                "Most GAN architectures are restricted by several factors when it comes to text and sequential generation.",
                "[44] introduced SeqGAN to overcome these challenges by defining the generator as stochastic policy in reinforcement learning (RL).",
                "The SeqGAN architecture is illustrated in Figure 8.",
                "SeqGAN",
                "[44] \nThe discriminator in SeqGAN is trained using both the real as well as generated data.",
                "The SeqGAN generated poems outperformed the baseline maximum likelihood estimation on the BLEU score (statistically significant with p-value less than 10^-6).",
                "Additionally, 70 experts on Chinese poems were asked to evaluate between 20 real poems, 20 generated using maximum likelihood estimation, and 20 generated using SeqGAN.",
                "The SeqGAN outperformed the baseline with a 0.54 average score and a statistically significant p-value.",
                "The approach of extending GANs to generate sequences of discrete tokens is suitable for poetry and other text generations because of the sequential nature of text datasets.",
                "The proposed image to poetry GAN (I2P-GAN) was evaluated on several metrics such as relevance, novelty, and BLEU scores against different architectures including SeqGAN.",
                "The proposed I2P-GAN outperformed all the models with a 7.18 overall score which is close to the ground truth overall score of 7.37.",
                "Figure 9 Poems Generated from a Given Image of the Proposed I2P-GAN in [45] and Comparison with Baseline Models Kashyap et al.",
                "A sample generation of the poem (center) and prose (right) of the proposed model is shown in Figure 11.  Figure 11 A sample Poem and Prose Generation from [46] GANs are excellent at modeling continuous distributions, making them suitable for tasks like image generation.",
                "As a result, a maximum likelihood augmented discrete GAN was proposed in [47].",
                "The proposed model outperformed \nseveral baselines such as SeqGAN in terms of several metrics including BLUE score.",
                "For objective function, Wasserstein GAN (WGAN), as well as WGAN with gradient penalty (WGAN-GP)",
                "Both LSTM-based, as well as CNN-based GAN architectures, were experimented and the proposed model with LSTM outperformed the existing works on the aforementioned Poem-5 and Poem-7 datasets.",
                "[50] presented RankGAN by taking inspiration from the learning to rank concept from information retrieval, where given a reference, the required information is integrated into the ranking function encouraging relevant documents to be returned quicker.",
                "The overall architecture of RankGAN is presented in Figure 12.",
                "Figure 12 Proposed RankGAN Architecture in [50] The RankGAN differs from traditional GAN by including a sequence of generators and a ranker.",
                "In terms of BLUE score, the proposed RankGAN outperformed both SeqGAN and maximum likelihood estimation.",
                "Additionally, 57 native mandarin Chinese speakers were asked to rate the generations from SeqGAN, RankGAN, and human-written poems.",
                "The average scores provided by the evaluators were 3.4, 4.6, and 6.4 for SeqGAN, RankGAN, and human-written poems, respectively.",
                "The proposed RankGAN model was also applied to Shakespeare\u2019s Romeo and Juliet play to learn lexical dependency and use rare phrases in the play.",
                "The high BLEU-2 score of 0.91 indicates that RankGAN was able to capture the transition pattern among the words despite the training sentences being novel, subtle, and complex.",
                "[51] proposed a GAN architecture for creative text generation.",
                "In terms of perplexity scores, the proposed Creative-GAN outperformed the existing works.",
                "This section presented a comparison of recent applications of GAN with regards to poetry and literary text generations.",
                "Moreover, the GAN type, loss function, and architectural details are also highlighted.",
                "Recent Advances in Literary Text Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Dataset Result",
                "[44] Chinese Poetry generation RL GAN Cross entropy and policy gradient RNN Generators and CNN discriminators 16394 Chinese quatrains BLEU-2 score of 0.74, overall score of 0.54 by human evaluators [45] Generate poetry from an image Multiadversarial GAN with an embedding model Cross entropy and policy gradient RNN Generators, GRU-based discriminator, CNN image encoder and RNN poem decoder Novel dataset with paired image and poetry Overall BLEU score of 0.77, 7.18 out of 10 overall score by human evaluators [46] Generate Shakespearean prose from a painting Multiadversarial GAN with encoder and decoder Cross entropy and policy gradient RNN Generator, CNN-RNN agent for encoding and decoding painting, LSTM encoder and decoder for generating prose Two datasets for generating English poem from an image, and Shakespeare plays and their English translations for text style transfer Average scores of 3.7, 3.9, and 3.9 out of 5 by evaluators for content, creativity, and similarity to Shakespearean style respectively [47] Chinese Poetry generation RL GAN Maximum-likelihood A single layer LSTM generator, two-layer Bi-directional LSTMs discriminator Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.76 and 0.55 for the two datasets respectively",
                "[48] Chinese Poetry generation RNN GAN Wasserstein distance LSTM generator and discriminator with WGAN-GP training Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.88 and 0.67 for the two datasets respectively",
                "[50] Chinese Poetry generation GAN with a ranking function Ranking objective LSTM generator, CNN-based ranker Over 13,000 Chinese quatrains BLEU-2 score of 0.81, 4.6 out of 10 overall score by human evaluators [50] Learn rare words from Romeo and Juliet play GAN with a ranking function Ranking objective LSTM generator, CNN-based ranker Over 3000 sentences from Romeo and Juliet play BLEU-2 score of 0.914",
                "[51] Poetry and lyrics generation GAN with language model generator Cross entropy AWD-LSTM [52] and TransformerXL",
                "Although the previous section provided some promising recent developments in generating arts using GANs, there remains a few challenges.",
                "Moreover, the size of the dataset required for GAN training remains a challenge.",
                "Therefore, for novel applications, a lot of time is required in gathering the dataset first before applying the GAN training.",
                "There remain a few notable challenges in training GANs for music and melody generation.",
                "Therefore, to truly determine the ability of GAN with written arts, other applications such as novel writing and rhyme generations should be explored.",
                "This would lead to a better understanding of how well GANs can work for generating visual arts when the data is scarce.",
                "It could potentially lead to the development of a GAN framework that is well suited to dealing with such datasets.",
                "Therefore, training large scale GAN architectures will not be suitable.",
                "Therefore, future research should focus on implementing GAN architectures for generating music in raw audio format.",
                "Therefore, to find out the effectiveness of GANs in producing larger text pieces such as novels and dramas, researchers are encouraged to take on the challenge with longer texts.",
                "The previous section of this paper clearly highlighted the remarkable progress in recent years for generating various artworks utilizing GANs.",
                "In summary, for future work on art generation using GANs, we recommend the following: \u2022 Experiment with smaller dataset and GAN architectures for visual arts generation.",
                "Implement GANs to generate music in raw audio format as opposed to MIDI file format.",
                "Fortunately, due to the advancement in the field of deep learning, GANs emerged as a promising technology for computer generated arts.",
                "After a brief overview on different types of GANs and loss functions, the paper presented the recent works on generating visual arts, music, and literary texts using GANs.",
                "[10] J. Brownlee, \u201cA gentle introduction to generative adversarial networks (GANs),\u201d Retrieved June, vol. 17, p. 2019, 2019.",
                "[33] L. Kang, P. Riba, Y. Wang, M. Rusi\u00f1ol, A. Forn\u00e9s, and M. Villegas, \u201cGANwriting: Content-Conditioned Generation of Styled Handwritten Word Images,\u201d in Computer Vision \u2013 ECCV 2020, Cham, 2020, pp.",
                "[42] O. Mogren, \u201cC-RNN-GAN: Continuous recurrent neural networks with adversarial training,\u201d arXiv preprint arXiv:1611.09904, 2016.",
                "[51] A. Saeed, S. Ili\u0107, and E. Zangerle, \u201cCreative GANs for generating poems, lyrics, and metaphors,\u201d arXiv preprint arXiv:1909.09534, 2019."
            ],
            "Generative Adversarial Network": [
                "A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network  Sakib Shahriar  Department of Computer Science and Engineering American University of Sharjah, UAE b00058710@aus.edu  Abstract\u2014 \u201cArt is the lie that enables us to realize the truth.\u201d",
                "Generative Adversarial Networks (GANs) use deep learning architectures to facilitate generative modeling.",
                "[25] B. Kuriakose, T. Thomas, N. E. Thomas, S. J. Varghese, and V. A. Kumar, \u201cSynthesizing Images from Hand-Drawn Sketches using Conditional Generative Adversarial Networks,\u201d in 2020 International Conference on Electronics and Sustainable Communication Systems (ICESC), 2020, pp.",
                "A Hybrid Approach to Intelligent Musical Composition Based on Generative Adversarial Networks with a Variational Autoencoder,\u201d in Proceedings of the Future Technologies Conference, 2020, pp.",
                "[36] Y. Xu, X. Yang, Y. Gan, W. Zhou, H. Cheng, and X. He, \u201cA Music Generation Model Based on Generative Adversarial Networks with Bayesian Optimization,\u201d in Chinese Intelligent Systems Conference, 2020, pp."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "CONTROLLABLE DEEP MELODY GENERATION VIA HIERARCHICAL MUSIC STRUCTURE REPRESENTATION",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "One such approach is\nto use Generative Adversarial Networks (GANs) to model\nthe distribution of music [25\u201327].",
                "GANs learn a mapping\nfrom a point zsampled from a prior distribution to an in-\nstance of generated music xand hence represents the dis-\ntribution of music with z."
            ],
            "Generative Adversarial Network": [
                "One such approach is\nto use Generative Adversarial Networks (GANs) to model\nthe distribution of music [25\u201327]."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer",
            "RNN": [
                "The fame of the Transformer\ndecoder can be attributed to its \u201cself-attention\u201d mechanism,\nwhich provides the generative model a much longer memory\ncompared to previous statistical or recurrent neural network\n(RNN) models.",
                "[33] presented a hierarchical latent vector model\nthat uses a higher-level RNN to deal with bar-level dependency\nof up to 16 bars long, and a lower-level RNN to generate notes\nin each bar independently.",
                "Efforts have been made to improve the\noverall repetitive structure of the music generated by either\na Transformer model [34], [35], an RNN model",
                "[68] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, \u201cTransformers\nare RNNs: Fast autoregressive Transformers with linear attention,\u201d in\nProc."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "[23] A. Muhamed, L. Li, X. Shi, S. Yaddanapudi, W. Chi, D. Jackson,\nR. Suresh, Z. C. Lipton, and A. J. Smola, \u201cSymbolic music generation\nwith Transformer-GANs,\u201d in Proc."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [
                "In Transformer-based music generation, a music piece is\nusually represented as a sequence of \u201cevent tokens,\u201d such\nas N OTE-ONand N OTE-DURATION",
                "[31] T. Wang and X. Wan, \u201cT-CV AE: Transformer-based conditioned\nvariational autoencoder for story completion,\u201d in Proceedings of the\nTwenty-Eighth International Joint Conference on Arti\ufb01cial Intelligence,\nIJCAI-19 ."
            ],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "RNN": [
                "We analyse music samples\ngenerated by three models \u2013 SampleRNN, Jukebox, and DDSP\n\u2013 and employ a homogeneous framework across all methods\nto allow for objective comparison.",
                "We analyse the generated samples from three deep music\ngeneration methods of different philosophies: the autoregres-\nsive SampleRNN [3], Vector Quantised V AE (VQ-V AE)",
                "We functionally evaluate the perfor-\nmance of three generative models, which subscribe to different\ngeneration paradigms.\nSampleRNN.",
                "The autoregressive SampleRNN",
                "[3] uses\ntiers of RNN modules that work on different timescales of\nthe signal.",
                "The tiered architecture of\nthe SampleRNN allows for different computational focus to be\napplied to different levels of abstraction of the audio, which\nallows long term dependencies to be modelled ef\ufb01ciently.\nJukebox.",
                "SampleRNN Primed 10 4 16\nDDSP Recons 4 n/a 16\nJukebox Primed 24 8 44\nTABLE II\nMODEL PROPERTIES PERTAINING TO GENERATING SAMPLES .",
                "We trained\nSampleRNN and DDSP on the training partition of each\ndataset and used performance on the validation partition to\ntune hyperparameters.",
                "We generate samples with SampleRNN and Jukebox by\n\u2018priming\u2019 the model with an input length of real music to\nseed the sampling process.",
                "0.0450.150.300.450.600.75\n(a) SampleRNN\nactivatedpleasant deactivatedpleasantactivated\nunpleasantdeactivatedunpleasant\nPredictedactivated\n   pleasant\ndeactivated\npleasant\nactivated\nunpleasant\ndeactivated\nunpleasantTrue0.37 0.16 0.42 0.044\n0.15 0.35 0.44 0.064\n0.12 0.13 0.73 0.015\n0.14 0.31 0.44 0.110.150.300.450.60\n (b) Jukebox\nactivatedpleasant deactivatedpleasantactivated\nunpleasantdeactivatedunpleasant\nPredictedactivated\n   pleasant\ndeactivated\npleasant\nactivated\nunpleasant\ndeactivated\nunpleasantTrue0.45 0.18 0.26 0.099\n0.086 0.34 0.44 0.13\n0.071 0.12 0.72 0.092\n0.072 0.28 0.31 0.33 0.150.300.450.60\n (c) DDSP\nFig.",
                "F1 PR-AUC ROC-AUCModelMacro Micro Macro Micro Macro Micro\nSampleRNN 0.063 0.137 0.134 0.158 0.752 0.799\nDDSP 0.066 0.146 0.134 0.159 0.761 0.807\nJukebox 0.063 0.133 0.125 0.151 0.744 0.795\n(a) Mood/theme labels\nF1 PR-AUC ROC-AUCModelMacro Micro Macro Micro Macro Micro\nSampleRNN 0.483 0.523 0.515 0.565 0.764 0.774\nDDSP 0.489 0.526 0.511 0.565 0.759 0.771",
                "Samples generated by\nSampleRNN and Jukebox failed to result in consistent per-\nformance increases to the same degree, although they were\nat least as good as the baseline in terms of PR-AUC and\nROC-AUC, and competitive in terms of F1.",
                "For emotional labels, we observe from Table III(b) that\nsamples generated by SampleRNN and DDSP yield negligible\nperformance increase.",
                "We demonstrate, by\ncomparison to the MediaEval 2019 competition, that music\ngenre classi\ufb01ers are improved by using reconstructed samples0369121518212427303336394245485154\nPredicted0\n3\n6\n9\n12\n15\n18\n21\n24\n27\n30\n33\n36\n39\n42\n45\n48\n51\n54True\n0.000.150.300.450.60\n(a) SampleRNN\n0369121518212427303336394245485154\nPredicted0\n3\n6\n9\n12\n15\n18\n21\n24\n27\n30\n33\n36\n39\n42\n45\n48\n51\n54True\n0.000.150.300.450.60\n (b) Jukebox\n0369121518212427303336394245485154\nPredicted0\n3\n6\n9\n12\n15\n18\n21\n24\n27\n30\n33\n36\n39\n42\n45\n48\n51\n54True\n0.000.150.300.450.60\n (c) DDSP\nFig.",
                "The SampleRNN samples tend to be predicted as\nbelonging to the \u2018dark\u2019 class.",
                "We see that existing music generation methods implicitly bias the generated samples with speci\ufb01c\nthematic indices.\nfrom DDSP (less so from SampleRNN) to augment training\ndata; however, this effect was not observed in the emotion\nclassi\ufb01cation experiment, where only JukeBox achieved some\nimprovement in Micro averages of the performance measures\nused in MediaEval 2019."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [
                "Sequence-level rec.e m b e d\n Bidirectional \nEncoder\n Auto-Regressive \nDecoder Auto-Regressive \nDecoderGlobal, fine-grained control\nVQ-VAE description-to-sequenceco nc a t\nper column c"
            ],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Music Generation Using an LSTM",
            "RNN": [
                "We sought to demonstrate an approach of music generation using \nRecurrent Neural Networks (RNN).",
                "3  \n 2 Methods and Project Pipeline  \n2.1 RNNs  \nImagine that you were to start humming along to a song on the radio .",
                "To \nsolve this limitation of traditional neural  networks, RNNs (recurrent neural networks) can \nbe applied to information that comes in a sequence.",
                "Simple RNNs only have one \ndifference from traditional neutral networks: they have an output that feeds back into the \nnetwork .",
                "[3] \n \nAn RNN cell   \nThe looping output is a vector which encodes the state of all the network\u2019s previous \ninputs .",
                "[3] The looping nature of RNN s allows state about a \nsequence to be carried from one input to the next.",
                "Unraveling this loop gives a clearer \npicture of how RNNs remember information  to generate a sequence .",
                "[3] \n \n   \n \n  \n4  \n  \nUnraveled RNN loop  \nEach rectangle  represents  different iterations of the s ame RNN.",
                "The horizontal channel of \narrows represents  the looping state of the RNN being passed from one iteration to the \nnext.",
                "An RNN could  be thought of as a regular neural network with the addition of the \nnetwork\u2019s input  being the item of a  sequence and  with outputs dedicated to encoding and \nremembering state about the previous items sent through the network .",
                "A simple RNN might only consist of a single layer dedicated to both remembering a \nsequence and calculating  the output .",
                "The reason we didn\u2019t use a simple RNN for our \nproject is because of an undesirable property  of simple RNNs where previous inputs \n\u2018disappear\u2019 as more inputs are fed to the network.",
                "The inner workings of an RNN cell  \nA simple RNN has a single layer (a tanh layer in this diagram) that calculates the output \nusing both the state of previous inputs and the current input."
            ],
            "Recurrent Neural Network": [
                "We sought to demonstrate an approach of music generation using \nRecurrent Neural Networks (RNN)."
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Prote\u00e7\u00e3o intelectual de obras produzidas por sistemas baseados em intelig\u00eancia artificial: uma vis\u00e3o tecnicista sobre o tema",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Dentretaism\u00e9todos,destacoas Generative\nAdversarial Networks (GANs)"
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "An adaptive music generation architecture for games based on the deep learning Transformer model",
            "RNN": [
                "[15] (illustrated in Fig. 1) is multi-agent and multi-technique:\n2\u2022theharmony role agent , which generates a chord progression, using an RNN (trained on a corpus of\nsymbolic chord sequences, actually an extension of the harmony system from the same authors [14]);\n\u2022themelody role agents (one for each instrument), which instantiate characteristics (length, pitch, propor-\ntion of diatonic notes. . . ) of pre-existing melodies, using an evolutionary rule system (XCS, for eXtended\nlearning Classi\fer System",
                "adapting them to the harmony;\n\u2022therhythm role agent , which uses another RNN model.",
                "Transformer [39] is an important evolution of a Sequence-\nto-Sequence architecture (based on RNN Encoder-Decoder), where a variable length sequence is encoded into a\n\fxed-length vector representation which serves as a pivot representation to be iteratively decoded to generate\na corresponding sequence (see more details, e.g., in [8, Section 10.4]).",
                "Interactive music generation with positional constraints using\nAnticipation-RNN, September 2017."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [
                "In particular, we want to\nmove towards collaborative and interactive control of the music components generated by the Transformer-based\narchitecture.",
                "5.2 Interactive Coordination\nA more radical approach is to substitute the sequencer-like platform (currently, Ableton Live) by a more\ngeneral platform for interactive and collaborative control of musical components (being generated by our current\nTransformer-based architecture)."
            ],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "WHAT IS MISSING IN DEEP MUSIC GENERATION? A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "[20,24], Generative Ad-\nversarial Networks (GANs)",
                "Unfortunately, most deep\nlearning models can generate only a limited length: V AEs\nand GANs usually have a \ufb01xed length from 2 to 8 bars,\nabout the length of just one phrase."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "VIS2MUS: EXPLORING MULTIMODAL REPRESENTATION MAPPING FOR CONTROLLABLE MUSIC GENERATION",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "[13] Elena Rivas Ruzafa, Pix2Pitch: generating music from\npaintings by using conditionals GANs , Ph.D. thesis,\nETSI Informatica, 2020."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "GENERATING MUSIC WITH SENTIMENT USING TRANSFORMER-GANS",
            "RNN": [
                "In this same work, the authors also discover an\nanalogy between the Transformer and the RNN.",
                "These sequences are generated step-by-\nstep in an autoregressive manner, which is done via the\nTransformer-RNN analogy made in [18]."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "GENERATING MUSIC WITH SENTIMENT USING\nTRANSFORMER-GANS\nPedro L. T. Neves\nState University of Campinas\np185770@dac.unicamp.brJose Fornari\nState University of Campinas\nfornari@unicamp.brJo\u00e3o B. Florindo\nState University of Campinas\nflorindo@unicamp.br\nABSTRACT\nThe \ufb01eld of Automatic Music Generation has seen signi\ufb01-\ncant progress thanks to the advent of Deep Learning.",
                "The model is a Transformer-GAN trained with labels that\ncorrespond to different con\ufb01gurations of the valence and\narousal dimensions that quantitatively represent human af-\nfective states.",
                "Attribution: Pedro L. T. Neves, Jose\nFornari, and Jo\u00e3o Batista Florindo, \u201cGenerating music with sentiment\nusing Transformer-GANs\u201d, in Proc. of the 23rd Int.",
                "This is the motivation behind the use\nof Generative Adversarial Networks (GANs)",
                "This addition has a positive effect on the generated\nsamples, and we demonstrate, through evaluations of our\nmodel both via automatic metrics and human feedback,\nthat the proposed Transformer GAN obtains a performance\nthat competes with a current state-of-the-art model, even\nwhile having a smaller set of parameters and using a sim-\npler representation of music.",
                "To summarise, our contribu-\ntions are as following:\n\u2022 We present a neural network that, up to our knowl-\nedge, is the \ufb01rst generative model based on GANs to\nproduce symbolic music conditioned by sentiment.arXiv:2212.11134v1",
                "2. RELATED WORKS\n2.1 Generative Adversarial Networks\nGenerative Adversarial Networks, or simply GANs, con-\nsist of a theoretical framework in which two Neural Net-\nworks, the Generator and the Discriminator, through com-\npetition, optimize a model that implicitly approximates a\ndata distribution by generating samples that try to mimic\nthe features that it observes on a given set of samples orig-\ninating from that distribution.",
                "Due to the inherent instability of the adversarial pro-\ncess, several works have focused on improving the conver-\ngence and the quality of the samples generated by GANs\nvia new objective functions, regularization and normaliza-\ntion techniques, and model architectures.",
                "Of particular in-\nterest to this work is RSGAN",
                "[8], which substitutes the\nstandard GAN loss for the non-saturating Relativistic Stan-\ndard Loss.",
                "LRSGAN;D =\n\u0000E(xr;xf)\u0018(P;Q)[log( sigmoid (D(xr)\u0000D(xf)))](2)LRSGAN;G =\n\u0000E(xr;xf)\u0018(P;Q)[log( sigmoid (D(xf)\u0000D(xr)))];(3)\nin which PandQare, in this order, the real and fake disri-\nbutions.",
                "In order to provide more stability to the training process,\nWGAN-GP",
                "Nevertheless, generating\ndiscrete sequences with GANs is notoriously hard.",
                "[25] and MuseGAN",
                "[26] are GAN-based gen-\nerative models of music that use Convolutional Neural\nNetworks (CNNs) as both the Generator and Discrimina-\ntor.",
                "This tech-\nnique, often used in image generating-GANs [42], ensures\nthat the model prioritizes local structure.",
                "The networks were trained via a combination of the\nteacher forcing objective plus the RSGAN objective",
                "The overall objective for the gener-\nator in this training stage is:\nLG=LMLE+\u000bLRSGAN G-global +\fLRSGAN G-local;(9)\nwhereLmle=\u0000Ex\u0018P[logG\u0012(x)]",
                "(10)\nwhere the factorsLRSGAN G-global andLRSGAN G-local are re-\nspectively the global and local GAN losses detailed above,\n\u000band\f, which we empirically chose to be equal to 1, are\nhyperparameters controlling the relative intensity of each\nloss factor, andLMLE is the Maximum Likelihood.",
                "The\nDiscriminator was simply trained with the local and global\nRSGAN objectives given previously in Equation 2 plus the\nglobal and local gradient penalties based on Equation 4\nand regulated by a hyperparameter \u0015(which as per [9], we\nchose to be 10) .",
                "LD=LRSGAN D-global +\fLRSGAN D-local+\n\u0015(LGP-global +LGP-local ):(11)4.",
                "To achieve this purpose, we used both the automatic evalu-\nation metrics proposed in [26,45] and a set of human eval-\nuation metrics to compare our GAN with the system that,\nas far as we know, corresponds to a state-of-the-art gen-\nerative model of symbolic music conditioned by sentiment\ncurrently available in the literature.",
                "[37] 49:76 8:52 4:36\nTransformer 48:79 8 :65 4 :37\nTransformer GAN 50:73 9:45 4:43\nTable 1 : Comparison between the samples generated by\nours and a state-of-art model.",
                "Since the focus\nof our work was to implement the GAN framework within\nthe context of symbolic music generation conditioned by\nsentiment, and given the complexity of this framework, es-\npecially when it is applied to the discrete domain, we chose\nto use a simpler representation in order to maintain the fo-\ncus of our work on the implementation of the GAN.",
                "These results suggest that both\nthe proposed Transformer, and the Transformer GAN, are\ncompetitive with a state-of-the-art model with respect to\nthe four qualitative metrics.",
                "The acronyms TG, T and CP corre-\nspond, respectively, to the Transformer GAN, Transformer\nand Compound-Word Transformer Baseline models.",
                "Between the\nthree, by observing the boxplots, it seems that while the\nTransformer and the Compound-Word baseline sometimes\nproduce samples that situate themselves more strongly to\nthe side to which they theoretically pertain (e.g., for the\nhigh valence and low arousal categories), the Transformer\nGAN surpasses the simple Transformer due to the fact that\nit never situates more than 50% of the excerpts on the in-\ncorrect side of the middle line, which in this case is rep-\nresented by the number 3.",
                "Overall, given the superior ratings of the Transformer\nGAN respective to the automatic metrics and its compet-\nitiveness with a state-of-the art model with respect to the\nhuman evaluations, and given the considerations above\nabout model size and representation, the Transformer GAN\nseems to be a promising model for music generation con-\nditioned by sentiment.",
                "[37] 3.32\u00061.29 2.93 \u00061.13 3.18 \u00061.30 3.49 \u00061.04\nTransformer 3.75\u00061.24 3.22 \u00061.19 3.76 \u00061.14 3.89 \u00061.14\nTransformer-GAN 3.56\u00061.34 3.06 \u00061.21 3.38 \u00061.09 3.44 \u00061.15\nTable 2 : Results of the Survey where participants were asked to rate the samples generated by several models.",
                "[8] A. Jolicoeur-Martineau, \u201cThe relativistic discrim-\ninator: a key element missing from standard\nGAN,\u201d in International Conference on Learn-\ning Representations , 2019.",
                "[41] T. Miyato and M. Koyama, \u201ccGANs with projec-\ntion discriminator,\u201d in International Conference on\nLearning Representations , 2018."
            ],
            "Generative Adversarial Network": [
                "This is the motivation behind the use\nof Generative Adversarial Networks (GANs)",
                "\u2022 We show that promising results come from using\nGenerative Adversarial Networks within the context\nof music generation conditioned by sentiment, as our\nmodel obtains a good performance despite having\nless parameters, using a simpler symbolic represen-\ntation, and, being, up to our knowledge, the \ufb01rst on\nthis task to be trained via an adversarial scheme.",
                "2. RELATED WORKS\n2.1 Generative Adversarial Networks\nGenerative Adversarial Networks, or simply GANs, con-\nsist of a theoretical framework in which two Neural Net-\nworks, the Generator and the Discriminator, through com-\npetition, optimize a model that implicitly approximates a\ndata distribution by generating samples that try to mimic\nthe features that it observes on a given set of samples orig-\ninating from that distribution."
            ],
            "Transformer-based": [
                "2.3 Generative Models of Music\nAs of the writing of this paper, most of the state-of-the\nart generative models of symbolic music are Transform-\ners or Transformer-based."
            ],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning",
            "RNN": [
                "[40] G. Hadjeres, F. Nielsen, Anticipation-RNN: enforcing unary constraints in sequence generation, with\napplication to interactive music generation."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [
                "We then train an autoregressive\ndecoder-only Transformer-based network (38) on the collected melodic skeleton data to construct\nnew melodic skeletons (Fig. 1C, a).",
                "At the stage of melody inpainting, we employ the recurrent Transformer-based\nencoder\u2013decoder architecture (18) in a sequence-to-sequence setup as the melody inpainting module\nto complete the melody conditioned on the melodic skeleton, i.e., \ufb01lling the missing information\nbetween the melodic skeleton notes.",
                "2.6 Comparisons with other melody generation methods\nTo prove the effectiveness of the proposed hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, we compared WuYun-RS (i.e., using the rhythmic\nskeleton setting) to \ufb01ve public SOTA Transformer-based melody generation models, namely, Music\nTransformer (15), Pop Music Transformer (17), Compound Word Transformer (19), Melons (33)\nand MeMIDI, that follow an end-to-end left-to-right note-by-note generative paradigm and treat\neach note equally.",
                "4.3 WuYun architecture\nHere, we brie\ufb02y elaborate on the con\ufb01guration details of the two Transformer-based generative\nmodules of WuYun architecture, i.e., the melodic skeleton generation module for the melodic skeleton\nconstruction stage and the melodic prolongation generation module for the melody inpainting stage.",
                "We use a conditional sequence-to-sequence model based on Transformer-based recurrent en-\ncoder\u2013decoder neural networks for the melodic prolongation generation module (18)."
            ],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "An investigation of the reconstruction capacity of stacked convolutional autoencoders for log-mel-spectrograms",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "[2] or Generative Adversarial Networks (GANs)"
            ],
            "Generative Adversarial Network": [
                "[2] or Generative Adversarial Networks (GANs)",
                "[4] A. Radford, L. Metz, and S. Chintala, \u201cUnsupervised Representation\nLearning with Deep Convolutional Generative Adversarial Networks,\u201d\nJan. 2016."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Byte Pair Encoding for Symbolic Music",
            "RNN": [
                "Early research in-\ntroduced representations speci\ufb01cally tied to the training\ndata being used, such as DeepBach (Hadjeres et al., 2017),\nFolkRNN (Sturm et al., 2015) or BachBot (Liang et al.,\n2017)."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Non-sequential models such as MuseGAN (Dong\net al., 2018) often represent music as pianoroll matrices."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "A Symbolic-domain Music Generation Method Based on Leak-GAN",
            "RNN": [
                "While Midinette [6] choose \nto use CNN in their model, RNN has also been applied in such \noccasion."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "2021 3rd International Academic E xchange Conference on Science and Technology Innovation (IAECST) \n978-1-6654-0267-5/21/$31.00 \u00a92021 IEEE \n549 \n A Symbolic-domain Music Generation Method Based \non Leak-GAN \n \nZihan Li \nXihua Honor College,  \nXihua University,   \nChengdu, China,  \nkkli19@outlook.com \n \nYangcheng Liu \nSchool of Electrical and El ectronic Information, \nXihua University,  \nChengdu, China, \nluming9704@163.com Yi Guo * \nSchool of Electrical and El ectronic Information, \nXihua University,  \nChengdu, China, \nlpngy@vip.163.com \n \nQianxue Zhang \nSchool of Electrical and El ectronic Information, \nXihua University,  \nChengdu, China, \nlouxzqx@163.com",
                "In recent \nyears the deep neural network such as convolutional neural \nnetwork (CNN) and generative adversarial networks (GANs) \nhave been applied in this field [6,7,8,9,10] and have shown \ntremendous potential in their capability for content generation  \nand modification.",
                "[10] proposed three \ndifferent methods combined with GAN to handle the \ninteraction among tracks and generate polyphonic music with \nharmonic and rhythmic structure.",
                "However, to our knowledge the GAN approach is still \nlimited and suffers from exposure bias and mode collapse [12].",
                "In this paper we implement symbolic-domain music melody \ngeneration experiment on a model of LeakGAN",
                "MODEL STRUCTURE  \nWith transferring midi files to text files, the melody \ngeneration is taken as a text generation problem and then the \nLeak-GAN [14] generation model is used to generate text in \nthis work.",
                "B. LeakGAN in Music Generation \nInspired by the progressive GAN [16] approach, we choose \nLeak-GAN [14] model to generate our music.",
                "Composed of a \ndiscriminate net and a generate net, the Leak-GAN model \nallow the discriminator to leak its extracted features that is from higher level to the generator to help the guidance more distant, which effectively addresses the problem for long text-represented-music generation.",
                "Additionally, as LeakGAN model is skilled in long sequence generation, it produces promising results of adequate length as well as fine quality.",
                "Following we first introduce the general idea of generative \nadversarial network, then we take a closer look at the \nLeakGAN model. \n1) Generative Adversarial Networks \nGenerative adversarial network",
                "[1 log( ( (z))]\ndata zpx pG DVD G Dx D G\uf03d\uf045 \uf02b\uf045 \uf02d \uff0c ( 1 )  \nwheredatap is the real data\u2019s distribution andzp represent the \nprior distribution of z.   \n2) Leak-GAN On condition that we focus on symbolic-domain music \ngeneration, the sequence data of text-represented melody generation is considered as a sequential decision-making \nprocess [15].",
                "With the approval of leaking discriminator D\u2019s high level \nfeature representation to the generator who is not supposed to receive such information, the LeakGAN model addresses the problem that D\u2019s guiding signals are only available when the \nwhole melody sequence \nTs has been generated.",
                "To alleviate this situation, the LeakGAN model does not only provide generator with discriminator\u2019s information, it additionally applies a hierarchical reinforcement learning architecture to the generator in order to coordinate the leaked information with \ngeneration process of \nG\uf071.  \nWe introduce a MANAGER module and a WORKER \nmodule of the generator, which both start from an all-zero \nhidden state, denoted as 0Wh and 0Mh.",
                "FB \n2) GAN Setting \nWe choose LSTM to be the architecture of both \nMANAGER and WORKER in the generator.",
                "In this way, not only \nwe can reduce the problem of mode collapse, but also bring the GAN\u2019S solution closer to that of the supervised training.",
                "For the comparison experiment, LeakGAN model is mainly compared with LSTM.",
                "A few samples generated by LeakGAN are illustrated in Fig.",
                "2, which to a certain extent provide evidence for LeakGAN\u2019s ability of generating creative and harmonic music.",
                "The results are provided in Table 2, from which we see that the first four values for LeakGAN model is relatively high and the last is lower than that of LSTM.",
                "This indicates that the music generated by the LeakGAN tends to perform well both in statistics and imitating real music.",
                "e  \nmusic generated by LeakGAN is better.",
                "OBJECTIVE EVALUATION PERFORMANCE  \nMetrics LeanGAN LSTM Type \nWilcoxon \nTest  0.89105026  0.60805510 CB \nMWU Test 0.86661874  0.65296427 CB \nKWH)",
                "The comprehensive evaluation chart of LeakGAN and LST M. \nFig.4 (a) shows the high accuracy of our model even at the \nvery beginning, and Fig.4(b) is the training loss of D which reveals that there is a rapid decrease at the beginnings and th en \nit saturates."
            ],
            "Generative Adversarial Network": [
                "Following we first introduce the general idea of generative \nadversarial network, then we take a closer look at the \nLeakGAN model. \n1) Generative Adversarial Networks \nGenerative adversarial network"
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "AI Music Therapist: A Study on Generating Specific Therapeutic Music based on Deep Generative Adversarial Network Approach",
            "RNN": [
                "The three mainstays of the recurrent \nneural network family are: recurrent neural network (RNN), \nlong-short time memory model (LSTM), and gated recurrent \nunit (GRU).",
                "As is shown in Fig 1. \nRNN | LSTM     \n      \n  Fig.",
                "Schematic diagram of recurrent neural networks \nThe standard LSTM model is a special type of RNN, \nwith four special structures in each repetitive module, \ninteracting in a special way."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Adversarial Generative Network (GAN), \nLong-Short Time Memory (LSTM), and other neural networks \nlearn data from prepared MIDI (Musical Instrument Digital \nInterface) files with therapeutic effects.",
                "The experiments demonstrate that \nthe designed GAN neural network makes the music more \nnatural and smooth.",
                "Keywords\u2014Music therapy, Music analysis, LSTM, GAN, \nCNN \nI. INTRODUCTION \nA. Origin of Music Therapy",
                "Therefore, three dimensions need to be discussed in this \nstudy: whether the generated content is melody or \naccompaniment; whether single-track or multi-track music is \ngenerated; and whether the network architecture is CNN \n(convolutional neural network) or LSTM (recurrent neural \nnetwork), or GAN (generative adversarial neural network) \nwith CNN as the underlying architecture, which performs \nwell in various image video tasks.",
                "B. GAN \nTraining a, \nFake image Discriminator \n_",
                "GAN diagram \nAs shown in Figure 5, the simplest GAN (Adversarial \nGenerative Network) consists of two parts, G and D. The \ngenerator tries to generate data from some probability \ndistribution.",
                "6. CNN diagram \nCurrently, most of the underlying perceptrons of GANs \nuse the CNN _ framework, ie., convolutional shared \ncomputation method.",
                "C. GAN Results and Discussions \nGAN itself is a game-like network.",
                "The GAN superparameters are as follows: the maximum \nnumber of iterations was set to 50,000, the batch size was set \nto 64, the initial learning rate was set to 0.001, and the Adam \noptimizer (betal=0.5, beta2=0.9) was chosen.",
                "Diagram of GAN music generator \nAs is shown in Fig.9.",
                "GAN multi-track generation results \nFigure 10(a) shows the output of a random sequence in \nthe dataset, with the horizontal and vertical coordinates \nindicating time and notes, respectively.",
                "The experimental platform was \nbuilt on two deep neural network models: LSTM and GAN."
            ],
            "Generative Adversarial Network": [
                "2022 IEEE 2nd International Conference on Electronic Technology, Communication and Information (ICETCI) \nAI Music Therapist: A Study on Generating \nSpecific Therapeutic Music based on Deep \nGenerative Adversarial Network Approach \nYurui Hou \nWalnut Hill School for The Arts \nNatick, MA 01760, United States \nyurui.hou2023 @walnuthillarts.org"
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "An intelligent music generation based on Variational Autoencoder",
            "RNN": [
                "Unsupervised learning networks are most suitable for \ngenerating data with time-series information, especially for \nthe recursive neural networks (RNN) and long-term and \nshort-term memory networks (LSTM), which perform quite \nwell in this field.",
                "The encoder and decoder network in VAE, specifically \nthe encoder \u074d\u0c12\u123a\u0756\u0201\u0754\u123b  is a cyclic neural network (RNN) that \nprocesses the input sequence \u0754\u0d4c\u123c",
                "The decoder \u074c\u0c0f\u123a\u0754\u0201\u0756\u123b   sets the initial state \nof the decoder RNN using the sampled potential vector z, \nand the decoder RNN automatically generates the output \nsequence \u0003\u0755\u0d4c\u0003\u123c\u0755\u123a\u0373\u123b\u01e1\u0755\u123a\u0374\u123b\u01e1\u01e4\u01e4\u01e4\u01e1\u0755\u123a\u0750\u123b\u123d .",
                "For the encoder \u074d\u0c12\u123a\u0756\u0201\u0754\u123b , we use \na bidirectional RNN network.",
                "By analyzing this decoder, a \nhierarchical RNN decoder is used [12].",
                "This model is composed of two parts: one is to \ncalculate the occurrence probability of rhythm and \ndownbeat by recurrent neural network RNN, the other is to \napply dynamic Bayesian network DBN to the output of \nRNN to make the binary decision of the occurrence of \nrhythm and downbeat."
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "Abstract \u2014 In this paper, GAN and VAE are combined with \ndeep learning network to generate intelligent music based on \nmusic theory rules, and to explore intelligent music \ngeneration algorithm.",
                "Different from the traditional \nalgorithmic composition, it is not necessary to manually add \ncomplex rules, but trains the initial music set, evaluates and \nfilters the music collection, and ultimately generates music \nvia the RVAE-GAN neural network.",
                "On this \nbasis, the semi-supervised algorithm is used to form the chord \nstructure model, combined with the feature extraction of \nmusic, and the intelligent generation music based on GAN \nconfrontation generation network and VAE network \ncombined with music theory rules is proposed and proposed.",
                "Keywords-VAE; GAN; Music theory rules; Rhythm \nI.  INTRODUCTION  \nAlgorithmic composition, or automated composition, is \nan attempt to use a formal process to minimize the \ninvolvement of a person (or composer) in the use of \ncomputers for music creation",
                "VAE network \nA variational auto-encoder (VAE) is a directional \nmodel that uses good approximation inference and can be \ntrained purely using a gradient-based approach.",
                "The VAE \nfirst samples z from the code distribution \u0732\u0003\u0b6b\u0b6d\u0b62\u0b63\u0b6a\u123a\u009c\u123b.",
                "The VAE consists of an \nencoder \u074d\u0c12\u123a\u0754\u0201\u0756\u123b  approximating a posteriori \u074c\u123a\u0756\u0201\u0754\u123b  and a \ndecoder \u074c\u0c0f\u123a\u0754\u0201\u0756\u123b  of parametric likelihood \u074c\u123a\u0754\u0201\u0756\u123b .",
                "The encoder and decoder network in VAE, specifically \nthe encoder \u074d\u0c12\u123a\u0756\u0201\u0754\u123b  is a cyclic neural network (RNN) that \nprocesses the input sequence \u0754\u0d4c\u123c",
                "Compared to VAE, it has no \nlower limit of variation.",
                "In other words, the various \nantagonistic generation networks will be asymptotically \nconsistent, while the VAE has some bias.",
                "MULTIMODAL NEURAL NETWORK - RVAE-GAN \n1.",
                "In view of  the above two, combined with the \nadvantages and disadvantages of each network, we propose \na new multi-modal neural network model: rule-based neural \nnetwork (RVAE-GAN).",
                "The VAE decoder and GAN generator are combined \ninto one by having them share parameters and training \ntogether.",
                "Experiment procedure \nThe VAE decoder and the GAN generator are merged \ninto one by sharing parameters and training together.",
                "This paper introduces an RVAE-GAN model for \ngenerating sequence data.",
                "Semi-Recurrent CNN-Based VAE-\nGAN for Sequential Data Generation."
            ],
            "Variational Autoencoder": [
                "An intelligent music generation based on Variational Autoencoder \nTao Wang \nSchool of Information Engineering \nZhengzhou University \nHenan, China \n794574617@qq.com \n \n \n Junzhe Liu,Cong Jin*,Jianguang \nLi \nSchool of Information and \nCommunication Engineering \nCommunication University of China \nBeijing, China \nCorresponding Author:"
            ],
            "GAN": [
                "Abstract \u2014 In this paper, GAN and VAE are combined with \ndeep learning network to generate intelligent music based on \nmusic theory rules, and to explore intelligent music \ngeneration algorithm.",
                "Different from the traditional \nalgorithmic composition, it is not necessary to manually add \ncomplex rules, but trains the initial music set, evaluates and \nfilters the music collection, and ultimately generates music \nvia the RVAE-GAN neural network.",
                "On this \nbasis, the semi-supervised algorithm is used to form the chord \nstructure model, combined with the feature extraction of \nmusic, and the intelligent generation music based on GAN \nconfrontation generation network and VAE network \ncombined with music theory rules is proposed and proposed.",
                "Keywords-VAE; GAN; Music theory rules; Rhythm \nI.  INTRODUCTION  \nAlgorithmic composition, or automated composition, is \nan attempt to use a formal process to minimize the \ninvolvement of a person (or composer) in the use of \ncomputers for music creation",
                "Currently, variable-point automatic \nencoders (V AEs) and generative confrontation networks \n(GANs) are widely used to generate complex structural \nmodels.",
                "Music is generated intelligently based on the V AE-\nGAN network in combination with the rules we have \nspecified.",
                "2. GAN network \nGenerative Adversarial Networks (GAN) is a deep \nlearning model and one of the most promising methods for \nunsupervised learning in complex distribution in recent \nyears.",
                "In the \ntraining process, GAN adopts a very direct alternating \noptimization method, which can be divided into two stages.",
                "MULTIMODAL NEURAL NETWORK - RVAE-GAN \n1.",
                "In view of  the above two, combined with the \nadvantages and disadvantages of each network, we propose \na new multi-modal neural network model: rule-based neural \nnetwork (RVAE-GAN).",
                "The VAE decoder and GAN generator are combined \ninto one by having them share parameters and training \ntogether.",
                "Experiment procedure \nThe VAE decoder and the GAN generator are merged \ninto one by sharing parameters and training together.",
                "This paper introduces an RVAE-GAN model for \ngenerating sequence data.",
                "Semi-Recurrent CNN-Based VAE-\nGAN for Sequential Data Generation."
            ],
            "Generative Adversarial Network": [
                "2. GAN network \nGenerative Adversarial Networks (GAN) is a deep \nlearning model and one of the most promising methods for \nunsupervised learning in complex distribution in recent \nyears."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [
                "This model is composed of two parts: one is to \ncalculate the occurrence probability of rhythm and \ndownbeat by recurrent neural network RNN, the other is to \napply dynamic Bayesian network DBN to the output of \nRNN to make the binary decision of the occurrence of \nrhythm and downbeat."
            ],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "APE-GAN: A Novel Active Learning Based Music Generation Model With Pre-Embedding",
            "RNN": [
                "One of the earliest\npapers on music generation using neural networks is published\nin 2013, in which J.Bayer [1] discussed generating musics\nby using RNNs with fast dropout, a regularization method\nfor generalized linear models.",
                "[2] proposed\nRNN-DBNs by stacking Restricted Boltzmann Machine in\nRNN models to create a better representation of the data\nfor music generation.",
                "In the same year, the model C-RNN-GAN is proposed inwhich",
                "[4] used a generative adversarial architecture\nto combine two RNNs and resulted in more complexity in\nthe generated musics and increased intensity span similarity\nbetween the generated musics and real musics.",
                "C-RNN-GAN: Continuous recurrent neural networks\nwith adversarial training.",
                "Fundamentals of recurrent neural network (RNN)\nand long short-term memory (LSTM) network."
            ],
            "Recurrent Neural Network": [
                "Understanding LSTM\u2013a\ntutorial into Long Short-Term Memory Recurrent Neural Networks.\narXiv preprint arXiv:1909.09586 ."
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "APE-GAN: A Novel Active Learning Based Music\nGeneration Model With Pre-Embedding\n1stWenyi Su\nMars Laboratory\nWhittle School\nShenzhen, China\nwsu111@whittleschool.org2ndYixuan Fang\nMars Laboratory\nWhittle School\nShenzhen, China\nyfang@whittleschool.org3rdZheng Li\u0003\nMars Laboratory\nWhittle School\nBeijing, China\nzli@whittleschool.org4thXin Steven\u0003\nEECS\nPeking University\nBeijing, China\nmengxinpku2018@gmail.org\nAbstract \u2014Being able to generate realistic musics is one of\nthe biggest challenges for Arti\ufb01cial Intelligence, and the current\nmodels do not have musical descriptive ability as humans have\nand the musics they produce are not highly realistic.",
                "This\npaper proposed an active learning based music generation model\nwith pre-embedding (APE-GAN) that can use textual inputs\nto generate musics, and have increased performance by active\nlearning and a checking mechanism with the discriminator\nin GAN model.",
                "Through experiments, this work shows that\nAPE-GAN only needs 5% to 10% humans labelled data to\nachieve relatively good music generation ability.",
                "After using Lakh\nPianoroll Dataset to train APE-GAN, the similar meaning textual\ninputs result in outputs with KL divergence approximate to 0,\nwhile different meaning textual inputs result in outputs with KL\ndivergence approximate to 1.\nKeywords \u2014Music Generation; Generative Adversarial Net-\nworks(GAN); BERT; Active Learning; KL Divergence\nI. I NTRODUCTION",
                "In the same year, the model C-RNN-GAN is proposed inwhich",
                "One of the\nclosest attempts to generate \u201creal\u201d musics was made in 2017\nby H.W.Dong [5] in the model named MuseGAN.",
                "We named our model A ctive learning based\nPre-Embeding - G enerative A dversarial N etwork (APE-GAN),\na model that can hopefully result in more realistic music\ngeneration.",
                "\u000fWe use active learning technology to ef\ufb01ciently train\nour APE-GAN to achieve high performance under the\n123\nICEICT 2021 \nIEEE 4th International Conference on Electronic Information and Communication Technology\n978-1-6654-3203-0/21/$31.00 \u00a92021 IEEE\nXi'an, China \u2022",
                "Section III ex-\npatiates the complete work\ufb02ow of APE-GAN.",
                "M ODEL COMPONENTS\nA. Music Generative Adversarial Network\nGenerative adversarial network, or GAN, is a kind of\ndeep neural network with two components, the generator ( G)\nand the discriminator ( D).",
                "For generation of musics, because the music sequences for\neither real or generated musics are represented as \ufb01xed-size\nmatrix, we use CNNs for both GandD. The training process\nof GAN can be modeled by:\nmin\nGmax\nDEx\u0018pd[log(D(x))]",
                "In this work, we reference MuseGAN as our baseline\narchitecture.",
                "In MuseGAN, the generator is divided into two\nsub networks, the temporal structure generator Gtand music\nsequence generator Gm, in whichGtmaps the input vector\nzto",
                "Then, ~zis used byGmto generate melody sequences\nbar by bar:\nG(z) =fGm(~z)gT\nt=1 (3)\nB. Pre-Word-Embedding\nFor our model, instead of adding a random noise as the\ninput to let the model generate musics by itself, we decide\nto add a layer of word embedding (representation of text in\nthe form of vectors) that takes a piece of text as the input\nbefore the data enter the GAN model.",
                "The output vector z\nfrom the word embedding layer will be used as the input for\nthe GAN model.",
                "Due to the temporal correlation feature for\nmusics and the GAN model, generated musics that deviates\ntoo much from the input text will be discriminated as \u201cfake\u201d\nby the discriminator.",
                "By using BERT,\nAPE-GAN can create contextual word embedding vector zfor\nany sentences and use it as the input for the GAN model as\nreplace the random noise.",
                "As GAN model is learning in each\nround, the discriminator will label some data as \u201cuncertain\u201d\nif it \ufb01nds that it deviates only by a small amount from\nbeing either \u201creal\u201d or \u201cfake\u201d.",
                "Because of the selective nature of active\nlearning, it is rather cost ef\ufb01cient in increasing the performance\nof APE-GAN by getting a relatively small amount of labelled\ndata [18].",
                "Active Learning Process\nD. Discriminator Checking Mechanism\nFor many GAN models used in music generation, though\nboth the generator and discriminator are trained in the training\nprocess, only the generator is used to generate musics, while\nthe discriminator\u2019s function is yet to be exploited.",
                "APE-GAN I NFERENCE WORKFLOW\nA. Steps of Algorithm\nWith the knowledge in Section III, we can now introduce\nthe entire process of music generation inference of APE-GAN,\ndivided in the following steps, as illustrated in Figure 3 and\nFigure 4:\n1) Choose a piece of text that one wants to describe with\nAPE-GAN.\n2) Use word embedding with BERT as model input.",
                "3. APE-GAN Inference Work Flow\n\u000fIf the sequence is discriminated as \u201creal\u201d, then\noutput the sequence.",
                "4. APE-GAN Inference Work Flow\nIV.",
                "[21]\nC. Results\nFor our results, Figure 5 demonstrates the pianorolls of\n4 generated sequences of APE-GAN.",
                "In this paper, we proposed a model\nnamed \u201cAPE-GAN\u201d that can improve the performance of\nmusic generation models in three ways.",
                "To evaluate APE-GAN more objectively, we\nmeasure APE-GAN\u2019s descriptive ability by comparing the\nmusic sequences generated by different textual inputs with\nthe metric of KL divergence.",
                "Therefore, APE-GAN is able to learn semantic\nprior knowledge ef\ufb01ciently, which allows it to generate more\nrealistic and creative musics.",
                "C-RNN-GAN: Continuous recurrent neural networks\nwith adversarial training.",
                "MIDI-Sandwich: Multi-model\nMulti-task Hierarchical Conditional V AE-GAN networks for Symbolic\nSingle-track Music Generation.",
                "Classical Music Generation in Distinct Dastgahs with AlimNet\nACGAN. arXiv preprint arXiv:1901.04696 .",
                "Can GAN originate new electronic dance music\ngenres?\u2013Generating novel rhythm patterns using GAN with Genre\nAmbiguity Loss."
            ],
            "Generative Adversarial Network": [
                "M ODEL COMPONENTS\nA. Music Generative Adversarial Network\nGenerative adversarial network, or GAN, is a kind of\ndeep neural network with two components, the generator ( G)\nand the discriminator ( D)."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [
                "[2] proposed\nRNN-DBNs by stacking Restricted Boltzmann Machine in\nRNN models to create a better representation of the data\nfor music generation."
            ],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Automatic Music Generation System based on RNN Architecture",
            "RNN": [
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n294\n \n978-1-6654-7657-7/22/$31.00 \u00a92022 IEEE  Auto\nmatic Music Generation System based on RNN \nArchitecture \n \nSandeep Kumar \nDepartment of Computer Science & \nEngineering \nKoneruLakshmaiah Educational \nFoundation  \nVaddeswaram, Andhra Pradesh, India \ner.sandeepsahratia@gmail.com \n \nShilpa Rani \nDepartment of Computer Science & \nEngineering \nNeil Gogte Institute of Technology \nHyderabad, India \nshilpachoudhary1987@gmail.com \n \n KeerthiGudiseva \nDepartment of Computer Science & \nEngineering \nKoneruLakshmaiah Educational \nFoundation  \nVaddeswaram, Andhra Pradesh, India \nkeerthigudiseva0611@gmail.com",
                "By \nthe concept of Recurrent Neural Network (RNN), computers \ncan study the patterns from present song portions and \nconvert them into a possible map",
                "The natural \nchoice will be RNN or RNN variations since our music are a \nseries of characters",
                "An ANN that processes \nsequential or time series data is known as a RNN as shown \nin Fig.1.",
                "In this \nresearch, a straightforward RNN is used to produce musical \nnotes.",
                "RNN Block Diagram. \n \nII\n.",
                "LITERATURE SURVEY  \n \nThis paper shows how to use a simple RNN to make \nmusical notes.",
                "13 Muhammad Nadeem et \nal., 2019 LSTM  Nottingham \nMusicDataba\nse The majority of participants enjoyed listening to the music created by \nthe proposed musical data structure and RNN architecture.",
                "In the proposed system, we \nused RNN to generate musical notes.",
                "The detailed \ndescription of RNN is given below and the architectural \ndiagram is shown in Fig. 4. \n \nA. Recurrent Neural Network (RNN)",
                "For simulating sequence data, neural networks \nbelonging to the RNN class are helpful.",
                "Feedforward \nnetworks' offspring, RNNs, exhibit behavior akin to that of \nhuman brains.",
                "Natural Language Processing is a \ngreat application for RNN.",
                "Sample file \n \nTh\ne potential to send information over the years-steps \nmakes the RNN distinctive from the opposite participants of \nthe own family of the neural community.",
                "The working style \nof RNN chooses sequential information as input and gives \ninformation in sequential/order form instead of accepting \ninput and producing an outcome.",
                "This RNN \nmodel can help learn the track collection and generate the \nseries of track records.",
                "RNN Structure I\nV. RESULT AND DISCUSSION  \n \nA. D\nataset \n \nMore than 200 hours of paired audio and MIDI \nrecordings from the International Piano-e-first Competition's \nten years are included in the MAESTRO dataset."
            ],
            "Recurrent Neural Network": [
                "By \nthe concept of Recurrent Neural Network (RNN), computers \ncan study the patterns from present song portions and \nconvert them into a possible map",
                "The detailed \ndescription of RNN is given below and the architectural \ndiagram is shown in Fig. 4. \n \nA. Recurrent Neural Network (RNN)",
                "Recurrent Neural Networks contain many copies of the \nneural community linked and working in a series."
            ],
            "VAE": [
                "TABLE I. C OMPARATIVE ANALYSIS OF DIFFERENT METHODS \nS.NO Author Name& Year \nof Publication Methodology Database Remarks \n1 Chih-Fang Huang  et \nal., 2020 CVAE-GAN Model Self database Scoring Statistics Table for the Generated Music: \n                A        B     C      D \nAverage 2.52 1.52 2.27 4.10  \nSD1.24",
                "RVAE-GAN neural \nnetwork \n Self database Objective Evaluation: The average probability of a beat, abbreviated \nas APB, the standard deviation of the beat length, the standard \ndeviation of the bar length.",
                "REFERENCES  \n \n[1] Huang, Chih-Fang, and Cheng-Yuan Huang, \u201cEmotion-based AI \nmusi\nc generation system with CVAE-GAN,\u201d In IEEE Eurasia \nConference on IoT, Communication and Engineering (ECICE), pp. \n220-222, 2020."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "TABLE I. C OMPARATIVE ANALYSIS OF DIFFERENT METHODS \nS.NO Author Name& Year \nof Publication Methodology Database Remarks \n1 Chih-Fang Huang  et \nal., 2020 CVAE-GAN Model Self database Scoring Statistics Table for the Generated Music: \n                A        B     C      D \nAverage 2.52 1.52 2.27 4.10  \nSD1.24",
                "RVAE-GAN neural \nnetwork \n Self database Objective Evaluation: The average probability of a beat, abbreviated \nas APB, the standard deviation of the beat length, the standard \ndeviation of the bar length.",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n296 \n 9 Sarthak Agarwal et al., \n2018 PRECON-LSTM  Nottingham \nMus\nic \nDatabase  Sco\nreMeanMean Realness \nArtif3.120 2.747 \nGen \nHum3.613 3.516 \nComp  \n10 AdvaitMaduskar et al. \n2020 Autoregressive GAN \nmodel Bach\u2019s \nmusical \nsymphonies \ndataset Outlined a generation model for note sequence generation using the \nGAN framework.",
                "12 Hongyu Chen et al., \n2019 DCGAN model The Lakh \nMIDI",
                "Dataset A GAN-based model that was trained on a dataset of Bach's \norchestral symphonies produced the desired outcomes.",
                "14 Belinda M. Dungan et \nal. 2020 Generative model GAN-based \nMidinet PERFORMANCE OF THE MELODY GENERATION",
                "REFERENCES  \n \n[1] Huang, Chih-Fang, and Cheng-Yuan Huang, \u201cEmotion-based AI \nmusi\nc generation system with CVAE-GAN,\u201d In IEEE Eurasia \nConference on IoT, Communication and Engineering (ECICE), pp. \n220-222, 2020."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Development of Application Software for Generating Music Composition Inspired by Nature Using Deep Learning",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Deep learning, Machine learning, Computational intelligence, Mobile application \nsoftware, Artificial intelligence \nIntroduction: \nThe generative models in Deep learning like GAN (Generative Adversarial Network), Auto \nencoders are capable of generating the new data that have identical statistical characteristics of the \ntraining data.",
                "Figure 1 Illustration of (a) GAN (b) Variational En coder \n \nb) Variational Auto encoder: It is the simple feed forward network, in which the neurons are \nstacked in the layered architecture.",
                "Mobile application sofGenerative models (GAN and Auto-encoders) are used to g enerate   \nthe features that follows the statistical characteristics of the feat ures obtained from the \nnatural database."
            ],
            "Generative Adversarial Network": [
                "Deep learning, Machine learning, Computational intelligence, Mobile application \nsoftware, Artificial intelligence \nIntroduction: \nThe generative models in Deep learning like GAN (Generative Adversarial Network), Auto \nencoders are capable of generating the new data that have identical statistical characteristics of the \ntraining data.",
                "a) Generative Adversarial Network consists of two blocks."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "RNN": [
                "We analyse music samples\ngenerated by three models \u2013 SampleRNN, Jukebox, and DDSP\n\u2013 and employ a homogeneous framework across all methods\nto allow for objective comparison.",
                "We analyse the generated samples from three deep music\ngeneration methods of different philosophies: the autoregres-\nsive SampleRNN [3], Vector Quantised V AE (VQ-V AE)",
                "We functionally evaluate the perfor-\nmance of three generative models, which subscribe to different\ngeneration paradigms.\nSampleRNN.",
                "The autoregressive SampleRNN",
                "[3] uses\ntiers of RNN modules that work on different timescales of\nthe signal.",
                "The tiered architecture of\nthe SampleRNN allows for different computational focus to be\napplied to different levels of abstraction of the audio, which\nallows long term dependencies to be modelled ef\ufb01ciently.\nJukebox.",
                "SampleRNN Primed 10 4 16\nDDSP Recons 4 n/a 16\nJukebox Primed 24 8 44\nTABLE II\nMODEL PROPERTIES PERTAINING TO GENERATING SAMPLES .",
                "We trained\nSampleRNN and DDSP on the training partition of each\ndataset and used performance on the validation partition to\ntune hyperparameters.",
                "We generate samples with SampleRNN and Jukebox by\n\u2018priming\u2019 the model with an input length of real music to\nseed the sampling process.",
                "0.0450.150.300.450.600.75\n(a) SampleRNN\nactivatedpleasant deactivatedpleasantactivated\nunpleasantdeactivatedunpleasant\nPredictedactivated\n   pleasant\ndeactivated\npleasant\nactivated\nunpleasant\ndeactivated\nunpleasantTrue0.37 0.16 0.42 0.044\n0.15 0.35 0.44 0.064\n0.12 0.13 0.73 0.015\n0.14 0.31 0.44 0.110.150.300.450.60\n (b) Jukebox\nactivatedpleasant deactivatedpleasantactivated\nunpleasantdeactivatedunpleasant\nPredictedactivated\n   pleasant\ndeactivated\npleasant\nactivated\nunpleasant\ndeactivated\nunpleasantTrue0.45 0.18 0.26 0.099\n0.086 0.34 0.44 0.13\n0.071 0.12 0.72 0.092\n0.072 0.28 0.31 0.33 0.150.300.450.60\n (c) DDSP\nFig.",
                "F1 PR-AUC ROC-AUCModelMacro Micro Macro Micro Macro Micro\nSampleRNN 0.063 0.137 0.134 0.158 0.752 0.799\nDDSP 0.066 0.146 0.134 0.159 0.761 0.807\nJukebox 0.063 0.133 0.125 0.151 0.744 0.795\n(a) Mood/theme labels\nF1 PR-AUC ROC-AUCModelMacro Micro Macro Micro Macro Micro\nSampleRNN 0.483 0.523 0.515 0.565 0.764 0.774\nDDSP 0.489 0.526 0.511 0.565 0.759 0.771",
                "Samples generated by\nSampleRNN and Jukebox failed to result in consistent per-\nformance increases to the same degree, although they were\nat least as good as the baseline in terms of PR-AUC and\nROC-AUC, and competitive in terms of F1.",
                "For emotional labels, we observe from Table III(b) that\nsamples generated by SampleRNN and DDSP yield negligible\nperformance increase.",
                "0369121518212427303336394245485154\nPredicted0\n3\n6\n9\n12\n15\n18\n21\n24\n27\n30\n33\n36\n39\n42\n45\n48\n51\n54True\n0.000.150.300.450.60\n(a) SampleRNN\n0369121518212427303336394245485154\nPredicted0\n3\n6\n9\n12\n15\n18\n21\n24\n27\n30\n33\n36\n39\n42\n45\n48\n51\n54True\n0.000.150.300.450.60\n (b) Jukebox\n0369121518212427303336394245485154\nPredicted0\n3\n6\n9\n12\n15\n18\n21\n24\n27\n30\n33\n36\n39\n42\n45\n48\n51\n54True\n0.000.150.300.450.60\n (c) DDSP\nFig.",
                "The SampleRNN samples tend to be predicted as\nbelonging to the \u2018dark\u2019 class.",
                "We see that existing music generation methods implicitly bias the generated samples with speci\ufb01c\nthematic indices.\nfrom DDSP (less so from SampleRNN) to augment training\ndata; however, this effect was not observed in the emotion\nclassi\ufb01cation experiment, where only JukeBox achieved some\nimprovement in Micro averages of the performance measures\nused in MediaEval 2019."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Generating Music Algorithm with Deep Convolutional Generative Adversarial Networks",
            "RNN": [
                "\"C-RNN-GAN: Continuous recurrent neural networks \nwith adversarial training.\""
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "Compared with VAE, GAN has no variation lower bound.",
                "In other words, GAN is progressive, but VAE is biased.",
                "Training GAN needs to reach Nash equilibrium\n [9], \nsometimes it can be done by gradient descent method or not, We have not found a good way to reach Nash equilibrium, so training GAN is unstable \ncompared to VAE or PixelCNN\n[10], but I think in \npractice it is still more stable than training the Boltzmann machine."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "This paper proposes an advanced arithmetic for generating music using Generative Adversarial Networks (GAN).",
                "In most cases, Although GAN excel in image generation, the model adopts a \nfull-channel lateral deep convolutional network structure \naccording to the music data characteristics in this paper, generate music more in line with human hearing and aesthetics.",
                "; deep convolution GAN; full-\nchannel lateral; piano-roll; time sequence data structure algorithm optimization \nI.  INTRODUCTION  \nA.  Background  \nMusic is an art that reflects the emotions of human life.",
                "At present, the integration of AI and music has been \nstudied at home and abroad, mainly in deep learning models such as GAN\n[1], CNN[2] and LSTM[3].",
                "C. Introduction to GAN \nGAN (generative adversarial network) is an artificial \nintelligence algorithm for unsupervised learning.",
                "GAN is constituted \nby two networks, generation network G and discriminator \nnetwork D, G is responsible for generating target objects.",
                "The generator will generate objects that are very similar to the genuine objects, so that the \ndiscriminator network cannot distinguish between the \ngenerated objects and the real objects, thereby achieving GAN training.",
                "GAN Network architecture.",
                "GAN has been widely researched and apply.",
                "Here's a \nbrief list of GAN's advantage and disadvantages.",
                "GAN is a generative model that uses only \nbackpropagation compared to other generation \nmodels (Boltzmann machines\n[5] and GSNs[6]) \nwithout the need for complex Markov chains[7].",
                "GAN can produce a more clear, realistic sample \ncloser to the real object.",
                "GAN adopts an unsupervised learning style training, \nwhich can be widely used in unsupervised learning and semi-supervised learning.",
                "Compared to the variational autoencoders\n[8], GAN \ndoes not introduce any deterministic bias, and the variational method introduces deterministic bias because they optimize the lower bound of the log likelihood rather than the likelihood itself.",
                "Compared with VAE, GAN has no variation lower bound.",
                "In other words, GAN is progressive, but VAE is biased.",
                "Training GAN needs to reach Nash equilibrium\n [9], \nsometimes it can be done by gradient descent method or not, We have not found a good way to reach Nash equilibrium, so training GAN is unstable \ncompared to VAE or PixelCNN\n[10], but I think in \npractice it is still more stable than training the Boltzmann machine.",
                "GAN is not appropriate for processing discrete forms \nof data, such as text.",
                "GAN has problems of unstable training, gradient \ndisappearance, and mode collapse.",
                "DCGAN:",
                "[11], greatly improves the \nstability of GAN training and the quality of the resulting results.",
                "/g120 \n\u6fcb\u6fbb\u6fb5\u6fc2\u6fae\u6f94 Wasserstein GAN[12], solve problems such \nas mode collapse, improve learning stability, and \nprovide meaningful learning curves useful for debugging and hyperparametric searches.",
                "/g120 \n\u6fb6\u6fb9\u6fbb\u6fb5\u6fc2\u6fae  Boundary Equilibrium GAN[14], training \nauto-encoder based Generative Adversarial \nNetworks which Replace the similarity between the distribution by estimating the similarity between the distribution errors of the distribution to achieve fast \nand stable training. \nII.",
                "DCGAN generator is used for LSUN scene modeling.",
                "In addition, the discriminator of the GAN model has been \nspecially optimized and adjusted according to the nature of \nthe music data.",
                "DCGAN almost completely uses the convolutional layer \ninstead of the full-connection layer.",
                "C. GAN Model \n5HDO\n)DNH\n1RL]H5HDO\u0003GDWD\n*HQDHUDWRU\u0003\nGDWDGenerator ModelDiscriminator Model\nFigure 5.",
                "GAN train process.",
                "GAN train process follows as pseudo code: \n##start \ngenerator = Generator_build()discriminator = Discriminator_build ()\ncombiner = discriminator (generator(noise_z))\nfor epoch in range(epochs): \n# Train Discriminator # Generate a half batch of new music \ngen_musics=generator.predict(noise) \n# Train the discriminator \ndiscriminator.train(gen_musics)",
                "C. GAN Train",
                "After establishing the G-network and D-network models, \nwe started to train the network GAN and there is some tips. \n/g120When",
                "/g120Each side of the GAN can overpower the other.",
                "/g120GAN model training is a quite time-consuming \nprocess, so we can consider using parallel GPU for training under conditions, otherwise, we can only do other things while training.",
                "In this paper, we propose a generation model for \ngenerating note sequences under the GAN framework.",
                "\"C-RNN-GAN: Continuous recurrent neural networks \nwith adversarial training.\"",
                "\"BEGAN: \nboundary equilibrium generative adversarial networks.\""
            ],
            "Generative Adversarial Network": [
                "This paper proposes an advanced arithmetic for generating music using Generative Adversarial Networks (GAN)."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Generating Music with Emotions",
            "RNN": [
                "Similarly, recurrent neural network\n(RNN) based GAN is proposed in C-RNN-GAN [23], which\ncan generate polyphonic continuous music sequence."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Generative adversarial\nnetwork (GAN)",
                "MuseGAN",
                "[7] is a convolutional\nneural network (CNN) based GAN to compose polyphonic\nmusic with 5 sound-tracks.",
                "Similarly, recurrent neural network\n(RNN) based GAN is proposed in C-RNN-GAN [23], which\ncan generate polyphonic continuous music sequence.",
                "In [25], a model\ncalled CV AE-GAN was proposed for emotion-conditioned\nsymbolic music generation, which synthesized Conditional-\nV AE and Conditional-GAN [26].",
                "[28], [29] utilized conditional-GAN to generate\nmelody conditioned on the given lyric, in which the generator\nand discriminator were both LSTM networks with lyric as\ncondition."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Generating Music with Generative Adversarial Networks and Long Short-Term Memory",
            "RNN": [
                "Inspired by the image generation [6], continuous Recurrent \nNeural Network with Generative Adversarial Network (C\u2043RNN\u2043GAN)",
                "C\u2043RNN\u2043GAN was trained by using the standard GAN loss function, and tone lengths, note frequencies, intensifies,  and \ntiming were constructed to characterize the music.",
                "Restrictions apply.  \n269 \n more realistic than those generated by the conventional \nRecurrent Neural Network (RNN) model.",
                "LSTM is an artificial cyclic neural network (RNN) structure for deep learning.",
                "[7] Mogren O. C-RNN-GAN: continuous recurrent neural networks with \nadversarial training [EB/OL].  (2016-11-29)."
            ],
            "Recurrent Neural Network": [
                "Restrictions apply.  \n269 \n more realistic than those generated by the conventional \nRecurrent Neural Network (RNN) model."
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "In the paper, a deep learning m odel \ncomposed of Generative Adversarial Networks (GANs) and Long Short-Term Memory (LSTM) was introduced to implement the \ngeneration of music.",
                "The GANs are mainly composed of a \ngenerator and a discriminator, while LSTM serves as the basic \nunit of the generator and discriminator.",
                "Through analyzing the \nmusic file generated in different iteration times, the results \ndemonstrate that the pieces of music we got via the model made up \nof GAN and LSTM are in a similar pattern.",
                "In recent years, Generative Adversarial Networks (GANs) have become one of the hottest research fields among scholars because of their outstanding performance in data generation.",
                "Thus, it is sensibl e \nto come up with the idea of how to use GAN for music creation, which is intriguing and deserves attention.",
                "The basic idea of GANs is derived from the zero-sum game, \nso generally GANs are composed of a generator and a discriminator.",
                "Inspired by the image generation [6], continuous Recurrent \nNeural Network with Generative Adversarial Network (C\u2043RNN\u2043GAN)",
                "C\u2043RNN\u2043GAN was trained by using the standard GAN loss function, and tone lengths, note frequencies, intensifies,  and \ntiming were constructed to characterize the music.",
                "With the help of GAN, a normal person like you and me who don\u2019t have any previous experience in music is able to generate unique music through a simple click.",
                "Moreover, we would like to make some innovations by joining Long Short-term Memory (LSTM) into our GANs model.",
                "B. Generating Music based on GANs and LSTM",
                "The deep learning model used in this paper is mainly GAN, \nwhich is composed of an Encoder, a Generator and a Discriminator, as shown in Figure 1, and the Generator and \nDiscriminator are mainly composed of LSTM.",
                "The GAN structure used for music generation.",
                "The GAN mainly follows the game confrontation theory, and \nthe generator and the discriminator are balanced through a zero -\nsum game.",
                "In the GANs we designed, both the discriminator and \ngenerator are composed of two LSTM layers with one layer stacked on the top of another layer, which is suitable for retention of time-dependent patterns, especially when learning patterns cover a relatively long time.",
                "The reason why LSTMs were embedded in our GANs was \nthat the time dependent patterns could be kept through it, and the \nstructure of the LSTM can be summarized as in Figure 4 .",
                "B. Results \nThe configuration of GAN model is shown in Table 1  and \nthe structure of the signal is shown in Table 2 .",
                "TABLE 1 CONFIGURATION OF GAN  MODEL  \nGenerator \nNumber of \nEpoch Valid  \nLabel \nShape Fake  \nLabel \nShape True \nShape Fake \nShape \n5 (3, 1) (3, 1) (3, 6, 1) (3, 6, 1) \nTABLE 2 STRUCTURE OF SIGNAL  \nSample Rate Length of Signal Signal Data Type \n22050 44100",
                "Besides, the loss keeps decreasing after each epoch by definition and the validation loss is always smaller than the training loss, which means that there is some under-fitting of the model. \nTABLE 3  LOSS AND ACCURACY OF LSTM-GAN",
                "We improved the structure of the GAN model and fixed some mistakes that appeared in the \nfunction train GAN.",
                "273 \n of GAN training.",
                "Since by lookin g \ncloser to the sequences generated by the GAN, the range and \nstyle of the sequences are very close to the sequences converte d \nfrom the sample data.",
                "This means the GAN is probably still functioning its job; however, the lack of training epochs makes  \nit hard to capture the features imbedded in the sample clips.",
                "D. Potential reasons and further improvements \nWith the increase of training times, the generated WAV files \nare broken, all of these clues point in one direction: there ma y \nbe some problems with the GAN model architecture.",
                "Future research may combine new ideas, such as Musical Instrument Digital Interface (MIDI), to train the new GAN model.",
                "In the paper, we tried to use GANs and LSTM to generate \nmusic.",
                "LSTM is the basic unit of the generator and discriminato r \nin GANs.",
                "[7] Mogren O. C-RNN-GAN: continuous recurrent neural networks with \nadversarial training [EB/OL].  (2016-11-29).",
                "[9] Engel J,Agrawal K K, Chen S,et al. GANSynth: adversarial neural  audio \nsynthesis  \n[10] https://en.wikipedia.org/wi ki/Long_short-term_memory"
            ],
            "Generative Adversarial Network": [
                "In the paper, a deep learning m odel \ncomposed of Generative Adversarial Networks (GANs) and Long Short-Term Memory (LSTM) was introduced to implement the \ngeneration of music.",
                "In recent years, Generative Adversarial Networks (GANs) have become one of the hottest research fields among scholars because of their outstanding performance in data generation.",
                "Inspired by the image generation [6], continuous Recurrent \nNeural Network with Generative Adversarial Network (C\u2043RNN\u2043GAN)"
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Generation of Music With Dynamics Using Deep Convolutional Generative Adversarial Network",
            "RNN": [
                "A. Recurrent Neural Network \nRecurrent Neural Network (RNN)",
                "However, RNN suffers from the \nvanishing and exploding gradient problem due to the \nconstant updates of the weight matrix during \nbackpropagation.",
                "A variation of RNN, Long Short-Term Memory (LSTM)",
                "[3], solved the problem in a novel RNN by using various \ngates.",
                "LSTM is used in an expressive music generation system \nPerformance RNN [4], developed by a google group \nMagenta."
            ],
            "Recurrent Neural Network": [
                "A. Recurrent Neural Network \nRecurrent Neural Network (RNN)"
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "With the piano-roll data \nrepresentation, Deep Convolutional Generative Adversarial \nNetwork (DCGAN) learned the data distribution from the \ngiven dataset and generated new data derived from the same \ndistribution.",
                "The \nevaluation results verified that DCGAN could generate \nexpressive music comprising of music dynamics and \nsyncopated rhythm.",
                "Keywords-music generation; DCGAN; dynamics; pianoroll \nI.  INTRODUCTION  \nMusic Generation is composing music with a computer \nthat uses machine learning methods.",
                "DCGAN was chosen to be the deep \nlearning architecture used in this paper.",
                "B. Generative Adverserial Network  \nGenerative Adversarial Network (GAN)",
                "MuseGAN",
                "PROPOSED METHOD  \nIn this paper, we designed and implemented a music \ngeneration system that can generate polyphonic music with \ndynamic using Deep Convolutional Generative Adversarial \nNetwork (DCGAN).",
                "With the image-like data representation \nand DCGAN ability to generate sample from the same data \ndistribution, we hypothesized that the DCGAN can generate \nmusic that incorporated with music dynamics.",
                "In this paper, a music generation system was \nimplemented under the framework of GANs, DCGAN.",
                "Future \nresearch using GAN in music generation could focus on \ngenerating arbitrary length of music or gather a larger data \nset for training."
            ],
            "Generative Adversarial Network": [
                "Generation of Music With Dynamics Using \nDeep Convolutional Generative Adversarial Network \n \nRaymond Kwan How Toh \nSchool of Computer Science and Engineering \nNanyang Technological University \nSingapore \nrtoh004@ntu.edu.sg Alexei Sourin \nSchool of Computer Science and Engineering \nNanyang Technological University \nSingapore \nassourin@ntu.edu.sg\n \n \nAbstract \u2014Following the rapid advancement of Artificial \nIntelligence and transition into the era of Big Data, researchers \nhave started to explore the possibility of using machine \nlearning in creative domains such as music generation.",
                "B. Generative Adverserial Network  \nGenerative Adversarial Network (GAN)"
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Monophonic Music Generation With a Given Emotion Using Conditional Variational Autoencoder",
            "RNN": [
                "Figs. 5 and 6 show the encoder and decoder of CVAE, which\nwere implemented using the recurrent neural network (RNN).",
                "For faster RNN learning, the sequences are\nnormalized (mean: 0.00, std: 1.00).",
                "[49], which\nmake up RNN.",
                "After combining, two inputs are used\nto layer RepeatVector to prepare the data size for the next\nlayer which is RNN with 512 GRU.",
                "CVAE RNN encoder.",
                "CVAE RNN decoder.",
                "[49] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y. Bengio, ``Learning phrase representations using RNN\nencoder\u0015decoder for statistical machine translation,'' in Proc."
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "In [36], a generative VAE model to control\ntonal tension in generated music was used.",
                "V. CONDITIONAL VAE\nA generative model based on variational autoencoder\n(VAE)",
                "The advantage of VAE is the\nability to move in the continuous latent space of trained VAE,\nwhich allows to generate new musical sequences.",
                "In order\nto add the possibility of controlling the type of emotions in\nthe generated musical sequences, the model was extended to\nconditional VAE (CVAE)",
                "What makes CVAE different\nfrom VAE is the addition of a condition, which in our case is\nan emotion label (Fig. 4).",
                "CVAE model structure.",
                "A. IMPLEMENTATION OF GENERATIVE MODEL\nFor building implementation of the CVAE network and con-\nducting the experiments, the Keras4deep learning library\nwritten in Python with Tensor\u001dow5as backend was used.",
                "Figs. 5 and 6 show the encoder and decoder of CVAE, which\nwere implemented using the recurrent neural network (RNN).",
                "CVAE allows to generate musical sequences with a speci\u001cc\nemotion through random sampling from the latent space,\nwhich in our case has 20 dimensions.",
                "The CVAE network consist of the encoder and the decoder\njoined together.",
                "The shape of the music sequences on\nthe CVAE input and output is the same (None, 64, 31).",
                "CVAE RNN encoder.",
                "As a loss function to\ntrain the CVAE network, categorical crossentropy was used,\nwhich computes the crossentropy loss between the one-hot\npitch values and predictions.",
                "The CVAE was trained with RMSprop optimizer\n(lrD0.001).",
                "CVAECDense was chosen as a baseline for comparing the\nresults of the obtained models.",
                "It differed from CVAE CGRUTABLE 2.",
                "From\nthe obtained results, we can see that models CVAE CGRU\nwith more than 64 GRU units are superior to the baseline\nmodel (CVAECDense).",
                "We can see that the recurrent units in\nCVAE are better suited for encoding and decoding sequential\ndata, which is of course well known.",
                "Testing how the use of\nthe baseline model (CVAE CDense) and the proposed model\n(CVAECGRU) affects the obtained metrics for the generated\nVOLUME 9, 2021 129093J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 6.",
                "CVAE RNN decoder.",
                "music depending on the type of emotion will be presented\nin Section VI-B.\nFig. 7 presents the stages of CVAE training with the use\nof input and output data visualization.",
                "CVAE is not yet suf\u001cciently trained and is\nunable to generate a sequence close to the input sequence.",
                "A trained CVAE model was used to generate new music\nsequences with a speci\u001cc emotion.",
                "Stages of CVAE training over epochs, illustrated with (a) input\nexample and output example during training with (b) 50, (c) 100,\n(d) 500",
                "Latent space of CVAE.",
                "Additionally, KS statistic was calculated between the\nmusic generated by the baseline model (CVAE CDense) and\nthe training sets, which are presented in Table 9.",
                "This is con\u001crmed\nby the use of the CVAE CGRU model with recurrent units for\nsequence processing; it is better suited than CVAE CDense.",
                "The use of CVAECDense as the baseline model showed\nthat the non-recursive model is worse at generating music\nwith e1 and e2 emotions than the CVAE CGRU model\n(Tables 8 and 9).",
                "Assessment of\nthe generated examples pertained two models: the baseline\n(CVAECDense) and the proposed model (CVAE CGRU).",
                "Expert annotations of the generated set by the base-\nline model (CVAECDense) and by the proposed model\n(CVAECGRU) are presented in Table 10 and 11.",
                "An interesting observation is that the more complex\nmodel, which is the proposed model (CVAE CGRU), is\nbetter at generating \u001cles with positive emotions (e1 - accu-\nracy 100%; e4 - accuracy 100%) and slightly worse at gen-\nerating \u001cles with negative emotions (e2 - accuracy 85%;\ne3 - accuracy 70%).",
                "In the case of \u001cles generated by the baseline model\n(CVAECDense), we notice a deterioration in the genera-\ntion of \u001cles with positive emotions (e1 - 80%, e4 - 60%),\ni.e. the opposite situation than in the case of the proposed\nmodel (greater accuracy for emotions e2 and e3, and worse\nfor e1 and e4).",
                "Available:\nhttp://arxiv.org/abs/1704.01444\n[34] G. Hadjeres, F. Nielsen, and F. Pachet, ``GLSR-VAE: Geodesic latent space\nregularization for variational autoencoder architectures,'' in Proc."
            ],
            "Variational Autoencoder": [
                "Digital Object Identifier 10.1 109/ACCESS.2021.31 13829\nMonophonic Music Generation With\na Given Emotion Using Conditional\nVariational Autoencoder\nJACEK GREKOW\n AND TEODORA DIMITROVA-GREKOW\nFaculty of Computer Science, Bialystok University of Technology, 15-351 Bialystok, Poland\nCorresponding author: Jacek Grekow (j.grekow@pb.edu.pl)"
            ],
            "GAN": [
                "Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang, ``MuseGAN:\nMulti-track sequential generative adversarial networks for symbolic music\ngeneration and accompaniment,'' in Proc."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Music Deep Learning: A Survey on Deep Learning Methods for Music Processing",
            "RNN": [
                "DL METHODS FOR MIR\nThe DL architectures that are most frequently employed for\nMIR tasks are: i) Recurrent Neural Networks (RNNs), and\nii)Convolutional Neural Networks (CNNs).",
                "I\nDL METHODS FOR MIR\nDL Architectures Applications Research Paper\nRNNs Feature extraction [11] - [14]\nLSTMs Emotion prediction [10]\nCNNs Feature extraction [16] - [25], [27]\nUnsupervised Learning Sound representations",
                "[28]\nA. RNNs\nRNNs are a family of neural networks for processing\nsequential data [1].",
                "A subset of RNNs which has been suc-\ncessfully applied in many different areas including MIR is the\nLong Short Term Memory networks (LSTM)",
                "[14] different variants of RNN\narchitectures are employed in order to tackle such problems.",
                "In addition a combination of\na feed-forward neural network with a RNN performed better\nthan the individual models themselves.",
                "The most common approaches to MG are: i)\nRNNs - LSTMs , ii)Generative Adversarial Networks (GANs),\nand iii) Transformers .",
                "TABLE II\nDL METHODS FOR MG\nDL Architectures Applications Research Paper\nRNNs Music generation",
                "[52] - [55]\nA. RNNs\nRNNs have been proved powerful for MIR tasks.",
                "Classical\nRNN architectures have been tested on various MG tasks [30]\n- [35].",
                "In [33] a novel RNN model, DeepBach, is proposed aimed\nat modeling polyphonic music and specifically hymn-like\npieces, while in [34] the model produces only drums\u2019 sounds.",
                "Instead of using simple RNNs one can test LSTM archi-\ntectures for MG tasks [36] - [42].",
                "Interactive Music Genera-\ntion with Positional Constraints using Anticipation-RNNs.",
                "Explicitly Conditioned\nMelody Generation: A Case Study with Interdependent RNNs.",
                "[35] Soroush M., et al. SampleRNN: An Unconditional End-to-End Neural\nAudio Generation Model."
            ],
            "Recurrent Neural Network": [
                "DL METHODS FOR MIR\nThe DL architectures that are most frequently employed for\nMIR tasks are: i) Recurrent Neural Networks (RNNs), and\nii)Convolutional Neural Networks (CNNs)."
            ],
            "VAE": [],
            "Variational Autoencoder": [
                "The\nraw audio data were first compressed into compressed codes\nusing Vector Quantization - Variational Autoencoders (VQ-\nV AE), a variant of classical V AE which produces discrete data."
            ],
            "GAN": [
                "The most common approaches to MG are: i)\nRNNs - LSTMs , ii)Generative Adversarial Networks (GANs),\nand iii) Transformers .",
                "[36] - [42]\nGANs Symbolic music generation [44] - [51]\nTransformers Longer sequences generation",
                "B. GANs\nAnother popular approach in the field of MG is the use of\nGANs.",
                "GANs were first introduced in [43].",
                "The core idea\nbehind GANs is the existence of two antagonistic entities;\nthe generator and the discriminator.",
                "GANs have found great\nsuccess in the image generation task and since they were\nintroduced many researchers have trained GAN models for\nMG problems [44] - [51].",
                "Symbolic music is music stored in a notation-based format,\nwhich makes it easier for GANs to train on.",
                "Many different\nGANs have been applied on this task [45], [46], [48].",
                "Poly-\nphonic music generation is discussed in [47], while DRUM-\nGAN",
                "The authors of\n[49] demonstrated that GANs are able to generate high-fidelity\nand locally-coherent audio by modeling log magnitudes and\ninstantaneous frequencies with sufficient frequency resolution\nin the spectral domain.",
                "Self-attention mechanism is combined\nwith GANs in [51] in order to extract more temporal features\nto generate multi-instruments music.",
                "SeqGAN:\nsequence generative adversarial nets with policy gradient.",
                "Symbolic\nMusic Genre Transfer with CycleGAN.",
                "[48] Hao-Wen Dong,* Wen-Yi Hsiao,* Li-Chia Yang and Yi-Hsuan Yang,\n\u201dMuseGAN: Multi-track Sequential Generative Adversarial Networks\nfor Symbolic Music Generation and Accompaniment,\u201d Proceedings of\nthe 32nd AAAI Conference on Artificial Intelligence (AAAI), 2018.",
                "GANSynth:",
                "DrumGAN: Synthesis\nof drum sounds with timbral feature conditioning using Generative\nAdversarial Networks.",
                "\u2329hal-\n03233337 \u232a\n[51] F. Guan, C. Yu and S. Yang, \u201dA GAN Model With Self-attention\nMechanism To Generate Multi-instruments Symbolic Music,\u201d 2019\nInternational Joint Conference on Neural Networks (IJCNN), 2019, pp.\n1-6, doi: 10.1109/IJCNN.2019.8852291."
            ],
            "Generative Adversarial Network": [
                "The most common approaches to MG are: i)\nRNNs - LSTMs , ii)Generative Adversarial Networks (GANs),\nand iii) Transformers .",
                "A Convolutional\nGenerative Adversarial Network for Symbolic-Domain Music Gener-\nation.",
                "Polyphonic Music\nGeneration with Sequence Generative Adversarial Networks.",
                "[48] Hao-Wen Dong,* Wen-Yi Hsiao,* Li-Chia Yang and Yi-Hsuan Yang,\n\u201dMuseGAN: Multi-track Sequential Generative Adversarial Networks\nfor Symbolic Music Generation and Accompaniment,\u201d Proceedings of\nthe 32nd AAAI Conference on Artificial Intelligence (AAAI), 2018."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "In \u00a1i\u00bfProceedings of the 15th International Workshop on\nContent-Based Multimedia Indexing\u00a1/i\u00bf (\u00a1i\u00bfCBMI \u201917\u00a1/i\u00bf).",
                "Polyphonic Music Mod-\nelling with LSTM-RTRBM."
            ],
            "Hopfield Network": []
        },
        {
            "title": "Music Generation using Deep Generative Modelling",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "In juxtaposition, Generative Adversarial \nNetworks  (GANs)  are effective  for modeling  globally  coherent \nsequence structures, but struggle to generate localized sequences.",
                "Through this project, we  aim to propose  a system that combines \nthe random subsampling  approach  of GANs  with  a recurrent \nautoregressive  model.",
                "The models we \nstudied were largely  based on deep generative modeling and \nconsisted of LSTM (Shin A., Crestel L.), Transformer (Cheng - \nZhi, et al.), AutoEncoders (Engel J., Resnick C., et al.) and \nGAN (Hao -Wen Dong, et al.)",
                "Based on our literature survey, we identified shortcoming s and \nopportunities in WaveNet and MuseGAN, the two models that \nproduced the most significant results.",
                "However, GANs fail when it comes to \ngenerating short term sequences due to overfitting on training \ndata and are extremely expensive from a computational \nperspective.",
                "This literature review has allowed us to propo se the \ndevelopment of a model that combines the lightweight \nautoregressive approach of WaveNet with the random \nsubsampling methodology of GANs to develop coherent \nmedium -to-long term musical subsequences without having to \nconform to computing or memory lim itations.",
                "We aim to conceptualize a \nmodel that amalgamates the properties of Auto Encoders and \nGANs in a recurrent feedback loop.",
                "The concept of GAN is fundamentally unambiguous and \nsimple.",
                "` The GAN based Autoregressive approach, which we have \nillustrated in Figure 1, can prove to be fruitful because GANs \nbeing very effective in modelling an underlying pattern will \nproduce quality results.",
                "The proposed system which combines a GAN based system with an \nautoregressive approach  \nVIII.",
                "The following results, as seen in Figures 2,3 and 4, were \nachieved when a GAN based model was trained on a dataset \nconsisting of Bach\u2019s musical symphonies.",
                "2. Representation of musical waveforms  \nHere, we have used random interpolation to generate music on \na scaled  GAN  architecture.",
                "Future scope includes development of an autoregressive GAN \nmodel  in a recurrent  feedback  loop that can be used not just for \nmusic generation, but also for detection of plagiarism in music \nby introducing original work as a sample data point and the \ndubious work as the fake samples in the discriminator function \nof the GAN.",
                "Combining Autoencoders with GANs  and allowing  the model to \nregress  on itself  can reduce  the amount  of data  required  while \nspeeding  up  computation.",
                "This  is possible due  to the \ndimensionality  reduction  being  done  by autoencoders  working \nwith the sampling  and  generative  abilities  of GANs.",
                "[2] H. W. Dong, W. Y. Hsiao, L. C. Yang and Y. H. Yang, \u201cMuseGAN\ndemonstration of a convolutional GAN based model for generating multi-track \npiano-rolls.,\u201d in International Society of Music Information Retrieva\nConference , 2017."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Music Generation using Deep Learning with Spectrogram Analysis",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "ZH\u0003 VWLOO\u0003 KDYH\u0003\nSURPLVLQJ\u0003UHVXOWV\u0003JHQHUDWHG\u0003IURP\u0003RXU\u0003PRGHOV\u0011\u0003)RU\u0003IXWXUH\u0003ZRUNV\u000f\u0003\nZH\u0003PLJKW\u0003EH\u0003DEOH\u0003WR\u0003JHQHUDWH\u0003XQGLVWLQJXLVKDEOH\u0003PXVLF\u0003ZLWK\u0003OLWWOH\u0003\nHIIRUW\u0011\u0003\nKeywords-music generation; LSTM; GAN; spectrogram; deep \nlearning; generative model; MIDI \n,\u0011\u0003\u0003,1752'8&7,21 \u0003\n)",
                "WR\u0003PDNLQJ\u0003SUHGLFWLRQV\u0003EDVHG\u0003RQ\u0003WLPH\u0003VHULHV\u0003GDWD\u0003>\u0016\u001b@\u0011\u0003\n2) GAN \n$\u0003 JHQHUDWLYH\u0003 DGYHUVDULDO\u0003 QHWZRUN\u0003 \u000b*$1\f\u0003 LV\u0003 D\u0003 FODVV\u0003 RI\u0003\nPDFKLQH\u0003OHDUQLQJ\u0003IUDPHZRUNV\u0003GHVLJQHG\u0003E\\\u0003,DQ\u0003*RRGIHOORZ\u0003DQG\u0003KLV\u0003FROOHDJXHV\u0003LQ\u0003\u0015\u0013\u0014\u0017\u0011\u0003,Q\u0003D\u0003JHQHUDWLYH\u0003DGYHUVDULDO\u0003QHWZRUN\u000f\u0003\nWZR\u0003QHXUDO\u0003QHWZRUNV\u0003FRQWHVW\u0003HDFK\u0003RWKHU\u0003LQ\u0003D\u0003]HUR\u0010VXP\u0003JDPH\u000f\u0003\nZKHUH\u0003RQH\nV\u0003JDLQ\u0003LV\u0003DQRWKHU\nV\u0003ORVV\u0011\u0003*$1\u0003OHDUQV\u0003WR\u0003JHQHUDWH\u0003GDWD"
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Music Generation with AI technology: Is It Possible?",
            "RNN": [],
            "Recurrent Neural Network": [
                "[1]  M. M. Team, \"Recurrent Neural Networks \u2013 with memory,\" [Online]."
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "These three models are Biaxial-LSTM, DeepJ and \nMuseGAN.",
                "Keywords \u2014Music Generation, LSTM, Biaxial LSTM, DeepJ, \nMuseGAN, Deep Learning, GAN \nI. INTRODUCTION  \nIn the era of big data, the demand for short videos and game \nsoundtracks has grown by leaps and bounds with the rapid \ndevelopment of streaming platforms.",
                "In this paper, we did a literature review focused on three \nmain deep learning models for music generation, Biaxial-LSTM, \nDeepJ, and MuseGAN.",
                "MuseGAN \nis an innovational approach based on Generative Adversarial \nNetwork that can be used to generate pop music with multiple \ntracks.",
                "Moreover, MuseGAN used many statistical indicators \nsuch as the ratio of empty bars and the number of used pitch \nclasses per bar to avoid the response errors from the Turing test.",
                "Therefore , we will then introduce two deep learning models, \nDeepJ and MuseGAN, with corresponding implementations in \nthe following sections.",
                "Both of these two models are related to \nmusic style, while the main difference is, the DeepJ model is \napplicable to all musi c styles, but the MuseGAN model is mostly \nused to generate pop music which has multiple instruments and tracks.",
                "Restrictions apply.  \nFig. 4. MuseGAN Framework  \nAnother improvement compared with previous Biaxial \nLSTM is that we add a 1\ud835\udc37 convolution layer with \ud835\udc58\ud835\udc52\ud835\udc5f\ud835\udc5b\ud835\udc52\ud835\udc59 =2. \nWith the rapid development of generative models in recent \nyears, generative adversarial neural networks have made very \nsignificant progress in data depth falsification.",
                "The mathematical theory that underpins GAN is extremely \ncomplex.",
                "Simply speaking, GAN is a class of machine learning \ntechniques consisting of two simultaneously trained models: a \ngenerator, which is trained to generate pseudo -data, and a \ndiscriminator, which is trained to identify pseudo -data from real \ndata.",
                "Key information for generators and discriminators  \nThe training process of GAN can be seen as a zero -sum game, \nand eventually the GAN model will reach a Nash equilibrium, \ni.e., the pseudo -samples generated by the generator are no \ndifferent from the real data in the training set.",
                "4) Apply GAN into pop music generation  \nPop music is usually composed of multiple \ninstruments/tracks.",
                "In the next experiments, the authors use three models, Jamming \nmodel, Composer model , Hybrid model, which have different \nimplementation mechanisms, but they are all based on \nGenerative Adversarial Neural Network (GAN).",
                "7. Flow chart of pre -processing  \n6) Architecture  \nGANs  implements adversarial learning mainly by \nconstructing two sub -network generators and discriminators.",
                "The \nauthors used three models based on GANs, which are Jamming \nmodel, Composer model and Hybrid model respectively.",
                "For the Jamming model, it is equivalent to each member of \nthe band having an independent random noise \ud835\udc4d. Each member \ngenerates music based on its own GAN model.",
                "Introduction of three GANs models  \nIII.",
                "COMPARISON  \nA. Between Biaxial LSTM, DeepJ and MuseGAN",
                "GAN  with temporal  struc ture \nTherefore, in the next section, we will focus on comparing \nthe DeepJ and MuseGAN models, as two models related to \nmusic styles, in order to do better processing for the music style.",
                "The MuseGAN model, on the other \nhand, is mainly targeted at modern pop music and usually \ncontains a variety of instruments such as drums, bass, guitar, \norchestra and piano.",
                "the research objectives, features in datasets and the \nevaluation methods taken by DeepJ model and the MuseGAN \nmodel.",
                "Certain \u201cmood\u201d can be generated also, to fit the \nstyle that the user wishes to compose.  \n2) MuseGAN model  \nSince the existing convolutional generative adversarial \nnetworks (GANs) models for generating music have some limits \nand cumbersome pre -processing such as, hard thresholding (HT) \nor Bernoulli sampling (BS), the research objective of MuseGAN \nis to improve a  convolutional GAN model that can use binary \nneurons to directly creates binary -valued piano -rolls.",
                "2) MuseGAN model  \nThe goal of Mu seGAN is to generate pop music of multiple \ntracks in piano -roll format.",
                "B. MuseGAN",
                "This also makes the evaluation of the model \nfor MuseGAN more reliable than the questionnaire.",
                "In MuseGAN model, the evaluation is mainly based on \nseveral indicators of the generated music, which show how \ncomplex a music piece is.",
                "This model \nhas the ability to generate polyphonic music, but since the \nalgorithm still does not have the ability to filter music styles in \norder to customize music generated, two cutting -edge researches \non AI music gen eration  were  further explored, which are the \nDeepJ and MuseGAN models.  \n \nFig. 12.",
                "The \nMuseGAN model is another style -related model that is used to \ngenerate music with multiple instrument tracks, normally used \nto generate modern pop music.",
                "Compared with DeepJ model, the MuseGAN model is \nmore suitable for generating pop music w ith multiple \ninstruments and tracks, and can be used in the future for game \nsoundtracks, etc.",
                "Compared to the \nMuseGAN model, the DeepJ model introduces human  \nevaluation and therefore has a deeper dimensionality.",
                "[4]  V. Bok, GANs in Action, 2017.",
                "[5]  H. W. Dong,  W. Y. Hsiao, L. C. Yang and Y. H. Yang  \"MuseGAN: Multi -\ntrack Sequential Generative Adversarial Networks for Symbolic Music \nGeneration and Accompaniment,\" 2019."
            ],
            "Generative Adversarial Network": [
                "[5]  H. W. Dong,  W. Y. Hsiao, L. C. Yang and Y. H. Yang  \"MuseGAN: Multi -\ntrack Sequential Generative Adversarial Networks for Symbolic Music \nGeneration and Accompaniment,\" 2019."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Music Generation with Bi-Directional Long Short Term Memory Neural Networks",
            "RNN": [
                "Recurrent  Neural  Network  (RNN) \nunits  have  been  used  in the form  of Generative  Adversarial \nNetworks,  Auto-Encoders  and Long  Short-Term  Memory \nNetworks but they have been plagued by issues with regard to \nvarying  offsets  and different  sequence  lengths.",
                "A. Bidirectional LSTM  \nRNNs maintain a memory based on historic sequential \ndata, where the output is determined by long -distance attributes of the incoming sequence.",
                "Thus, it is ideal to emp loy \nRNN for predictive music generation.",
                "Bi-directional RNNs are a method of stacking two LSTM \nnetworks into a single layer, where the first RNN processes \ndata in the forward direction while the second RNN processes \ndata in the backward direction, as shown  below  in figure 1.",
                "[5] O. Mogren, \u201cC -RNN"
            ],
            "Recurrent Neural Network": [
                "Recurrent Neural Networks,\u201d 2016."
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Mogren et al. have generated polyphonous music using \nGenerative Adversarial Networks (GANs).",
                "GANs suffer from probl ems such as vanishing gradient, \nmode collapse and divergence mismatching",
                "This gave \nthem better results than traditional GANs [6].",
                "Further, the proposed \nmodel is able to overcome the drawbacks of the other \narchitectures such as GANs and Auto -Encoders by not being \nsusceptible to mode -collapse or generating overly repetitive \nmusic respectively.",
                "-GAN: Continuous recurrent neural networks \nwith adversarial t raining,\u201d arXiv [cs.AI], 2016.",
                "Wass erstein \nGAN. arXiv preprint arXiv:1701.07875, 2017."
            ],
            "Generative Adversarial Network": [
                "Mogren et al. have generated polyphonous music using \nGenerative Adversarial Networks (GANs)."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "\u201cModelling \nhigh-dimensional sequences with LSTM -RTRBM: application to \npolyphonic music generation\u201d 24th International Conference on \nArtificial Intelligence (IJCAI'15)."
            ],
            "Hopfield Network": []
        },
        {
            "title": "RE-RLTuner: A topic-based music generation method",
            "RNN": [
                "To apply music theory to music generation, a novel\nsequence learning approach, RLTuner was proposed in [12].In this model, Reinforcement Learning is used to imposestructure on an RNN.",
                "Task-related rewards and the probabil-ity of a given action, which is learned from the pre-trainedRNN, are combined by the reward function in RLTuner.",
                "NoteRNN is trained as a single-layer LSTM network,\nconsisting of Q network, Target Q network and RewardRNN.Target Q network is the delayed copy of the Q network.",
                "RewardRNN is \ufb01xed together with the reward function tomaintain its characteristics.",
                "The accuracy of the veri\ufb01cationset was 76%.RL: In the RL module, NoteRNN\u2019s structure is similar to thereward function for it only takes the music sequence as input.",
                "The LSTM we used isNoteRNN.",
                "C-RNN-GAN: Continuous recurrent neural networks with\nadversarial training[J]. 2016."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Generative AdversarialNetwork (GAN)[2], [3], [4] has also been applied to musicgeneration, where random noises serve as an input to thegenerator whose goal is to transform random noises intothe objective.",
                "GAN-based methods often achieve outstandingresults by carefully designing the generating strategy ofthe discriminator.",
                "C-RNN-GAN: Continuous recurrent neural networks with\nadversarial training[J]. 2016.",
                "[3] Dong H W , Hsiao W Y , Yang L C , et al. MuseGAN: Symbolic-\ndomain Music Generation and Accompaniment with Multi-track Se-\nquential Generative Adversarial Networks[J]. 2017.",
                "Conditional LSTM-GAN for Melody Generation\nfrom Lyrics[J]. 2019."
            ],
            "Generative Adversarial Network": [
                "[3] Dong H W , Hsiao W Y , Yang L C , et al. MuseGAN: Symbolic-\ndomain Music Generation and Accompaniment with Multi-track Se-\nquential Generative Adversarial Networks[J]. 2017."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Some Reflections on the Potential and Limitations of Deep Learning for Automated Music Generation",
            "RNN": [
                "B. Recurrent Neural Networks\nRecurrent Neural Networks (RNN) are an archi-\ntecture created to work with sequential data: the\nbasic idea is to have each neuron in the network\nreceive as its input both the current step of the\nsequence and the previous, carrying on those values\nacross all time steps and allowing the network to\nmodel temporal dependencies.",
                "Long Short Time\nMemory(LSTM) cells solve the vanishing/exploding\ngradient problem of simple RNN introducing three\ngates (input, output and forget gates) to regulate the\n\ufb02ow in each cell [7].",
                "[16] Simon, Ian and Oore, Sageev, \u201cPerformance\nRNN: Generating Music with Expressive Tim-\ning and Dynamics,\u201d 2017."
            ],
            "Recurrent Neural Network": [
                "B. Recurrent Neural Networks\nRecurrent Neural Networks (RNN) are an archi-\ntecture created to work with sequential data: the\nbasic idea is to have each neuron in the network\nreceive as its input both the current step of the\nsequence and the previous, carrying on those values\nacross all time steps and allowing the network to\nmodel temporal dependencies."
            ],
            "VAE": [],
            "Variational Autoencoder": [
                "C. Variational Autoencoders"
            ],
            "GAN": [
                "D. Generative Adversarial Networks\nIntroduced by Ian Goodfellow in [20] GANs are a\nnew paradigm that is based on an adversarial game\nbetween a Generator network and a Discriminator\nnetwork.",
                "GANs output is more sharp and de\ufb01ned that those\nof V AEs, but training is very unstable and compu-\ntationally intense.",
                "It has to be noted that\nGANs are still affected by the issues of LSTM and\nCNN if those architecture are part of the generator\nand discriminator networks.",
                "Yang,\n\u201cMuseGAN: Symbolic-domain music generation and ac-\ncompaniment with multi-track sequential generative adver-\nsarial networks,\u201d arXiv preprint arXiv:1709.06298, 2017."
            ],
            "Generative Adversarial Network": [
                "D. Generative Adversarial Networks\nIntroduced by Ian Goodfellow in [20] GANs are a\nnew paradigm that is based on an adversarial game\nbetween a Generator network and a Discriminator\nnetwork."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation",
            "RNN": [
                "In particular, we explore the effect of explicit\narchitectural encoding of musical structure via comparing two\nsequential generative models: LSTM (a type of RNN) and\nWaveNet (dilated temporal-CNN).",
                "As far as we know, this is the\n\ufb01rst study of applying WaveNet to symbolic music generation,\nas well as the \ufb01rst systematic comparison between temporal-\nCNN and RNN for music generation.",
                "We did a systematic\ncomparison between two main-stream approaches of handling\nmusic structure representation using two sequential representa-\ntion generative models: LSTM (a type of RNN) and WaveNet\n(dilated temporal-CNN).",
                "To our knowledge, this is the \ufb01rst systematic\ncomparison between temporal-CNN and RNN for symbolic\nmusic application.",
                "With Recurrent Neural Network (RNN)",
                "The\nwork by [30] demonstrated that RNNs is capable of revealing\nsome higher-level information in melody generation.",
                "The work by [31] de\ufb01ned\nseveral measurements (Tone division, Mode, Number of Oc-\ntaves, etc.) and create melody sequences by RNN by varies\ninspirations.",
                "DeepBach [4] introduced an innovated bidirectional RNN for\nmusic harmonization.",
                "[29] K. Cho et al., Learning Phrase Representations using RNN Encoder-\nDecoder for Statistical Machine Translation, The Conference on Empir-\nical Methods on Natural Language Processing, 2014."
            ],
            "Recurrent Neural Network": [
                "With Recurrent Neural Network (RNN)",
                "[27] Z. C. Lipton, A Critical Review of Recurrent Neural Networks for\nSequence Learning, CoRR, abs/1506.00019, 2015."
            ],
            "VAE": [],
            "Variational Autoencoder": [
                "[33] A. Roberts, J. Engel and D. Eck, Hierarchical Variational Autoencoders\nfor Music, The 31st Conference on Neural Information Processing\nSystems, Workshop for Machine Learning for Creativity and Design,\n2017."
            ],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "PopMNet: Generating structured pop music melodies using neural networks",
            "RNN": [
                "PopMNet  \nconsists  of a Convolutional  Neural  Network (CNN)-based  Structure  Generation  Net (SGN) \nand a Recurrent  Neural  Network (RNN)-based  Melody  Generation  Net (MGN).",
                "The proposed  model  is compared  with four existing  \nmodels  AttentionRNN,  LookbackRNN,  MidiNet  and Music Transformer.",
                "Some models  are based  \non recurrent  neural  networks  (RNN)",
                "Second,  melodies  are generated  conditioned  on the structure  by the \nMelody  Generation  Net (MGN),  which  consists  of three RNNs (see Section 4for details).",
                "With the development  of deep learning,  many RNN-based  methods  are proposed  for melody  generation.",
                "Two more advanced  versions  were proposed  recently [ 4]\u2014 the Lookback  RNN, where  a lookback  feature  is employed  to \nmodel  the repetition  in melodies,  and the Attention  RNN, where  an attention  mechanism  is added  to LSTM.",
                "Deep Arti\ufb01cial  \nComposer (DAC)  divides  pitches  and durations  of notes into two sequences  and employs  two RNN to learn them jointly",
                "Song From Pi employs  a hierarchical  RNN that uses multiple  recurrent  layers  to generate  melodies,  drums  and chords [ 16].",
                "These  RNN-based  methods  are \nmainly  based  on modeling  the conditional  distribution  of notes given previous  notes.",
                "RNN-based  and CNN-based  GANs are also proposed  to learn the \ndistribution  of melody  clips",
                "An RNN-based  language  model  is proposed  for speech  recognition [ 25].",
                "GraphRNN  models  graphs  as sequences  via breadth-\ufb01rst  search  and generates  the \nsequence  of node and edge with a deep auto-regressive  model",
                "Melody  generation  net\nThe embedding  size and the hidden  size of all RNNs were set to 256.",
                "Each RNN consists  of two recurrent  layers.",
                "Dropout  with the ratio of 0.5w a s  used for the outputs  \nof the \ufb01rst recurrent  layer of all RNNs.",
                "\u2022AttentionRNN: an RNN-based  model  with attention  mechanism [ 4].",
                "\u2022LookbackRNN: an RNN-based  model  with Lookback  feature [ 4].",
                "The AttentionRNN,  LookbackRNN,  and MidiNet  were trained  on our dataset  with the same setting  in their original  papers.",
                "Input of models\nMGN,  LookbackRNN,  AttentionRNN,  MidiNet,  and Music  Transformer  needed  chord  progressions  and primer  melodies  as \ninput to generate  melodies.",
                "The melodies  generated  \nby AttentionRNN,  LookbackRNN,  and MidiNet  hardly  had any clear structures.",
                "There  were almost  no repetitions  in melodies  \nof AttentionRNN",
                "The percents  of repetition  and \nrhythmic  sequence  of MidiNet  and LookbackRNN  are comparable  with those of PopMNet.",
                "However,  most repetitions  of \nMidiNet  and LookbackRNN  had distances  within  4 bars (Fig.8).",
                "However,  it was inferior  to LookbackRNN  and MidiNet  in capturing  \nrhythmic  sequences.",
                "In this experiment,  we evaluated  \ufb01ve models,  PopMNet,  PopMNet-real,  AttentionRNN,  LookbackRNN,  and \nMidiNet.",
                "Repetition Rhythmic sequence No relation\nReal data 29.06% 32.64% 38.30%\nAttentionRNN 0.42% 24.73% 74.85%\nLookbackRNN 9.13 % 37.78% 53.09%MidiNet 18.75% 18.752% 62.29%Music Transformer 35.64% 25.15% 39.21%\nPopMNet 27.05% 28.91% 44.04%\nPopMNet-Real 31.45% 31.33% 37.22%PopMNet-NC 26.28% 29.45% 44.27%\nin the revision  phase  and it would  be costly  to redo this experiment  by incorporating  a new model.",
                "Repetition Rhythmic sequence\nAttentionRNN 8.272 1.973LookbackRNN",
                "Pleasure Reality Smooth Integrity\nAttentionRNN 2 .57\u00b11.43 2 .39\u00b11.43 2 .53\u00b11.39 2 .60\u00b11.41\nLookbackRNN 3 .18\u00b11.17",
                "Pleasure Reality Smooth Integrity\nAttentionRNN 1 .59\u00b10.93 1",
                ".32\u00b10.81 1 .62\u00b10.92 1 .75\u00b11.16\nLookbackRNN 2",
                "In experiments,  \nwe compared  PopMNet  with four existing  models  AttentionRNN,  LookbackRNN,  MidiNet,  and Music  Transformer.",
                "[35]J. You, R. Ying, X. Ren, W. Hamilton,  J. Leskovec,  GraphRNN:  generating  realistic  graphs with deep auto-regressive  models,  in: Proceedings  of the 35th \nInternational  Conference  on Machine  Learning,  Stockholm,  Sweden,  2018, pp.",
                "[41]K. Cho, B. van Merrienboer,  \u00c7. G\u00fcl\u00e7ehre,  D. Bahdanau,  F. Bougares,  H. Schwenk,  Y. Bengio,  Learning  phrase representations  using RNN encoder-decoder  \nfor statistical  machine  translation,  in: Proceedings  of the 2014 Conference  on Empirical  Methods  in Natural  Language  Processing,  Doha, Qatar, 2014, \npp."
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "A typical  model  is the \nhierarchical  variational  auto-encoder  named  MusicVAE [ 3].",
                "GraphVAE  employs  CNNs to encode,  reconstruct  and generate  \ngraphs  in an end-to-end  way",
                "Similar  to GraphVAE,  we represent  melody  structures  \nas adjacency  matrices  and utilize  GAN to generate  adjacency  matrices.",
                "MusicVAE [ 3]i s also available  open source,  but not included  in comparison  here.",
                "All the models  we compared  are \nvariable-length  autoregressive  models,  while MusicVAE  is a \ufb01xed-length  latent  variable  model.",
                "Despite  efforts  in tuning  its \nhyperparameters,  we failed to obtain  satisfactory  results  with MusicVAE  after training  it on our dataset.",
                "To obtain  good \nresults,  MusicVAE  may require  a large dataset  for training."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "First, structures  are generated  by the Structure  Generation  Net (SGN),  which  is a convolutional  generative  \nadversarial  network  (GAN)  (see Section 3for details).",
                "RNN-based  and CNN-based  GANs are also proposed  to learn the \ndistribution  of melody  clips",
                "Similar  to GraphVAE,  we represent  melody  structures  \nas adjacency  matrices  and utilize  GAN to generate  adjacency  matrices.",
                "To generate  images  with high \ndiversity,  FaceFeat-GAN  generates  diverse  features  \ufb01rst and then renders  features  to images [ 38].",
                "As illustrated  in Fig.3, a convolutional  GAN is designed  to generate  the adjacency  matrix  of the melody  structure  graph.",
                "The generator  Gand critic Dare trained  jointly  under  the Wasserstein  GAN with gradient  penalty  framework [ 39].",
                "The choice  of GAN for generating  the adjacency  matrix  is somehow  arbitrary.",
                "Other  methods  may perform  as well as GAN, or even better,  but that \nis not the focus of this paper.",
                "\u2022MidiNet: a CNN-based  GAN",
                "[29]N. De Cao, T. Kipf, MolGAN:  an implicit  generative  model for small molecular  graphs,  in: International  Conference  on Machine  Learning,  Workshop  on \nTheoretical  Foundations  and Applications  of Deep Generative  Models,  2018."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Singability-enhanced lyric generator with music style transfer",
            "RNN": [
                "The Google Brain\nteam introduced the Transformer [5], which incorporates an encoder\u2013\ndecoder architecture to create a sequence to sequence (Seq2Seq) model\nwithout using convolutional (CNN) or recurrent (RNN) neural net-\nworks.",
                "Traditional CNNs and RNNs were discarded in Transformer,\nwith the entire network composed entirely of attention mechanisms.",
                "This paper used a multilayer RNN encoder and\nmultilayer RNN with attention for decoding, creating a style transfer\nspecific architecture.",
                "The model used a bidirectional\nlong term short memory model to encode the input modern English\nsentence, and the decoder combined an RNN and pointer network\nmodule."
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "[17] built a neural generative model with VAE and holistic\nattribute discriminators or effectively generate sentences with control-\nlable attributes by learning latent representations.",
                "[3] Y. Zhao, P. Yu, S. Mahapatra, Q. Su, C. Chen, Discretized bottleneck in VAE:\nPosterior-collapse-free sequence-to-sequence learning, 2020, arXiv preprint arXiv:\n2004.10603, December."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "Many proposed methods borrow concepts from generative adver-\nsarial network (GAN) frameworks for generative adversarial training\ndiscriminators."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Human, I wrote a song for you: An experiment testing the influence of machines\u2019 attributes on the AI-composed music evaluation",
            "RNN": [
                "Apart from GANs and Transformer, a hierarchical recurrent \nneural network (HRNN), which Deep Bach uses, is based on \npseudo-Gibbs sampling in order to produce notes in the style of Bach \nchorales, providing more techniques to create music (Hadjeres et al., \n2017 ; Wu et al., 2020 )."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Generative Adversarial Networks \n(GANs) have been a popular structure type for developing creative machines.",
                "In music generation, \nMidiNet and MuseGAN are both based on GANs (Dong et al., 2018 ; Yang \net al., 2017 ).",
                "While using GANs is a well-known method to develop \nmusic-composing machines, it is not the only one.",
                "Apart from GANs and Transformer, a hierarchical recurrent \nneural network (HRNN), which Deep Bach uses, is based on \npseudo-Gibbs sampling in order to produce notes in the style of Bach \nchorales, providing more techniques to create music (Hadjeres et al., \n2017 ; Wu et al., 2020 )."
            ],
            "Generative Adversarial Network": [
                "Generative Adversarial Networks \n(GANs) have been a popular structure type for developing creative machines."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "BMC Public Health, 13(1), 1\u20138."
            ],
            "Hopfield Network": []
        },
        {
            "title": "Rethinking musicality in dementia as embodied and relational",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "deepsing: Generating sentiment-aware visual stories using cross-modal music translation",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "For example, Generative Adversarial Networks (GANs)\n(Brock, Donahue, & Simonyan , 2018 ; Goodfellow, Pouget-Abadie, Mir-\nza, Xu, Warde-Farley, Ozair, et al. , 2014 ; Karras, Aila, Laine, & Lehti-\nnen, 2017 ) are capable of synthesizing highly realistic visual content\nthat has not been encountered during the training process, neural style\ntransfer methods can re-paint images to match the style of reference\nimages ( Luan, Paris, Shechtman, & Bala , 2017 ), or even to follow\nthe style of well-known artists ( Gatys, Ecker, & Bethge , 2015 ; Wang,\nOxholm, Zhang, & Wang , 2017 ), while deep dreaming methods have\ndemonstrated that neural networks can exhibit a behavior known as\npareidolia in humans, i.e., recognize and synthesize patterns on seem-\ningly random data ( Mordvintsev, Olah, & Tyka , 2015 ).",
                "Finally, a generator model, e.g., a GAN, is employed to\ngenerate the final content.",
                "Our work go beyond\nexisting approaches by exploiting the power of Generative Adversarial\nNetworks (GANs) to generate unconstrained visualizations (Goodfellow\net al., 2014).",
                "Instead of directly training GANs for music visualization,\nwhich would be very challenging given the lack of appropriate datasets,\nwe propose employing an efficient cross-modal translation approach\nthat allows for translating the audio sentiment space into the visual\nsentiment space, through any pre-trained GAN model.",
                "To the best of\nour knowledge, this is the first work that proposed and evaluated an\nefficient cross-modal translation approach for sentiment-aware music\nvisualization using GANs.",
                "In this work, we employ a GAN for generating the\nfinal images from the intermediate generator vectors.",
                "(4)\nIt is worth noting that for the case of GANs, sampling the generator\nspace is easy, since GANs are typical trained to generate images from a\nGaussian distribution (Brock et al., 2018).",
                "However, in practice we observed that it was very difficult to fit\nsuch translation models, especially if GANs with very complex genera-\ntor spaces are used, e.g., GANs capable generating 1000 classes ( Brock\net al. , 2018 ).",
                "To understand why this happens, we have to consider\nthe mapping between the (sub)-classes produced through the GAN and\nthe attribute space.",
                "This behavior was also experimentally confirmed for the GAN\nused in the experiments conducted in this paper, as shown in Fig. 3(c).",
                "The plot shown in Fig. 3(c) was generated by clustering the sound\nattribute space and then measuring the number of GAN classes mapped\nin each cluster.",
                "[]\n5: foreach cluster do\n6: Sample one GAN category \ud835\udc50from each cluster according to\nthe probability of observing each class in the generator space\n7: Add every instance (\ud835\udc2d(\ud835\udc61)\n\ud835\udc56,\u0303\ud835\udc2d(\ud835\udc4e)",
                "In this work, we assume that a class-based\nGAN is employed.",
                "However, the proposed method can be also extended\nto handle any kind of GANs.",
                "For each cluster, we\nsample a GAN category with probability proportional to its cardinality\nin the cluster.",
                "This process allows to effectively\nkeep only one GAN category per cluster, leading to a smoother and\nmuch stabler matching, since every remaining attribute vector in each\ncluster is mapped to the same class.",
                "Note that, as demonstrated in\nFig. 3(c), for a GAN capable of generating images belonging to 1,000\ndifferent categories, each initial cluster could be mapped to more than\n500 different categories.",
                "Proposed method for fitting the translation model (a), an unstable mapping can occur between the attribute and generator spaces (b), and a toy example demonstrating\nthe unstable mapping for a GAN with 1000 classes (c).",
                "Hyper-stylization\nEven though GANs can offer a satisfactory degree of variation for\nthe generated content, they currently mostly fail to also simultaneously\nstylize the generated images according to the requirements of the\nusers.",
                "A pretrained BigGAN ( Brock et al. , 2018 ) model was used for\ngenerating images of 512 \u00d7512 pixels.",
                "The translation model consists\nof two hidden layers with 64 and 256 neurons respectively which\nbranches out into two streams: (a) a 1000 classification (softmax) layer\nused for predicting the category that will be used by the GAN, and\n(b) a 128 fully connected layer with no activation function used for\npredicting the latent vector to be fed to the GAN.",
                "First, note that even though the employed GAN wasExpert Systems With Applications 164 (2021) 114059\n7N. Passalis and S. Doropoulos\nFig.",
                "The proposed method works by\nfirst extracting the sentiment of a music track, which is then appro-\npriately translated into a space, from which a GAN can be employed\nfor generating the frames of the visual story."
            ],
            "Generative Adversarial Network": [
                "I C L E I N F O\nKeywords:\nMusic-to-image translation\nVisual stories\nGenerative Adversarial Networks\nNeural style transferA B S T R A C T",
                "For example, Generative Adversarial Networks (GANs)\n(Brock, Donahue, & Simonyan , 2018 ; Goodfellow, Pouget-Abadie, Mir-\nza, Xu, Warde-Farley, Ozair, et al. , 2014 ; Karras, Aila, Laine, & Lehti-\nnen, 2017 ) are capable of synthesizing highly realistic visual content\nthat has not been encountered during the training process, neural style\ntransfer methods can re-paint images to match the style of reference\nimages ( Luan, Paris, Shechtman, & Bala , 2017 ), or even to follow\nthe style of well-known artists ( Gatys, Ecker, & Bethge , 2015 ; Wang,\nOxholm, Zhang, & Wang , 2017 ), while deep dreaming methods have\ndemonstrated that neural networks can exhibit a behavior known as\npareidolia in humans, i.e., recognize and synthesize patterns on seem-\ningly random data ( Mordvintsev, Olah, & Tyka , 2015 )."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "ComposeInStyle: Music composition with and without Style Transfer",
            "RNN": [
                "Although (Hantrakul,\nEngel, Roberts, & Gu, 2019) applies WaveNet (WaveRNN) for mu-\nsical audio synthesis, (Engel, Agrawal, Chen, Gulrajani, Donahue, &\nRoberts, 2019) demonstrate the practical advantages of using GANs\nover WaveNets (Oord et al., 2016) by generating log magnitude spec-\ntrograms of musical data.",
                "In the paper (Johnson, 2017), a set of\nparallel, tied weight Recurrent Neural Network (RNN) is used to predict\nand generate polyphonic music compositions which is transposition\ninvariant.",
                "Sheet music is explored by Agarwala, Inoue, and Sly (2017) in which\nthey use RNN models to generate sheet music.",
                "RNN is used in 2 more recent works by Colombo, Muscinelli, Seeholzer,\nBrea, and Gerstner (2016), Mogren (2016) to generate music.",
                "(Mogren,\n2016) proposes a Convolutional-RNN-GAN (C-RNN-GAN) to generate\nclassical music trained n sequential data.",
                "C-RNN-GAN: Continuous recurrent neural networks with adversarial\ntraining."
            ],
            "Recurrent Neural Network": [
                "In the paper (Johnson, 2017), a set of\nparallel, tied weight Recurrent Neural Network (RNN) is used to predict\nand generate polyphonic music compositions which is transposition\ninvariant."
            ],
            "VAE": [
                "Among many attempts at generating music at par with paintings,\nresearchers have made several attempts starting from Variational Auto\nEncoder (VAE) (Brunner, Konrad, Wang, & Wattenhofer, 2018a; Luo,\nYang, Ji, & Li, 2020) to modern GANs (Abdulatif et al., 2019; Dong\net al., 2018; Dong & Yang, 2018; Kaneko et al., 2017; Marafioti et al.,\n2019).",
                "The Luo et al. (2020) explored the field of Chinese folk song\ngeneration by capturing specific music styles using VAE.",
                "Brunner et al.\n(2018b ) also performs genre style transfer and interpolation between\nmedleys and song mixtures using neural network based VAE.",
                "MIDI-VAE: Modeling\ndynamics and instrumentation of music with applications to style transfer.",
                "Mg-VAE: Deep Chinese folk songs generation\nwith specific regional styles."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "Contents lists available at ScienceDirect\nExpert Systems With Applications\njournal homepage: www.elsevier.com/locate/eswa\nComposeInStyle: Music composition with and without Style Transfer\nSreetama Mukherjeea, Manjunath Mulimanib,\u2217\naMicrosoft R&D Pvt. Ltd., Hyderabad, 500 033, India\nbDepartment of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576 104, India\nA R T I C L E I N F O\nKeywords:\nMusic composer classification\nStyle transfer\nGenerative Adversarial Networks (GAN)\nHybrid model\nMusical Instrument Digital Interface (MIDI)",
                "The GAN architecture of the style transfer step\nis built out of the GAN architecture of the second step.",
                "Then, the compositions\nspecific to a particular composer of choice is generated using vanilla\nGenerative Adversarial Network (GAN) (Goodfellow, 2016).",
                "Although various attempts have\nbeen recorded in the literature to compose music using vanilla GAN\nsuch as (Abdulatif, Armanious, Guirguis, Sajeev, & Yang, 2019; Dong,\nHsiao, & Yang, 2018; Dong & Yang, 2018; Kaneko, Takaki, Kameoka,\n& Yamagishi, 2017; Marafioti, Perraudin, Holighaus, & Majdak, 2019),\nthey are not specifically for composers.",
                "Among many attempts at generating music at par with paintings,\nresearchers have made several attempts starting from Variational Auto\nEncoder (VAE) (Brunner, Konrad, Wang, & Wattenhofer, 2018a; Luo,\nYang, Ji, & Li, 2020) to modern GANs (Abdulatif et al., 2019; Dong\net al., 2018; Dong & Yang, 2018; Kaneko et al., 2017; Marafioti et al.,\n2019).",
                "In this section, the most recent development in music compo-\nsition using GAN in the (2016\u20132020) timeframe has been discussed\nin detail.",
                "GAN was introduced by Goodfellow (2016).",
                "It describes the\nadvantages of using GAN over other generative networks.",
                "Another\npaper (Zhu, Park, Isola, & Efros, 2017) introduces the architecture\nfor cycle GAN which is also a significant advancement in the field\nof generative networks.",
                "Cycle GANs were made keeping in mind the\nuse case in which paired data is unavailable for training GANs.",
                "It\nintroduces a cycle consistency loss in addition to the adversarial loss\nin order to evaluate the GAN performance in training with unpaired\ndata.",
                "Now beginning with\nthe various applications of GAN in practical use cases.",
                "The next model GNACM uses\na GAN based framework with the output of NACM as input to the GAN\nin Zhang et al.",
                "Other various applications of GANs have been\nexplained in the survey paper by Yu (2020).",
                "Here, they give a detailed\ndescription of the pros and cons of using GANs over other generative\nnetworks.",
                "In the field of music, GAN is found to be used only in the very\nrecent works (Abdulatif et al., 2019; Dong et al., 2018; Dong & Yang,\n2018; Kaneko et al., 2017; Marafioti et al., 2019).",
                "Although (Hantrakul,\nEngel, Roberts, & Gu, 2019) applies WaveNet (WaveRNN) for mu-\nsical audio synthesis, (Engel, Agrawal, Chen, Gulrajani, Donahue, &\nRoberts, 2019) demonstrate the practical advantages of using GANs\nover WaveNets (Oord et al., 2016) by generating log magnitude spec-\ntrograms of musical data.",
                "They have used a non-autoregressive CNN based GAN for this purpose.",
                "(Marafioti et al., 2019) proposes GAN models to generate time\nfrequency based raw audio waveforms.",
                "The GAN model is trained on\ntime-frequency features which produce reliable and invertible Short\nTerm Fourier Transform (STFT) representations which can be converted\nto raw audio waveforms.",
                "(Abdulatif et al.,\n2019) have explored the domain of denoising audio tracks using GANs.",
                "(Dong & Yang, 2018) aims to solve this process of\nbinarizing generated piano rolls using GANs.",
                "In another work by Dong et al. (2018), 3 GAN models\n(jamming, composer and the hybrid model) are trained for music\ncomposition generation.",
                "They have proposed a GAN based\npost filter to reconstruct high fidelity STFT spectrograms.",
                "The GAN is\ntrained by divide and conquer by dividing the dataset with overlap and\nthen concatenating the generated output again with overlap in order\nto minimize information loss.",
                "In this, the authors attempt to solve\nthe problem using a sequential framework called SeqGAN.",
                "The corresponding intermediate state\u2013action steps get the rewards\nusing Monte-Carlo search after the GAN discriminator judges on a\ncomplete generated sequence.",
                "Another work by Yang, Chou, and Yang\n(2017) demonstrates the generation of symbolic music using GAN\neither from scratch, followed by a chord sequence or a priming melody.",
                "(Mogren,\n2016) proposes a Convolutional-RNN-GAN (C-RNN-GAN) to generate\nclassical music trained n sequential data.",
                "Yet another work on genre style transfer\nby Brunner et al. (2018a) is performed using cycle GAN.",
                "Unsupervised multi-\nmodal music style transfer for one-to-one generation is done using\nRelativistic average Generative Adversarial Networks (RaGAN).",
                "Moreover, in our approach we propose a cycle GAN way of training\nin presence of unpaired data.",
                "The\nentire procedure is performed in a step by step manner by training\nthe classifiers first, followed by the training of the GAN models for\ngenerating random compositions in the style of a composer A from\nnoise and then performing style transfer using the final style transfer\nmodel to generate composer A\u2019s compositions in the style of the target\ncomposer B.",
                "There are 2 GANs (each with 2\ngenerators) along with 2 common discriminators.",
                "The 2 GANs have\ndifferent functionalities.",
                "The vanilla GAN (in step 2) as shown in Fig.",
                "Given noise as input, the vanilla\nGAN can generate compositions in the style of the preferred composer\nof choice.",
                "The style transfer GAN architecture focuses on keeping the\nFig.",
                "Architecture of using Vanilla GAN for generating compositions from noise in\nStep 2.\ncontent of composition by composer A and outputs the melody in the\nstyle of the preferred composer B of choice.",
                "Architecture of the vanilla GAN\nThe audio melodies thus generated are in MIDI format which is then\nconverted into raw audio format (wav).",
                "Description of GAN architectures \ud835\udc3a\ud835\udc34,\ud835\udc3a\ud835\udc35,\ud835\udc37\ud835\udc34and\ud835\udc37\ud835\udc35\nIn step 2, paired vanilla GAN has been trained to generate multi-\ntrack polyphonic music.",
                "The overall architecture of a single GAN consists\nmainly of 2 parts: Generator and Discriminator 4.",
                "They are the 2 players\nin the minimax game of the GAN.",
                "Individual Architecture of a single GAN:\n1.Generator: The generator which is used here for step 2 com-\nprises of the following high-level components:",
                "The style transfer composite model consists of 4 generators and 2\ndiscriminators forming a cycleGAN.",
                "Step 2: Generate composer specific melody using a single composite\nGAN model.",
                "Steps of preprocessing the data\nBefore the data can be used to train the vanilla GAN and subse-\nquently the hybrid style transfer model, the data must be preprocessed\nin the appropriate format.",
                "These piano rolls are used in the next step for training the vanilla\nGAN and the hybrid style transfer GAN described in the next sections.",
                "The GAN generates piano rolls as the output.",
                "However, for training the\nhybrid style transferred GANs, the data type used is piano rolls.",
                "As a result, MIDI\nformat is most suitable for structured music composition generation\nby GANs as described in the later sections.",
                "Description of algorithm\nFor each epoch, real training audio data is taken for composer A\nand B. From noise, fake audio data in the style of composer A and B\nare then generated after training the GAN.",
                "In the comparison paper, the discriminators\nare 4 in number which are trained as a part of the cycle GANs.",
                "Metric Proposed Hybrid Style Transfer Model Comparison model\nGenerators Paired Vanilla GAN (2 GANs) and\nCycle GAN (2 GANs)Cycle GAN (only 2 GANs)\nDiscriminators Discriminator for composer A,\nDiscriminator for composer BDiscriminator for composer A,\nDiscriminator for composer B,\nDiscriminator for composer A\n(mixed) and\nDiscriminator for composer B\n(mixed)",
                "C-RNN-GAN: Continuous recurrent neural networks with adversarial\ntraining."
            ],
            "Generative Adversarial Network": [
                "Contents lists available at ScienceDirect\nExpert Systems With Applications\njournal homepage: www.elsevier.com/locate/eswa\nComposeInStyle: Music composition with and without Style Transfer\nSreetama Mukherjeea, Manjunath Mulimanib,\u2217\naMicrosoft R&D Pvt. Ltd., Hyderabad, 500 033, India\nbDepartment of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576 104, India\nA R T I C L E I N F O\nKeywords:\nMusic composer classification\nStyle transfer\nGenerative Adversarial Networks (GAN)\nHybrid model\nMusical Instrument Digital Interface (MIDI)",
                "Then, the compositions\nspecific to a particular composer of choice is generated using vanilla\nGenerative Adversarial Network (GAN) (Goodfellow, 2016).",
                "Unsupervised multi-\nmodal music style transfer for one-to-one generation is done using\nRelativistic average Generative Adversarial Networks (RaGAN)."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [
                "2 models namely, Tied Parallel - Long Short Term Memory\n- Neural Autoregressive Distribution Estimator (TP-LSTM- NADE) and\nBiaxial Long Short Term Memory (BALSTM) are used for this purpose."
            ],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "The mixed data indicates\nthat the corresponding discriminators have been trained with mixed\nreal data from all the 3 composers and fake generated data for that\nspecific composer (say dBM is trained with both mixed real data and\nfake data for composer B).",
                "Predicting TBM penetration rate in hard rock condition: A comparative study\namong six XGB-based metaheuristic techniques."
            ],
            "Hopfield Network": []
        },
        {
            "title": "The algorithmic composition for music copyright protection under deep learning and blockchain",
            "RNN": [
                "In this work, the music audio\nisdeemedastheresearchobject,andanewautomaticmusic\nsynthesisalgorithmisproposedbasedonLSTMRNN.Theriseof\ndigitalcontentoperationshasdemonstratedstrongdevelopment\npotentialandacceleratedthediversifieddevelopmentofthecom-\nmerciallayoutofthecontentindustry.",
                "JaechandOstendorf(2018)proposedtoadoptRNNandits\nvariants as a music language model in the field of automatic\nmusictranscription.",
                "LSTM\nLSTMisatypeofRNNnetwork,whichisoftenusedtoprocess\nandpredictimportanteventswithverylongintervalsanddelays\nintimeseries.",
                "Algorithm performance test\nToverifythesuperiorityoftheMCNNalgorithm,experiments\nareconductedbasedontheCPMGdataset,andtheresultsare\ncompared with three music generation algorithms: VAE\u2013GAN,\nSeqGAN,andRL-RNN.VAE\u2013GANaddstheencodinganddecoding\nprocesstotheGANnetwork.",
                "RL-RNNisanRNNalgorithm\nbased on Q-learning reinforcement learning.",
                "For the sequence\ngeneratedbytheRNNnetwork,theactionwiththelargestQ\nvalue is output.",
                "ThesamplecoincidencerateoftheRL-\nRNNalgorithmreaches0.946,whilethesamplecoincidencerate\noftheMCNNalgorithmreaches0.95,whichisthehighest,far\nexceedingGANandVAE\u2013GAN,andslightlybetterthanRL-RNN.",
                "Comparison results of performance of different algorithms\nInFig.26,theresultsofthisalgorithmarecomparedwith\nVAE\u2013GAN,GAN,andRL-RNN.Fromtheexperimentalresults,the\nmusic samples generated by MCNN network have the highestTable 3\nMusiccopyrightsecurityanalysisresults.",
                "Technology Safety Protectionrate\nDeeplearning 65.36% 65.23%\nBigdata 66.14% 69.43%\nBlockchain 65.12% 70.05%\nDeeplearning+blockchain 72.14% 82.33%\ncompliance rate, which is far better than GAN and VAE\u2013GAN,\nandslightlybetterthanRL-RNN.Themusicgenerationalgorithm\nbasedonMCNNnetworkcanbasicallyguaranteetheintegrityof\nthegeneratedmusic.",
                "[17] A. Jaech, M. Ostendorf, Low-rank RNN adaptation for context-aware\nlanguagemodeling,Trans.Assoc."
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "Algorithm performance test\nToverifythesuperiorityoftheMCNNalgorithm,experiments\nareconductedbasedontheCPMGdataset,andtheresultsare\ncompared with three music generation algorithms: VAE\u2013GAN,\nSeqGAN,andRL-RNN.VAE\u2013GANaddstheencodinganddecoding\nprocesstotheGANnetwork.",
                "theVAE\u2013GANalgorithmreaches0.81,whichishigherthanthat\noftheGANalgorithm.",
                "ThesamplecoincidencerateoftheRL-\nRNNalgorithmreaches0.946,whilethesamplecoincidencerate\noftheMCNNalgorithmreaches0.95,whichisthehighest,far\nexceedingGANandVAE\u2013GAN,andslightlybetterthanRL-RNN.",
                "Comparison results of performance of different algorithms\nInFig.26,theresultsofthisalgorithmarecomparedwith\nVAE\u2013GAN,GAN,andRL-RNN.Fromtheexperimentalresults,the\nmusic samples generated by MCNN network have the highestTable 3\nMusiccopyrightsecurityanalysisresults.",
                "Technology Safety Protectionrate\nDeeplearning 65.36% 65.23%\nBigdata 66.14% 69.43%\nBlockchain 65.12% 70.05%\nDeeplearning+blockchain 72.14% 82.33%\ncompliance rate, which is far better than GAN and VAE\u2013GAN,\nandslightlybetterthanRL-RNN.Themusicgenerationalgorithm\nbasedonMCNNnetworkcanbasicallyguaranteetheintegrityof\nthegeneratedmusic."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "Algorithm performance test\nToverifythesuperiorityoftheMCNNalgorithm,experiments\nareconductedbasedontheCPMGdataset,andtheresultsare\ncompared with three music generation algorithms: VAE\u2013GAN,\nSeqGAN,andRL-RNN.VAE\u2013GANaddstheencodinganddecoding\nprocesstotheGANnetwork.",
                "SeqGANintroducesareinforcement\nlearningalgorithmPolicyGradient.",
                "Figs.20and21showthatthesamplecoincidencerateofthe\nGANalgorithmisonly0.62,andthesamplecoincidencerateof\nFig.",
                "theVAE\u2013GANalgorithmreaches0.81,whichishigherthanthat\noftheGANalgorithm.",
                "ThesamplecoincidencerateoftheRL-\nRNNalgorithmreaches0.946,whilethesamplecoincidencerate\noftheMCNNalgorithmreaches0.95,whichisthehighest,far\nexceedingGANandVAE\u2013GAN,andslightlybetterthanRL-RNN.",
                "IntheGAN\nnetwork,therelationshipbetweenthegeneratorandthediscrim-\ninatormakesthegeneratedmusicsequenceverysimilartothe\ntrainingset,andthediversityislacking.\nFig. 21.Qualifiedrateoffourkindsofalgorithminmusicgeneration.\nFig.",
                "Comparison results of performance of different algorithms\nInFig.26,theresultsofthisalgorithmarecomparedwith\nVAE\u2013GAN,GAN,andRL-RNN.Fromtheexperimentalresults,the\nmusic samples generated by MCNN network have the highestTable 3\nMusiccopyrightsecurityanalysisresults.",
                "Technology Safety Protectionrate\nDeeplearning 65.36% 65.23%\nBigdata 66.14% 69.43%\nBlockchain 65.12% 70.05%\nDeeplearning+blockchain 72.14% 82.33%\ncompliance rate, which is far better than GAN and VAE\u2013GAN,\nandslightlybetterthanRL-RNN.Themusicgenerationalgorithm\nbasedonMCNNnetworkcanbasicallyguaranteetheintegrityof\nthegeneratedmusic.",
                "Fromtheprincipleofthealgorithm,\nit is not difficult to find that the game relationship between\ngeneratoranddiscriminatorintheGANnetworkmakesthemusic\nsequencegeneratedandthetrainingsetextremelysimilar,and\nthediversityissomewhatmissing.",
                "ThesupportrateoftheGANs1\nmodelproposedbyYe(2019)wasabout33.3%,thesupportrate\nofDeepBachwasabout38.3%,thesupportrateofrealsongswas\nabout38.3%,andthesupportrateofreconfigurationharmony\nwasabout33.3%.ThesupportrateoftheGANs2modelproposed\nisabout46.7%,whichiscloseto50%andfarmorethanthatof\nYe\u2019s[35].ThemusicgeneratedbytheZhang(2020)modelisless\nthan50%ofthesatisfactionrateofthemusicgeneratedbythe\nBiLSTM-GANsunderthecategoryofpianomusicians.",
                "[35] W.H.Ye,AutomaticcompositionbasedonBiLSTMandGANsalgorithm,J.\nJilinUniv.21(2)(2019)193\u2013194."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Towards a Deep Improviser: a prototype deep learning post-tonal free music generator",
            "RNN": [
                "Considering the case\nof music with symbolic representation (and thus potentially\nconventional musical notation), the highly successfulFolkRNN",
                "Performance RNN is a recent output of the\nGoogle Magenta project [ 28].",
                "The online illustrative sound excerpts of Performance RNN\n(as in the training corpus of piano music used) are largelytonal, and the authors themselves describe the outputs\nsomewhat dismissively as \u2018noodling\u2019.",
                "In the work on audio generation from audio inputs, Sam-\npleRNN",
                "Because of the apparently greater\ncapacity of recurrent neural nets (RNN) and in particularthose using the LSTM (long short-term memory) nodes, we\nthen considered RNNs receiving outputs from an initial\nCNN layer.",
                "Fig-\nure2summarises the form of the CNN/RNN model.",
                "Bidirectional\nRNN was ineffective (as might be expected, bar the\noccurrence of signi\ufb01cant retrogradation, where, for exam-ple, a pitch sequence may occur both forwards and back-\nwards).",
                "Table 2summarises the performance of the resultant\npair of models (one CNN alone and one CNN followed by\nRNN) as applied to both the Algorithmic and Improvised\nCorpora (different weights in each case, but identicalmodel form).",
                "Compared\nto the CNN-only model, the CNN\u2013RNN gives an improved\noverall RMSE (root mean squared error) in the case of theAlgorithmic Corpus, but not with the Improvised Corpus\nFig. 2 CNN/RNN model.",
                "IOI p1\nAlgorithmic Corpus\nNa\u0131\u00a8ve model 562.22 1004.23 24.55 N.A. N.A.\nCNN-only 397.04 415.66 15.49 95.25 7565\nCNN/RNN 180.36 488.57 11.54 96.44 10,445\nImprovised Corpus\nNa\u0131\u00a8ve model 254.18 477.91 19.76 N.A.",
                "N.A.\nCNN-only 185.56 417.72 16.01 83.32 7565CNN/RNN 196.30 450.24 19.20 80.90 10,445\nThe na\u0131 \u00a8ve model, as indicated in the text, was one in which the next event is predicted to be the same as the present one.",
                "CNN/RNN: CNN 32 \ufb01lters, kernel of 4, dilation 8, RNN LSTM 32.",
                "Note that the purpose of adding the RNN with LSTM\nnodes (with only a c.50% increase in model parameters) isnot solely to enhance the model precision, but also in the\nhope of enhancing model \u2018memory\u2019 (autoregressive and\ncross-parameter temporal relationships), such that it mightpredict longer sequences.",
                "The results of assessing this question were\nthat both CNN and CNN/RNN models de\ufb01ned above\nregressed to a static value within about 60 predictions, after\nseeding once.",
                "This required that both the CNN and CNN/\nRNN models are reseeded very regularly for generativeapplications.",
                "For\nassessments below, both CNN and CNN/RNN models werereseeded every 10 events with a randomly chosen (thus\nnormally new) subsequence from the seed (reseeding every\n11, being the number of input vector sequences in thelearning phase, or 20 events was also functional).",
                "Separate\nAnderson\u2013Darling analyses (not shown) of the outputs\nfrom the CNN/RNN model of the Algorithmic Corpus (and\nthe parallel analyses with the CNN\u2013RNN ImprovisedCorpus model) support a general conclusion: that our\napproach permits the generation of sequences statistically\ndistinct from either the learned corpus or the input seeddistribution which are yet organised rather than random.\nGoing from statistical distinction to substantive human\nevaluation of computational artistic generativity is a hugelydif\ufb01cult task [ 38\u201341] as for that matter is evaluation of\n(manually) composed work, and there is also an argument\nthat computational creativity (to which this paper poten-tially contributes) should be assessed in relation to its own\nspeci\ufb01ed objectives, partly or even solely by an internal\nmechanism [ 42].",
                "On the other hand, one-hotencoding (where, for example, pitches 12\u2013113 would be\nrepresented each by a vector of 101 zeros with a 1 at the\nspot in the vector corresponding to the pitch) is also asparse representation, yet has many bene\ufb01ts including\ncategorical prediction and may be useful here (it is used as\nthe basis of Performance RNN encoding).",
                "The present Deep Improviser , while yet\nshallow, when comprised of CNN and RNN probably\nimitates some of the sequential aspects of time series\nmodels.",
                "SampleRNN: an unconditionalend-to-end neural audio generation model.",
                "Performance RNN: generating music\nwith expressive timing and dynamics."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "From artificial neural networks to deep learning for music generation: history, concepts and trends",
            "RNN": [
                "7and described in Sect. 9.3); and\n\u2013 Architectures with multiple time/clocks, such as Clock-\nwork RNN [ 32] (shown in Fig. 8) and SampleRNN",
                "Reproduced from [ 53] with\npermission of the authors\nFig. 8 Clockwork RNN\narchitecture.",
                "The RNN-likehidden layer is partitioned into\nseveral modules each with its\nown clock rate.",
                "MIDI is\nprobably the most versatile, as it can encompass the case ofhuman interpretation of music (live performances), with\narbitrary/expressive timing and dynamics of note events,\nas, e.g., used by the Performance RNN system [ 58].",
                "7.2 Recurrent architecture\nArecurrent neural network (RNN) is a feedforward neural\nnetwork extended with recurrent connexions in order to\nlearn series of items (e.g., a melody as a sequence of notes).",
                "We will see that, from an\narchitectural point of view, various types of combination42\nmay be used:\n8.1 Composition\nSeveral architectures, of the same type or of different types,\nare composed, e.g.:\n\u2013 A bidirectional RNN, composing two RNNs, forward\nand backward in time, e.g., as used in the C-RNN-GAN\n[44] (see Fig. 16) and the MusicVAE [ 53] (see Fig. 7\nand Sect. 9.3) architectures; and\n\u2013 The RNN-RBM architecture [ 1], composing an RNN\narchitecture and an RBM architecture.",
                "33:39\u201365\n123one hidden layer with the same cardinality (number of\nnodes) for the input layer and the output layer; and\n\u2013 A variational autoencoder (VAE) architecture, which is\nan autoencoder with an additional constraint on the\ndistribution of the variables of the hidden layer (see\nSect. 9.2), e.g., the GLSR-VAE architecture [ 20].\n8.3 Nesting\nAn architecture is nested into the other one, e.g.:\n\u2013 A stacked autoencoder architecture,43e.g., the Dee-\npHear architecture [ 60]; and\u2013 A recurrent autoencoder architecture (Sect. 9.3), where\nan RNN architecture is nested within an autoencoder,44\ne.g., the MusicVAE architecture [ 53] (see Sect. 9.3).",
                "8.4 Pattern\nAn architectural pattern is instantiated onto a given archi-\ntecture(s),45e.g.:\n\u2013 The anticipation-RNN architecture [ 19] that instantiates\ntheconditioning pattern46onto an RNN with the output\nof another RNN as the conditioning input; and\n\u2013 The C-RNN-GAN architecture [ 44], where the GAN\n(Generative Adversarial Networks) pattern (to be\nintroduced in Sect. 9.4) is instantiated onto two RNN\narchitectures, the second one (discriminator) beingbidirectional (see Fig. 16); and\n\u2013 The MidiNet architecture [ 67] (see Sect. 9.4), where\ntheGAN pattern is instantiated onto two convolu-\ntional\n47feedforward architectures, on which a condi-\ntional pattern is instantiated.",
                "precisely, an RNN is nested within the encoder and another\nRNN within the decoder.",
                "Therefore, it is also named an RNN\nEncoder\u2013Decoder architecture.",
                "16 C-RNN-GAN architecture with the D(iscriminator)",
                "GAN\ncomponent being a bidirectional RNN (LSTM).",
                "The motivation is to combine:\n\u2013 The variational property of the VAE architecture for\ncontrolling the generation; and\n\u2013 The arbitrary length property of the RNN architecture\nused with the recursive strategy.54\nAn example (also hierarchical) is the MusicVAE archi-tecture [ 53] (shown in Fig. 7, with an example of con-\ntrolled generation in Fig. 21).",
                "Reproduced from [ 34] with permission of the authors\n62As in the case of a good cook, whose aim is not to simply mix all\npossible ingredients but to discover original successful combinations.58 Neural Computing and Applications (2021) 33:39\u201365\n123the input (in the case of creation by re\ufb01nement, as intro-\nduced in Sect. 4.2, or by using an extra conditioning input,\nas in Anticipation-RNN [ 19]), the output (in the case of\nconstrained sampling, as used by C-RBM), or an encap-sulation (in the case of a reformulation through reinforce-\nment learning as in RL Tuner [ 29]).",
                "Examples of types of architecture are:\nfeedforward (aka multilayer perceptron), recurrent\n(RNN), autoencoder and generative adversarial networks(GAN).",
                "This is the basis of a recurrent\nneural network (RNN) architecture.62 Neural Computing and Applications (2021) 33:39\u201365\n123Recurrent neural network (RNN)",
                "Reinforcement strategy A strategy for content gener-\nation by modeling generation of successive notes as areinforcement learning problem while using an RNN as a\nreference for the modeling of the reward.",
                "Interactive music generation with\npositional constraints using anticipation-RNN. arXiv:1709.",
                "A clock-\nwork RNN.",
                "Mehri S, Kumar K, Gulrajani I, Kumar R, Jain S, Sotelo J,\nCourville A, Bengio Y (2017) SampleRNN: an unconditionalend-to-end neural audio generation model.",
                "Mogren O (2016) C-RNN-GAN: continuous recurrent neural\nnetworks with adversarial training.",
                "Performance RNN: generating music\nwith expressive timing and dynamics."
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "An example is the experiment\nconducted by the YACHT dance music band with theMusicVAE architecture\n8from the Google Magenta Project\n[39].",
                "\u2019\u2019\nThus, these early designs may be seen as precursors of\nsome recent proposals:\n\u2013 Hierarchical architectures, such as MusicVAE [ 53]\n(shown in Fig.",
                "\u2019\u2019\nThe idea of an attention mechanism , although not yet\nvery developed, may be seen as a precursor of attentionmechanisms in deep learning architectures: at \ufb01rst as an\nadditional mechanism to focus on elements of an input\nsequence during the training phase [ 15, Section 12.4.5.1],\nnotably for translation applications, until being proposed as\nthefundamental and unique mechanism (as a full alterna-\ntive to recurrence or convolution) in the Transformerarchitecture [ 64], with its application to music generation,\nnamed MusicTransformer [ 28].\nFig. 7 MusicVAE architecture.",
                "We will see that, from an\narchitectural point of view, various types of combination42\nmay be used:\n8.1 Composition\nSeveral architectures, of the same type or of different types,\nare composed, e.g.:\n\u2013 A bidirectional RNN, composing two RNNs, forward\nand backward in time, e.g., as used in the C-RNN-GAN\n[44] (see Fig. 16) and the MusicVAE [ 53] (see Fig. 7\nand Sect. 9.3) architectures; and\n\u2013 The RNN-RBM architecture [ 1], composing an RNN\narchitecture and an RBM architecture.",
                "33:39\u201365\n123one hidden layer with the same cardinality (number of\nnodes) for the input layer and the output layer; and\n\u2013 A variational autoencoder (VAE) architecture, which is\nan autoencoder with an additional constraint on the\ndistribution of the variables of the hidden layer (see\nSect. 9.2), e.g., the GLSR-VAE architecture [ 20].\n8.3 Nesting\nAn architecture is nested into the other one, e.g.:\n\u2013 A stacked autoencoder architecture,43e.g., the Dee-\npHear architecture [ 60]; and\u2013 A recurrent autoencoder architecture (Sect. 9.3), where\nan RNN architecture is nested within an autoencoder,44\ne.g., the MusicVAE architecture [ 53] (see Sect. 9.3).",
                "9.2 Variational autoencoder architecture\nAlthough producing interesting results, an autoencoder\nsuffers from some discontinuity in the generation when\nexploring the latent space.49Avariational autoencoder\n(VAE)",
                "50The implementation of the encoder of a VAE actually generates a\nmean vector and a standard deviation vector [ 31].",
                "51This constraint is implemented by adding a speci\ufb01c term to the\ncost function to compute the cross-entropy between the distribution oflatent variables and the prior distribution.52 Neural Computing and Applications (2021) 33:39\u201365\n123As with an autoencoder, a VAE will learn the identity\nfunction, but furthermore the decoder will learn the relation\nbetween the prior (Gaussian) distribution of the latent\nvariables and the learnt examples.",
                "9.2.1 Variational generationExamples of possible dimensions captured by latent vari-\nables learnt by the VAE are the note duration range (the\ndistance between shortest and longest note) and the note\npitch range (the distance between lowest and highest pitch).This latent representation (vector of latent variables) can beused to explore the latent space with various operations to\ncontrol/vary the generation of content.",
                "Another issue is that the semantics (meaning) of the\ndimensions captured by the latent variables is automati-\ncally \u2018\u2018chosen\u2019\u2019 by the VAE architecture in function of thetraining examples and the con\ufb01guration and thus can only\nbe interpreted a posteriori .",
                "The motivation is to combine:\n\u2013 The variational property of the VAE architecture for\ncontrolling the generation; and\n\u2013 The arbitrary length property of the RNN architecture\nused with the recursive strategy.54\nAn example (also hierarchical) is the MusicVAE archi-tecture [ 53] (shown in Fig. 7, with an example of con-\ntrolled generation in Fig. 21).",
                "21 Example of a melody generated (bottom) by MusicVAE by\nadding a \u2018\u2018high note density\u2019\u2019 attribute vector to the latent space of anexisting melody (top).",
                "Structure imposition is a \ufb01rst direction, as in C-RBM, or by\nusing hierarchical architectures as in Music-VAE.",
                "Variational autoencoder (VAE) An autoencoder with\nthe added constraint that the encoded representation (its\nlatent variables) follows some prior probability distribu-tion, usually a Gaussian distribution.",
                "Hadjeres G, Nielsen F, Pachet F (2017) GLSR-VAE: geodesic\nlatent space regularization for variational autoencoder architec-tures.",
                "Understanding variational autoencoders (VAEs):\nbuilding, step by step, the reasoning that leads to VAEs.",
                "represen-\ntation learning in VAEs (Pt. 1)."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "We will see that, from an\narchitectural point of view, various types of combination42\nmay be used:\n8.1 Composition\nSeveral architectures, of the same type or of different types,\nare composed, e.g.:\n\u2013 A bidirectional RNN, composing two RNNs, forward\nand backward in time, e.g., as used in the C-RNN-GAN\n[44] (see Fig. 16) and the MusicVAE [ 53] (see Fig. 7\nand Sect. 9.3) architectures; and\n\u2013 The RNN-RBM architecture [ 1], composing an RNN\narchitecture and an RBM architecture.",
                "8.4 Pattern\nAn architectural pattern is instantiated onto a given archi-\ntecture(s),45e.g.:\n\u2013 The anticipation-RNN architecture [ 19] that instantiates\ntheconditioning pattern46onto an RNN with the output\nof another RNN as the conditioning input; and\n\u2013 The C-RNN-GAN architecture [ 44], where the GAN\n(Generative Adversarial Networks) pattern (to be\nintroduced in Sect. 9.4) is instantiated onto two RNN\narchitectures, the second one (discriminator) beingbidirectional (see Fig. 16); and\n\u2013 The MidiNet architecture [ 67] (see Sect. 9.4), where\ntheGAN pattern is instantiated onto two convolu-\ntional\n47feedforward architectures, on which a condi-\ntional pattern is instantiated.",
                "16 C-RNN-GAN architecture with the D(iscriminator)",
                "GAN\ncomponent being a bidirectional RNN (LSTM).",
                "9.4 Generative adversarial networks (GAN)\narchitecture\nAn interesting example of architectural pattern is the\nconcept of Generative Adversarial Networks (GAN) [ 16],\nas illustrated in Fig.",
                "An example of the use of GAN for generating music is\nthe MidiNet system [ 67], aimed at the generation of single\nor multitrack pop music melodies.",
                "The architecture, illus-\ntrated in Fig. 23, follows two patterns: adversarial (GAN)\nand conditional (on history and on chords to condition\nmelody generation).",
                "22 Generative adversarial networks (GAN) architecture.",
                "A notable attempt has been proposed for creating\npaintings in [ 9], by extending a GAN architecture to favor\nthe generation of content dif\ufb01cult to classify within exist-ing styles and therefore favoring the emergence of new\nstyles.",
                "Examples of types of architecture are:\nfeedforward (aka multilayer perceptron), recurrent\n(RNN), autoencoder and generative adversarial networks(GAN).",
                "Discriminator The discriminative model component of\ngenerative adversarial networks (GAN) which estimates\nthe probability that a sample came from the real data\nrather than from the generator.",
                "Generative adversarial networks (GAN) A compound\narchitecture composed of two component architectures,the generator and the discriminator, who are trained\nsimultaneously with opposed objectives.",
                "The generative model component of gener-\native adversarial networks (GAN) whose objective is to\ntransform a random noise vector into a synthetic (faked)\nsample which resembles real samples drawn from adistribution of real data.",
                "Mogren O (2016) C-RNN-GAN: continuous recurrent neural\nnetworks with adversarial training."
            ],
            "Generative Adversarial Network": [
                "8.4 Pattern\nAn architectural pattern is instantiated onto a given archi-\ntecture(s),45e.g.:\n\u2013 The anticipation-RNN architecture [ 19] that instantiates\ntheconditioning pattern46onto an RNN with the output\nof another RNN as the conditioning input; and\n\u2013 The C-RNN-GAN architecture [ 44], where the GAN\n(Generative Adversarial Networks) pattern (to be\nintroduced in Sect. 9.4) is instantiated onto two RNN\narchitectures, the second one (discriminator) beingbidirectional (see Fig. 16); and\n\u2013 The MidiNet architecture [ 67] (see Sect. 9.4), where\ntheGAN pattern is instantiated onto two convolu-\ntional\n47feedforward architectures, on which a condi-\ntional pattern is instantiated.",
                "9.4 Generative adversarial networks (GAN)\narchitecture\nAn interesting example of architectural pattern is the\nconcept of Generative Adversarial Networks (GAN) [ 16],\nas illustrated in Fig."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "We will see that, from an\narchitectural point of view, various types of combination42\nmay be used:\n8.1 Composition\nSeveral architectures, of the same type or of different types,\nare composed, e.g.:\n\u2013 A bidirectional RNN, composing two RNNs, forward\nand backward in time, e.g., as used in the C-RNN-GAN\n[44] (see Fig. 16) and the MusicVAE [ 53] (see Fig. 7\nand Sect. 9.3) architectures; and\n\u2013 The RNN-RBM architecture [ 1], composing an RNN\narchitecture and an RBM architecture.",
                "There are\nstochastic versions of arti\ufb01cial neural networks\u2014the Restricted\nBoltzmann Machine (RBM) [ 24] is an example\u2014but they are not\nmainstream.",
                "An example of use of RBM is described in Sect.",
                "An example of application to music is the generation\nalgorithm for the C-RBM architecture [ 34].",
                "The architec-\nture is a re\ufb01ned (convolutional58) restricted Boltzmann\nmachine (RBM59).",
                "Reproduced from [ 21] with\npermission of the authors\n58The architecture is convolutional (only) on the time dimension, in\norder to model temporally invariant motives, but not pitch invariantmotives which would break the notion of tonality.\n59Because of space limitation, and the fact that RBMs are not\nmainstream, we do not detail here the characteristics of RBM (see,e.g., [ 15, Section 20.2] or [ 3, Section 5.7] for details).",
                "In a \ufb01rst\napproximation for this article, we may consider an RBM as analog to\nan autoencoder, except with two differences: the input and outputlayers are merged (and named the visible layer), and the model isstochastic.60This is named structure imposition , with the same basic approach\nthat of style transfer [ 7], except that of a high-level structure.",
                "Control is necessary to inject constraints (e.g., tonality,\nrhythm) in the generation, as witnessed by the C-RBMarchitecture (see Sect. 9.6).",
                "28 C-RBM Architecture generation algorithm.",
                "Reproduced from [ 34] with permission of the authors\n62As in the case of a good cook, whose aim is not to simply mix all\npossible ingredients but to discover original successful combinations.58 Neural Computing and Applications (2021) 33:39\u201365\n123the input (in the case of creation by re\ufb01nement, as intro-\nduced in Sect. 4.2, or by using an extra conditioning input,\nas in Anticipation-RNN [ 19]), the output (in the case of\nconstrained sampling, as used by C-RBM), or an encap-sulation (in the case of a reformulation through reinforce-\nment learning as in RL Tuner [ 29]).",
                "Structure imposition is a \ufb01rst direction, as in C-RBM, or by\nusing hierarchical architectures as in Music-VAE.",
                "Restricted Boltzmann machine (RBM) A speci\ufb01c type\nof arti\ufb01cial neural network that can learn a probability\ndistribution over its set of inputs."
            ],
            "Hopfield Network": []
        },
        {
            "title": "Conditional hybrid GAN for melody generation from lyrics",
            "RNN": [
                "When given Chinese lyrics, melody and exactalignment are predicted in a lyrics-conditioned melody\ncomposition system by [ 7], which is an end-to-end neural\nnetwork model including RNN-based lyrics encoder, RNN-based context melody encoder, and a hierarchical RNN\ndecoder."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "ORIGINAL ARTICLE\nConditional hybrid GAN for melody generation from lyrics",
                "Inthis paper, we propose a novel conditional hybrid GAN (C-Hybrid-GAN) for melody generation from lyrics.",
                "Through extensive experiments using evaluation metrics, e.g., maximum mean\ndiscrepancy, average rest value, and MIDI number transition, we demonstrate that the proposed C-Hybrid-GAN outper-forms the existing methods in melody generation from lyrics.",
                "Keywords Melody generation from lyrics /C1GAN /C1AI music /C1Conditional sequence generation\n1 Introduction\nGenerating melody from lyrics to compose a song has been\na challenging research task in the \ufb01eld of arti\ufb01cial intelli-\ngence and music, which falls under the \ufb01eld of conditional\ndiscrete-valued sequence generation.",
                "An earlier study by [ 1] has shown the feasibility\nof exploiting conditional long short-term memory\u2014gen-\nerative adversarial network (LSTM-GAN) for melodygeneration from lyrics.",
                "On the one\nhand, the continuous-valued sequence, as the output of thegenerator in the GAN, is not in accordance with the dis-\ncrete-valued music attributes.",
                "On this basis,a novel conditional hybrid generative adversarial network\n(C-Hybrid-GAN) is suggested to generate melodies from\nlyrics, where three discrete sequences of music attributesare separately generated by the melody generation model\nconditioned on the same lyrics.",
                "Through the extensive\nexperiments, we show that the proposed C-Hybrid-GANoutperforms the existing melody generation methods and\nhas the capability of generating more natural and plausible\nmelodies.",
                "Consequently, in this work, we mainly discuss\nmelody generation from lyrics and GAN-based discretesequence generation.",
                "2.2 GAN-based discrete sequence generation\nGenerative adversarial networks (GANs) in [ 11] were\noriginally developed to generate continuous data [ 12],\nwhich have been applied successfully in the conditional\nsequence generation such as dialogue in [ 13], text-to-video\nin [14], and lyrics-to-melody in [ 1] generation.",
                "However,\nGANs have the limitation in generating discrete sequence\ndue to the non-differentiable problem of the discrete-val-\nued outputs from the generator.",
                "SeqGAN by [ 15] models the generator as a\nstochastic policy in reinforcement learning, which\navoids the generator differentiation problem by\ndirectly performing policy-gradient updates.",
                "Rank-GAN by [ 16] uses the ranking score as the rewards\nto learn the generator, which is optimized through\nthe policy gradient method.",
                "LeakGAN by [ 17]\naddresses a mechanism of providing richer infor-\nmation from the discriminator to the generator by\nexploiting hierarchical reinforcement learning.",
                "InMaskGAN, [ 18] proposes the actor-critic GAN\narchitecture that uses reinforcement learning to\ntrain the generator, where the in-\ufb01lling techniquemay alleviate mode collapse.",
                "In TextGAN, [ 19] utilizes a kernelized\ndiscrepancy metric to map high-dimensional latent\nfeature distributions of real and synthetic sentences,\nwith the aim of mitigating the model collapse.",
                "Instead of applying standard GAN objective, FM-\nGAN by [ 20] suggests to match the latent feature\ndistributions of real and synthetic sentences exploit-ing the feature-movers distance.",
                "In ARAE, [ 21]\nutilizes the adversarial autoencoder to transform the3192 Neural Computing and Applications (2023) 35:3191\u20133202\n123discrete data into a continuous latent space for\nGAN training.",
                "In GAN for sequences of discreteelements by [ 2] and RelGAN by [ 22], Gumbel-\nSoftmax approaches are suggested to approximate\nthe discrete-valued distribution for continuous-valued distribution.",
                "Here, we propose a hybrid GAN structure for learning\nmultiple melody attributes which contains two noveltechniques to improve the quality of lyrics-conditioned\nmelody generation: (i) Relational reasoning technique is\napplied to modeling not only dependency inside eachsequence of music attributes, but also consistency among\nthree sequences of music attributes during the training\nstage.",
                "(ii) Gumbel-Softmax technique is utilized toapproximate the discrete-valued distribution of music\nattributes in a conditional hybrid GAN.",
                "Conditional GAN\nWe propose an end-to-end deep generative model for\ngenerating melodies conditioned on lyrics.",
                "The proposedC-Hybrid-GAN model is trained by considering the\nalignment relationship between sequences of music attri-\nbutes and their corresponding lyrics.",
                "The Gumbel-\nSoftmax relaxation technique is exploited to train GAN fordirectly generating discrete-valued sequences.",
                "The RMC layer followingthe fully connected layer uses a single memory slot with\nthe head size set to 16, the number of heads set to 2, and the\nnumber of blocks set to 2.\n3.3 Gumbel-Softmax\nTraining GANs for the generation of discrete data faces a\nnon-differentiable problem due to discrete-valued output\nfrom the generator.",
                "1 Architecture of\nconditional hybrid GAN3194 Neural Computing and Applications (2023) 35:3191\u20133202\n123yp\nt\u00fe1/C24softmax \u00f0ot\u00de: \u00f03\u00de\nHere, softmax \u00f0ot\u00derepresents the multinomial distribution\non the set of all possible MIDI numbers.",
                "Then, the discriminator loss\nis given by\nloss\nD\u00bclogsigmoid1\nTXT\nt\u00bc1ot/C01\nTXT\nt\u00bc1^ot !\n: \u00f05\u00de\nHere, we use the relativistic standard GAN (RSGAN) loss\nfunction in [ 29].",
                "4 Experiments\nIn this section, we discuss the experimental setup andresults to demonstrate the feasibility of our proposed C-\nHybrid-GAN.",
                "Moreover, [ 22] showed that GAN\nwith RMC and Gumbel-Softmax outperforms other exist-\ning state-of-the-art generative models in terms of samplequality and diversity in text generation.",
                "In these research\nreports, we have seen that GAN with RMC and Gumbel-\nSoftmax performs best.",
                "This work aims to discuss theeffectiveness of melody generation from lyrics where three\ndiscrete sequences corresponding to music attributes,\nnamely pitch, duration, and rest, are separately generatedby GAN with RMC and Gumbel-Softmax when given\nlyrics.",
                "In addition, four competitive methods are implemented\nto compare with the proposed C-Hybrid-GAN as follows:\nTBC-LSTM-MLE:",
                "TBC-LSTM-GAN: It is similar to TBC-LSTM-MLE, except that GAN is used and the Gumbel-Softmax\ntechnique is exploited to train a GAN for discrete-valued\nsequence generation.",
                "C-Hybrid-GAN: The proposed\nmethod uses both RMC and Gumbel-Softmax.",
                "C-LSTM-GAN in [ 1]: It contains a deep LSTM generator and a deep\nLSTM discriminator both conditioned on lyrics, without\nusing RMC and Gumbel-Softmax.",
                "As Gumbel-Softmaxand RMC are mainly involved in the proposed C-Hybrid-\nGAN, their impacts are further investigated as the ablation\nstudy, and the results of TBC-LSTM-MLE and TBC-LSTM-GAN are shown in Table 2.\n4.1 Experimental setup\nWe use the Adam [ 32] optimizer with b1\u00bc0:9 and b2\u00bc\n0:99 and perform gradient clipping if the norm of the\ngradients exceeds 5.",
                "The value of the Self-BLEU score ranges between 0 and 1with a smaller value of Self-BLEU implying a higher\nsample diversity hence a less chance of mode collapse in\nthe GAN model.",
                "During the adversarial training,\nSelf-BLEU values of our C-Hybrid-GAN architecturereach the peak around 45 epochs, decrease until 100\nepochs, and then approach to the stability.",
                "4.5 Comparison with state-of-the-art methods\nTo study if C-Hybrid-GAN can generate sequences that\nresemble the same distribution as training samples, quan-titative evaluation is performed to compare existing state-\nof-the-art approaches following the previous quantitative\nmeasurements in [ 1], for example, 2-MIDI numbers repe-\ntitions, 3-MIDI numbers repetitions, MIDI numbers span,\nthe number of unique MIDI, the number of notes without\nrest, average rest value in a song, and song length.",
                "It is very obvious that the\noverall performance of the proposed C-Hybrid-GAN out-\nperforms other competitive methods in most aspects.",
                "Forpitch-related attributes such as MIDI number span and the\nnumber of unique MIDI numbers, the proposed C-Hybrid-\nGAN method is closest to the ground truth.",
                "In addition, for\nmetrics on temporal attributes such as average rest value\nand the number of notes without rest, C-Hybrid-GAN isalso closest to the ground truth.",
                "4 Training curves of MMD\nscores on testing dataset3198 Neural Computing and Applications (2023) 35:3191\u20133202\n123melodies generated by our model C-Hybrid-GAN,\nC-Hybrid-MLE, and C-LSTM-GAN.",
                "According to theoccurrence of MIDI number transition in the \ufb01gures, it is\nvery clear that the proposed C-Hybrid-GAN model can\nbetter capture the distribution of MIDI number transition.",
                "Mean values\narelrs\u00bc1:3666, lrn\u00bc1:3692,\nandlrns\u00bc1:3679, respectively\nTable 2 Metrics evaluation of attributes\nGround truth C-LSTM-GAN C-Hybrid-MLE C-Hybrid-GAN TBC-LSTM-MLE TBC-LSTM-GAN\n2-MIDI repetitions 7.4 9.7 6.8 6.5 9.1 10.6\n3-MIDI repetitions 3.8 2.2 2.8 2.7 2.1 3.0MIDI span 10.8 7.7 12.7 12.0 13.7 12.1Unique MIDI number 5.9 5.1 6.0 6.1 6.2 6.1Average rest value 0.8 0.6 1.4 0.7 1.1 0.8Non-rest note number 15.6 16.7 12.7 16.1 12.7 15.9Song length 43.3 39.2 60.9 39.1 51.0 41.4\nFig.",
                "Generally, it can be seen that the overall result of the\nproposed method C-Hybrid-GAN is the closest to that ofthe ground truth, especially for melody and rhythm scores.",
                "To avoid the problem ofnon-differentiability in GANs for discrete data generation,\nwe exploit the Gumbel-Softmax to approximate the dis-\ntribution of discrete-valued sequences.",
                "Through extensiveexperiments of melody generation from lyrics including the\ndiversity and quality of generated melody samples, the\neffect of lyrics-based context conditioning, and the com-parison with existing works, we indicate that the proposed\nC-Hybrid-GAN outperforms the existing cutting-edge\nmethods in lyrics-conditioned melody generation withmultiple music attributes."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Scene2Wav: a deep convolutional sequence-to-conditional SampleRNN for emotional scene musicalization",
            "RNN": [
                "https://doi.org/10.1007/s11042-020-09636-5\nScene2Wav:adeepconvolutional\nsequence-to-conditionalSampleRNNforemotional\nscenemusicalization\nGwenaelleCunhaSergio1\u00b7MinhoLee1\nReceived:30September2019/Revised:4July2020/Accepted:13August2020/",
                "The decoder Scene2Wav is a proposed conditional Sam-\npleRNN which uses that emotional visual feature embedding as condition to generate novel\nemotional music.",
                "Keywords Sequence-to-conditional SampleRNN \u00b7Convolutional neural network \u00b7Deep\nrecurrent neural network \u00b7Domain transformation \u00b7Emotional music generation\n1 Introduction\nArt may very well be one of the defining characteristics of the human species, and are key\nto effective human interactions.",
                "Lastly, a conditional Scene2Wav decoder, composed\nof a proposed conditional SampleRNN, takes the emotional visual vector obtained from the\nencoder as a condition to generate novel emotional music signals.",
                "\u2013 Conditional Music Generator : Scene2Wav decoder using a proposed conditional Sam-\npleRNN that considers emotional visual embedding obtained from a convolutional\nrecurrent neural network encoder.",
                "A typi-\ncal Seq2Seq model has a encoder-decoder structure, each with a Recurrent Neural Network\n(RNN) responsible for dynamically modeling a sequence of samples.",
                "Considering that our\nvideo data involves audio, it\u2019s of utmost importance that we consider its dynamic time prop-\nerties with RNN, which considers information in previous time steps, unlike feedforward\nnetworks.",
                "SampleRNN [ 15] is the state-of-the-art, alongside\nWaveNet [ 33], for music generation according to the authors human evaluation perfor-\nmance.",
                "This model hierarchically combines sample-level modules as multilayer perceptrons\nand frame-level modules as RNNs in order to capture long-term dependencies in the\ntemporal sequences, and it does so on three different datasets.",
                "The original SampleRNN model has the limitation of being unconditional, meaning that\nit produces random music when instructed to generate audio, but researchers have recently\nextended it for applications such as voice conversion and text-to-speech [ 30,45].",
                "To alle-\nviate this limitation, we propose a conditional SampleRNN that decodes emotional visual\ninformation into audio eliciting the same emotion from users.",
                "The conditional SampleRNN\nis trained on sequentially encoded visual features as initial hidden state and target audio,\ngenerating emotionally charged music.",
                "The previ-\nously mentioned ConvSeq2Seq model is then chosen as our baseline model to evaluate the\neffect our proposed conditional decoder SampleRNN has over a regular decoder RNN.",
                "4 Proposedmodel\n4.1 Scene2Wav\nOur proposed model, Scene2Wav, is an end-to-end deep neural network composed of a\nConvolutional Neural Network (CNN), an Encoder Deep Recurrent Neural Network (RNN)\nand a proposed conditional Decoder SampleRNN [ 15] with Gated Recurrent Units (GRUs).",
                "This is followed by further sequence encod-\ning with an Encoder Deep RNN framework.",
                "This decoder is our pro-\nposed conditional SampleRNN that conditionally considers emotional aspects in music.",
                "In\nother words, the proposed decoder is conditioned on the encoded emotional visual sequence\nfeatures vector v. The original unconditional SampleRNN model is not able to generate\nmusic corresponding to a desired video, instead simply generating random music.",
                "The scene input is given to the CNN module, responsible for the frame by frame fea-\nture extraction, and these features are given to the Encoder RNN, responsible for encoding\nthe visual features as a sequence.",
                "It\nconsists of three modules:\nemotional visual feature\nextraction with CNN, sequence\nencoding with an Encoder Deep\nRNN framework, and music\ngeneration with our proposed\nconditional SampleRNN,\nconditioned on the encoded\nemotional visual sequence\nfeatures vector v\nScene Input \n(3 channels)",
                "Audio Output CNN \nv  CNN CNN \nSequence \nEncoder Scene2Wav \n Encoder  Emotional visual \nembedding z1 \ne1 z2 \ne2 zF \neF Conditional  \nDecoder \nSampleRNN \nand that evoke similar emotions to the original visual input.",
                "Max Pooling (kernel, stride) \nFully Connected + Softmax\nFig.3 Convolutional Neural Network (CNN) for visual emotion feature extraction\n4.3 DomaintransformationwithdeepencoderRNNs\nAfter obtaining each frames features, we need to consider our data\u2019s dynamic time prop-\nerties.",
                "The model chosen for this task is a Recurrent Neural Network (RNN), since it\nconsiders dynamic properties of data in previous timesteps unlike feedforward networks.",
                "Whereas in our proposed model, we substitute the decoder\npart for a conditional SampleRNN (see Section 4.4) while still keeping the pre-trained\nencoder.",
                "Whereas in the proposed model, the decoder part is substituted\nfor a conditional SampleRNN (see more in Section 4.4)1800 MultimediaToolsandApplications(2021)80: 1793\u20131812Vanilla RNNs, however, have limitations when dealing with data with long term depen-\ndencies, such as audio.",
                "In our proposed model, the 2-layer GRU decoder is\nsubstituted for a conditional SampleRNN, which is explained in more detail in the following\nsection.",
                "4.4 ConditionalSampleRNNformusicgeneration\nThe original SampleRNN [ 15] is originally proposed to generate random audio samples\nby modeling the probability of a sequence of audio samples X=x1,x2,...,xTas the prod-\nuct of the probabilities of each sample conditioned on all previous samples, as shown in\n(10):\np(X)=T\u22121/productdisplay\ni=0p(xi+1|x1,...,xi) (10)",
                "In this section, we propose a conditional\nSampleRNN to be used as our Scene2Wav decoder for music generation with the scene\nvisual features.",
                "It is critical to consider the emotional visual vector in the SampleRNN to\ngenerate music that carries the same emotional aspect as the visual input.",
                "The\nframe-level modules are deep RNNs, and they are responsible for modeling chunks of audio\nconditioned on the sequentially encoded emotional visual features v. As can be seen in\nFig.",
                "The proposed conditional SampleRNN consists of two modules: frame-level and sample-level.",
                "The\nframe-level is composed of two deep RNNs and is responsible for the audio modeling conditioned on\nthe sequentially encoded emotional visual features v. The sample-level is composed of an MLP and it is\nresponsible for the next sample prediction given the previous samples and conditioned features\ncharacteristics of the audio, and Tier 2 processes smaller windows, allowing for modeling of\nmore refined audio characteristics.",
                "In our proposed Scene2Wav decoder, unlike in the conventional model,\nTiers 2 and 3\u2019s cells also take into consideration the emotional visual embedding vobtained\nfrom the CNN and RNN encoder.",
                "p(2)\nt,v)\nc(2)\n(t\u22121)\u22174+j=W(2)\njh(2)\nt,1\u2264j\u22644(15)\nwhere W(2)\nxis a matrix used to obtain the linear combination between f(2)\ntandc(3)\nt.\nSimilarly to the conventional SampleRNN, the final tier, ( 16) is an MLP that models\nthe probability of input xi+1given all previous samples and v, which is encoded in the\nconditional vector from the previous tier c(2)",
                "i=W(1)\nxf(1)\ni+c(2)\ni\np(xi+1|x1,...,xi,v)=So ftmax(MLP(inp(1)\ni))(16)\nwhere W(1)\nxis used to obtain the linear combination between f(1)\niandc(2)\ni.\n4.5 Baselinemodel:convolutionalsequence-to-sequence\nDue to its proven performance and effectiveness in multimodal tasks, a Convolutional\nSeq2Seq model [ 35] is chosen as our baseline model in order to evaluate the effect our pro-\nposed conditional decoder SampleRNN has over a regular decoder RNN.",
                "Sim-\nilarly to our proposed model, the baseline model extracts emotional visual features using a\nCNN, encodes the sequence of features with a Deep RNN and decodes it into a sequence\nof audio sample with another Deep RNN in an Encoder-Decoder framework, as shown in\nFig.",
                "6.\n5 Resultsanddiscussion\n5.1 Trainingcon\ufb01guration\nBoth the proposed and baseline models are trained on the previously discussed 3 second\nspliced dataset (see Section 3) and both use the same CNN and Encoder RNN with heuris-\ntically determined configurations.",
                "The encoder is a 2-layer deep GRU-RNN with 128 hidden units, trained for 20\nepochs, with ASGD Optimizer, learning rate of 0.001, momentum of 0.98, weight decay of\n1e\u22125, and scheduler of 0.8.",
                "Lastly, in the proposed model, is the conditional SampleRNN\ndecoder, a 2-layer deep RNN with 1,024 hidden units, batch size of 128, and quantization\nlevelqof 256, corresponding to a per-sample bit depth of 8.",
                "6 Baseline model architecture for ConvSeq2Seq consisting of two modules: emotional visual feature\nextraction with CNN, and sequence encoding and decoding with an Encoder-Decoder Deep RNN framework\n5.2 Examplesofgeneratedsamples\nExperimental results are showcased in Figs.",
                "The proposed model uses\na conditional SampleRNN decoder that takes into account sequentially encoded emotional\nvisual features, obtained from a CNN followed by an RNN encoder, to generate related\nemotional music.",
                "By considering the encoded emotional visual features as condition, the\nmodel is able to generate music corresponding to a given video, thus alleviating the limi-\ntation in the unconditional SampleRNN model."
            ],
            "Recurrent Neural Network": [
                "This is followed by an\nEncoder Deep Recurrent Neural Network with Gated Recurrent Units (GRUs) responsible\nfor sequentially encoding the extracted emotional visual features, transforming an emotional\nscene into a more abstract embedding.",
                "A typi-\ncal Seq2Seq model has a encoder-decoder structure, each with a Recurrent Neural Network\n(RNN) responsible for dynamically modeling a sequence of samples.",
                "4 Proposedmodel\n4.1 Scene2Wav\nOur proposed model, Scene2Wav, is an end-to-end deep neural network composed of a\nConvolutional Neural Network (CNN), an Encoder Deep Recurrent Neural Network (RNN)\nand a proposed conditional Decoder SampleRNN [ 15] with Gated Recurrent Units (GRUs).",
                "The model chosen for this task is a Recurrent Neural Network (RNN), since it\nconsiders dynamic properties of data in previous timesteps unlike feedforward networks.",
                "4 Encoder-Decoder Recurrent Neural Network (Seq2Seq)."
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "Splices\nBMI A Beautiful Mind 31.3 625\nCHI Chicago 30.15 602\nFNE Finding Nemo 30.3 605\nGLA Gladiator 30.05 600\nLOR Lord of the Rings 37.57 750\nCRA Crash 26.63"
            ],
            "Hopfield Network": []
        },
        {
            "title": "Attentional networks for music generation",
            "RNN": [
                "Keywords Recurrent neural network (RNN) \u00b7Long short term memory (LSTM) \u00b7\nAttention \u00b7Bidirectional LSTM \u00b7MIDI format\n/envelopebackPrerana",
                "Depiction of different AI empowered procedures can be found in[3,4,13] including a probabilistic model utilizing RNNs, Anticipation RNN and Recur-\nsive Artificial Neural Networks (RANN), an adaptation of artificial neural networks [1]f o rcreating the consequent notes, resulting note duration and rhythm.",
                "Next, in the subsequent sections we provide thebackground on different components relevant to this work.\n2.1 RNN\nRecurrent Neural Networks (RNNs) include intermittent associations inside the hidden lay-ers between past and current states in the neural network.",
                "The primary issue with a standard RNN is that it stores the data of just the previouslyattended state; this implies the setting expands just a single strand back.",
                "2.2 LSTM\nLong Short Term Memory networks \u2013 generally called \u201cLSTMs\u201d \u2013 are an extraordinary sortof RNN, equipped for adapting to long term conditions.",
                "In standard RNNs, this rehashing module will have an extremely basic structure,for example, a solitary tanh layer.",
                "3.2 MusicgenerationusingattentionbasedLSTM\nRecurrent neural networks (RNNs) are quite widely used to process sequential data infor-mation.",
                "[ 2] proposed the standard RNN model."
            ],
            "Recurrent Neural Network": [
                "Next, in the subsequent sections we provide thebackground on different components relevant to this work.\n2.1 RNN\nRecurrent Neural Networks (RNNs) include intermittent associations inside the hidden lay-ers between past and current states in the neural network."
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "More recent generative models such as gener-ative adversarial networks (GANs) or variational auto-encoders (V AEs) can even generatenovel timbral spaces as well as render novel songs while directly working in the waveformdomain.",
                "Generative AdversarialNetworks (GANs)",
                "MuseGAN"
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "They utilized a RecurrentTemporal Restricted Boltzmann machine (RTRBM) so as to demonstrate unconstrainedpolyphonic music.",
                "Utilizing the RTRBM modelling enabled them to speak to a confoundeddispersion over each time step as opposed to a solitary token as in most character language\nFig."
            ],
            "Hopfield Network": []
        },
        {
            "title": "Monophonic music composition using genetic algorithm and Bresenham\u2019s line algorithm",
            "RNN": [
                "[8] used deep recurrent neural network (RNN) with gated26487 Multimedia Tools and Applications (2022)",
                "[ 2] Generative RNN model for sheet music Uses dataset in ABC music notation26488 Multimedia Tools and Applications (2022) 81:26483\u201326503recurrent unit (GRU) to generate convincing monophonic melodies.",
                "[ 2] proposed generative RNN models to build a music gener-\nator using Seq2Seq and Character RNN, which is trained by Abc formatted music datasetwith approximately 34,000 songs of different genres."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "IBM Syst J 4(1):25\u2013305."
            ],
            "Hopfield Network": []
        },
        {
            "title": "Polyphonic music generation generative adversarial network with Markov decision process",
            "RNN": [
                "[ 14] have solved the shortcomings of the traditional recurrent neural network (RNN),\nwhich is neither interactive nor creative.",
                "They propose a new structure \u2014an anticipation-\nRNN \u2014for interactive music generation, and they have proven its efficiency at generating\nmelodies satisfying the unitary constraints of the soprano style in a J. S. Bach chorus.",
                "[ 23] propose an\nalgorithm to generate notes using RNN, mainly via an LSTM network.",
                "In this paper, GAN is used instead of RNNto generate music, and LSTM units are used in the generator.",
                "Hadjeres G, Nielsen F (2020) Anticipation-RNN: enforcing unary constraints in sequence generation, with\napplication to interactive music generation."
            ],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Therefore, this paper proposes a novel polyphonic music creation\nmodel, combining the ideas of the Markov decision process (MDP) and Monte Carlo tree\nsearch (MCTS) and improving the Wasserstein Generative Adversarial Network\n(WGAN) theory.",
                "Generative Adversarial Network (WGAN)\n1 Introduction\nProducing various independent melodies and combining them harmoniously through technical\nprocessing are key to creating polyphonic music.",
                "i n a#The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022classic Generative Adversarial Network (GAN) [ 7,13] is limited for polyphonic music\ncreation.",
                "For example, it is difficult for a classic GAN to create a new melody outside thetraining dataset, and it is difficult to break through the shackles of melody and tone in that\ndataset.",
                "Therefore, the present research team designed a GAN model based on the Markov decisionprocess (MDP) [ 21] and Monte Carlo tree search (MCTS) [ 4] to generate polyphonic music.",
                "Based on the above research, Goodfellow et al. have drawn lessons from a novel pair theory\n(GAN) for the network construction of image generation and learning.",
                "[ 9] proposed a GAN model which can have a convolution, which needs to add an\nadditional refiner network to the generator.",
                "The team has also proposed two versions of music generation models MuseGAN V1[10] and MuseGAN V2 [ 11].",
                "MuseGAN V1 uses convolution in the generator and discrim-\ninator, which can generate multi-channel pop/rock music from scratch or can accompany a\ntrack provided by the user.",
                "MuseGAN V2 uses three symbolic, multi-channel, music gener-\nation models in a GAN framework: the interference model, the composer model, and thehybrid model.",
                "Experiments show that, given a specific track composed by humans, MuseGANV2 can generate four additional accompaniment tracks.",
                "However, since GAN was proposed,29866 Multimedia Tools and Applications (2022)",
                "Many studies, including those concerning MuseGAN V1 and MuseGAN\nV2, are trying to solve this problem, but the effect is not yet satisfactory.",
                "For example, some\nresearchers [ 22,32\u201334] have improved algorithms in the original GAN.",
                "[ 2] propose using the Wasserstein Generative Adversarial Network (WGAN),\nbased on the work of Goodfellow et al.",
                "[ 13], to remedy the instability of GAN training and\ncollapse mode.",
                "However, in polyphonic musicgeneration, as the length of the music sequence generated by WGAN increases, sequencecoherence is broken, and the discriminator in the WGAN model finds it difficult to evaluatethe incomplete sequence.",
                "Therefore, aiming atthe problems of WGAN polyphonic music generation, this paper improves the structure of the\ngenerator and discriminator in the WGAN model and adds a policy gradient algorithm [ 29]t o\nupdate the generator parameters.",
                "Forpolyphonic music generation, the generator in the GAN model supervises learning needs to beinformed of the appropriate notes required for each small segment sequence.",
                "However, thereare two problems in the existing GAN model; (1) the generator finds it difficult to transfer\ngradient update, and (2) the discriminator finds it difficult to evaluate the incomplete sequence.",
                "Therefore, the present research team has the introduced MDP mechanism into the GAN modelto generate polyphonic music.",
                "This effectively solves the problem of the GAN generator finding it difficult totransfer the gradient update, and MDP is suitable for the generation model construction in thispaper.",
                "This paper combines WGAN and MDP theory, integrates MCTS, and proposes a poly-\nphonic music generation method based on MDP and WGAN, as shown in Fig.",
                "GAN model is unable to evaluate incomplete music sequences.",
                "(1) The model is built based on MDP and\nWGAN.",
                "(2) The construction of this model draws\nlessons from WGAN principles and adds Wasserstein distance [ 2], which stabilizes model\ntraining by resolving the issue in which, the better the discriminator is trained in the existing\nGAN, the more seriously the generator gradient disappears.",
                "(3) After training, the generator in\nthe GAN model can be used independently of the discriminator.",
                "(4) This paper is the first to integrateMCTS into WGAN for polyphonic music generation, removing the issues in which it isdifficult for the discriminator, in the existing GAN model, to evaluate incomplete sequences.",
                "2 Technical details\n2.1 Network construction\nIn this paper, the researchers have built a neural network based on WGAN and MDP to learnand generate polyphonic music.",
                "This model was constructed by referring to the WGAN model presented by Martinet al.",
                "This study combines\nWGAN and MDP to generate polyphonic music.",
                "Discrete music sequences differ fromcontinuous image processing, which is not conducive for training typical GAN.",
                "Moreover,GAN can only evaluate the whole sequence, rather than the local sequence, which leads to thegenerator producing a discrete output and makes it difficult for the discriminator to return agradient to update the generator.",
                "As mentioned above, the generator network has been built with reference to the WGAN\nstructure proposed by Martin et al.",
                "2.2 Network strategy gradient\nWhen designing the strategy gradient for the polyphonic music generation model, it wasnecessary for the authors to introduce the relevant GAN theory proposed by Goodfellow et al.[13].",
                "To learn the target generator distribution from standard data, the GAN input is defined as\nP\nzz\u00f0\u00de, and the generation model is defined as G\u03b8yt\u00f0jY1:t/C01\u00de.G\u03b8is the model parameter; ytis the\noutput at time t;a",
                "min\nGmax\nDVD;G\u00f0\u00de \u00bc E\nx~Pdatax\u00f0\u00delog D x\u00f0\u00de\u00f0\u00de\u00bd/C138 \u00fe Ez~Pzz\u00f0\u00delog1/C0DGz\u00f0\u00de\u00f0\u00de\u00f0\u00de\u00bd/C138 \u00f0 1\u00de\nHowever, classic GAN presents a large problem.",
                "[ 2] propose WGAN,\nbased on Goodfellow et al.",
                "[ 13], and replace Jensen-Shannon divergence with Wasserstein\ndistance, which effectively solves the problem of instability in the classic GAN trainingprocess.",
                "Therefore, the team introduced WGAN into the polyphonic music generation model.",
                "The discriminator in the original GAN accom-\nplishes the task of true and false dichotomy, so the sigmoid function is used in the last layer.",
                "However, the discriminator, f\n!, in WGAN does approximate fitting Wasserstein distance,\nwhich belongs to the regression task, so it is necessary to remove the sigmoid function of thelast layer.",
                "Considering that the first term of Lis independent of the generator, two\nWGAN losses can be obtained, as shown in Eqs.",
                "Because this study introduces MDP into the GAN model, the goal of the generator is to\ngenerate sequences to maximize the expectation of return, as shown in Eq.",
                "The experiment has employed three models to generate music results: the WGAN model, the\nWGAN and MDP model, and the proposed model for comparative training (Fig. 6).",
                "At the\nsame time, the research team has compared the effects of the model in this study with theMuseGAN v1 model [ 10], the MuseGAN v2 model [ 11], and the LSTM music generation\nmodel [ 23] for polyphonic music generation.",
                "In the training process, whenusing WGAN for music generation, the research team has employed a convolution network as\nthe generator.",
                "Then, theteam used WGAN + MDP to generate music, and this has improved the original WGAN byreplacing the convolution layer of the generator with an LSTM layer to better extract thecharacteristics of the original music sequence.",
                "The MDP algorithm also makes the generatedmusic more fluent, and, as the generated music sequence grows, the model will not alwaysgenerate single melody music, thereby giving the GAN model a certain creative ability.",
                "81:29865 \u201329885 29875Figure 7shows the change of loss value with iteration times for the WGAN model, the\nWGAN and MDP model, and the proposed model.",
                "WGAN WGAN+MDPWGAN+\nMDP+MCTS Original Music\nFig.",
                "The music generated by the proposed model, the music generated by the WGAN model, and the music\ncontrast map generated by the WGAN and MDP model\n0102030405060Loss\nEpochLoss curves of generator\nWGAN\nWGAN+MDP\nOur model\n0510152025303540\n0 100 200 300 400 500 600 700 800 900 1000Loss\nEpochLoss curves of discriminator\nWGAN\nWGAN+MDP\nOur model\nFig. 7 Curve of functions changing with iterations29876 Multimedia Tools and Applications (2022)",
                "During the training process, it is difficult for the convolutionlayer of the generator in the WGAN model to learn complex music sequence features.",
                "When comparing the WGAN and MDP model with the proposed model, it is evident thatthe generator with the MCTS mechanism has the fastest convergence effect.",
                "3.3 Experimental result\nBy studying the existing music generation methods and analyzing the WGAN model, through\na large number of experiments, it has been proven that the model proposed in this paper is\nmore suitable for polyphonic music generation than both the WGAN model and the WGANand MDP model.",
                "It should be noted that, with the change of test range, the value obtained formacro/C0F1 is always higher than that obtained for macro/C0F1\nD. Thus, it is proven that the\npolyphonic music generation model based on WGAN theory, which integrates the ideas of\nMDP and MCTS, is better than the MuseGAN model proposed by Dong et al.",
                "To more effectively verify the effectiveness of the model proposed in this paper, the\nresearch team has compared this model with the latest music generation models, includingthe MuseGAN v1 model [ 10], the MuseGAN v2 model [ 11], and the LSTM model [ 23].",
                "To\navoid generator collapse in the MuseGAN model and to optimize the training speed, theWGAN gradient penalty (WGAN-GP) network is used.",
                "Compared with the above research, the model in this paper combines MDP theory and MCTS,\nbased on WGAN.",
                "Therefore, the research team did notadd a gradient penalty term to the original WGAN.",
                "In this paper, GAN is used instead of RNNto generate music, and LSTM units are used in the generator.",
                "The music visualization diagrams generated by the proposed model, theMuseGAN v1 model, the MuseGAN v2 model, and the LSTM models are shown in Fig. 12.",
                "Combining GAN and MDP theory can also effectively enhance\nFig.",
                "Afterlistening to music, subjects rated them on a 10-point scale ranging from 1 (very low) to 10\nMuseGAN v1",
                "[8] MuseGAN v2",
                "i o n s\nBased on the WGAN model, a novel polyphonic music generation method using MDP",
                "Wasserstein GAN. arXiv, 1701.07875.",
                "Dong HW, Hsiao WY, Yang LC (2017) MuseGAN: demonstration of a convolutional GAN based model\nfor generating multi-track piano-rolls.",
                "Dong HW, Hsiao WY, Yang LC (2017) MuseGAN: multi-track sequential Generative Adversarial\nNetworks for symbolic music generation and accompaniment.",
                "FusionGAN:",
                "Xie Y, Franz E, Chu MY, Thuerey N (2018) tempoGAN:",
                "A temporally coherent, volumetric GAN for\nsuper-resolution fluid flow.",
                "StackGAN plus plus: Realistic image synthesis with stacked generative adversarial\nnetworks."
            ],
            "Generative Adversarial Network": [
                "Therefore, this paper proposes a novel polyphonic music creation\nmodel, combining the ideas of the Markov decision process (MDP) and Monte Carlo tree\nsearch (MCTS) and improving the Wasserstein Generative Adversarial Network\n(WGAN) theory.",
                "Generative Adversarial Network (WGAN)\n1 Introduction\nProducing various independent melodies and combining them harmoniously through technical\nprocessing are key to creating polyphonic music.",
                "i n a#The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022classic Generative Adversarial Network (GAN) [ 7,13] is limited for polyphonic music\ncreation.",
                "[ 2] propose using the Wasserstein Generative Adversarial Network (WGAN),\nbased on the work of Goodfellow et al."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "A combination of multi-objective genetic algorithm and deep learning for music harmony generation",
            "RNN": [
                "However recently, the neural networks, and evolutionary algorithms have been more\nwidely used in the AMG literature, such as conditional rhythms generation of drum sequences\nwith neural networks [ 21], and using a Hierarchical Recurrent Neural Network (HRNN) for\nmelody generation [ 35] or combining two types of music generation models, namely symbol-\nic, and raw audio models based on the WaveNet architecture [ 22].",
                "The LSTM model is a type of RNNs that uses a memory cell built to show long-termdependencies on time-series data."
            ],
            "Recurrent Neural Network": [
                "However recently, the neural networks, and evolutionary algorithms have been more\nwidely used in the AMG literature, such as conditional rhythms generation of drum sequences\nwith neural networks [ 21], and using a Hierarchical Recurrent Neural Network (HRNN) for\nmelody generation [ 35] or combining two types of music generation models, namely symbol-\nic, and raw audio models based on the WaveNet architecture [ 22]."
            ],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "One of the recent AMG approaches is the use of Generative Adversarial Networks (GANs)\nas well as reinforcement learning methods.",
                "In 2017, Yang et al. proposed a system called\nMidiNet, which in two separate experiments generated melody and melody with harmony bycombining the GAN and Convolutional Neural Networks (CNN)"
            ],
            "Generative Adversarial Network": [
                "One of the recent AMG approaches is the use of Generative Adversarial Networks (GANs)\nas well as reinforcement learning methods."
            ],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "A Style-Specific Music Composition Neural Network",
            "RNN": [
                "a polyphonic generation framework by combining RNN and general neural network.",
                "GoogleBrain\u2019s Magenta project [ 15] presents further optimizations based on LSTM to enhance long-\nterm associations between multiple measures; Attention RNN adds mask vectors to loop joinsto control model weights for different historical states for the effect of music generation, butthe resulting fragments are short and have a relative poor structure.",
                "(1)RNN+DQN",
                "This network is proposed by Google DeepMind, which mainly consists\nof a value-based Q-learning network and RNN network.",
                "Different time steps have beenassessed: RNN +DQN, SeqGAN, V AE\u2013GAN and MCNN, and considered in the truncated\nback propagation.",
                "Execution are shownfor RNN +DQN, SeqGAN, V AE\u2013GAN and MCNN with 100 timesteps.",
                "By contrast, there is a slightand relatively large variety between the experiment with 100 time steps in RNN +DQN and\nMCNN models respectively.",
                "In this occasion, the results indicate that the MCNN performsbetter than RNN +DQN in terms of the problem solving, reaching the minimum loss below\n1.0 using 1000 time steps while RNN +DQN reaching the loss above 1.0 using the same time\nsteps.",
                "123A Style-Speci\ufb01c Music Composition Neural Network 1907\nTable 1 Compliance rate of generation music of 4 algorithms\nAlgorithm SeqGAN V AE\u2013GAN RNN\u2013DQN MCNN\nCompliance rate 0.6234 0.8152 0.9489 0.951\n1.",
                "In the experiment, the algorithm in this paper is compared with the results of three music\ngeneration algorithms of V AE\u2013GAN, SeqGAN and RNN\u2013DQN, as shown in Table 1.",
                "According to the experimental results, music sample generated by MCNN network has the\nhighest compliance rate, far exceeding SeqGAN and V AE\u2013GAN, slightly exceeding RNN\u2013DQN.",
                "In the experiment, as shown in Fig. 8, the ROC curves\nof V AE\u2013GAN and SeqGAN are more twisted and sharp, and relatively speaking, RNN\u2013DQNand MCNN are more smooth.",
                "On the other hand,if we use reinforcement learning individually, considering there is a bottleneck to construct\n123A Style-Speci\ufb01c Music Composition Neural Network 1909\nTable 2 Score sheet of subjective validation\nModel Melody Rhythm Harmony of chord Musical texture Emotion\nRNN\u2013DQN 6.14 5.13 6.21 6.36 6.00\nSeqGAN 6.27 7.07 7.36 6.29 6.07V AE\u2013GAN 5.71 5.93 5.60 6.14 5.27MCNN 7.86 7.73 8.00 7.93 7.00\na suitable reward function, it is hard to put it into use to generate music based musical theory\nrule.",
                "In MCNN model, we treat the probability\noutput of CNN discriminator as a part of the reward of RNN, which takes part in the feedbackof LSTM and contributes to the speci\ufb01c music style element in the music."
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "While we add musicrules as the reward function in order to generate music with speci\ufb01c styles.\n(3)VAE\u2013GAN [18] The variational auto-encoder and generative adversarial networks are\nboth the generative models based on the unsupervised learning method, which canintegrate the adversarial ideas into the variational auto-encoder."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "[ 18] proposes a sequence gen-\neration algorithm and a combined network based on V AE\u2013GAN structure, in which eachgenerated continuous frame is encoded, then the generator predicts the generated content ofthe next frame according to the coding sequence of the previous frame, \ufb01nally the discrimi-nator determines between the real data and the generated data, but this model is dif\ufb01cult tomodel long sequence.",
                "Nowadays,generative adversarial network (GAN) has achieved remarkable results in image generationand processing, and Yang et al.",
                "[ 21] introduces MidiNet, which combines GAN network\nand CNN network to generate popular melody.",
                "In addition to this, MuseGAN proposed by Dong et al.",
                "Based on above, SeqGAN\nproposed by Y u et al.",
                "GAN network is dif\ufb01cult to update the\ngeneration model of discrete sequence, which can be used to model the data generator as arandom policy in reinforcement learning, and to bypass the difference of the generator byimplementing updates to the gradient strategy.",
                "[24] proposes that the CycleGAN model can be applied to the style migration of symbolic\nmusic, adding another discriminator to keep the generator\u2019s structure features of the originalmusic, but the generated style is not rich enough.",
                "(2)SeqGAN [23] The novel collision of adversarial network and reinforcement learning,\nwhich is devoted to generating discontinuous sequence, such as text sequence generation.",
                "While we add musicrules as the reward function in order to generate music with speci\ufb01c styles.\n(3)VAE\u2013GAN [18] The variational auto-encoder and generative adversarial networks are\nboth the generative models based on the unsupervised learning method, which canintegrate the adversarial ideas into the variational auto-encoder.",
                "Thereby, we can trainencoder, generator and discriminator synchronously to generate images on the basisof mutual compensation between GAN and V AE.",
                "Different time steps have beenassessed: RNN +DQN, SeqGAN, V AE\u2013GAN and MCNN, and considered in the truncated\nback propagation.",
                "Execution are shownfor RNN +DQN, SeqGAN, V AE\u2013GAN and MCNN with 100 timesteps.",
                "SeqGAN and V AE\u2013GAN models shown in Fig.",
                "123A Style-Speci\ufb01c Music Composition Neural Network 1907\nTable 1 Compliance rate of generation music of 4 algorithms\nAlgorithm SeqGAN V AE\u2013GAN RNN\u2013DQN MCNN\nCompliance rate 0.6234 0.8152 0.9489 0.951\n1.",
                "In the experiment, the algorithm in this paper is compared with the results of three music\ngeneration algorithms of V AE\u2013GAN, SeqGAN and RNN\u2013DQN, as shown in Table 1.",
                "According to the experimental results, music sample generated by MCNN network has the\nhighest compliance rate, far exceeding SeqGAN and V AE\u2013GAN, slightly exceeding RNN\u2013DQN.",
                "In the experiment, as shown in Fig. 8, the ROC curves\nof V AE\u2013GAN and SeqGAN are more twisted and sharp, and relatively speaking, RNN\u2013DQNand MCNN are more smooth.",
                "On the one hand, using GAN individually on generating music creates much similarities\non the music sequence and the training data, causing a lack of diversity.",
                "On the other hand,if we use reinforcement learning individually, considering there is a bottleneck to construct\n123A Style-Speci\ufb01c Music Composition Neural Network 1909\nTable 2 Score sheet of subjective validation\nModel Melody Rhythm Harmony of chord Musical texture Emotion\nRNN\u2013DQN 6.14 5.13 6.21 6.36 6.00\nSeqGAN 6.27 7.07 7.36 6.29 6.07V AE\u2013GAN 5.71 5.93 5.60 6.14 5.27MCNN 7.86 7.73 8.00 7.93 7.00\na suitable reward function, it is hard to put it into use to generate music based musical theory\nrule.",
                "This network makes use ofthe advantages of AC reinforcement learning algorithm to select the diversity of actions, aswell as avoids the drawback of GAN network generating samples, which are too similar, andimproves the creativity of style music generation.",
                "Semi-recurrent CNN-based V AE\u2013GAN for sequential data generation.",
                "Dong HW, Hsiao WY , Yang LC, Yang YH (2018) MuseGAN: symbolic-domain music generation and\naccompaniment with multitrack sequential generative adversarial networks.",
                "SeqGAN: sequence generative adversarial nets with policy gradient.",
                "Symbolic music genre transfer with CycleGAN.",
                "Generalizing GANs: a turing perspective."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [
                "[ 17] raises the RBM network adding some constraint\nrules.",
                "Although the RBMnetwork can constrain the sampling for gradient descent optimization, it will cost too longtime to calculate and sensitive to sampling noise."
            ],
            "Hopfield Network": []
        },
        {
            "title": "Self-Supervised Music Motion Synchronization Learning for Music-Driven Conducting Motion Generation",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Therefore, we propose a novel Music Motion Synchronized Generative\nAdversarial Network (M2S-GAN), which generates motions according to the automatically learned music representations.",
                "More speci\fcally, M2S-GAN is a cross-modal generative network comprising four components: 1) a music encoder that\nencodes the music signal; 2) a generator that generates conducting motion from the music codes; 3) a motion encoder\nthat encodes the motion; 4) a discriminator that di\u000berentiates the real and generated motions.",
                "Extensive experiments on ConductorMotion100 demonstrate the e\u000bectiveness of M2S-GAN.",
                "Finally, the proposed Music\nMotion Synchronization-Based Generative Adversarial\nNetwork (M2S-GAN) is trained jointly with sync loss\nand Wasserstein distance based adversarial loss[18,19],\nsuch that both music motion synchronization and mo-\ntion realism are guaranteed.",
                "The model \frst\nlearns music and motion feature representations in a\nReal\nMotionE\nIn-Sync/\nOut-of-Sync\nEE\nDSync \nLoss\nAdversarial \nLoss\nE GGenerative Learning Stage\n(M2S-GAN)Contrastive Learning Stage\n(M2S-Net)Real\nMotion\nMusicMusic Generated\nMotion\nMotion\nz\nMotion\nGenerated\nNoise\nFig.1.",
                "Therefore, an increasing number of researchers have\nbegun to apply advanced deep generative models, par-\nticularly generative adversarial nets (GAN)[42].",
                "We\nbelieve that the objective of regression loss is in con\rict\nwith the adversarial loss: the optimal output of the re-\ngression loss is over-smoothed, while the discriminator\nof GAN can easily learn the over-smooth pattern and\nprovide con\rict gradients to the generator.",
                "Subsequently, the trained Emusic andEmotion are\ntransferred to the generative learning stage, where these\ntwo encoders, together with the generator and discrim-\ninator, form the Music Motion Synchronized Genera-\ntive Adversarial Network (M2S-GAN).",
                "We use the word\n\\synchronized\" here to imply that M2S-GAN requires\na preparation stage and that the weights of the trans-\nferred encoders are frozen.",
                "In the\nfollowing generative learning stage, all four of these net-\nworks form M2S-GAN.Ggenerates a motion sequence\naccording to zand the output of Emusic.",
                "Training Procedure of M2S-Net and M2S-GAN\nInput : datasetD=f(Xi;Yi)gN\ni=1; loss function weights \u0015adv,\u0015sync,wGP;\nOutput : trained music encoder Emusic , generator G;\n/",
                "Network structure of M2S-Net and M2S-GAN.",
                "(b) M2S-GAN. Conv: convolutional layer.",
                "Note that several GAN-based\naudio-to-motion translation methods also feed the au-\ndio into the discriminator[1,37,43].",
                "Negative pairs\nare mismatched sequences, of which the details will be\nintroduced in Subsection 4.5.\nLM2S-Net\n=MX\ni;j=1cijlog2\u0010\nf[Emusic(Xi)\bEmotion (Yj)]\u0011\n+\n(1\u0000cij)log2\u0010\n1\u0000f[Emusic(Xi)\bEmotion (Yj)]\u0011\n;(1)wherecijis de\fned by\ncij=(\n1;ifi=j;\n0;otherwise:\n4.4.2 Loss Function for M2S-GAN\nIn the generative learning stage, the generator gene-\nrates the conducting motion by ^Y=G(Emusic(X);z).",
                "The third term is the gradient penalty\nterm of the Wasserstein GAN[19], where ~Yis obtained\nvia the random linear interpolation between ^YandY.\nNote that, here, GandDminimizeLGandLDre-\nspectively, but Emusic andEmotion do not participate in\nthe optimization; their weights are directly transferred\nfrom the trained M2S-Net and frozen in M2S-GAN.",
                "Chen et al.[57]added a self-supervised loss to the dis-\ncriminator of a conditional generative adversarial net-\nwork (CGAN); afterwards Hao et al.[58]extended the\nmodel to a cross-modal cycle generative adversarial net-\nwork (CMCGAN).",
                "Choi et al.[60]\frst de-\nployed a cross-modal identity matching task, and then\ntransferred the learned features to a CGAN.",
                "For its part, our proposed approach has two\nlearning stages: we \frst obtain an optimal M2S-Net,\nand then apply it to M2S-GAN.\n5.2 Sync Loss vs Perceptual Loss\nPerceptual loss[61]is a popular choice in many ill-\nposed image manipulation tasks.",
                "The third type involves using the\nfeature of a GAN's discriminator to calculate percep-\ntual loss[63].",
                "As suggested in [19], our\nM2S-GAN is trained by the RMSprop optimizer[64]with a learning rate of 0.000 5.",
                "Overall, the training of our approach (M2S-\nNet + M2S-GAN) takes approximately 48 hours.",
                "Following WGAN-\nGP[19], in each step, we train the discriminator \fve\ntimes and train the generator once.",
                "How-\never, it would not be suitable to use the sync loss as\nan evaluation metric, since our proposed M2S-GAN\ndirectly minimizes it.",
                "Note that this\napproach does not result in any data leakage, since\nEmotion trained on the testing set is not involved in the\ntraining of M2S-GAN.",
                "parallel with the generator,\neven if the generator is not trained with the GAN loss.",
                "(GAN )[43].",
                "The beat data is modi\fed\nto sawtooth the wave-like data, as in [ 12].The performances of the above comparison meth-\nods and our proposed M2S-GAN are listed in Table 2 .",
                "As the ta-\nble shows, our M2S-GAN outperforms all comparison\nmethods on SE, RDE, SCE, and W-dis.",
                "The superior\nresults on SE, RDE, and SCE indicate that M2S-GAN\ncan model the music-motion relationships the most ac-\ncurately.",
                "M2S-GAN's advantages on SDP and W-dis\nindicate that it generates the most realistic conducting\nmotion.",
                "Notably, M2S-GAN does not achieve the low-\nest MSE.",
                "Comparatively, the motion distri-\nbutions generated by GAN-based approaches (GAN[43]\nand Our M2S-GAN) look much closer to the real mo-\ntion.",
                "Com-\nparing these two GAN-based approaches, we \fnd that\nthe motion generated by our M2S-GAN conforms far\nmore closely with the music.",
                "Performances of Music-Driven Conducting Motion Generation\nModel MSE ( \u0002103) SE W-dis ( \u0002103) RDE SCE SDP (%)\nShlizerman et al. , LSTM[32]3.50 1.301 87.470 0 0.973 9 2.511 38.98\nYelta et al. , CNN-LSTM[28]3.08 0.911 50.850 0 0.991 1 2.482 27.11\nGinosar et al. , GAN[43]6.60 1.371 29.980 0 0.943 7 2.864 97.93\nOurs, M2S-GAN 5.40 0.883 1.426 4 0.049 0 2.046 99.62\n(b) (a) (c) (d) (e)\nFig.8.",
                "(c) GAN[43], SD = 0.043 51, SDP = 97.93%.",
                "(d) M2S-GAN, SD = 0.043 51, SDP = 99.62%.",
                "In our experiment, although\nthe super-hard negatives under-perform hard negatives\nin M2S learning, the training process under super-hard\nnegatives is reasonably smooth, as shown in Fig.9.6.6 Impact of Training Set Scale\nWe next conduct another experiment to demon-\nstrate the necessity of discarding the MSE loss: we\ntrain the MSE model and our proposed M2S-GAN us-\ning di\u000berent scales of the training set.",
                "Comparatively, the SDP of our M2S-GAN (red\nlines) does not change with the training set scale, re-\nmaining stable at around 100%.",
                "Since M2S-GAN does\nnot seek to regress the ground truth motion, it has a\nmuch higher MSE than the MSE model.",
                "7\u25cbhttps://github.com/ChenDelong1999/VirtualConductor, Mar. 2022.Fan Liu et al. : Self-Supervised Music Motion Synchronization Learning 555\n0 20 120\n100\n80 \n60 \n40 \n20 \n0\n         40      60 80 100 0 20         40      60 80 100\nSDP (%) \nTraining Set Scale (h) MSE \nTraining Set Scale (h) \u03a410 -3 \n7\n6\n5\n4\n3\n2\n1\n0Ours, MSE (Testing Set)\nOurs, M2S-GAN (Testing Set)\nOurs, MSE (Training Set)\nM2S-GAN (Training Set)Ours, MSE (Testing Set)\nOurs, M2S-GAN (Testing Set)\nOurs, MSE (Training Set)\nM2S-GAN (Training Set)\nFig.10.",
                "The ConductorMotion100\ndataset enables M2S-GAN to learn rich music seman-\ntics.",
                "An M2S-Net is \frst trained\nwith a self-supervised loss, and then an M2S-GAN\nis trained with adversarial loss and a proposed sync\nloss, which measure the realism and the perceptual\nsimilarity between the real motion and the generated\nmotion respectively.",
                "the Annual Conference on Neural Informa-\ntion Processing Systems , December 2018, pp.7774-7785.[18] Arjovsky M, Chintala S, Bottou L. Wasserstein GAN.\narXiv:1701.07875, 2017.",
                "[19] Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville\nA C. Improved training of Wasserstein GANs.",
                "[58] Hao W, Zhang Z, Guan H. CMCGAN:"
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Integration of a music generator and a song lyrics generator to create Spanish popular songs",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [
                "Awavegenerative\nadversarialnetwork(WaveGAN)architectureisalsoemployedtogenerateaudiofilesfordataaugmentation.",
                "3 Weexplorethetime-domainstrategyofsynthetic\nmusicaudiogenerationfordataaugmentationusing\nWaveGAN.Theproposedtaskisaddressedwithand\nwithoutdataaugmentation.",
                "Asapartofdataaug-\nmentation, additional training files are generated using\nWaveGAN (Fig.",
                "Fig.1Visualrepresentationofanaudioexcerptwithacousticguitarasleading,UpperpanerepresentstheMel-spectrogram,modgdgram,and\ntempogramoftheoriginalaudiofileandlowerpanerepresentstheWaveGANgeneratedfilesReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page4of14\n3.1.2 Modifiedgroupdelayfunctionsandmodgdgram\nGroup delay features are being employed in numerous\nspeech and music processing applications [ 18],[33].",
                "5.2 DataaugmentationusingWaveGAN\nGenerative adversarial networks (GAN) have been suc-\ncessfully applied to a variety of problems in image gen-\neration [41] and style transfer [ 42].",
                "WaveGAN architec-\nture is similar to deep convolutional GAN (DCGAN),\nwhich is used for Mel-spectrogram generation in vari-\nous music processing applications.",
                "The DCGAN gener-\nator uses transposed convolution to iteratively upsample\nlow-resolutionfeaturemapsintoahigh-resolutionimage.",
                "In WaveGAN architecture, the transposed convolution\noperation is modified to widen its receptive field.",
                "Thediscriminatorisalsomodifiedsimilarly,usinglength-\n25 filters in one dimension and increasing stride from\ntwo to four which results in WaveGAN architecture [ 43].The transposed convolution in the generator produces\ncheckerboard artifacts [ 43].",
                "Finally,thesystemistrainedusingtheWasser-\nstein GAN with gradient penalty (WGAN-GP) strategy",
                "For training, the WaveGAN optimizes\nWGAN-GP using Adam for both generator and discrim-\ninator.",
                "WaveGAN is trained for 2000 epochs on the three-sec\naudio files of each class to generate similar audio files\nbased on a similarity metric ( s)[45] with an acceptance\ncriterion of s>0.1.",
                "The values of parameters and hyper-\nparameters associated with WaveGAN for our experi-\nments are listed in Table 2.",
                "This value is comparable to the MOS\nscoreobtainedin[ 43]and[46]usingW aveGAN.\n5.3 Experimentalset-up\nTheexperimentisprogressedinfourphases,namelyMel-\nspectrogram-based, modgdgram-based, and tempogram-\nTable2VarioushyperparameterschosenforWaveGAN\nName Value\nWavGANLatentdimension 100\nNumberofchannels 1\nWavGANdimension 32\nTrainingbatchsize 64\nKernellength 25\nGenerationlength 65,536samples\nLoss WGAN-GP( \u03bb=10)\nDupdatesperGupdates 5ReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page8of14\nbased, followed by soft voting.",
                "The time-domain strat-\negy of synthetic music generation for data augmentation\nusing WaveGAN is explored.",
                "WaveGAN data augmenta-\ntion for instrument detection is probably a new attempt\nin predominant instrument recognition.",
                "I.Gulrajani,F.Ahmed,M.Arjovsky,V.Dumoulin,A.Courville,in Proc.ofthe\n31stInternationalConferenceonNeuralInformationProcessingSystems,\nLongBeachCaliforniaUSADecember4-9,2017 .Improvedtrainingof\nwassersteinGANs(CurranAssociatesInc.,MorehouseLaneRedHookNY,\n2017)\n45.",
                "J.Kong,J.Kim,J.Bae,in Proc.of34thConferenceonNeuralInformation\nProcessingSystems(NeurIPS2020),Vancouver,Canada .HiFi-GAN:\nGenerativeAdversarialNetworksforEfficientandHighFidelitySpeech\nSynthesis,vol.33(CurranAssociates,Inc,2020),pp.17022\u201317033\nPublisher\u2019sNote\nSpringerNatureremainsneutralwithregardtojurisdictionalclaimsin\npublishedmapsandinstitutionalaffiliations."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [
                "ReghunathandRajan EURASIPJournalonAudio,Speech,andMusic\nProcessing         (2022) 2022:11 \nhttps://doi.org/10.1186/s13636-022-00245-8\nEMPIRICAL RESEARCH OpenAccess\nTransformer-basedensemblemethod\nformultiplepredominantinstruments\nrecognitioninpolyphonicmusic\nLekshmiChandrikaReghunath*and RajeevRajan\nAbstract\nMultiplepredominantinstrumentrecognitioninpolyphonicmusicisaddressedusingdecisionlevelfusionofthree\ntransformer-basedarchitecturesonanensembleofvisualrepresentations."
            ],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "Genre Recognition from Symbolic Music with CNNs: Performance and Explainability",
            "RNN": [],
            "Recurrent Neural Network": [],
            "VAE": [],
            "Variational Autoencoder": [],
            "GAN": [],
            "Generative Adversarial Network": [],
            "Transformer-based": [],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        },
        {
            "title": "CycleDRUMS: automatic drum arrangement for bass lines using CycleGAN",
            "RNN": [
                "In [23], RNNs are used for the prediction and \ncomposition of polyphonic music; in [24], highly convincing chorales in the style of Bach were automatically generated \nusing note names [25]; added higher-level structure on generated polyphonic music, whereas in [26] an end-to-end \ngenerative model capable of composing music conditioned on a specific mixture of composer styles was designed.",
                "The \napproach described in [27], instead, relies on notes as an intermediate representation to a suite of models\u2014namely, a \ntranscription model based on a CNN and an RNN network [ 28], a self-attention-based music language model [29] and a \nWaveNet model",
                "Mogren O. C-RNN-GAN: continuous recurrent neural networks with adversarial training.",
                "Mehri S, Kumar K, Gulrajani I, Kumar R, Jain S, Sotelo J, Courville A, Bengio Y. SampleRNN: an unconditional end-to-end neural audio \ngeneration model."
            ],
            "Recurrent Neural Network": [],
            "VAE": [
                "Finally, in [12], the authors tackled \nthe long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes and modeled such context \nthrough Sparse Transformers, in order to generate music with singing in the raw audio domain."
            ],
            "Variational Autoencoder": [],
            "GAN": [
                "Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7\n1 3Discover Artificial IntelligenceResearch\nCycleDRUMS: automatic drum arrangement for\u00a0bass lines using \nCycleGAN\nGiorgio\u00a0Barnab\u00f21\u00a0\u00b7 Giovanni\u00a0Trappolini1\u00a0\u00b7 Lorenzo\u00a0Lastilla1\u00a0\u00b7 Cesare\u00a0Campagnano2\u00a0\u00b7 Angela\u00a0Fan3\u00a0\u00b7 Fabio\u00a0Petroni3\u00a0\u00b7 \nFabrizio\u00a0Silvestri1\nReceived: 12 September 2022 / Accepted: 4 January 2023",
                "We formulated this task as an unpaired \nimage-to-image translation problem, and we addressed it with CycleGAN, a well-established unsupervised style transfer \nframework designed initially for treating images.",
                "Keywords Automatic music arrangement\u00a0\u00b7 Cycle-GAN\u00a0\u00b7 Deep learning\u00a0\u00b7 Source separation\u00a0\u00b7 Audio and speech \nprocessing\n1 Introduction\nThe development of home music production has brought significant innovations into the process of pop music composi-\ntion.",
                "To solve this task, we tested an unpaired image-to-image translation strategy known as CycleGAN [7 ].",
                "In particular, we \ntrained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset [8 ] and the musdb18 dataset [9 ].",
                "To sum up, our main contributions are the following:\n\u2022 we trained a CycleGAN architecture on bass and drum mel-spectrograms in order to automatically generate drums \nthat follow the beat and sound credible for any given bass line;\n\u2022 our approach can generate drum arrangements with low computational resources and limited inference time, if \ncompared to other popular solutions for automatic music generation [12];\n\u2022 we developed a metric\u2014partially based on or correlated to human (and expert) judgment\u2014to automatically evaluate \nthe obtained results and the creativity of the proposed system, given the challenges of a quantitative assessment of \nmusic;\n\u2022 we compared our method to Pix2Pix, another popular image transfer network, showing that the music arrangement \nproblem can be better tackled with an unpaired approach and adding a cycle-consistency loss.",
                "It is worth recalling that the application of GANs to music generation tasks is not \nnew: in [45], GANs are applied to symbolic music to perform music genre transfer, while in [46, 47], authors construct \nand deploy an adversary of deep learning systems applied to music content analysis; however, to the best of our knowl-\nedge, GANs have never been applied to raw audio in the mel-frequency domain for music generation purposes.",
                "[48], and (ii) the definition of an objective \nmetric and loss is a common problem to generative models such as GANs: as of now, generative models in the music \ndomain are evaluated based on the subjective response of a pool of listeners, because an objective metric for the raw \naudio representation has never been proposed so far.",
                "3.3  Image to\u00a0image translation\u2014CycleGAN\nWe cast the automatic drum arrangement generation task as an unpaired image-to-image translation task, and we \nsolved it by adapting the CycleGAN model to our purpose.",
                "CycleGAN is a framework designed to translate between \ndomains with unpaired input\u2013output examples.",
                "This property is achieved by training both the mapping G  and F  simultaneously \nwith a \u201cstandard\u201d GAN loss of the form\nFig. 1  To solve the automatic drum arrangement task, we tested an unpaired image-to-image translation strategy known as CycleGAN [7].",
                "In particular, we trained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset",
                "For the discriminator networks, we use PathcGANs [ 11, 57, 58], which aim to classify \nwhether overlapping image patches are real or fakes.",
                "Since the CycleGAN model takes 256\u00d7256 images as input, each mel-spectrogram is chunked into smaller pieces \nwith an overlapping window of 50 time frames, obtaining multiple samples from each song (each equivalent to 5 s of \nmusic); finally, in order to obtain one channel images from the original spectrograms, we performed a discretization \nstep in the range [0\u2013255].",
                "In the final stage of our pipeline, we fed CycleGAN architecture with the obtained dataset.",
                "As previously anticipated, this task is an appropriate first step toward fully automated LGAN(G,DY,X,Y)=/u1D53Cy\u223cpdata(y)[logDY(y)]\n+/u1D53Cx\u223cpdata(x)[log(1 \u2212DY(G(x)))],\nLcyc(G,F)=/u1D53Cx\u223cpdata(x)[\u2016F(G(x))",
                "L(G,F,DX,DY)=LGAN(G,DY,X,Y)",
                "+LGAN(F ,DX,Y,X)+/u1D706Lcyc(G,F).",
                "Fig. 2  CycleGAN is a framework designed to translate between domains with unpaired input\u2013output examples.",
                "The \nhop size, 256, was chosen according to recommendations from [35].\n4.2  Training of\u00a0the\u00a0CycleGAN model",
                "As to the CycleGAN model used for training, we relied on the default network5.",
                "As a result, the model uses a resnet_9blocks ResNet generator and a basic 70 \u00d7 70 PatchGAN as a discriminator.",
                "Cycle GAN- and-",
                "FID-based features: in the context of GANs result evaluation, the Fr\u00e9chet Inception distance (FID) is supposed to \nimprove on the Inception Score by actually comparing the statistics of generated samples to authentic samples [63].",
                "Unlike CycleGAN, Pix2Pix learns to translate between domains when \nfed with paired input\u2013output examples.",
                "We then asked the same four evaluators to grade the new drum samples according to the \nprinciples presented in Sect.\u00a0 4.4.\n4.6  Experimental results\nFigure\u00a0 3 shows the distribution of grades for the 400 test drums for both CycleGAN and Pix2Pix\u2014averaged among all \nfour independent evaluators and over all four dimensions.",
                "Given this pretty good result, we could then use this trained logistic model to label 14,000 different 5s fake drum clips \nproduced from as many real bass lines using both CycleGAN and Pix2Pix.",
                "Cycle GAN- and-",
                "We applied \nCycleGAN to real bass lines, treated as gray-scale images (mel-spectrograms), obtaining good ratings, especially com-\npared to another image-to-image translation approach (Pix2pix).",
                "It is not unlikely that in the future, artists and composers will \nstart creating their music almost like they were drawing.\nFig. 3  (left) The distribution of grades for the 400 test drums for both CycleGAN and Pix2Pix (baseline)\u2014averaged among all four independ-\nent evaluators and over all four dimensions.",
                "Given this pretty good result, we could then use this trained logistic \nmodel to label 14,000 different 5s fake drum clips produced from as many real bass lines using both CycleGAN and Pix2Pix (baseline).",
                "The patch size refers to the pixel dimensions of the patches considered for the Patch-\nGAN, that is used as the building block for the discriminator.",
                "Mogren O. C-RNN-GAN: continuous recurrent neural networks with adversarial training."
            ],
            "Generative Adversarial Network": [],
            "Transformer-based": [
                "As to \nthe arrangement generation task, the large majority of approaches proposed in the literature is based on a symbolic \nrepresentation of music: in [5 ], a novel multi-track MIDI representation (MuMIDI) is presented, which enables simultane -\nous multi-track generation in a single sequence and explicitly models the dependency of the notes from different tracks \nutilizing a Transformer-based architecture; in [4 ], a deep reinforcement learning algorithm for online accompaniment \ngeneration is described."
            ],
            "Tranformer": [],
            "Neural Autoregressive": [],
            "NAM": [],
            "Deep Belief Network": [],
            "DBN": [],
            "Markov Chain Monte Carlo": [],
            "MCMC": [],
            "Boltzmann Machines": [],
            "BM": [],
            "Hopfield Network": []
        }
    ]
}