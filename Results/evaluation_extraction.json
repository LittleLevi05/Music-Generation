{
    "data_collection": [
        {
            "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Hierarchical Recurrent Neural Networks for Conditional Melody Generation with Long-term Structure",
            "evaluation metrics": [
                "Below, we \ufb01rst describe our dataset and pre-processing\nmethod, followed by a description of the evaluation metrics\nand the listening test setup."
            ],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Melodies\ngenerated by the proposed model were extensively evaluated in\nquantitative experiments as well as a user study to ensure the\nmusical quality and long-term structure of the output.",
                "Finally, the model output is evaluated in a\nlistening study.",
                "By combining the generated pitch and rhythmic pattern, the\ngenerated monophonic melodies are qualitatively evaluated\nthrough multiple listening tests and are shown to outperform\nthe LookbackRNN",
                "Moreover, the\nmusical quality of HRNN-generated music is only evaluated\nthrough a subjective listening test.",
                "In contrast, we evaluate\nour proposed CM-HRNN with extensive analytical measures\nas well as a user study.",
                "V. E XPERIMENTS\nA. Experimental setup\nWe set up several experiments to determine the optimal\nnetwork architecture; evaluate the model\u2019s ability to generate\nhigh-quality music with structure, and to compare it with a\nstate-of-the-art system, AttentionRNN.",
                "[19] as a proxy to evaluate the long\nterm structures contained in the generated music as in [11].",
                "D. Listening test setup\nAn online listening test was conducted to evaluate the\nproposed model subjectively.",
                "R ESULTS\nA. Ability to learn correct bar timing and effectiveness of the\naccumulated time information\nTo evaluate the in\ufb02uence of adding the accumulated time\ninformation in the bottom generation tier on the predicted\nduration of each note, as well as the correct bar placement,\nwe trained 4 variants of our model (see Table II): a model\nwith 2-tiers and 3-tiers, each with and without the accumulated\ntime information added as input."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "LEARNING TO GENERATE MUSIC WITH SENTIMENT",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "We evaluate the accuracy of the model\nin classifying sentiment of symbolic music using a new\ndataset of video game soundtracks.",
                "In order to evaluate this approach, we need a dataset of\nmusic in symbolic format that is annotated by sentiment.",
                "5. SENTIMENT ANALYSIS EVALUATION\nTo evaluate the sentiment classi\ufb01cation accuracy of our\nmethod (generative mLSTM + logistic regression), we\ncompare it to a baseline method which is a traditional\nclassi\ufb01cation mLSTM trained in a supervised way.",
                "We evaluated each variation of the generative mLSTM\nwith a forward pass on test shard using mini-batches of size\n32.",
                "We evaluate both methods using a\n10-fold cross validation approach, where the test folds have\nno phrases that appear in the training folds.",
                "For each generation,\nthe GA (i) evaluates the current population, (ii) selects 100\nparents via a roulette wheel with elitism, (iii) recombines\nthe parents (crossover) taking the average of their genes\nand (iv) mutates each new recombined individual (new\noffspring) by randomly setting each gene to an uniformly\nsampled random number \u00002\u0014r\u00142.",
                "We evaluated this model both as a\ngenerator and as a sentiment classi\ufb01er."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Personalized Popular Music Generation Using Imitation and Structure",
            "evaluation metrics": [],
            "subjective evaluation": [
                "15Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n5 Evaluation\nWe conducted both objective and subjective evaluations of our stylistic music generation\nsystem.",
                "This is consistent with subjective evaluations discussed in the next section."
            ],
            "objective evaluation": [],
            "evaluate": [
                "Therefore, it is easier to compare, discuss, and\nevaluate popular music than other music.",
                "In Section 5, we describe both objective and sub-\njective experiments to evaluate the model output.",
                "Thus, we\ndesigned a number of melody style rating functions to evaluate the suitability of the next\nnote given previous notes in the context of a chord progression.",
                "For rest notes in the middle of a phrase, we use both rest note duration statistics from the\nseed song and music theory rules to evaluate the rating.",
                "Some also told us that the chord progression\nis too hard for non-musical experts to evaluate, and even hard to tell the di\u000berence for\nprofessional musicians, which explains why the results for chord progression alone are not\nconsistent with the other parts."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Such approaches iteratively generate examples and then use a fitness function to evaluate them.",
                "Fr\u00e9chet Inception Distance (FID) is used to evaluate the generations and classification accuracy is used to evaluate the style classification.",
                "For human evaluation, 100 participants were asked to evaluate between the proposed model and the baselines, with 63.3% preferring the generations made by the proposed model.",
                "Further case studies were conducted to evaluate the originality and complexity of the generations.",
                "The authors did not evaluate the performance of the generations but rather introduced the \ndataset and encouraged researchers to work on improving the quality of the generations.",
                "Several datasets including MNIST was used to evaluate the network.",
                "The discriminator simply evaluates between the ground truth and the generated sample.",
                "[29] Generate pre-modern Japanese art facial expression StyleGAN WGAN-GP Configurations not provided No evaluation [31] Generate face photo from a sketch and vice versa Conditional GAN Cross entropy Configurations not provided No evaluation, the loss values were reported [32] Generate digits and character strokes Modified DCGAN + agent MSE Generator contains CONV+LeakyReLU, Agent is VGG Generated images not evaluated, classification accuracy 91% on MNIST [33] Generate calligraphy and handwritten digits Modified conditional GAN with multiple encoders for style & content-encoding Binary Cross entropy discriminator, cross-entropy style loss & Kullback-Leibler content loss Two residual blocks then 4 CONV modules generator, 1 CONV layer then 6 residual blocks discriminator FID 120.1, 49.3% of the images were identified to be synthesized C. Music and Melody Generation using GANs  Next, GAN approaches for music and melody generation are presented.",
                "Human evaluators consisting of expert musicians as well as amateurs were provided three samples to evaluate, and these samples were generated with three different epochs.",
                "Finally, nineteen participants were invited to evaluate the quality of the music.",
                "A case study with 21 participants, including 10 people with a music background, was conducted to evaluate the generations.",
                "Moreover, the reward in RL arrives from the discriminator which is evaluated on the entire sequence.",
                "Additionally, 70 experts on Chinese poems were asked to evaluate between 20 real poems, 20 generated using maximum likelihood estimation, and 20 generated using SeqGAN.",
                "The proposed image to poetry GAN (I2P-GAN) was evaluated on several metrics such as relevance, novelty, and BLEU scores against different architectures including SeqGAN.",
                "Human evaluators were also invited to evaluate out of 10 on relevance, coherence, imaginativeness, and the generations overall.",
                "The proposed architecture was evaluated for poetry generation on a Chinese poem dataset containing over 13,000 five-word quatrain poems.",
                "Therefore, researchers should be encouraged to invite poets and writers to evaluate the generated literary texts on their creativity."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "CONTROLLABLE DEEP MELODY GENERATION VIA HIERARCHICAL MUSIC STRUCTURE REPRESENTATION",
            "evaluation metrics": [],
            "subjective evaluation": [
                "Both objective and subjective evaluations show the im-\nportance of having music frameworks."
            ],
            "objective evaluation": [],
            "evaluate": [
                "Our work is with pop music because structures are rel-\natively simple and listeners are generally familiar with the\nstyle and thus able to evaluate compositions.",
                "4.3 Subjective Evaluation\n4.3.1 Design of the listening evaluation\nWe conducted a listening test to evaluate the generated\nsongs."
            ],
            "model accuracy": [
                "Also, in both\nrhythm and melody generation, the MusicFrameworks ap-\nproach signi\ufb01cantly improves the model accuracy."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer",
            "evaluation metrics": [],
            "subjective evaluation": [
                "We report on objective and subjective evaluations of variants of\nthe proposed Theme Transformer and the conventional prompt-\nbased baseline, showing that our best model can generate, to\nsome extent, polyphonic pop piano music with repetition and\nplausible variations of a given condition.",
                "Speci\ufb01cally, we train the\nmodels using the training split of the POP909 dataset [25],\nand conduct objective and subjective evaluations on its test\nsplit.",
                "The subjective evaluation en-\ntails an online listening test that involves listeners familiar\nand unfamiliar with the music pieces in the POP909 dataset.",
                "Conducted objective\nand subjective evaluations demonstrate that, compared to the\ndecoder-only Music Transformer and the simple Seq2seq\nTransformer, Theme Transformer does better in repeating the\nthematic materials with perceptible variations, and enhances\nthe quality of the generated music."
            ],
            "objective evaluation": [
                "In the objective evaluation, weuse some general metrics from [26] and some novel theme-\nspeci\ufb01c metrics proposed here.",
                "In the objective evaluation, we let each model generate 64-\nbar polyphonic piano music using the thematic conditions\nretrieved from each of the 29 testing songs of POP909.",
                "0.05\u00060.05 0.04\u00060.04 12.24\u000611.32\nTABLE II: Result of the objective evaluation."
            ],
            "evaluate": [
                "Prompt-based Theme transformer Original pieceFig. 1: Given either an original or generated piece, we split\nit into non-overlapping two-bar fragments, and evaluate the\ndifference between each of these fragments with the beginning\n(i.e., \ufb01rst) fragment of the same piece using a distance measure\ncomputed in a \u201cmelody embedding\u201d space (see Section IV-C\nfor de\ufb01nition).",
                "Because the embedding model is trained to discriminate\nsimilar and dissimilar fragments, we will also use it to set up\nobjective metrics to evaluate the performance of our model:\nD(Si;Sj)",
                "We evaluate the result using two sets of metrics.",
                "We develop theme inconsistency to\nevaluate the average difference in melody embedding among\nall pairs of different theme regions (delimited by T HEME -\nSTART and T HEME -ENDtokens) in a generated piece, i.e.,\n2\nN(N\u00001)P\ni;jD(\u0000i;\u0000j), where \u0000idenotes a theme region\nandNthe number of theme regions in a piece.",
                "By theme\nuncontrollability , we evaluate the closeness between the theme\nregions and the condition,1\nNPN\ni=1D(c1:\u001c;\u0000i).",
                "0.04\u00060.04 12.24\u000611.32\nTABLE III: Result of an ablation study that evaluates variants of the implemented models\nthe generation, suggesting the effectiveness of the proposed\ntheme-aligned positional encoding and gated parallel attention.",
                "Here, we evaluate a\nvariant of the Theme Transformer that uses the SEinstead.",
                "We intend to have subjects evaluate both the machine gen-\nerated pieces and original pieces from the test set.",
                "We evaluate not only CL but also the following methods:"
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "evaluation metrics": [],
            "subjective evaluation": [
                "While each method\nwas presented with some subjective evaluation (often by the\nauthors), there has yet been little effort to quantitatively and\nobjectively evaluate generated samples."
            ],
            "objective evaluation": [
                "We \ufb01rmly believe that objective evaluation of music gen-\neration methods is key, in order to fairly monitor progress in\nthis domain."
            ],
            "evaluate": [
                "Any engineered measures to\nevaluate generated music typically attempt to de\ufb01ne the samples\u2019\nmusicality, but do not capture qualities of music such as theme or\nmood.",
                "Finally, we use a classi\ufb01er\ntrained on real data to evaluate the label validity of class-\nconditionally generated samples.",
                "While each method\nwas presented with some subjective evaluation (often by the\nauthors), there has yet been little effort to quantitatively and\nobjectively evaluate generated samples.",
                "It is com-\nmon to employ multiple human annotators to evaluate each\nsample [11] in order to elicit con\ufb01dent insights, something\nthat hinders scalability.",
                "Recent work [12] proposes simple,\nmusically informed metrics to evaluate the musicality of gen-\nerated music.",
                "In the emotional speech synthesis domain (another sub-\njective audio domain), the generative adversarial network\nbased studies performed in [13]\u2013[15] quantitatively evaluated\nsynthesised speech using a classi\ufb01er to demonstrate that the\ngenerated speech contained meaningful emotional information.",
                "We functionally evaluate the perfor-\nmance of three generative models, which subscribe to different\ngeneration paradigms.\nSampleRNN.",
                "We\nevaluate performance on the test partition of each dataset with\nrespect to micro and macro averaged metrics, calculated in the\nsame manner as the MediaEval 2019 competition submission."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control",
            "evaluation metrics": [
                "We see a drop in performance in all evaluation metrics for\nevery model compared to the conditional generation task,\nwhich is expected due to distributional shifts in the data."
            ],
            "subjective evaluation": [
                "To\nevaluate sample quality, we employ subjective evaluation in\nthe form of a user study."
            ],
            "objective evaluation": [],
            "evaluate": [
                "We evaluate the proposed method on its ability to adhere to\nthe prescribed condition by comparing it to state-of-the-art\nmethods for controllable symbolic music (Choi et al., 2020;\nWu & Yang, 2021).",
                "To\nevaluate sample quality, we employ subjective evaluation in\nthe form of a user study.",
                "We also perform an ablation study\nto validate different parts of the proposed model and \ufb01nally\nevaluate the subjective quality of generated samples.",
                "D ESCRIPTION FIDELITY\nWe also quantitatively evaluate the \ufb01delity of generated\nsequences to the given condition.",
                "We also evaluate chroma and groov-\ning similarity as a way to quantify similarity in sound and\nrhythm as proposed by Wu & Yang (2021).",
                "Zero-Shot Medley Generation\nWe evaluate the out-of-distribution performance of our mod-\nels by combining two sequences x(1)andx(2)into a new\nsequence that does not belong to the training distribution.",
                "Ablation Study\nTo evaluate which parts of the expert description are essen-\ntial, we group it into three components: instruments, chords\nand meta-information.",
                "Subjective Evaluation\nFinally, we evaluate the subjective quality of generated sam-\nples through a user study, comparing our best model to the\nbaselines."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Music Generation Using an LSTM",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Once this \noutput is achieved, we evaluated each model on how \nwe liked the songs.",
                "Among these includes how to evaluate non -deterministic outputs, how the \ntime between notes should be represented, how the data as a whole should be represented, \nand how to have multiple tracks of music generated."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Prote\u00e7\u00e3o intelectual de obras produzidas por sistemas baseados em intelig\u00eancia artificial: uma vis\u00e3o tecnicista sobre o tema",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "An adaptive music generation architecture for games based on the deep learning Transformer model",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "WHAT IS MISSING IN DEEP MUSIC GENERATION? A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "[32\u201335] such as pitch class, pitch intervals, and\nrhythm density to evaluate music statistically.",
                "However,\nmost of them ignore even short patterns, and none evaluate\nmusic structure.",
                "In contrast, we offer quantitative and ob-\njective methods to evaluate music repetition and structure."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "VIS2MUS: EXPLORING MULTIMODAL REPRESENTATION MAPPING FOR CONTROLLABLE MUSIC GENERATION",
            "evaluation metrics": [],
            "subjective evaluation": [
                "[14] and develop\na method that combines representation learning with user\nsubjective evaluations."
            ],
            "objective evaluation": [],
            "evaluate": [
                "In speci\ufb01c, we i) propose several\ntransformations that can be applied to both music and image\nrepresentations, ii) synthesize image-music pairs data using\ndeep generative models by applying the proposed transforma-\ntions, and iii) conduct user studies to evaluate the synthesized\npairs to explore properties of visual-to-image mappings."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "GENERATING MUSIC WITH SENTIMENT USING TRANSFORMER-GANS",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "EXPERIMENTS\nWe evaluated our models both with respect to the overall\nquality of the samples they produce and their ability to gen-\nerate songs that convey the conditioning emotional signals.",
                "For each model, we generated 400samples\n(100for each class) and evaluated these samples with re-\nspect to the characteristics above, then averaged the results\nto produce the overall model score."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning",
            "evaluation metrics": [
                "We demonstrate that WuYun can generate melodies with better\nlong-term structure and musicality and outperforms other state-of-the-art methods\nby 0.51 on average on all subjective evaluation metrics.",
                "The hierarchical\nskeleton-guided melody generation architecture effectively improves generated melodies\u2019 long-term\nstructure and musicality and outperforms other state-of-the-art methods by 0.51 on average on all\nsubjective evaluation metrics.",
                "Consequently, almost all the proposed objective\n7evaluation metrics are dif\ufb01cult to apply for comparing different music creation systems and lack\nsustainability for future development demands.",
                "We tried to calculate the averaging overlapped area\nof some musical feature distributions between generated musical pieces and ground-truth musical\npieces as the objective evaluation metrics like (18, 60) using the public evaluation toolbox.",
                "Each subject\nwas required to rate 18 musical pieces, which cost approximately 25 min.\nFigure 3A shows the mean opinion scores of WuYun architecture\u2019s melody generation performances\nwith nine different settings on the \ufb01ve subjective evaluation metrics from all subjects in the form of\nhistograms.",
                "The rhythmic skeleton setting (No. 3) achieved the best\nresult on all subjective evaluation metrics, followed by the tonal skeleton setting (No. 4).",
                "Among\nthe three types of melodic skeletons associated with rhythm (Nos. 1, 2, and 3), the melodic skeleton\ncomposed of a single type of accent (e.g., metrical accents or agogic accents (31)) has a large gap with\nthe rhythmic skeleton in richness, expectation, and overall quality and even surpassed by the random\nmelodic skeletons (Nos. 7, 8, and 9) on most subjective evaluation metrics.",
                "In this study, we chose the setting of the rhythmic skeleton (No. 3) that performed best on all\nsubjective evaluation metrics in this experiment as the default skeleton con\ufb01guration (denoted as\nWuYun-RS) for the next experiment to compare with other melody generation models.",
                "The WuYun architecture with the rhythmic skeleton setting achieves the best performance in\nall melodic skeleton settings on all subjective evaluation metrics.",
                "The average experiment time\ncost each subject about 2 h.\nFigure 3B shows the mean opinion scores and one-tailed t-test results of the different music generation\nsystems on the \ufb01ve evaluation metrics in the form of violin plots.",
                "Furthermore, WuYun-RS and WuYun-RRS demonstrate highly similar\nperformances in terms of the quality of generated melodies on all evaluation metrics."
            ],
            "subjective evaluation": [
                "We demonstrate that WuYun can generate melodies with better\nlong-term structure and musicality and outperforms other state-of-the-art methods\nby 0.51 on average on all subjective evaluation metrics.",
                "The hierarchical\nskeleton-guided melody generation architecture effectively improves generated melodies\u2019 long-term\nstructure and musicality and outperforms other state-of-the-art methods by 0.51 on average on all\nsubjective evaluation metrics.",
                "The human listening test is currently an indispensable and viable method\nfor subjective evaluation to measure the quality of the generated musical pieces.",
                "Therefore, we conducted two subjective evaluation experiments to evaluate the performance of our\nproposed WuYun, including different melodic skeleton settings in rhythm and pitch dimensions and\ncomparisons with public state-of-the-art (SOTA) music generation models.",
                "Therefore, before formal experiments, we conducted multiple rounds of discussions,\ntesting, and validation with musicians and nonmusicians regarding the above subjective evaluation\nmetrics and their descriptions until they could easily understand and grasp them.",
                "To ensure that\nthe recruited subjects have a common understanding of the metrics and scales in the questionnaire,\nwe also conducted evaluation training for them, including the explanation of subjective evaluation\nmetrics and preliminary experiments.",
                "Each subject\nwas required to rate 18 musical pieces, which cost approximately 25 min.\nFigure 3A shows the mean opinion scores of WuYun architecture\u2019s melody generation performances\nwith nine different settings on the \ufb01ve subjective evaluation metrics from all subjects in the form of\nhistograms.",
                "The rhythmic skeleton setting (No. 3) achieved the best\nresult on all subjective evaluation metrics, followed by the tonal skeleton setting (No. 4).",
                "Among\nthe three types of melodic skeletons associated with rhythm (Nos. 1, 2, and 3), the melodic skeleton\ncomposed of a single type of accent (e.g., metrical accents or agogic accents (31)) has a large gap with\nthe rhythmic skeleton in richness, expectation, and overall quality and even surpassed by the random\nmelodic skeletons (Nos. 7, 8, and 9) on most subjective evaluation metrics.",
                "In this study, we chose the setting of the rhythmic skeleton (No. 3) that performed best on all\nsubjective evaluation metrics in this experiment as the default skeleton con\ufb01guration (denoted as\nWuYun-RS) for the next experiment to compare with other melody generation models.",
                "The WuYun architecture with the rhythmic skeleton setting achieves the best performance in\nall melodic skeleton settings on all subjective evaluation metrics.",
                "Since the second subjective evaluation\nexperiment relies on the result of the \ufb01rst subjective evaluation experiment and requires some time to\ncollect, process, and analyze, we recruited 13 participants again (i.e., six females and seven males,\nages 18 and 25 years) to evaluate the musical pieces with payment.",
                "4.5 Statistical analysis\nAll subjective evaluation results were expressed as mean \u0006standard deviation."
            ],
            "objective evaluation": [
                "2.4 Evaluation metrics\nSubjective and objective evaluations are the two essential aspects of evaluating the performance of\nmusic generation systems.",
                "However, for the\nobjective evaluation, many efforts have been made to design quantitative metrics; there is not a set of\nconvincing and uni\ufb01ed metrics.",
                "We tried to calculate the averaging overlapped area\nof some musical feature distributions between generated musical pieces and ground-truth musical\npieces as the objective evaluation metrics like (18, 60) using the public evaluation toolbox.",
                "We\narrived at a similar conclusion as PopMNet (32) that a better result of objective evaluation does not\nmean better structure and musicality of generated music.",
                "The same objective evaluation result can be\ncalculated and veri\ufb01ed with the provided melody MIDI \ufb01les of this study\u2019s next two experiments."
            ],
            "evaluate": [
                "To prove the effectiveness of the architecture, we evaluate WuYun on a publicly available melody\ndataset.",
                "Therefore, we conducted two subjective evaluation experiments to evaluate the performance of our\nproposed WuYun, including different melodic skeleton settings in rhythm and pitch dimensions and\ncomparisons with public state-of-the-art (SOTA) music generation models.",
                "2.5 Model performance based on different melodic skeleton settings\nTo compare the effectiveness of variants of melodic skeleton extracted from rhythm and pitch\ndimensions, we comprehensively evaluated the performance of WuYun based on different settings of\nthe melodic skeleton.",
                "We recruited 30 subjects (13 females\nand 17 males, ages 18 and 30 years) from Zhejiang University and Zhejiang Conservatory of Music\nto evaluate the musical pieces with payment.",
                "Since the second subjective evaluation\nexperiment relies on the result of the \ufb01rst subjective evaluation experiment and requires some time to\ncollect, process, and analyze, we recruited 13 participants again (i.e., six females and seven males,\nages 18 and 25 years) to evaluate the musical pieces with payment.",
                "4 Materials and Methods\n4.1 Details of dataset preprocessing\nWe evaluate the effectiveness of WuYun architecture on a commonly used and publicly available\nsymbolic melody dataset of Wikifonia (32, 33, 67).",
                "Data and materials availability: All data needed to evaluate the conclusions in the paper are present\nin the paper and/or the Supplementary Materials."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": [
                "Accents can be expressed in various ways to increase musical\nexpressiveness and add character to the movement of music."
            ]
        },
        {
            "title": "An investigation of the reconstruction capacity of stacked convolutional autoencoders for log-mel-spectrograms",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "In both of the previous studies, they evaluated their\nmethods of measuring the mean square error (MSE) between\nthe original and generated frame of the magnitude spectrogram\nframe.",
                "The architecture was evaluated\nby applying Inverse-STFT (ISTFT) on the generated frames\ndeveloping also a deterministic method for reconstructing the\nphase.",
                "The model was\nevaluated subjectively using the MSE but also objectively\nby statistically analysing quality ratings.",
                "The model was objectively evaluated\nusing MSE between the original and generated spectrogram\nbut also visually compared using rainbowgrams , which are\ninstantaneous frequency colored spectrograms.",
                "In\nthis research, we evaluate the effectiveness of autoencoders\nfor sound synthesis purposes and we measure possible im-\nprovement using additional techniques that are expected to\ncalibrate the model.",
                "The experiments\nwere conducted on a Tesla P100 GPU using the TensorFlow\nlibrary3.\n2https://magenta.tensor\ufb02ow.org/datasets/nsynth\n3https://www.tensor\ufb02ow.org/V. E VALUATION\nTo evaluate the effectiveness of a generative network, many\nmethods have been proposed.",
                "In order to evaluate the generated spectrograms, we devel-\noped a metric that highlights the signi\ufb01cance of harmonics in\ntimbre synthesis.",
                "The calculation of the original, generated and identical\nfrequencies can then be used to evaluate the predicted samples\nusing a precision-recall schema",
                "D. Dimensionality of the latent space\nTo evaluate the effectiveness of neural autoencoders regard-\ning their compression ability, an experimentation on the latent\nspace dimension was conducted."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Byte Pair Encoding for Symbolic Music",
            "evaluation metrics": [
                "Section 4 describes our experimental settings and\nSection 5 describes the evaluation metrics that we use for\nthe experimental evaluation."
            ],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "5. Evaluation metrics\nGenerative models are often evaluated with automatic met-\nrics on the generated results."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": [
                "Kermarec, M., Bigo, L., and Keller, M. Improving tokeniza-\ntion expressiveness with pitch intervals."
            ]
        },
        {
            "title": "A Symbolic-domain Music Generation Method Based on Leak-GAN",
            "evaluation metrics": [
                "Furthermore, we take the music theory into account to test the generated data on two  \nmusic theory evaluation metrics, the Smooth-saltatory \nprogression (SSP) comparison and Note-level mode test.",
                "And then, several objective \nevaluation metrics are designed to judge the quality of \ngenerated music."
            ],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "To prove good quality of the \ngenerated pieces, we conduct comparative test on LSTM model \nand evaluate the randomly-sampled music on five metrics in the \nlight of statistics and music theory."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "AI Music Therapist: A Study on Generating Specific Therapeutic Music based on Deep Generative Adversarial Network Approach",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "An intelligent music generation based on Variational Autoencoder",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [
                "In the objective evaluation, we chose the music samples \ngenerated by magenta"
            ],
            "evaluate": [
                "Different from the traditional \nalgorithmic composition, it is not necessary to manually add \ncomplex rules, but trains the initial music set, evaluates and \nfilters the music collection, and ultimately generates music \nvia the RVAE-GAN neural network.",
                "Therefore, we \nuse \u0743\u0b35\u123a\u0754\u123b to evaluate the overall outline of a piece of \nmusic."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "APE-GAN: A Novel Active Learning Based Music Generation Model With Pre-Embedding",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "To evaluate APE-GAN more objectively, we\nmeasure APE-GAN\u2019s descriptive ability by comparing the\nmusic sequences generated by different textual inputs with\nthe metric of KL divergence."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Automatic Music Generation System based on RNN Architecture",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "We described the findings of benchmark datasets in section \n4, and in the following subsections, we evaluated by \ncontrasting the existing methods with the suggested method."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Development of Application Software for Generating Music Composition Inspired by Nature Using Deep Learning",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "evaluation metrics": [],
            "subjective evaluation": [
                "While each method\nwas presented with some subjective evaluation (often by the\nauthors), there has yet been little effort to quantitatively and\nobjectively evaluate generated samples."
            ],
            "objective evaluation": [
                "We \ufb01rmly believe that objective evaluation of music gen-\neration methods is key, in order to fairly monitor progress in\nthis domain."
            ],
            "evaluate": [
                "Any engineered measures to\nevaluate generated music typically attempt to de\ufb01ne the samples\u2019\nmusicality, but do not capture qualities of music such as theme or\nmood.",
                "Finally, we use a classi\ufb01er\ntrained on real data to evaluate the label validity of class-\nconditionally generated samples.",
                "While each method\nwas presented with some subjective evaluation (often by the\nauthors), there has yet been little effort to quantitatively and\nobjectively evaluate generated samples.",
                "It is com-\nmon to employ multiple human annotators to evaluate each\nsample [11] in order to elicit con\ufb01dent insights, something\nthat hinders scalability.",
                "Recent work [12] proposes simple,\nmusically informed metrics to evaluate the musicality of gen-\nerated music.",
                "In the emotional speech synthesis domain (another sub-\njective audio domain), the generative adversarial network\nbased studies performed in [13]\u2013[15] quantitatively evaluated\nsynthesised speech using a classi\ufb01er to demonstrate that the\ngenerated speech contained meaningful emotional information.",
                "We functionally evaluate the perfor-\nmance of three generative models, which subscribe to different\ngeneration paradigms.\nSampleRNN.",
                "We\nevaluate performance on the test partition of each dataset with\nrespect to micro and macro averaged metrics, calculated in the\nsame manner as the MediaEval 2019 competition submission."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Generating Music Algorithm with Deep Convolutional Generative Adversarial Networks",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [
                "The first set of evaluation systems is based on \nstatistically objective evaluations [16]."
            ],
            "evaluate": [
                "So, we use two sets of programs to evaluate our experimental results.",
                "We invited some \nexperts in the music field to evaluate and analyze some high-quality music and got some indicators from the midi data level."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Generating Music with Emotions",
            "evaluation metrics": [],
            "subjective evaluation": [
                "First, we evaluate the accuracy of the music emotion classifier\nin Section V-A. Then, the experimental setup and objective\nevaluation of the lyric-melody generator are demonstrated in\nSection V-B. Finally, the subjective evaluation of the generated\nmusic is shown in Section V-C. The code of this work can\nbe downloaded at https://github.com/BaoChunhui/Generate-\nEmotional-Music.",
                "Music composition is a human creative\nprocess, so we adapt the subjective evaluation method to\nevaluate generated lyrics and melodies by our ELMG system.",
                "We invited 20 participants for our subjective evaluation,\nwhere 10 are male and 10 are female."
            ],
            "objective evaluation": [
                "C. Subjective Evaluation\nAlthough objective evaluation indicate that the model is\nable to generate harmonious lyric and melody to capture\nthe required emotion, it is still difficult to conclude that the\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ],
            "evaluate": [
                "\u2022We evaluate the proposed ELMG system with both ob-\njective and subjective methods.\nII.",
                "In order to answer this question, we\ntrain deep learning models on the EMOPIA dataset [15] and\nevaluate if they can be used on our dataset.",
                "Therefore, there\u2019s b3segments of each length, and for\nevery segment, b1\u2217b2candidate lyric-melody pairs should\nbe evaluated.",
                "First, we evaluate the accuracy of the music emotion classifier\nin Section V-A. Then, the experimental setup and objective\nevaluation of the lyric-melody generator are demonstrated in\nSection V-B. Finally, the subjective evaluation of the generated\nmusic is shown in Section V-C. The code of this work can\nbe downloaded at https://github.com/BaoChunhui/Generate-\nEmotional-Music.",
                "We evaluate the classifiers using a 8-fold cross validation\napproach, in which the testing fold and the training folds\nhave no overlapping data.",
                "9/10 of them are\nused in the training process and 1/10 are used to evaluate the\ntrained sequence to sequence model.",
                "After training, the GRU and Transformer based networks\nare evaluated by using the test data.",
                "Then we use the fine-tuned Bert modelintroduced in Section III-C to objectively evaluate them.",
                "Music composition is a human creative\nprocess, so we adapt the subjective evaluation method to\nevaluate generated lyrics and melodies by our ELMG system.",
                "We invited volunteers to evaluate the music data selected from\nthe ground-truth dataset, music segments generated by GRU\nbased model and Transformer based model.",
                "Then, each participant needs to evaluate 18 music\npieces."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Generating Music with Generative Adversarial Networks and Long Short-Term Memory",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Generation of Music With Dynamics Using Deep Convolutional Generative Adversarial Network",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "The generated music was evaluated based on its \nincorporation of music dynamics and a user study.",
                "Hence, quantitative \nand qualitative measures were used to evaluate the \nexperimental results.",
                "Music Dynamics Values \nB. User Study \nThe user study was designed to determine if the \nparticipants could identify the expressive elements of music \ndynamics and evaluate how musical the generated music are \n139\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": [
                "Good flow and dynamics  \nSome no tes end abruptly  More dynamics and m usicality  \n \nThe excerpts rating on expressiveness are found in Figure \n7."
            ]
        },
        {
            "title": "Monophonic Music Generation With a Given Emotion Using Conditional Variational Autoencoder",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "The generated examples were evaluated with two methods, in the \u001crst using metrics for\ncomparison with the training set and in the second using expert annotation.",
                "The authors evaluated the affective expression\nperceived in the music generated by the proposed system,\nbased on human annotation.",
                "The loss was evaluated on\na validation set (20% of the training data).",
                "B. EVALUATION OF RESULTS USING METRICS\nTo evaluate the generated music sequences, they were tested\nusing the following metrics",
                "The task of each music expert was to listen and determine the\nemotions for all the examples generated by a given model,\ni.e. making 80 annotations for the evaluated model.",
                "After conducting both evaluations (using metrics and\nexpert opinions), we can see that using additional objective\nmetrics to evaluate the model (Section VI-B) is helpful in\nthis case."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Music Deep Learning: A Survey on Deep Learning Methods for Music Processing",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Music Generation using Deep Generative Modelling",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Music Generation using Deep Learning with Spectrogram Analysis",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Music Generation with AI technology: Is It Possible?",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "However, current music generation models lack \nmainstream algorithms and specifications, including algorithm \ndesign criteria and model evaluation criteria, and exactly how to \nevaluate that music generated by a deep learning model is a good \npiece of music.",
                "However, in this paper the authors give some metrics tha t can be \nused to quantitatively evaluate the goodness of computer \ngenerated music.",
                "This is because music is very subjective fundamentally, \nand cannot be simply evaluated by mathematical indicators."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Music Generation with Bi-Directional Long Short Term Memory Neural Networks",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Hewahi et al. conducted experiments to evaluate various \nparameters of their model but their work had one drawback.",
                "To evaluate the value of each input feature, the output and \ncorresponding sequential input function together  across \nmultiple timesteps."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "RE-RLTuner: A topic-based music generation method",
            "evaluation metrics": [],
            "subjective evaluation": [
                "Even though subjective evaluations onpreferences show that the proposed algorithm generatesmusic pieces of higher quality than the RLTuner method,the main disadvantage of data-driven reward models is thatit only focuses on the super\ufb01cial contextual relationshipsof the notes, but can not make the most of the structuralcharacteristics of the music [1]."
            ],
            "objective evaluation": [],
            "evaluate": [
                "In addition, they were also asked to evaluate how closeeach of the three types of music samples to the music fromthe dataset."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Some Reflections on the Potential and Limitations of Deep Learning for Automated Music Generation",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation",
            "evaluation metrics": [
                "To\ndate, most of the evaluation metrics for neural music models\nwere done in terms of immediate prediction error, incapable\nof capturing longer terms salience structures."
            ],
            "subjective evaluation": [
                "The subjective evaluation results of the ratings on three models.",
                "We conducted a\nsubjective evaluation in our experiments, which let subjects to\njudge the generated samples by interactivity, complexity, and\nstructure."
            ],
            "objective evaluation": [],
            "evaluate": [
                "In order to evaluate the performance of the neural model,\nwe conducted a subjective survey to evaluate the quality\nof generated music."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "PopMNet: Generating structured pop music melodies using neural networks",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Another  kind of \ufb01tness  function  adopts  explicit  rules to evaluate  generated  compositions.",
                "We did not exhaustively  search  for the optimal  hyper-\nparameters,  since it is di\ufb03cult  to quantitatively  evaluate  a music  generation  model.",
                "[44\u201346]e m p l o y e d  the likelihood  to evaluate  generative  models  for sequence  generation.",
                "We did not use it as a metric  to evaluate  models  because  we found  that a higher  likelihood  on the training  set or the \nevaluation  set does not imply  better  performance  by a preliminary  behavior  experiment  by the \ufb01rst author.",
                "[6,5,3], we conducted  two human  behavior  experiments  to evaluate  the melodies  generated  by \ndifferent  models.",
                "In this experiment,  we evaluated  \ufb01ve models,  PopMNet,  PopMNet-real,  AttentionRNN,  LookbackRNN,  and \nMidiNet.",
                "Music  Transformer  was evaluated  in a separate  experiment  (see Experiment 2) because  it was added  to the paper  10 J. Wu et al. /",
                "In this experiment,  we evaluated  PopMNet  and Music  Transformer.",
                "Besides  melodies  generated  by PopMNet  and Music  Transformer,  10 melodies  sampled  from the dataset  were also evaluated.",
                "We calculated  the correlation  between  structure  metrics  and human  evaluation  scores  of all 50 melodies  evaluated  in \nSection 5.4.5(see Fig.10)."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Singability-enhanced lyric generator with music style transfer",
            "evaluation metrics": [
                "In the end, we conducted experiments with two\nevaluation metrics, including automatic and human evaluation."
            ],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Table 4-1 shows the five metrics used\nto evaluate the final lyrics, including thematic (T), structural (S),\noriginality (O), meaningfulness (M) and Fitness (F) scores; and Figs. 4-1\nand 4-2 show the interface for the web based survey for lyrics transfer\ngeneration.",
                "A total of 100 songs in the experimental section, 50\nPop to Rock and 50 Rock to Pop, will be evaluated for each of\nthe three different approaches."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Human, I wrote a song for you: An experiment testing the influence of machines\u2019 attributes on the AI-composed music evaluation",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "They found that people have different \nexpectations and perspectives on AI\u2019s creativeness, influencing how they \nevaluate AI music.",
                "Because AI performances are within people \u2019s expectations toward AI, \nthe machine heuristic research asked people to evaluate them based on \ncredibility coming from expertise level (Sundar, 2008)."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Rethinking musicality in dementia as embodied and relational",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Research on the impact of music programs is dominated by studies\nthat evaluate music as a therapeutic tool to achieve instrumental out-\ncomes ( DeNora & Ansdell, 2014)."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": [
                "In this sense, musicality is tantamount to theexistential expressiveness of the body that emerges from our active and\nresponsive propensity towards the world."
            ]
        },
        {
            "title": "deepsing: Generating sentiment-aware visual stories using cross-modal music translation",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "The\nproposed method was evaluated both quantitatively and qualitatively using a collection of songs that belong\nto 10 different genres, demonstrating that it is indeed possible to generate visual content that can match the\nsentiment expressed in songs.",
                "To the best of\nour knowledge, this is the first work that proposed and evaluated an\nefficient cross-modal translation approach for sentiment-aware music\nvisualization using GANs.",
                "Then, we provide three examples of visual stories generated using\nthe proposed method to further qualitatively evaluate the proposed\nmethod.",
                "On the other hand metal, disco and\nrock consistently led to the lowest precision compared to the rest of the\nevaluated methods."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "ComposeInStyle: Music composition with and without Style Transfer",
            "evaluation metrics": [
                "Human evaluation along with instrument activity detection has been\nused as the evaluation metrics for this step."
            ],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "This paper proposes a hybrid style transfer model which is an end-\nto-end approach to transfer style, compose as well as evaluate the style transfer accuracy in the domain of\nthe target music composer.",
                "The compositions generated from each architecture\nis finally evaluated by the common pre-trained classifier of the first step.",
                "It\nintroduces a cycle consistency loss in addition to the adversarial loss\nin order to evaluate the GAN performance in training with unpaired\ndata.",
                "Wei\u00df, Brand, and M\u00fcller (2019)\nexploits the Baum\u2013Welch algorithm to generate genre based western\nmusic which is in turn evaluated using Gaussian Mixture Model (GMM)\nclassifiers.",
                "The authors\nconvert MIDI music from one genre to another (say jazz to pop) which\nis evaluated using neural style classifiers.",
                "CNN in Siamese architecture have been used as the\nmodel which has been evaluated using KNN classifiers.",
                "Finally, the performance of\nthe individual models have been evaluated on the basis of the accuracy\nthat is achieved while classifying the generated compositions by the\nbasic and ensemble pre-trained classifiers of step 1 of the proposed\nhybrid algorithm.",
                "These\nmodels are now used to evaluate the performance of the generated\ncompositions by the corresponding prediction accuracies.",
                "The generated\ncompositions are then evaluated using the pre-trained classifiers of Step\n1 after extracting the features from the wav files of the generated style\ntransferred compositions.",
                "In a nutshell, in both the cases, using basic ML models as well\nas ensemble techniques, style transferred compositions by the pro-\nposed hybrid model is seen to perform better in terms of classification\naccuracy than the comparison model when evaluated by the classifiers.",
                "Subjective evaluation of music creativity\nThe creativity of the proposed hybrid model for composer style\ntransfer of music is evaluated using the Consensual Assessment Tech-\nnique (CAT) as done in De Prisco et al.",
                "In this work, CAT\nevaluates the quality of music in terms of creativity, technical quality,\nand expressiveness using domain experts.",
                "Each expert evaluates ten (5 target clips from each composer) style\ntransferred music clips.",
                "E2: How do you evaluate the listenability of the composition?\nTable 12\nThe mean ratings of experts for each question to measure the creativity of the proposed hybrid model that transfers the style from source to\ntarget composer.",
                "The final feature set is then evaluated as an unseen\ntest set for the classification models, pre-trained in objective 1 using\nmean as the frame flattening technique.",
                "Starting with the training of the classifiers, to\ngenerating compositions from noise to finally combining them together\nwith the addition of the style transfer model is an end to end approach\nto create as well as evaluate the success of style transfer."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": [
                "In this work, CAT\nevaluates the quality of music in terms of creativity, technical quality,\nand expressiveness using domain experts.",
                "E1 and E2: Questions correspond to expressiveness criteria."
            ]
        },
        {
            "title": "The algorithmic composition for music copyright protection under deep learning and blockchain",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [
                "II.From\nsubjectiveandobjectiveperspectives,thesubjectiveevaluation\nsystem based on expert scoring and the objective evaluation\nsystembasedonminimumdistanceareconstructed,respectively."
            ],
            "evaluate": [
                "Therewardnetworkevaluatestheprosandconsoftheaction\nstrategybycalculatingthecumulativerewardvalueoftheaction\nsequence.",
                "Composition evaluation\nTotestthegenerationeffectofalgorithmiccomposition,the\nqualityofmusicisevaluatedfromsubjectiveandobjectiveper-\nspectives.",
                "Throughtheformationofanexpertjury,theproduction\nqualityofmusicisevaluatedfromfiveaspectsofmelody,rhythm,\nharmony,musicsense,andexpressiondynamics."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": [
                "Themelodyof\nmusicisadoptedtomeasurethefluencyandnaturalnessofthe\nmusic,harmonycanmeasurethecomfortofthemusic,thesense\nof the music can express the integrity of the music, and the\nintensitycanhighlighttheexpressivenessofthemusic."
            ]
        },
        {
            "title": "Towards a Deep Improviser: a prototype deep learning post-tonal free music generator",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Future\nwork will aim to enhance the model such that it deserves to be fully evaluated in relation to expression, meaning and utility\nin real-time performance.",
                "This would also invite a\nhierarchical conditional model in which the \ufb01rst prediction\nis whether an event is a note or a chord, and the subsequentpredictions then evaluate the chosen case (note or chord\nexpressed as a rapid sequence of notes)."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "From artificial neural networks to deep learning for music generation: history, concepts and trends",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Most current works using deep learning to generate music\nare autonomous and the way to evaluate them is a musical\nTuring test, i.e., presenting to various human evaluators\n(beginners or experts) original music (of a given style of aknown compositor, e.g., Bach\n6) mixed with music gener-\nated after having learnt that style."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Conditional hybrid GAN for melody generation from lyrics",
            "evaluation metrics": [
                "Through extensive experiments using evaluation metrics, e.g., maximum mean\ndiscrepancy, average rest value, and MIDI number transition, we demonstrate that the proposed C-Hybrid-GAN outper-forms the existing methods in melody generation from lyrics."
            ],
            "subjective evaluation": [
                "4.6 Subjective evaluation of generated sequence\nTo show if the generated sequence \ufb01ts the lyrics-condi-\ntioning input, we also do subjective evaluation as shown in\nFig.",
                "In the subjective evalu-\nation, we use three different lyrics to generate 18 melodies,\nwhich are available at the link.1The different methods in\nthis subjective evaluation can generate music attributes(pitch, duration, and rest) when given the same lyrics."
            ],
            "objective evaluation": [],
            "evaluate": [
                "Large-scale Chinese language lyrics-melody\ndataset was built to evaluate the proposed learning model.",
                "Therefore, to evaluate our proposed architecture, we\nuse Self-BLEU in [ 30] to measure the diversity of gener-\nated samples and maximum mean discrepancy (MMD) in\n[31] to measure the quality of generated samples."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Scene2Wav: a deep convolutional sequence-to-conditional SampleRNN for emotional scene musicalization",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "The previ-\nously mentioned ConvSeq2Seq model is then chosen as our baseline model to evaluate the\neffect our proposed conditional decoder SampleRNN has over a regular decoder RNN.",
                "i=W(1)\nxf(1)\ni+c(2)\ni\np(xi+1|x1,...,xi,v)=So ftmax(MLP(inp(1)\ni))(16)\nwhere W(1)\nxis used to obtain the linear combination between f(1)\niandc(2)\ni.\n4.5 Baselinemodel:convolutionalsequence-to-sequence\nDue to its proven performance and effectiveness in multimodal tasks, a Convolutional\nSeq2Seq model [ 35] is chosen as our baseline model in order to evaluate the effect our pro-\nposed conditional decoder SampleRNN has over a regular decoder RNN.",
                "5.3 Humanevaluation:qualitativemetric\nThe most widely accepted measure of performance for researchers in this field to evaluate\nthe quality of generated audio is through human evaluation feedback.",
                "Note, however,\nthat PAM is still a distance metric nonetheless, meaning that it\u2019s extremely dependent on\nthe relationship between evaluated audio and target audio.",
                "We then analyze the detected chords in the generated\nmusics, both from our proposed model Scene2Wav and the baseline model ConvSeq2Seq.1807 MultimediaToolsandApplications(2021)80: 1793\u20131812Table3 Perceptual audio metric (PAM) used to evaluate the quality of generated music in Figs.",
                "The model is evaluated by human subjects\nin an online marketplace on their music preferences and an emotion evaluation is done by\ndetecting chords and analyzing them with an Emotional Circle of Fifths."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Attentional networks for music generation",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "The system is then evaluated by cross-entropy and MSE betweeninput and predicted sequences.",
                "All\nthe parameter tuning, training was performed on the training/validation set while the finalaccuracy has been evaluated on the test set."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Monophonic music composition using genetic algorithm and Bresenham\u2019s line algorithm",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "In each iteration, the fitness of individuals is evaluated and more fit individuals are selectedfor next generation.",
                "In the interactive method, fitness of the music sample is evaluated by audience them-\nselves.",
                "Thepopulation is evaluated based on fitness functions."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Polyphonic music generation generative adversarial network with Markov decision process",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [
                "3.5 Subjective evaluation\nOn the basis of the above objective evaluations"
            ],
            "evaluate": [
                "[ 31] have studied the problem of extracting\nsamples from music statistical models and have evaluated an improved iterated random walk\ntechnique using a four-part, multi-view system combined with a set of small general coordi-\nnation rules.",
                "However, in polyphonic musicgeneration, as the length of the music sequence generated by WGAN increases, sequencecoherence is broken, and the discriminator in the WGAN model finds it difficult to evaluatethe incomplete sequence.",
                "That is, the discriminator is unable to evaluate the generation statusof the music sequence in real time and provide feedback for the generator.",
                "However, thereare two problems in the existing GAN model; (1) the generator finds it difficult to transfer\ngradient update, and (2) the discriminator finds it difficult to evaluate the incomplete sequence.",
                "However, it is still difficult for the discriminator to evaluate no holonomic sequences.",
                "In this\npaper, MCTS is introduced to take advantage of its efficient search strategy for exploringpolyphonic music sequences and completing sequences from the generator, thus solving theproblem of the discriminator being unable to evaluate incomplete sequences.",
                "GAN model is unable to evaluate incomplete music sequences.",
                "(4) This paper is the first to integrateMCTS into WGAN for polyphonic music generation, removing the issues in which it isdifficult for the discriminator, in the existing GAN model, to evaluate incomplete sequences.",
                "Moreover,GAN can only evaluate the whole sequence, rather than the local sequence, which leads to thegenerator producing a discrete output and makes it difficult for the discriminator to return agradient to update the generator.",
                "Adding the MCTS algorithm resolves the problem in which the discriminator cannot evaluateincomplete music sequences.",
                ", this study also conducts a survey to evaluatethe quality of the model for music production."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "A combination of multi-objective genetic algorithm and deep learning for music harmony generation",
            "evaluation metrics": [],
            "subjective evaluation": [
                "Furthermore, in a subjective evaluation, it was observed\nthat more desirable samples of music could be achieved, which were able to obtain high scores\nfrom the audiences."
            ],
            "objective evaluation": [
                "One of the\nmain challenges in Automatic Music Generation is that there is no clear objective evaluation\ncriterion that can measure the music grammar, structural rules, and audience satisfaction."
            ],
            "evaluate": [
                "They used neural networks to evaluate the ability of semanticvector model patterns (word2vec) to record music text and semantic similarity [ 15]."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "A Style-Specific Music Composition Neural Network",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [
                "Objective evaluation\nThis paper introduces an objective evaluation method based on the minimum distance by\nmeasuring the distance between the generated sample and the training sample to evaluate."
            ],
            "evaluate": [
                "Therefore, the neural network architecture with the best performance in the trainingdata has been the MCNN model, evaluated in the iteration from 0 to 3500 time steps.",
                "5.2 Evaluation\nIt is a complicated task to evaluate musical samples because there is no standard procedurefor evaluating music\u2019s quality.",
                "In order to test the music generation effect of MCNN, thispaper evaluates the model from the subjective and objective perspectives.",
                "Objective evaluation\nThis paper introduces an objective evaluation method based on the minimum distance by\nmeasuring the distance between the generated sample and the training sample to evaluate.",
                "Through the establishment\nof an expert review panel, the performance of different models in generating classical musicwas evaluated from the following \ufb01ve aspects:\n1231908 C. Jin et al.\nFig. 8 ROC curve of style classi\ufb01er for 4 models\n\u2013Melody The melody of classical music sounds more balanced and symmetrical.",
                "In this subjective experiment, we invited 15 music professionals composed of 7 teachers\nfrom the composition department of the music conservatory, 4 performers from the symphonyorchestra and 4 music enthusiasts, including 8 males and 7 females aged between 30 and 40.They listened to the music generated by different models and gave a score on the above aspectsto evaluate the performance of those models."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Self-Supervised Music Motion Synchronization Learning for Music-Driven Conducting Motion Generation",
            "evaluation metrics": [
                "3) We conduct extensive experiments on Conductor-\nMotion100, using both standard evaluation metrics and\nseveral newly designed metrics."
            ],
            "subjective evaluation": [],
            "objective evaluation": [
                "In re-\ncent years, the objective evaluation of deep generative\nmodels has usually been based on the inception score\n(IS) or the frechet inception distance (FID)."
            ],
            "evaluate": [
                "Therefore, to evaluate the quality of conduct-\ning motions more e\u000bectively, we propose several new\nmetrics in addition to the existing metrics, as detailed\nin the below."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Integration of a music generator and a song lyrics generator to create Spanish popular songs",
            "evaluation metrics": [],
            "subjective evaluation": [
                "In a second stage, we produced several songs to perform a listening test and retrieve a subjective evaluation of the popular music and the lyrics generated.",
                "Following other examples of subjective evaluation by a group of human listeners\u00a0[e.g. Delgado et\u00a0al. (2009); Pearce and Wiggins (2007);"
            ],
            "objective evaluation": [],
            "evaluate": [
                "Several experiments were carried out to evaluate the quality of the results, based on human opinions towards generated pieces of music and lyrics.",
                "Once the melodies are generated, a listening test was \ndeveloped to evaluate the musical quality according to the Spanish popular music standards and to collect the users\u2019 opinion about the usefulness of the system to interact with them and generate music.",
                "In fact, for an evaluation of the final results, most works use objective measures and some optimization method without human evaluation at all, while a few use a human expert to evaluate the results.",
                "(2009) conducted a preliminary lis-tening test to evaluate their application.",
                "Their back -\nground makes the results more reliable, as they are asked to evaluate more \u201cobjective\u201d features (considering that music and art in general, are very subjective fields).",
                "They should evaluate the quality in a 5-point Likert scale, where 1 means \u201cVery poorly\u201d and 5 means \u201cPerfectly\u201d.",
                "More than the 54% of the responses evaluated the melody \nas \u201cGood\u201d or even better.",
                "The 60% of the Fig. 5  Screenshot of the valida-\ntion form to analyze melody \nand lyrics\n4432 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\nusers considered that the music adapts to the popular music \nstandards well or very well, as does the melodic rhythm, where the 54% of the users evaluated this item as good or very good."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "The\nproposedsystemissystematicallyevaluatedusingtheIRMASdatasetwithelevenclasses.",
                "4 Inthedevelopmentphase,theperformanceis\nevaluatedusingvariousschemeslike\nMel-spectrogram,modgdgram,andtempogram\nfollowedbyensemblevoting.",
                "5 Performanceevaluation\n5.1 Dataset\nThe performance of the proposed system is evaluated\nusing the IRMAS (Instrument Recognition in Musical\nAudio Signals) dataset, developed by the Music Technol-\nogy Group (MTG) of Universitat Pompeu Fabra (UPF).",
                "The quality of generated files is evaluated using a per-\nceptualtest.",
                "We also conducted the ablation study of the\nensemble to evaluate the contribution of the individ-\nual parts in the proposed ensemble classification frame-\nworkforpredominantinstrumentrecognition.",
                "The proposed method is evaluated using the IRMAS\ndataset."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "Genre Recognition from Symbolic Music with CNNs: Performance and Explainability",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "Evaluation and\u00a0Post Processing\nWe evaluate our models on the held-out test set for each of \nthe MASD and topMAGD datasets by computing precision, \nrecall, and micro f1 metric.",
                "Genetic Programming (GP), in general, generates a random \npopulation and evaluates the fitness of each individual, in \nterms of effectiveness in solving the problem, favoring the \nbetter individuals."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": []
        },
        {
            "title": "CycleDRUMS: automatic drum arrangement for bass lines using CycleGAN",
            "evaluation metrics": [],
            "subjective evaluation": [],
            "objective evaluation": [],
            "evaluate": [
                "To sum up, our main contributions are the following:\n\u2022 we trained a CycleGAN architecture on bass and drum mel-spectrograms in order to automatically generate drums \nthat follow the beat and sound credible for any given bass line;\n\u2022 our approach can generate drum arrangements with low computational resources and limited inference time, if \ncompared to other popular solutions for automatic music generation [12];\n\u2022 we developed a metric\u2014partially based on or correlated to human (and expert) judgment\u2014to automatically evaluate \nthe obtained results and the creativity of the proposed system, given the challenges of a quantitative assessment of \nmusic;\n\u2022 we compared our method to Pix2Pix, another popular image transfer network, showing that the music arrangement \nproblem can be better tackled with an unpaired approach and adding a cycle-consistency loss.",
                "[48], and (ii) the definition of an objective \nmetric and loss is a common problem to generative models such as GANs: as of now, generative models in the music \ndomain are evaluated based on the subjective response of a pool of listeners, because an objective metric for the raw \naudio representation has never been proposed so far.",
                "As a final step, the mel-spectrograms obtained were converted to the waveform \ndomain to evaluate the music produced.",
                "This \nis unlike Inception score which only evaluates the distribution of generated images.",
                "Given the novelty of the problem, we proposed a \nreasonable procedure to evaluate our model outputs properly."
            ],
            "model accuracy": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "harmonic complexity": [],
            "expressiveness": [
                "Indeed, even if it is possible to use synthesizers \nto produce sounds from symbolic music, MIDI, music sheets, and piano rolls are not always easy to find or produce, and \nthey sometimes lack expressiveness."
            ]
        }
    ]
}