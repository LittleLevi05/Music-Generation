{
    "data_collection": [
        {
            "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
            "architecture": [
                "The model architecture\nis a deep 2D convolutional network, where each\nsubsequent generator model block increases the\nresolution along the time axis and adds a higher\noctave along the frequency axis.",
                "An additional\nbene\ufb01t of the CNN-based model architecture is\nthat generation of new songs is almost instanta-\nneous.",
                "The architecture of our network is in-\nspired by the ProGAN model (Karras et al., 2018) with the\nnoticeable difference that we don\u2019t increase/decrease the\npixel density along the frequency axis with each successive\nmodel-block in the generator/discriminator.",
                "More-\nover, the model CNN-based architecture allows for almost\ninstantaneous generation of new samples.2.",
                "Network architecture\n3.1.",
                "2D convolutions to up- and downscale the MDCT\namplitude representation\nOur network architecture is based on the architecture of\nthe ProGAN network (Karras et al., 2018) with successive\nmodel blocks which scale up/down the image using strided\n2D convolutions in the generator and discriminator respec-\ntively.",
                "Overall model architecture\nThe model architecture of MP3net is shown in \ufb01gure 4.",
                "Model architecture of MP3net with 4 model blocks.",
                "The GANsynth architecture is based on Pro-\nGAN.",
                "Future work\nSince the model architecture of MP3net is very similar to\nthe convolutional networks well studies in the \ufb01eld of image\ngeneration, we can borrow many of the techniques of image\ngeneration.",
                "While the MP3net architecture is similar in many\nrespects to ProGAN, the characteristics of the data represen-\ntation is rather different.",
                "This modi\ufb01ed architecture\nwould allow us to control the generated audio at different\nscales.",
                "7. Conclusion\nIn this paper, we introduce MP3net, a 2D convolutional\nGAN with an architecture similar to some of the most suc-\ncessful image generation GANs (Karras et al., 2018; Zhang\net al., 2019; Karras et al., 2019).",
                "Compared to other gen-\nerational models generating multi-minute samples, training\ntimes of MP3net are much shorter and inference is quasi-\ninstantaneous given the inherent CNN-model architecture.",
                "Karras, T., Laine, S., and Aila, T. A style-based generator\narchitecture for generative adversarial networks, 2019."
            ],
            "dataset": [
                "Experiments with the MAESTRO dataset\n4.1.",
                "Dataset description\nOur experiments are based on the MAESTRO-V2.0.0\ndataset (Hawthorne et al., 2019).",
                "This dataset contains over\n200h of classical piano music, recorded over nine years of\nthe International Piano-e-Competition.",
                "This latter isMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nto be expected since \u00184%of the training dataset contains\natonal pieces (mostly from Alexander Scriabin).",
                "S AMPLE DIVERSITY\nThe MAESTRO dataset consists of music from the Baroque\nera over the Classical, Romantic and Impressionist styles,\nto the Expressionist period.",
                "In particular, training on the\nBlizzard (King & Karaiskos, 2011) and V oxCeleb2 datasets\n(Chung et al., 2018) would test the model\u2019s ability to synthe-\nsis speech.",
                "Hawthorne, C., Stasyuk, A., Roberts, A., Simon, I., Huang,\nC.-Z. A., Dieleman, S., Elsen, E., Engel, J., and Eck, D.\nEnabling factorized piano music modeling and generation\nwith the maestro dataset, 2019.Jenni, S. and Favaro, P."
            ]
        },
        {
            "title": "Hierarchical Recurrent Neural Networks for Conditional Melody Generation with Long-term Structure",
            "architecture": [
                "With this new data representation,\nthe proposed architecture is able to simultaneously model the\nrhythmic, as well as the pitch structures effectively.",
                "In Section III and\nIV, we describe the proposed event representation and CM-\nHRNN architecture in detail.",
                "The core design of the architecture is\nto \ufb01rst generate the rhythmic patterns and then condition the\npitch generation with both coarse and \ufb01ne rhythmic patterns.",
                "In this paper, we propose an architecture similar to HRNN\nbut with different design motivations.",
                "B. Hierarchical architectures for long term dependencies\nDrawing inspiration from other \ufb01elds in which long-term\ndependencies of sequential data have been modelled by RNNs,\nwe \ufb01nd that multiple hierarchical architectures have been\nproposed for this challenge [20], [21], [26]\u2013[28].",
                "These architectures can be applied to the domain of audio\ngeneration.",
                "P ROPOSED MODEL : CM-HRNN\nA. Model Architecture\nOur proposed CM-HRNN architecture consists of multiple\nhierarchical tiers (see Fig. 3).",
                "In this paper, we\nfocus on two variants of the proposed architecture: a 2-tier\nCM-HRNN and a 3-tier CM-HRNN.",
                "The architecture of the\nproposed 3-tier CM-HRNN is shown in Fig.",
                "By removing\nthe top tier of the 3-tier CM-HRNN, we obtain the architecture\nof the 2-tier CM-HRNN.",
                "3: Our proposed CM-HRNN architecture.",
                "V. E XPERIMENTS\nA. Experimental setup\nWe set up several experiments to determine the optimal\nnetwork architecture; evaluate the model\u2019s ability to generate\nhigh-quality music with structure, and to compare it with a\nstate-of-the-art system, AttentionRNN.",
                "Even though other research may share some similarities in\nterms of model architecture [22], [23], they either work on a\ndifferent problem domain or with different data representation,\nthus making comparison hard."
            ],
            "dataset": [
                "Below, we \ufb01rst describe our dataset and pre-processing\nmethod, followed by a description of the evaluation metrics\nand the listening test setup.",
                "B. Dataset and pre-processing\nAll training data (XML format) was parsed from Theory-\ntab",
                "[32] \u201cTheorytab dataset,\u201d https://www.hooktheory.com/theorytab."
            ]
        },
        {
            "title": "LEARNING TO GENERATE MUSIC WITH SENTIMENT",
            "architecture": [],
            "dataset": [
                "We evaluate the accuracy of the model\nin classifying sentiment of symbolic music using a new\ndataset of video game soundtracks.",
                "1. INTRODUCTION\nMusic Generation is an important application domain of\nDeep Learning in which models learn musical features\nfrom a dataset in order to generate new, interesting music.",
                "\u201cLearning to Generate\nMusic With Sentiment\u201d, 20th International Society for Music Informa-\ntion Retrieval Conference, Delft, The Netherlands, 2019.learn an excellent representation of sentiment (positive-\nnegative) on text, despite being trained only to predict the\nnext character in the Amazon reviews dataset [6].",
                "When\ncombined to a Logistic Regression, this LSTM achieves\nstate-of-the-art sentiment analysis accuracy on the Stan-\nford Sentiment Treebank dataset and can match the per-\nformance of previous supervised systems using 30-100x\nfewer labeled examples.",
                "In order to evaluate this approach, we need a dataset of\nmusic in symbolic format that is annotated by sentiment.",
                "To the best of our knowledge, there\nare no datasets of symbolic music annotated according to\nsentiment.",
                "Therefore, we created a new dataset composed\nof 95 MIDI labelled piano pieces (966 phrases of 4 bars)\nfrom video game soundtracks.",
                "The same dataset\nalso contains another 728 non-labelled pieces, which were\nused for training the generative LSTM.",
                "Another contribution of this paper is a la-\nbelled dataset of symbolic music annotated according to\nsentiment.",
                "A second-order Markov\nmodel is used to learn melodies from a dataset and are\nthen transformed by a rule-based system to \ufb01t the anno-\ntated emotions in the graph.",
                "With this\nnew representation and dataset, Oore et al.",
                "This mLSTM was trained on the Amazon product re-\nview dataset, which contains over 82 million product re-\nviews from May 1996 to July 2014 amounting to over 38\nbillion training bytes",
                "[13] used the\ntrained mLSTM to encode sentences from four different\nSentiment Analysis datasets.",
                "With the\nencoded datasets, Radford et al.",
                "By inspecting the relative contributions of features on\nvarious datasets, Radford et al.",
                "4. SENTIMENT DATASET",
                "[13] method to com-\npose music with sentiment, we also need a dataset of MIDI\n\ufb01les to train the LSTM and another one to train the lo-\ngistic regression.",
                "There are many good datasets of music\nin MIDI format in the literature.",
                "Thus, we created a new dataset called VGMIDI which iscomposed of 823 pieces extracted from video game sound-\ntracks in MIDI format.",
                "The data collection process provides a time series of\nvalence-arousal values for each piece, however to create a\nmusic sentiment dataset we only need the valence dimen-\nsion, which encodes negative and positive sentiment."
            ]
        },
        {
            "title": "Personalized Popular Music Generation Using Imitation and Structure",
            "architecture": [
                "The Transformer architecture does\nnot o\u000ber explicit representations of abstract music qualities such as chords, bass lines, or\nrhythms, so there is no straightforward way to control or bias the model to produce certain\nscales, hierarchical structure, repetition or even to limit repetition."
            ],
            "dataset": [
                "We identify distinc-\ntive chord sequences in the seed song by comparing statistical features between the seed song\nand a general dataset consisting of 300 annotated pop songs."
            ]
        },
        {
            "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network",
            "architecture": [
                "A performance comparison and description of the various GAN architecture are also presented.",
                "Visual art mainly contains works involving painting, sculpture, and architecture [1].",
                "Generative Adversarial Networks (GANs) use deep learning architectures to facilitate generative modeling.",
                "Following are the key contributions of this paper: \u2022 It provides an overview of the primary GAN architectures for generative arts.",
                "In this Section, the necessary background information including relevant GAN architectures will be discussed.",
                "Figure 2 depicts a basic GAN architecture consisting of a single generator and discriminator.",
                "Meanwhile, the discriminator has access to the ground truths, whether the data came from the generator or real dataset, which it can use to minimize its error.  \n Figure 2 A Basic GAN Architecture More sophisticated GAN architectures can make use of labels to generate data points for a specific category.",
                "Next, various GAN architectures relevant to art generation is discussed.",
                "1) Conditional GAN: Extended from the regular GAN, it is a conditional architecture if the generator and discriminator are conditioned on auxiliary information such as class labels [15].",
                "This type of architecture is suitable for multimodal generation of data points.",
                "[16], the generator and discriminator in this architecture is made up of convolutional networks.",
                "Given the success of convolutional neural network in image and video classification in recent years, DCGAN remains a suitable architecture for image generation applications.   3) Recurrent Adversarial Netwokrs: In this architecture, a recurrent computation is obtained by unrolling the gradient descent optimization [18].",
                "The two main components of this architecture are the convolutional encoder which extracts images of the current \u201ccanvas\u201d and the decoder which decides whether or not to update the \u201ccanvas\u201d by looking at the code for the reference image.",
                "Recurrent architectures are suitable for sequential and time-dependent data and is generally used for text and audio related applications.",
                "Other common GAN architectures include InfoGAN",
                "For a comprehensive comparison of GAN architectures, the readers are encouraged to refer to [17] and [22].",
                "The proposed architecture is a supervised learning approach such that given a black-and-white sketch, the model can create a painted colorful image.",
                "Based on this, the proposed architecture outperformed the three existing ones.",
                "A similar conditional GAN architecture using a U-Net generator for generating shoe image from a given shoe sketch is presented in [25].",
                "However, unlike previous works, their architecture allows the user to specify a style which could be based on the artist\u2019s name or a style category.",
                "The generator utilizes a U-Net architecture whereas the discriminator produces predictions of a style vector, sketch, and \u2018real\u2019 or \u2018fake\u2019 indicator by using the input image sketch and the image produced by the generator.",
                "In terms of both these metrics, the proposed architecture outperformed the existing works.",
                "The brushstrokes generated by the GAN architecture were rougher and more realistic.",
                "Figure 5 Samples from Proposed Architecture in [28]",
                "The authors then explored with an existing GAN architecture known as styleGAN",
                "[32] introduced StrokeNET, a GAN-based architecture to generate digits and character strokes.",
                "Furthermore, the table also summarizes the GAN type used, the loss function as well as the generator and discriminator architectures.",
                "Recent Advances in Visual Arts Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Result",
                "The authors in [37] presented an adversarial and convolutional based architecture known as MidiNet for generating pop music monophonic melodies using 1022 pop music from an online MIDI database called TheoryTab [38].",
                "The architecture of both the generator and discriminator contains 2 LSTM layers of 350 hidden units.",
                "The proposed \u2018MuseGAN\u2019 architecture utilized three different GANs namely the jamming model, the composer model, and the hybrid model.",
                "Moreover, the table also summarizes the GAN type used, the loss function as well as the generator and discriminator architectures.",
                "Recent Advances in Music and Melody Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Result",
                "Most GAN architectures are restricted by several factors when it comes to text and sequential generation.",
                "The SeqGAN architecture is illustrated in Figure 8.",
                "Architecture",
                "Among several experiments using this architecture, the authors also employed the model for Chinese poem generation.",
                "The generative model is a CNN-RNN architecture, acting as an agent.",
                "The proposed image to poetry GAN (I2P-GAN) was evaluated on several metrics such as relevance, novelty, and BLEU scores against different architectures including SeqGAN.",
                "The architecture used is displayed in Figure 10.",
                "Figure 10 Proposed Prose Generation Architecture in [46]",
                "This architecture of the CNN-RNN model as an agent and two discriminators is similar to that of [45].",
                "The overall goal of the poem generation architecture is to generate a sequence of words as a poem for an image by maximizing the expected return.",
                "This architecture introduced a low-variance objective function using the discriminator\u2019s result following the corresponding log-likelihood.",
                "Both LSTM-based, as well as CNN-based GAN architectures, were experimented and the proposed model with LSTM outperformed the existing works on the aforementioned Poem-5 and Poem-7 datasets.",
                "The overall architecture of RankGAN is presented in Figure 12.",
                "Figure 12 Proposed RankGAN Architecture in [50] The RankGAN differs from traditional GAN by including a sequence of generators and a ranker.",
                "The proposed architecture was evaluated for poetry generation on a Chinese poem dataset containing over 13,000 five-word quatrain poems.",
                "[51] proposed a GAN architecture for creative text generation.",
                "The discriminator contains an encoder-decoder pair with the encoder having the same architecture as the generator, with an added pooled decoder layer.",
                "Recent Advances in Literary Text Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Dataset Result",
                "Therefore, training large scale GAN architectures will not be suitable.",
                "Therefore, future research should focus on implementing GAN architectures for generating music in raw audio format.",
                "In summary, for future work on art generation using GANs, we recommend the following: \u2022 Experiment with smaller dataset and GAN architectures for visual arts generation.",
                "[30] T. Karras, S. Laine, and T. Aila, \u201cA style-based generator architecture for generative adversarial networks,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp."
            ],
            "dataset": [
                "This is achieved upon successful training where the adversarial network can identify patterns in the data and learn the distribution of the dataset.",
                "Also, due to the advancement of technology and digitization, the two main requirements of GANs, datasets and computing power, have become widely available.",
                "The generator tries to generate fake samples, having similar distribution to the examples in the real dataset and continues to improve its network to trick the discriminator.",
                "The generator has no access to data points from the real dataset.",
                "Meanwhile, the discriminator has access to the ground truths, whether the data came from the generator or real dataset, which it can use to minimize its error.  \n Figure 2 A Basic GAN Architecture More sophisticated GAN architectures can make use of labels to generate data points for a specific category.",
                "The training dataset contains the pair of sketches as well as ground truth colored images.",
                "The authors utilized two datasets, namely the Minions dataset containing 1100 colored minions and the Japanimation dataset containing 6000 pictures of Japanese anime with 90% of the data being used for training and the remaining 10% for evaluation.",
                "The dataset used contains 10k images of 55 different styles such as impressionism, realism, and symbolism.",
                "The authors utilized the WikiArt dataset which consists of more than 80k paintings from 1119 artists ranging from the 15th to 20th century.",
                "[29] introduced a pre-modern Japanese art facial expression dataset.",
                "This dataset contains more than 5500 RGB images alongside labeled gender and social status.",
                "The authors did not evaluate the performance of the generations but rather introduced the \ndataset and encouraged researchers to work on improving the quality of the generations.",
                "The training data used contained labeled face and sketch pairs from two different datasets which were combined.",
                "Several datasets including MNIST was used to evaluate the network.",
                "The total number of samples after adding all the genres was 1998 (10 seconds MIDI files) from the Nottingham dataset [35].",
                "The dataset provides two channels for each tab, one for melody and the other one for the underlying chord progression.",
                "To generate additional training examples, augmentation was performed by circularly shifting all melodies and chords to any of the 12 keys, resulting in a final dataset of 50,496 melody and chord pairs.",
                "[40] and Reddit MIDI datasets were combined, which resulted in a total of 12,197 MIDI files.",
                "A similar study was conducted in [41] using the exact dataset and conditional-LSTM GANs.",
                "The dataset used consists of 3697 classical music from 160 different composers.",
                "To implement the GANs, the Lakh MIDI dataset was transformed into a multi-track piano-rolls representation.",
                "The dataset used comprises of over 16, 000 poems with each poem containing four lines of twenty characters.",
                "The approach of extending GANs to generate sequences of discrete tokens is suitable for poetry and other text generations because of the sequential nature of text datasets.",
                "As part of this work, the dataset collected for pairing image and poem by human annotators is made available online.",
                "Due to the dataset of painting to Shakespearean prose not being available, the authors utilized intermediate English poem description of the paintings before applying language style transfer to obtain the relevant prose style.",
                "A total of three datasets were used to solve this application.",
                "Two of the datasets were used for generating an English poem for a given image.",
                "For text style transfer, Shakespeare plays, and their corresponding English translation dataset was used.",
                "Another limitation of this approach is that the generations may suffer in quality when the style transfer dataset does not have similar words in the training set of sequences and consequently, the dataset must be expanded.",
                "For poetry generation, two Chinese poem datasets, named Poem-5 and Poem-7, were used with each containing 5 or 7 characters in short sentences, respectively.",
                "Both LSTM-based, as well as CNN-based GAN architectures, were experimented and the proposed model with LSTM outperformed the existing works on the aforementioned Poem-5 and Poem-7 datasets.",
                "The objective function of the generator is to generate a sentence that can potentially obtain a greater ranking score than the ones taken from the real dataset.",
                "On the other hand, the ranker\u2019s task is to rank the generated sentence to be of lower importance than the sentences from the real dataset.",
                "The proposed architecture was evaluated for poetry generation on a Chinese poem dataset containing over 13,000 five-word quatrain poems.",
                "The dataset used for poetry generation consists of 740 classical and contemporary English poems.",
                "Recent Advances in Literary Text Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Dataset Result",
                "[44] Chinese Poetry generation RL GAN Cross entropy and policy gradient RNN Generators and CNN discriminators 16394 Chinese quatrains BLEU-2 score of 0.74, overall score of 0.54 by human evaluators [45] Generate poetry from an image Multiadversarial GAN with an embedding model Cross entropy and policy gradient RNN Generators, GRU-based discriminator, CNN image encoder and RNN poem decoder Novel dataset with paired image and poetry Overall BLEU score of 0.77, 7.18 out of 10 overall score by human evaluators [46] Generate Shakespearean prose from a painting Multiadversarial GAN with encoder and decoder Cross entropy and policy gradient RNN Generator, CNN-RNN agent for encoding and decoding painting, LSTM encoder and decoder for generating prose Two datasets for generating English poem from an image, and Shakespeare plays and their English translations for text style transfer Average scores of 3.7, 3.9, and 3.9 out of 5 by evaluators for content, creativity, and similarity to Shakespearean style respectively [47] Chinese Poetry generation RL GAN Maximum-likelihood A single layer LSTM generator, two-layer Bi-directional LSTMs discriminator Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.76 and 0.55 for the two datasets respectively",
                "[48] Chinese Poetry generation RNN GAN Wasserstein distance LSTM generator and discriminator with WGAN-GP training Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.88 and 0.67 for the two datasets respectively",
                "Moreover, the size of the dataset required for GAN training remains a challenge.",
                "The lowest dataset size from the existing works reviewed in Table 1 utilized at least 5000 images [29].",
                "Therefore, for novel applications, a lot of time is required in gathering the dataset first before applying the GAN training.",
                "B. Recommendations and Future Work In the context of visual arts generation, an experiment with smaller datasets should be carried out.",
                "It could potentially lead to the development of a GAN framework that is well suited to dealing with such datasets.",
                "In summary, for future work on art generation using GANs, we recommend the following: \u2022 Experiment with smaller dataset and GAN architectures for visual arts generation.",
                "[29] Y. Tian, C. Suzuki, T. Clanuwat, M. Bober-Irizar, A. Lamb, and A. Kitamoto, \u201cKaoKore: A Pre-modern Japanese Art Facial Expression Dataset,\u201d arXiv preprint arXiv:2002.08595, 2020.",
                "[35] \u201cjukedeck/nottingham-dataset,\u201d GitHub.",
                "https://github.com/jukedeck/nottingham-dataset (accessed May 25, 2021).",
                "[40] \u201cThe Lakh MIDI Dataset v0.1.\u201d"
            ]
        },
        {
            "title": "CONTROLLABLE DEEP MELODY GENERATION VIA HIERARCHICAL MUSIC STRUCTURE REPRESENTATION",
            "architecture": [
                "2 Sep 2021that analyze a song to derive music frameworks that can\nbe used in music imitation and subsequent deep learning\nprocesses, (4) a set of neural networks that generate a song\nusing the MusicFrameworks approach, (5) useful musical\nfeatures and encodings to introduce musical inductive bi-\nases into deep learning, (6) comparison of different deep\nlearning architectures for relatively small amounts of train-\ning data and a sizable listening test evaluating the musical-\nity of our method against human-composed music.",
                "Architecture of MusicFrameworks .",
                "3.2.2 Network Architecture\nWe use an auto-regressive model based on Transformer\nand LSTM.",
                "The architecture (Figure 5) consists of an en-\ncoder and a decoder.",
                "Transformer-LSTM architecture for melody, ba-\nsic melody and rhythmic pattern generation.",
                "We also use a Transformer-LSTM architecture (Figure\n5), but with different model settings (size).",
                "We also experimented\nwith other deep neural network architectures described in\nSection 4.1 for comparison.",
                "More details about the\nnetwork are in Section 4.1.\n4. EXPERIMENT AND EVALUATION\n4.1 Model Evaluation and Comparison\nAs a model-selection study, we compared the ability of\ndifferent deep neural network architectures implementing\nMusicFrameworks to predict the next element in the se-\nquence."
            ],
            "dataset": [
                "A listening test\nreveals that melodies generated by our method are rated\nas good as or better than human-composed music in the\nPOP909 dataset about half the time.",
                "We use a Chi-\nnese pop song dataset, POP909",
                "With the analysis algorithm, we can process a music\ndataset such as POP909 for subsequent machine learning\nand music generation.",
                "We used 4188 phrases from 528 songs in major mode\nfrom the POP909 dataset, using 90% of them as training\ndata and the other 10% for validation.",
                "The full Transformer model performed poorly on this\nrelatively small dataset due to over\ufb01tting.",
                "We con\ufb01rmed that our generation exhibits sim-\nilar structure-related distributions to that of the POP909\ndataset.",
                "The demographics information about the\nlisteners are as follows:\nGender male: 120, female: 75, other: 1;\nAge distribution 0-10: 0, 11-20: 17, 21-30: 149, 31-40:\n28, 41-50: 0, 51-60: 2, >60: 0;\nMusic pro\ufb01ciency levels lowest (listen to music <1\nhour/week): 16, low (listen to music 1\u201315 hours/week):\n62, medium (listen to music >15 hours/week): 21, high\n(studied music for 1\u20135 years): 52, expert ( >5 years of\nmusic practice): 44;\nNationality Chinese: 180, Others: 16 (note that the\nPOP909 dataset is primarily Chinese pop songs, and lis-\nteners who are more familiar with this style are likely to be\nmore reliable and discriminating raters.)",
                "The last two pairs show the ratings of our\nmethod compared to music in the POP909 dataset.",
                "This observation can be de-\nrived from similar distribution and near random prefer-\nence distribution in \u201c1 vs 2\u201d and \u201c1 vs 4,\u201d indicating that\npreference for the generated basic melody and rhythm\nform are close to those of music in our dataset.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset for\nmusic arrangement generation,\u201d in Proc. of 21st Inter-\nnational Conference on Music Information Retrieval,\nISMIR , 2020."
            ]
        },
        {
            "title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer",
            "architecture": [
                "Second,\nwe propose a novel gated parallel attention module to be used in\na sequence-to-sequence (seq2seq) encoder/decoder architecture\nto more effectively account for a given conditioning thematic\nmaterial in the generation process of the Transformer decoder.",
                "Furthermore, we show that the vanilla sequence-to-sequence\n(seq2seq) encoder/decoder architecture [13], [30], [31] is still\nnot suf\ufb01cient to enforce the in\ufb02uence of the condition when the\ngenerated music gets longer.",
                "The network uses a BERT-like\narchitecture",
                "From EncoderOnly for\ntop half layers \n(a) (b) (c)\nFig. 4: (a) Schematic overview of the proposed Transformer architecture for theme-conditioned music generation.",
                "Diagrams of\nthe decoder architecture for (b) the basic seq2seq model that utilizes segment embedding ( SE), and (c) the proposed Theme\nTransformer that utilizes parallel attention modules with XOR gating (the two \u201c \n\u201ds) and separate positional encodings ( PEs).\nof some toy sequences with perceptually tolerable alternations\nwe fed to the embedding model (when the model converges)."
            ],
            "dataset": [
                "The original piece here is an excerpt of the\nsong \u2018907.mid\u2019 from the test split of the POP909 dataset\n[25], starting from its thematic fragment.",
                "Note that the embedding distance as the\naverage of the whole POP909 dataset is 0.895.",
                "Speci\ufb01cally, we train the\nmodels using the training split of the POP909 dataset [25],\nand conduct objective and subjective evaluations on its test\nsplit.",
                "The subjective evaluation en-\ntails an online listening test that involves listeners familiar\nand unfamiliar with the music pieces in the POP909 dataset.",
                "The only\npublic theme-related dataset, the Musical Theme Dataset",
                "We identify the melody notes for each fragment using\nthe annotations of POP909 dataset.",
                "[64]), in our music generation task the length\nof the target sequences may well be longer than 64 bars, which\ntranslate to roughly 4,000 tokens (elements) for our dataset.",
                "I MPLEMENTATION DETAILS\nA. Dataset\nWe train our models using the piano covers of Mandarin\npop music from the POP909 dataset [25], which is composed\nof the piano covers of 909 Mandarin pop songs originally\ncomposed by 462 artists, released from the earliest in 1950s\nto the latest around 2010",
                "In total, our vocabulary for the piano contains 730 unique\ntokens.9The songs in our dataset have \u001895 bars on average,\nwhich translate to 5,249 tokens per song on average using\nthe piano representation.",
                "In our dataset, the\naverage length of such a melody sequence, which is fed to\nthe embedding model, is 24.3 tokens.10\nTable I lists the token types and the number of unique tokens\nfor each type in our token representation of the piano and the\nmelody, respectively.\nC. Model settings\nWe use a 6-layer encoder and a 6-layer decoder (i.e.,\nL= 6) for both the basic seq2seq Transformer and the\nproposed Theme Transformer .",
                "12We are aware of two other relevant dataset, the MTD",
                "[3] and JKU\nPattern Dataset",
                "A multi-\nmodal dataset of musical themes for MIR research,\u201d Transactions of the\nInt.",
                "Zhang, M. Xu, S. Dai, X. Gu, and G. Xia,\n\u201cPOP909: A pop-song dataset for music arrangement generation,\u201d in\nProc."
            ]
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "architecture": [
                "The tiered architecture of\nthe SampleRNN allows for different computational focus to be\napplied to different levels of abstraction of the audio, which\nallows long term dependencies to be modelled ef\ufb01ciently.\nJukebox.",
                "We use a classi\ufb01er architecture [25] that came fourth in the\nMediaEval 2019 competition with respect to macro-averaged\nPR-AUC.",
                "This analysis depends on the behaviour of the classi\ufb01er, so\nfuture work should explore the effect of different classi\ufb01er\narchitectures."
            ],
            "dataset": [
                "This is the \ufb01rst attempt at\naugmenting a music genre classi\ufb01cation dataset with conditionally\ngenerated music.",
                "We investigate the classi\ufb01cation performance\nimprovement using deep music generation and the ability of\nthe generators to make emotional music by using an additional,\nemotion annotation of the dataset.",
                "Finally, we\nperform the same experiments on an almost comprehensive\nsubset of our dataset with a relabelling, which we introduce,\npertaining to coarser arousal/valence emotion classes.",
                "MTG-J AMENDO DATASET",
                "For our experiments we use the MTG-Jamendo dataset [21],\nwhich is a large collection of labelled, high-quality commercial\nmusic.",
                "Distribution of total duration of music for each emotional class for\nthe dataset used in this study.",
                "We\nevaluate performance on the test partition of each dataset with\nrespect to micro and macro averaged metrics, calculated in the\nsame manner as the MediaEval 2019 competition submission.",
                "the same hyperparameters reported in its submission paper\n[25].\nA. Augmentation Policy\nFor each generative method, we generate an equal duration\nof music per class, with the total length of generated music\nequal to 5 % of the duration of the train split of each dataset\n(8 hours for mood/theme, 7.85 hours for emotional).",
                "We trained\nSampleRNN and DDSP on the training partition of each\ndataset and used performance on the validation partition to\ntune hyperparameters.",
                "C ONCLUSION & F UTURE WORK\nFigure 3 reveal that no model could generate music with\nmeaningful features for allclasses of each dataset.",
                "A gener-\native model for raw audio,\u201d arXiv preprint arXiv:1609.03499 , 2016.[6] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-Z. A. Huang,\nS. Dieleman, E. Elsen, J. Engel, and D. Eck, \u201cEnabling factorized\npiano music modeling and generation with the MAESTRO dataset,\u201d in\nInternational Conference on Learning Representations , 2019.",
                "[21] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra,\n\u201cThe mtg-jamendo dataset for automatic music tagging,\u201d in Machine\nLearning for Music Discovery Workshop, International Conference on\nMachine Learning (ICML 2019) , Long Beach, CA, United States,\n2019."
            ]
        },
        {
            "title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control",
            "architecture": [
                "Especially the Transformer architecture (Vaswani\net al., 2017), popularized in the context of Natural Language\nprocessing (Brown et al., 2020) and then successfully ap-\n1Department of Computer Science, ETH Z \u00a8urich, Z \u00a8urich,\nSwitzerland.",
                "Karras, T., Laine, S., and Aila, T. A Style-Based\nGenerator Architecture for Generative Adversarial Net-\nworks."
            ],
            "dataset": [
                "Dataset\nWe use the LakhMIDI dataset (Raffel, 2016) as training data\nin all of our experiments, which to the best of our knowl-\nedge is the largest publicly available symbolic music dataset.",
                "However, the model experiences\nposterior collapse in our experiments when trained on the\ndiverse LakhMIDI dataset, which is apparent by the low\nentropy of the model distribution (see Table 2) as well as the\nworse-than-unconditional performance on some description\nmetrics.",
                "In terms of sample quality, our method\nbeats other state-of-the-art symbolic music generative mod-\nels trained on the LakhMIDI dataset.",
                "Unlike the original work, we do not use data\naugmentation since the dataset is large enough and in order to allow for fair comparison between the models.",
                "We limit the training data to a subset of the entire dataset (20k samples) due to technical limitations.",
                "This is still considerably more training data than what was used in the original paper (1k samples) and should not affect\nperformance signi\ufb01cantly compared to using the full dataset."
            ]
        },
        {
            "title": "Music Generation Using an LSTM",
            "architecture": [
                "Enlisting a mapping \nfunction and designing a model architecture implementing fo ur-layer types, LSTM, \ndropout, dense, and the activation layer, we used the structure of the network presented \nhere as a springboard for our work.",
                "Although there are a multitude of methods that apply to artificially generating music, \narchitecture design r emains nontrivial, training parameters remain poorly understood, and \nissues of overfitting and ambiguity in the most effective method to model complex data \nfeatures abound.",
                "Ultimately, in their paper An Empirical Exploration of Recurrent Network Architectures \nthey found through a straightforward architecture search that  the existing  structure of \nLSTMs can\u2019t be beat for most applications.",
                "We then repeated the architecture \nsearch process on the best performing models.",
                "We \ntake inspiration from evolutionary architecture and \nhyperparameter search in this process, however as \nmusic quality is a subjective metric without a trivial \nautomatic validation metric there is no way that \nevolution can be done without human intervention.",
                "In the same sense, \nour network\u2019s tiered LSTM architecture  may allow it to pick apart the various  aspects of \nmusic such as if it is building tension , speeding up, or looking at any other type of pattern \nthat may be found in music.",
                "\u201cAn Empirical Exploration \nof Recurrent Network Architectures.\u201d"
            ],
            "dataset": [
                "To convert the MIDI files in our dataset into an input format that our model can train on, \nwe first utilized the Music21 library\u2019s  converter to parse MIDI data into a numerical \nformat."
            ]
        },
        {
            "title": "Prote\u00e7\u00e3o intelectual de obras produzidas por sistemas baseados em intelig\u00eancia artificial: uma vis\u00e3o tecnicista sobre o tema",
            "architecture": [],
            "dataset": []
        },
        {
            "title": "An adaptive music generation architecture for games based on the deep learning Transformer model",
            "architecture": [
                "An adaptive music generation architecture for games\nbased on the deep learning Transformer model\nGustavo Amaral Costa dos Santos1Augusto Ba\u000ba1Jean-Pierre Briot2;1",
                "furtado@inf.puc-rio.br\nAbstract: This paper presents an architecture for generating music for video games based on the Trans-\nformer deep learning model.",
                "[cs.SD]  10 Sep 2022With the above mentioned principles in mind, after several experiments, we opted for the Transformer\narchitecture [39], because it better captures the long-term structure of music [12].",
                "In particular, we want to\nmove towards collaborative and interactive control of the music components generated by the Transformer-based\narchitecture.",
                "The following sections introduce the design, implementation and preliminary experiments with a proto-\ntype architecture for generating personalized and adaptive music for games aligned with the above mentioned\nprinciples.",
                "2.2 Architecture of the Adaptive Music System\nThe architecture of (AMS)",
                "The comparison of our proposed model with the much more complete and robust AMS architecture is twofold.",
                "Furthermore, we use a deep learning architecture (Transformer) better suited to\ncapture long-term coherence in music.\nFigure 1: AMC architecture, reproduced from [15]\n3 Adaptability versus Continuity and other Design Issues\nA pure generative approach is some kind of ideal, as it could in principle combine personalization (learnt styles)\nwith real-time adaptation (to the game and players situation).",
                "In the following sub-sections, we\nwill describe and motivate various aspects and components of the architecture and of the generation process,\nnamely: the general design principles; the curation and pre-processing of the training musical examples; the way\nmusic generated is layered; the emotion model chosen to map the game play into some control of the generated\nmusic; the mapping discipline; the complete architecture; the implementation; and the preliminary evaluation.",
                "4.1 Design Principles\nAfter having at \frst experimented with a recurrent neural network architecture of type LSTM (part of Google's\nMagenta project library)",
                "[20], we selected the Transformer architecture for its ability to enforce consistency and\nstructure, by better handling long-term correlations.",
                "Transformer [39] is an important evolution of a Sequence-\nto-Sequence architecture (based on RNN Encoder-Decoder), where a variable length sequence is encoded into a\n\fxed-length vector representation which serves as a pivot representation to be iteratively decoded to generate\na corresponding sequence (see more details, e.g., in [8, Section 10.4]).",
                "For more details on the architecture, illustrated in Fig. 2, please see\nthe original article",
                "These layers are generated from the same learning corpus, but from di\u000berent seeds (starting sequences) and\nwith di\u000berent generation parameters (currently, we vary a temperature parameter that controls the determinism\nof the generation, for some more likely or more unpredictable result), depending on the controlling model (as\n4Figure 2: Transformer architecture, reproduced from [39]\nwill be presented in Section 4.5).",
                "4.6 Architecture\nThe \row logic of current architecture, illustrated in Fig. 6, is as follows:\n1.",
                "Figure 6: Final architecture \row\n4.7 Implementation\nTo optimize the music generation process, at least one music corresponding to each strategy is saved in memory.",
                "The architecture is designed as a server responsible for music generation, for various possible game clients, based\non game engines like Unity or Unreal, or speci\fc ones.",
                "4.8 Evaluation\nCurrent architecture has been tested with an emulated game model and with music generated from a corpus\nof ambient music.",
                "5.2 Interactive Coordination\nA more radical approach is to substitute the sequencer-like platform (currently, Ableton Live) by a more\ngeneral platform for interactive and collaborative control of musical components (being generated by our current\nTransformer-based architecture).",
                "It separates the macro-level coordination from the actual micro-level components, as for\narchitectural/coordination languages in software architectures",
                "The Skini platform\n9(whose architecture is shown in Fig.",
                "Figure 8: Skini architecture\n6 Conclusion\nIn this paper, we have presented an architecture, based on deep learning (more speci\fcally, the Transformer\narchitecture), for generating music for video games, personalized to the user musical preference.",
                "Our current architecture is a proof of concept, although it is complete and functional.",
                "We\nare currently working on the design of a next version architecture and its coupling with the coordination level\nbased on the Skini architecture.",
                "Software architecture { Perspectives on an emerging discipline ."
            ],
            "dataset": []
        },
        {
            "title": "WHAT IS MISSING IN DEEP MUSIC GENERATION? A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC",
            "architecture": [],
            "dataset": [
                "Analyses of two\npopular music datasets (Chinese and American) illustrate\nimportant music construction principles: (1) structure ex-\nists at multiple hierarchical levels, (2) songs use repetition\nand limited vocabulary so that individual songs do not fol-\nlow general statistics of song collections, (3) structure in-\nteracts with rhythm, melody, harmony and predictability,\nand (4) over the course of a song, repetition is not random,\nbut follows a general trend as revealed by cross-entropy.",
                "Mu-\nsic from recent music generation systems is analyzed and\ncompared to human-composed music in our datasets, often\nrevealing striking differences from a structural perspective.",
                "Another important effect of repetition is that song-\nspeci\ufb01c vocabulary of rhythm and pitch patterns is lim-\nited relative to what would be expected from the entire\ndataset.",
                "For training and testing, we use a Chinese pop\nsong dataset POP909",
                "[36], which has 909 pop song per-\nformances in MIDI, and an American pop song dataset\nPDSA",
                "Blue lines are real phrases in the dataset.",
                "Orange lines are sampled phrases constructed by choosing each pattern at random from the entire dataset distribution.",
                "This will of\ncourse be true necessarily if the entire dataset has a very\nlimited vocabulary, so as a baseline for comparison, we\nconstruct random phrases by sampling half-note onset pat-\nterns from the entire dataset distribution.",
                "This allows us to eval-\nuate whether patterns within phrases (blue) have similar\ndistributions to those of the entire dataset (orange).",
                "Again, we see that real song phrases have a\nlimited vocabulary compared to phrases assembled from\nrandom half-note units representing the entire dataset, and\nfewer real phrases go unrepeated.",
                "The vocabulary of pitch patterns within a song or phrase\nis also very limited compared to the whole dataset, im-\nplying pitch sequence repetitions within the phrase level.",
                "In Figure 6, we\ntrained on the entire dataset (background model) of melody\nFigure 6 : Average cross-entropy on diatonic pitches at dif-\nferent structure level positions in POP909 dataset predicted\nby background and foreground variable Markov models.\npitch sequences, holding out test songs; and also trained\non single songs (foreground model), holding out all phrase\nrepetitions to eliminate prediction by memorizing phrases,\nand holding out eight notes at a time for testing.",
                "The V AE generated\npatterns are signi\ufb01cantly more than the patterns in the real\nPDSA dataset, with p-value less than 10\u00005.\nmodel",
                "4.2 Discussion and New Directions\nRather than learning and reproducing general statistics of\ndatasets, we need to learn how songs strategically diverge\nfrom background or stylistic norms to create interest, sur-\nprise, and individuality.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset\nfor music arrangement generation,\u201d in Proc. of 21st\nInt."
            ]
        },
        {
            "title": "VIS2MUS: EXPLORING MULTIMODAL REPRESENTATION MAPPING FOR CONTROLLABLE MUSIC GENERATION",
            "architecture": [],
            "dataset": []
        },
        {
            "title": "GENERATING MUSIC WITH SENTIMENT USING TRANSFORMER-GANS",
            "architecture": [
                "Due to the inherent instability of the adversarial pro-\ncess, several works have focused on improving the conver-\ngence and the quality of the samples generated by GANs\nvia new objective functions, regularization and normaliza-\ntion techniques, and model architectures.",
                "Here, we use\nthe same schedule as in [13], that is, 1=\u001c= (1=\u001cmin)n=N,\nwherenis the index of the current global optimization, N\nis the total number of steps and \u001cminis a hyperparameter\nwhich we chose to be 10\u00002.\n2.2 Transformers\nShortly after its presentation in 2017, the Transformer [14]\nbecame the most popular architecture in the \ufb01eld of Natural\nLanguage Processing (NLP), and it has also been success-\nfully applied to other areas, such as image recognition",
                "METHODS\n3.1 Architecture and Loss functions\nBoth the Generator and Discriminator are Transformers\nwith linear versions of the Attention Mechanism [18]."
            ],
            "dataset": [
                "The EMOPIA\ndataset [37] contains musical passages separated into four\nquadrants that each corresponds to a combination of pos-\nitive or negative arousal and valence.",
                "The excerpts on\nthe dataset are from piano transcriptions of pop songs that\nwere labeled by its authors.",
                "In the \ufb01rst place, it has\nto predict each item of each sequence from the real dataset\nbased on the previous elements, that is, it has to complete\npieces of already existing sequences.",
                "The second objec-\ntive of the network is to generate sequences that are similar\nto those of the real set from scratch, that is, without context\nfrom the real dataset, such that these sequences can fool\nthe Discriminator.",
                "There is\none of these couples for each class on the dataset and one\nfor the unlabeled sequences.",
                "First, there is a sin-\ngle feature unit that indicates if the passage originates from\nthe real or fake datasets and if it exhibits the desired char-\nacteristics provided by the conditional signal.",
                "3.2 Datasets\nWe used two datasets to train our models, mainly as a\nmeans to allow the networks to use a larger training cor-\npus.",
                "The\nAILABS17k dataset [38] contains over 108hours of piano\ncovers of pop songs automatically transcribed by a state-of\nthe art piano transcription model",
                "The EMOPIA dataset [37] was constructed ina similar fashion to the one above, but its songs were af-\nterwards labeled by the authors of the dataset according to\nperceived sentiment [2].",
                "The clips on this dataset amount\nto approximately 11hours.",
                "In this stage, the Generator was\ntrained simultaneously on both datasets, and taking into\nconsideration the difference in size between these datasets,\nto balance the training process, we alternated between op-\ntimization steps on randomly sampled batches from each\nset.",
                "The Discriminator worked with subsequences\nof length 16, and the training was done exclusively on the\nEMOPIA dataset [37].",
                "H. Yang, \u201cEMOPIA: A multi-modal pop piano dataset\nfor emotion recognition and emotion-based music gen-\neration,\u201d in Proc."
            ]
        },
        {
            "title": "WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning",
            "architecture": [
                "Here, we present WuYun, a\nknowledge-enhanced deep learning architecture for improving the structure of\ngenerated melodies, which \ufb01rst generates the most structurally important notes to\nconstruct a melodic skeleton and subsequently in\ufb01lls it with dynamically decorative\nnotes into a full-\ufb02edged melody.",
                "Numerous specialized architectures of the language model for music generation have demonstrated\npromising performance in generating long-range coherent melodies, including effective attention\nmechanisms (15, 16), enhanced memory networks (17\u201319), large-scale deep neural networks (20),\nand explicit musicality regularization (21).",
                "In this study, we propose WuYun, a hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning that incorporates the melodic skeleton as deep structural\nsupport to provide explicit guidance on the development direction of melody generation (Fig. 1A).",
                "At the stage of melody inpainting, we adopt a Transformer encoder\u2013decoder\narchitecture (39) to elaborate the melodic skeleton into a full-\ufb02edged melody by encoding the melodic\nskeleton as additional knowledge into the decoder to guide the melody generation process (Fig.",
                "To prove the effectiveness of the architecture, we evaluate WuYun on a publicly available melody\ndataset.",
                "The hierarchical\nskeleton-guided melody generation architecture effectively improves generated melodies\u2019 long-term\nstructure and musicality and outperforms other state-of-the-art methods by 0.51 on average on all\nsubjective evaluation metrics.",
                "Input Module(Embedding)Positional Encoding\nTransformer-XL\uff084 blocks\uff09Output Module(Classifier)Ca) Melodic Skeleton Generation ModuleInput(Melodic skeleton)RepresentationInput Module(Embedding)Encoder\uff084 blocks\uff09Decoder\uff084 blocks\uff09Recurrent Transformerb) Melodic Prolongation Generation ModuleRepresentationInput Module(Embedding)Input(Melody)RepresentationInput(Melodic skeleton)Output Module(Classifier)TrainA\nMelodic SkeletonMelody (example)\nMelodic skeletonSequence learning modelY1Y2Yn\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nX1X2Xn\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7Partial sequenceMelodic skeleton\u00b7\u00b7\u00b7\nX1X2Xn\u00b7\u00b7\u00b7EncoderY2Y3Yn\u00b7\u00b7\u00b7Y1Y2Yn-1\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7Decoder\n\u2026\nPredicted tokens\nPredicted tokens\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7Sequence-to-sequencelearning modelMelodyFigure 1: Architecture of WuYun.",
                "( C) Architecture details of WuYun.",
                "In the following, we elaborate on the\nmelodic skeleton extraction framework from rhythm and pitch dimensions and introduce the design of\nWuYun melody generation architecture that \ufb01rst constructs the melodic skeleton and then completes\nthe melody instead of sequentially generating a melody note-by-note at once.",
                "2D shows the tonal skeleton of\nthe \ufb01rst eight bars from the song \u201cHey Jude.\u201d\n2.3 Design of WuYun\nFigure 1C shows the diagram of the proposed hierarchical melody generation architecture called\nWuYun.",
                "Then, we design a hierarchical melody generation architecture with two generative modules\nresponsible for melodic skeleton construction and melody inpainting, respectively.",
                "In this subsection,\nwe will introduce the hierarchical melody generation architecture about how we generate the melodic\nskeleton and incorporate it to guide the melody generation process.",
                "The details about the MeMIDI\nsymbolic music representation method and the word embedding technique used in this architecture\u2019s\ninput module are described in the Materials and Methods section.",
                "At the stage of melody inpainting, we employ the recurrent Transformer-based\nencoder\u2013decoder architecture (18) in a sequence-to-sequence setup as the melody inpainting module\nto complete the melody conditioned on the melodic skeleton, i.e., \ufb01lling the missing information\nbetween the melodic skeleton notes.",
                "In this work, we focus on designing a hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, following the hierarchical organization principle of\nstructure and prolongation.",
                "Therefore, we used common language models in NLP to make the WuYun\narchitecture accessible.",
                "Each subject\nwas required to rate 18 musical pieces, which cost approximately 25 min.\nFigure 3A shows the mean opinion scores of WuYun architecture\u2019s melody generation performances\nwith nine different settings on the \ufb01ve subjective evaluation metrics from all subjects in the form of\nhistograms.",
                "This phenomenon can be explained by\nthe structure and prolongation proportion tradeoffs in the design of two-stage melody generation\narchitecture using an end-to-end learning framework.",
                "2.6 Comparisons with other melody generation methods\nTo prove the effectiveness of the proposed hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, we compared WuYun-RS (i.e., using the rhythmic\nskeleton setting) to \ufb01ve public SOTA Transformer-based melody generation models, namely, Music\nTransformer (15), Pop Music Transformer (17), Compound Word Transformer (19), Melons (33)\nand MeMIDI, that follow an end-to-end left-to-right note-by-note generative paradigm and treat\neach note equally.",
                "However, the original music representation of Music\n9Figure 3: Subjective evaluation results of the WuYun melody generation architecture based on different\nmelodic skeleton settings, and the other public melody generation models.",
                "(A) Subjective comparison of the\nperformance of the WuYun architecture based on different melodic skeleton settings in Experiment 1.",
                "The WuYun architecture with the rhythmic skeleton setting achieves the best performance in\nall melodic skeleton settings on all subjective evaluation metrics.",
                "However, there is still an obvious gap between the WuYun melody generation architecture\nand human-composed music, leaving room for improvement.",
                "On the\nother hand, although the Compound Word Transformer and Melons (Nos. 4 and 5) were inferior to\nWuYun-RS, their effective compound word representation and the linear Transformer as the backbone\narchitecture enable it to process multidimensional music information in one step simultaneously and\nobtain a better result among these \ufb01ve public SOTA melody generation models.",
                "Thus, combining the\nproposed knowledge-enhanced hierarchical skeleton-guided music generation architecture with more\nef\ufb01cient music representation methods and advanced language models can bring a better result for\nmelody generation tasks.",
                "3 Discussion\nThe methodology we have taken in designing WuYun, a hierarchical skeleton-guided melody genera-\ntion architecture based on knowledge-enhanced deep learning, combines music analysis theory and\nmusical psychology.",
                "4 Materials and Methods\n4.1 Details of dataset preprocessing\nWe evaluate the effectiveness of WuYun architecture on a commonly used and publicly available\nsymbolic melody dataset of Wikifonia (32, 33, 67).",
                "4.3 WuYun architecture\nHere, we brie\ufb02y elaborate on the con\ufb01guration details of the two Transformer-based generative\nmodules of WuYun architecture, i.e., the melodic skeleton generation module for the melodic skeleton\nconstruction stage and the melodic prolongation generation module for the melody inpainting stage.",
                "For reproducibility, we do not tweak the architecture of\nreferenced models so that our music generation architecture can be easily assembled with the public\nimplementation of Transformers.",
                "144.4 Training\nWe implemented the WuYun architecture with Pytorch (v1.7.1) (69).",
                "The parameters of the WuYun\narchitecture were optimized by minimizing the cross-entropy loss on a single NVIDIA GTX 2080-Ti\nGPU with 11 GB memory."
            ],
            "dataset": [
                "To prove the effectiveness of the architecture, we evaluate WuYun on a publicly available melody\ndataset.",
                "We randomly selected ten melodies from the evaluation dataset for the listening materials.",
                "Therefore, in this experiment,\nwe applied the 16th note time grid as the MIDI quantization level to the melody dataset for all music\ngeneration models.",
                "4 Materials and Methods\n4.1 Details of dataset preprocessing\nWe evaluate the effectiveness of WuYun architecture on a commonly used and publicly available\nsymbolic melody dataset of Wikifonia (32, 33, 67).",
                "The Wikifonia dataset contains thousands of\nlead sheets in MusicXML format.",
                "Here, we describe the procedure below to clean\nup noisy data and arti\ufb01cial errors since the dataset is user-generated.",
                "We set one chord per beat\nand unify the chord representation of the Wikifonia dataset using the chord dictionary as\ndescribed in the following subsection.",
                "\u2022Chord\nTo cover the chord types in the Wikifonia dataset, we use a more comprehensive chord\nevent list.",
                "Here, we used the\nmelodic skeleton data extracted from the training part of the Wikifonia dataset to train the melodic\nskeleton generation module.",
                "Acknowledgements\nThanks to Huawei Technologies Co., Ltd for the help in dataset collection and comments."
            ]
        },
        {
            "title": "An investigation of the reconstruction capacity of stacked convolutional autoencoders for log-mel-spectrograms",
            "architecture": [
                "An autoencoder\n(AE) is a type of neural architecture composed of an encoder\nand a decoder.",
                "The architecture was evaluated\nby applying Inverse-STFT (ISTFT) on the generated frames\ndeveloping also a deterministic method for reconstructing the\nphase.",
                "A. Autoencoders\nAn autoencoder is a neural architecture composed of an\nencoder and a decoder [2] trained together in an unsupervised\nmanner.",
                "To expand the initial architecture to a deep neural autoen-\ncoder, a similar approach is adopted.",
                "Their architectureFig. 1.",
                "Illustration of a stacked convolutional autoencoders architecture for the reconstruction of the log-mel-spectrogram.",
                "[17] F. Girosi, M. Jones, and T. Poggio, \u201cRegularization Theory and Neural\nNetworks Architectures,\u201d Neural Computation , vol."
            ],
            "dataset": [
                "[3] have\ndemonstrated promising results in extracting information from\nrelatively big and complex datasets into a latent space.",
                "C. Regularization Techniques\nOne of the most considerable issues with deep neural\nnetworks is that they tend to over\ufb01t a training dataset.",
                "Regularization can be applied only on the weights W\n(kernel regularization ), or only on the bias term b(bias\nregularization ), or on the output layer y=Wx+b(activity\nregularization ).\n3) Data Augmentation: An attempt to increase the training\ndataset by alternating the original samples.",
                "In image processing, data augmentation\ncan be achieved by \ufb02ipping, scaling, or shifting the original\nimages [18] while in audio processing, common techniques\ninclude noise injection, time shifting, or speed alternation [20].\n4) Early Stopping: A validation dataset is used to calculate\nthe loss function after each epoch.",
                "E XPERIMENTS\nA. Dataset\nFor the conducted experiments, we used a subsample of\nthe NSynth dataset2, which is a dataset of four-second mono-\nphonic notes.",
                "The dataset was split into training, validation,\nand testing as 80/10/10.",
                "The experiments\nwere conducted on a Tesla P100 GPU using the TensorFlow\nlibrary3.\n2https://magenta.tensor\ufb02ow.org/datasets/nsynth\n3https://www.tensor\ufb02ow.org/V. E VALUATION\nTo evaluate the effectiveness of a generative network, many\nmethods have been proposed.",
                "However, training the autoencoder in the whole\nNSynth dataset conditioned by the pitch can expand the\nvariety of the produced sounds but also creates an additional\nperplexity which can reduce the performance of the network."
            ]
        },
        {
            "title": "Byte Pair Encoding for Symbolic Music",
            "architecture": [
                "Indeed, most recent models are based on the\nTransformer architecture (Vaswani et al., 2017).",
                "Model and training\nAs we speci\ufb01cally focus on sequential models, we exper-\niment with the state of the art deep learning architecture\nfor most NLP tasks at the time of writing, the Transformer(Vaswani et al., 2017) architecture."
            ],
            "dataset": [
                "This work aims at closing this gap by shedding light on the\nresults and performance gains of using BPE:\n\u2022We experiment on two public datasets (Wang et al.,\n2020b; Kong et al., 2021), with two base tokenizations,\non which BPE is learned with several vocabulary sizes,\non the generation and composer classi\ufb01cation tasks,\nand show that it improves the results;\n\u2022We compare BPE with other sequence reduction tech-\nniques introduced in recent research;\n\u2022We study the geometry of the learned embeddings, and\nshow that BPE can improve their isotropy;\n\u2022We show some limits of BPE, such as on the proportionarXiv:2301.11975v1",
                "Algorithm 1 Learning of BPE pseudo-code\nRequire: Base vocabularyV, target vocabulary size N,\ndatasetX\n1:whilejVj<N do\n2: Finds=ft1;t2g2V2, fromX, the most recurrent\ntoken succession\n3: Add a new token tinV, mapping to s\n4: Substitute every occurrence of sinXwitht\n5:end while\n6:returnV\nBPE is nowadays largely used in the NLP \ufb01eld as it allows\nto encode rare words and segmenting unknown or com-\nposed words as sequences of sub-word units (Sennrich et al.,\n2016).",
                "In\nthis context, BPE can allow to represent a note, or even a\nsuccession of notes, that is very recurrent in the dataset, as\na single token.",
                "Experimental settings\nThis section details the experimental protocol by describing\nthe models, the training and the datasets used along with the\nspeci\ufb01c tokenization processes.",
                "We split datasets\nin two subsets: one only used for training and updating\nthe models, one for validation to monitor trainings, that is\nalso used to test the models after training.",
                "These subsets\nrepresent respectively 65% and 35% of the original datasets.",
                "Datasets\nWe experiment with two datasets: POP909 (Wang et al.,\n2020b) and GiantMIDI (Kong et al., 2021).",
                "The POP909 dataset (Wang et al., 2020b) is composed of\n909 piano tracks of Pop musics, with aligned MIDI and\naudio versions.",
                "The GiantMIDI dataset (Kong et al., 2021) is composed\nof 10k piano MIDI \ufb01les, transcribed from audio to MIDI\nwithout downbeat and tempo estimation.",
                "Considering the com-\nplexity of its content, we make the assumption that it is a\ndif\ufb01cult dataset for a model to learn from.",
                "We perform data augmentation on the pitch dimension on\nboth datasets.",
                "Normalized distributions of the token types of the BPE\ntokens, per BPE factor for the POP909 dataset.\n0",
                "The distribution for\nthe GiantMIDI dataset are showned in Appendix C.\nFigure 4 shows the evolution of the average number of non-\nBPE token combinations represented by the BPE tokens.",
                "The\nPOP909 dataset being smaller than GiantMIDI, it naturally\nleads to a higher maximum number of combinations as the\nlatter is more diverse.",
                "Sim stands for similarity, the best results are\nthe closest to the datasets.",
                "These numbers,\ncorrelated with the model, dataset sizes and overall token\ndistribution of the dataset, might help to choose an optimal\nvocabulary size.",
                "Composer classi\ufb01cation\nComposer classi\ufb01cation is performed with the top-10 most\npresent composers of the GiantMIDI dataset.",
                "We also plan to experiment with larger model, dataset and\nvocabulary sizes, hoping to \ufb01nd guidelines for choosing an\noptimum vocabulary size.",
                "Giantmidi-\npiano: A large-scale midi dataset for classical\npiano music.",
                "URL https://openreview.\nnet/forum?id=ByxY8CNtvr .\nWang, Z., Chen, K., Jiang, J., Zhang, Y ., Xu, M., Dai,\nS., Bin, G., and Xia, G. Pop909: A pop-song dataset\nfor music arrangement generation.",
                "B. Data downsampling\n0 1 2 3 4 5 6 7\nduration0.00.20.40.60.81.01.21.4densityDataset\nPOP909\nGiantMIDI\n0 20 40 60 80 100 120\nvelocity0.0000.0050.0100.0150.0200.0250.030densityDataset\nPOP909\nGiantMIDI\nFigure 7.",
                "Distributions of the note durations and velocities of the POP909 and GiantMIDI datasets.",
                "Figure 7 shows the distributions of velocity and duration values of the notes from the two datasets we use.",
                "Normalized distributions of token types per BPE factor for the GiantMIDI dataset.",
                "The tokenization time could be\ndecreased if performed by a faster compiled language such as Rust or C. The Figure 8 complements the Figure 3, with the\nGiantMIDI dataset.",
                "Figure 10 shows the pairwise cosine similarity of the learned embedding vectors, for the TSD andRemi representation on\nthe POP909 dataset.",
                "Pairwise cosine similarity matrix of learned embedding of the generative models, on the POP909 dataset.",
                "UMAP 2d representations of the embeddings of classi\ufb01er models pre-trained with the GiantMIDI dataset and TSD tokenization.",
                "UMAP 3d representations of the embeddings of generative models with the POP909 dataset."
            ]
        },
        {
            "title": "A Symbolic-domain Music Generation Method Based on Leak-GAN",
            "architecture": [
                "[14] based on \nnetwork architecture, which allows the discriminator to leak th e \nextracted features from higher level to the generator to help t he \nguidance more distant, thus solving the problem of sparsity in \nguiding signals while training.",
                "To alleviate this situation, the LeakGAN model does not only provide generator with discriminator\u2019s information, it additionally applies a hierarchical reinforcement learning architecture to the generator in order to coordinate the leaked information with \ngeneration process of \nG\uf071.  \nWe introduce a MANAGER module and a WORKER \nmodule of the generator, which both start from an all-zero \nhidden state, denoted as 0Wh and 0Mh.",
                "FB \n2) GAN Setting \nWe choose LSTM to be the architecture of both \nMANAGER and WORKER in the generator."
            ],
            "dataset": [
                "Cutting \nAs the length of one complete song may be too long for our \ngeneration model, we cut long text by piece according to analysis results of the experimental dataset.",
                "Dataset \nAs we mainly focus on melody generation, the POP909[19] \npop music dataset is the best choice at our knowledge, as its music melody track can be explicitly distinguished and directly  \nderived.",
                "After our processing, the dataset consists of 11792 melody \ndata with an average length of 210-word.",
                "We randomly sample three tenths of the dataset as the test \ndata and the remaining seven tenths as the train data, on which  \nwe run the adversarial training pretrained by MLE for 200 \nepochs.",
                "We train the 8 layers LSTM model with the same dataset for 500 epochs.",
                "A pop-song dataset for music arrangement genera tion."
            ]
        },
        {
            "title": "AI Music Therapist: A Study on Generating Specific Therapeutic Music based on Deep Generative Adversarial Network Approach",
            "architecture": [
                "Therefore, three dimensions need to be discussed in this \nstudy: whether the generated content is melody or \naccompaniment; whether single-track or multi-track music is \ngenerated; and whether the network architecture is CNN \n(convolutional neural network) or LSTM (recurrent neural \nnetwork), or GAN (generative adversarial neural network) \nwith CNN as the underlying architecture, which performs \nwell in various image video tasks."
            ],
            "dataset": [
                "paragraph3 |   : paragraph | \n  \nphrase | | phrase 2 | phrase 3 phrase 4     \n    \n      \n| pixel1 | pixel2 |   \nFig. 7. Music composition \nE. Introduction to the Data Set \nThe data set is divided into two parts, one is the public \npiano dataset, and the other is the multiple self-selected \npiano pieces.",
                "The Lakh Pianoroll Dataset (LPD) is a collection of \n174,154 multitrack piano rolls derived from the Lakh MIDI \nDataset (LMD).",
                "The final dataset has a total of 127,731 bars, and \nthe goal of the model is to generate a five-track piano roll of \nfour bars.",
                "The network was trained 20, 40, 60, 80, 100, and 200 \ntimes on the dataset, and the reference values and the results \nof 100 and 200 times were sliced randomly in two segments \nfor comparison.",
                "GAN multi-track generation results \nFigure 10(a) shows the output of a random sequence in \nthe dataset, with the horizontal and vertical coordinates \nindicating time and notes, respectively."
            ]
        },
        {
            "title": "An intelligent music generation based on Variational Autoencoder",
            "architecture": [
                "The main architecture of the three networks used \nin this section has been added to the convolutional neural \nnetwork.",
                "The \nprimary architecture for the three networks used in this \nsection is CNN."
            ],
            "dataset": [
                "Experimental data \nThe data used in this paper is the classical piano MIDI \ndataset, which contains music files of different formats for \nall classical music.",
                "[14] toolkit to process the individual MIDI files in the \ndataset, marked the time intervals and note types to obtain \nrhythm sequences."
            ]
        },
        {
            "title": "APE-GAN: A Novel Active Learning Based Music Generation Model With Pre-Embedding",
            "architecture": [
                "[4] used a generative adversarial architecture\nto combine two RNNs and resulted in more complexity in\nthe generated musics and increased intensity span similarity\nbetween the generated musics and real musics.",
                "In this work, we reference MuseGAN as our baseline\narchitecture.",
                "Transformer Architecture",
                "[8]\nTransformer is a novel architecture introduced by the paper\n\u201cAttention Is All You Need\u201d."
            ],
            "dataset": [
                "After using Lakh\nPianoroll Dataset to train APE-GAN, the similar meaning textual\ninputs result in outputs with KL divergence approximate to 0,\nwhile different meaning textual inputs result in outputs with KL\ndivergence approximate to 1.\nKeywords \u2014Music Generation; Generative Adversarial Net-\nworks(GAN); BERT; Active Learning; KL Divergence\nI. I NTRODUCTION",
                "These \u201cuncertain\u201d data will be selected from the\ncomplete dataset and delivered to the oracle, or humans, who\nwill label these data via online interface.",
                "After labelling, these\ndata will be sent back to the original dataset to update it.",
                "The source data we used is the cleansed version\nof Lakh Pianoroll Dataset.",
                "The dataset provides 21,425\npianorolls in the format of .npz"
            ]
        },
        {
            "title": "Automatic Music Generation System based on RNN Architecture",
            "architecture": [
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n294\n \n978-1-6654-7657-7/22/$31.00 \u00a92022 IEEE  Auto\nmatic Music Generation System based on RNN \nArchitecture \n \nSandeep Kumar \nDepartment of Computer Science & \nEngineering \nKoneruLakshmaiah Educational \nFoundation  \nVaddeswaram, Andhra Pradesh, India \ner.sandeepsahratia@gmail.com \n \nShilpa Rani \nDepartment of Computer Science & \nEngineering \nNeil Gogte Institute of Technology \nHyderabad, India \nshilpachoudhary1987@gmail.com \n \n KeerthiGudiseva \nDepartment of Computer Science & \nEngineering \nKoneruLakshmaiah Educational \nFoundation  \nVaddeswaram, Andhra Pradesh, India \nkeerthigudiseva0611@gmail.com",
                "We \ndiscussed the architecture and approach in the third section.",
                "13 Muhammad Nadeem et \nal., 2019 LSTM  Nottingham \nMusicDataba\nse The majority of participants enjoyed listening to the music created by \nthe proposed musical data structure and RNN architecture.",
                "[14] Dungan, Belinda M., and Proceso L. Fernandez, \u201cNext Bar Predictor: \nAn Architecture in Automated Music Generation,\u201d In IEEE \nInternational Conference on Communication and Signal Processing \n(ICCSP), pp. 109-113, 2020.",
                "[17] Kumar S, Shilpa Rani, Arpit Jain, Chaman Verma, Maria \nSimonaRaboaca, Zolt\u00e1nIll\u00e9s and BogdanConstantinNeagu, \u201cFace \nSpoofing, Age, Gender and Facial Expression Recognition Using \nAdvance Neural Network Architecture-Based Biometric System, \u201d \nSensor Journal, vol."
            ],
            "dataset": [
                "The device is \ncomposed of a set of piano MIDI records from the MAESTRO \ndataset that are used to build song segments.",
                "Using a selection of piano MIDI files from the \nMAESTRO dataset, we can learn how to train a model.",
                "We described the findings of benchmark datasets in section \n4, and in the following subsections, we evaluated by \ncontrasting the existing methods with the suggested method.",
                "From the MAESTRO dataset, we can also \nlearn how to train a model using a group of piano MIDI \nfiles.",
                "Figures 9 and 10 show \nthe results for faster computation times using RW-hybrid for the \n30musicS dataset after two iterations.",
                "Four \nlayers total ( 3) Network of convolutional neurons: 2000 rows in the \ndataset Distribution of the dataset 70:20:10 50 pixels per second \nsquare spectrogram slices in the input layer 6 layers (4 Conv max \npools and 1 completely)",
                "WaveNet, \nWaveGlow, and the \nneural-source-filter \n(NSF) model NSynth  and \nMAESTRO \nDataset",
                "8 Brandon Royal,  et al. \n2020 preprocessing and \nreconstruction \ntechnique hash based Nottingham \ndataset Method Average (song 1) 4.14  Medeot et al.",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n296 \n 9 Sarthak Agarwal et al., \n2018 PRECON-LSTM  Nottingham \nMus\nic \nDatabase  Sco\nreMeanMean Realness \nArtif3.120 2.747 \nGen \nHum3.613 3.516 \nComp  \n10 AdvaitMaduskar et al. \n2020 Autoregressive GAN \nmodel Bach\u2019s \nmusical \nsymphonies \ndataset Outlined a generation model for note sequence generation using the \nGAN framework.",
                "11 Tianyu Jiang et al., \n2019 Bidirectional LSTM \nnetwork  Classical \nPiano \nDataset Bidirectional LSTM network was presented to produce harmonic \nmusic.",
                "Dataset A GAN-based model that was trained on a dataset of Bach's \norchestral symphonies produced the desired outcomes.",
                "RNN Structure I\nV. RESULT AND DISCUSSION  \n \nA. D\nataset \n \nMore than 200 hours of paired audio and MIDI \nrecordings from the International Piano-e-first Competition's \nten years are included in the MAESTRO dataset.",
                "16-bit \nPCM stereo) \n \nB. Training and testing of the Dataset \n \nBy removing notes from the MIDI documents, the \ntraining dataset is produced.",
                "C. Epochs \n \nA hyper-parameter that regulates how frequently the \nlearning algorithm iterates through the training dataset is the \nnumber of epochs.",
                "The underlying model parameters have \nbeen updated once every epoch for each sample in the \ntraining dataset.",
                "The musical \ninstrument comprises of a set of piano MIDI files from the \nMAESTRO dataset that serve as the basis for the song \nportions.",
                "[9] Agarwal, Sarthak, VaibhavSaxena, VaibhavSingal, and Swati \nAggarwal, \u201cLstm based music generation with dataset preprocessing \nand reconstruction techniques,\u201d In IEEE Symposium Series on \nComputational Intelligence (SSCI), pp. 455-462, 2018."
            ]
        },
        {
            "title": "Development of Application Software for Generating Music Composition Inspired by Nature Using Deep Learning",
            "architecture": [
                "Figure 1 Illustration of (a) GAN (b) Variational En coder \n \nb) Variational Auto encoder: It is the simple feed forward network, in which the neurons are \nstacked in the layered architecture."
            ],
            "dataset": [
                "2.   Formulating the Musical notes corresponding to the collected dataset: The earlier \nproposed methodology",
                "[10] will be adopted to collect the features from the natur al \ndataset and are further used to obtain the musical notes corresponding to the fea tures \ncollected."
            ]
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "architecture": [
                "The tiered architecture of\nthe SampleRNN allows for different computational focus to be\napplied to different levels of abstraction of the audio, which\nallows long term dependencies to be modelled ef\ufb01ciently.\nJukebox.",
                "We use a classi\ufb01er architecture [25] that came fourth in the\nMediaEval 2019 competition with respect to macro-averaged\nPR-AUC.",
                "This analysis depends on the behaviour of the classi\ufb01er, so\nfuture work should explore the effect of different classi\ufb01er\narchitectures."
            ],
            "dataset": [
                "This is the \ufb01rst attempt at\naugmenting a music genre classi\ufb01cation dataset with conditionally\ngenerated music.",
                "We investigate the classi\ufb01cation performance\nimprovement using deep music generation and the ability of\nthe generators to make emotional music by using an additional,\nemotion annotation of the dataset.",
                "Finally, we\nperform the same experiments on an almost comprehensive\nsubset of our dataset with a relabelling, which we introduce,\npertaining to coarser arousal/valence emotion classes.",
                "MTG-J AMENDO DATASET",
                "For our experiments we use the MTG-Jamendo dataset [21],\nwhich is a large collection of labelled, high-quality commercial\nmusic.",
                "Distribution of total duration of music for each emotional class for\nthe dataset used in this study.",
                "We\nevaluate performance on the test partition of each dataset with\nrespect to micro and macro averaged metrics, calculated in the\nsame manner as the MediaEval 2019 competition submission.",
                "the same hyperparameters reported in its submission paper\n[25].\nA. Augmentation Policy\nFor each generative method, we generate an equal duration\nof music per class, with the total length of generated music\nequal to 5 % of the duration of the train split of each dataset\n(8 hours for mood/theme, 7.85 hours for emotional).",
                "We trained\nSampleRNN and DDSP on the training partition of each\ndataset and used performance on the validation partition to\ntune hyperparameters.",
                "C ONCLUSION & F UTURE WORK\nFigure 3 reveal that no model could generate music with\nmeaningful features for allclasses of each dataset.",
                "A gener-\native model for raw audio,\u201d arXiv preprint arXiv:1609.03499 , 2016.[6] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-Z. A. Huang,\nS. Dieleman, E. Elsen, J. Engel, and D. Eck, \u201cEnabling factorized\npiano music modeling and generation with the MAESTRO dataset,\u201d in\nInternational Conference on Learning Representations , 2019.",
                "[21] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra,\n\u201cThe mtg-jamendo dataset for automatic music tagging,\u201d in Machine\nLearning for Music Discovery Workshop, International Conference on\nMachine Learning (ICML 2019) , Long Beach, CA, United States,\n2019."
            ]
        },
        {
            "title": "Generating Music Algorithm with Deep Convolutional Generative Adversarial Networks",
            "architecture": [
                "GAN Network architecture."
            ],
            "dataset": [
                "E\nXPERIMENT\nA. Data Set \nThe mentioned above dataset from The Lakh MIDI \nDataset1, which is a collection of 176,581 unique MIDI files, \n45,129 of which have been matched and aligned to entries in the Million Song Dataset.",
                "the same token, pretraining the discriminator \nagainst dataset before we start training the generator will establish a clearer gradient."
            ]
        },
        {
            "title": "Generating Music with Emotions",
            "architecture": [
                "Specifically, our\nELMG consists of the following three parts: 1) Lyric and\nmelody generator: a novel encoder-decoder architecture that\ncan generate lyric and melody by accepting a small piece\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "In this work, we propose a novel encoder-decoder\narchitecture for lyric and melody generation.",
                "The architecture of the proposed lyric-melody generator is\nshown in Figure 3, which is a sequential encoder-decoder\nmodel trained end-to-end to compose lyrics and melodies.",
                "The architecture of the proposed ELMG system."
            ],
            "dataset": [
                "There is no existing large-scale music datasets with the annotation\nof human emotion labels.",
                "In this paper, we propose an\nannotation-free method to build a new dataset where each sample\nis a triplet of lyric, melody and emotion label (without requiring\nany labours).",
                "Specifically, we first train the automated emotion\nrecognition model using the BERT (pre-trained on GoEmotions\ndataset) on Edmonds Dance dataset.",
                "We then train the encoder-decoder based model to\ngenerate emotional music on that dataset, and call our overall\nmethod as Emotional Lyric and Melody Generator (ELMG).",
                "The framework of ELMG is consisted of three modules: 1) an\nencoder-decoder model trained end-to-end to generate lyric and\nmelody; 2) a music emotion classifier trained on labeled data\n(our proposed dataset); and 3) a modified beam search algorithm\nthat guides the music generation process by incorporating the\nmusic emotion classifier.",
                "Given the advent of large-\nscale music datasets, e.g., LMD-full MIDI",
                "Dataset",
                "[5] and\nreddit MIDI dataset",
                "While training a deep model with the sense\nof \u201cmood\u201d categories (i.e., human emotions) is challenging,\ndue to the fact that most music datasets do not have emotion\nlabels.",
                "Though in the literature there are small datasets, they\nare limited to learn the mapping (from emotions to music).",
                "[13] built a music dataset VGMIDI com-\nposed of 95 labelled piano pieces and 728 unlabelled pieces,\nand trained a deep generative network to generate music with\na given emotion.",
                "In 2020, they expanded this VGMIDI dataset\nfrom 95 to 200 labelled pieces and presented a model called\nBardo Composer based on GPT-2",
                "[15] proposed a symbolic music dataset EMOPIA\nthat includes 1,078 music clips from 387 songs with Valence-\nArousal emotion labels.",
                "We aim\nfor the large-scale manual-labour-free dataset and the music\ngenerator for not only melodies but also lyrics with specific\nemotions.",
                "To this end, we first build the lyric-melody dataset with\nemotion labels.",
                "We use the songs with English lyrics se-\nlected from the LMD-full MIDI dataset [5] and reddit MIDI\ndataset",
                "The pipeline includes a few steps: 1) cutting each\nsong into lyric segments with fixed length; 2) fine-tuning a\nBert [16] model on Edmonds Dance dataset [17]; and 3)\nusing the result model to annotate the segments.",
                "We elaborate\nthe dataset construction in Section III.",
                "Beside of building the dataset, we design the Emotional\nLyric and Melody Generator (ELMG) system, which to our\nbest knowledge, is the first attempt to automatically and\nsimultaneously generate lyric and melody with a specific given\nemotion using deep learning.",
                "\u2022We build large-scale paired lyric-melody dataset with\nautomatic emotion labels consisting of 11,528 MIDI\nsongs.",
                "With the advent of large music\ndatasets, deep learning models have recently achieved high-\nquality results in music composition tasks.",
                "However, it is too expensive\nto manually annotate emotion labels for music datasets, which\ncauses great difficulties for music generation tasks conditioned\non emotions.",
                "They also built a new music dataset\nwith manually emotion labels called VGMIDI, which consists\nof 95 labelled piano pieces and 728 unlabelled pieces.",
                "[24]\nto generate music with a specific emotion and the VGMIDI\ndataset was extended to 200 labelled data.",
                "More recently, Hung et\nal.built an emotion-labeled symbolic music dataset called\nEMOPIA",
                "They also verified that the proposed dataset can be used\nfor generating music conditioned on emotions.",
                "Nevertheless,\nexisting music datasets with emotion labels are both smallin size.",
                "Therefore, we create a new large-scale paired lyric-\nmelody dataset with emotion labels for generating harmonious\nmusic that can evoke emotions.",
                "In recent years, with the ad-\nvent of music datasets with lyrics, deep learning was also\nresearched for mining musical knowledge between lyrics and\nmelodies.",
                "There is no large-scale music dataset with emotion labels\npublicly available for emotion-conditioned music generation.",
                "In this work, we build a paired lyric-melody music dataset,\nthe details of the new dataset used to generate lyric and\nmelody with emotions are introduced in this section.",
                "There\nare many different ways to represent music for deep learning,\nthe form of music representation in this work is introduced\nin Section III-A. The basic information of the paired lyric-\nmelody English songs dataset is introduced in Section III-B.\nThe method that we used to annotate music is introduced in\nSection III-C. The detailed analysis of the annotated dataset\nis given in Section III-D.\nA. Data Representation\nInspired by Yu et al.",
                "14, NO. 8, AUGUST 2021 3\nTABLE I\nEXAMPLES FROM GOEMOTIONS DATASET .",
                "B. Data Collection\nThe dataset used in our work comes from two large-\nscale MIDI music datasets: LMD-full MIDI dataset [5] and\nreddit MIDI dataset",
                "There are 176,581\ndifferent MIDI files in the LMD-full dataset, but most of them\ndo not contain lyrics.",
                "In this work we only use the music\ndata with English lyrics, so only 7,497 MIDI files are selected\nfrom the LMD-full dataset.",
                "Similarly, the reddit MIDI dataset\ncontains 130k different MIDI files but only 4,031 with English\nlyrics are selected.",
                "Altogether there are 11,528 MIDI files in\nour dataset.",
                "TABLE II\nEXAMPLES FROM EDMONDS DANCE DATASET .",
                "left me broken and\nbruised but now i know that you were wrong...sadness,\ndisgust,\nanger\nTABLE III\nCLASSIFICATION RESULTS (%) OFBERT MODELS TRAINED\nONGOEMOTIONS DATASET AND EDMONDS DANCE\nDATASET ,TESTED ON EDMONDS DANCE DATASET .",
                "\u201cB OTH\u201d\nMEANS FIRST TRAINED ON GOEMOTIONS DATASET AND\nTHE FINE -TUNED ON EDMONDS DANCE DATASET .",
                "Train dataset Acc Precision Recall F1 score\nGoEmotions 52.44 45.50 55.26 49.93\nEdmonds Dance 77.90 81.82 80.67 81.23",
                "Both 79.02 82.85 81.88 82.31\nC. Data Annotation\nFor the above large-scale dataset, manually labelling emo-\ntions expressed in music by humen is expensive.",
                "Therefore, in\nthis work we exploit the deep learning models to automatically\nannotate the paired lyric-melody dataset.",
                "There are many\ndatasets that can be used to train the annotator, such as large-\nscale social media or dialog datasets with emotion labels [32],\nrelatively small-scale lyric datasets for lyric emotion classifi-\ncation [17], [33], [34] and small-scale emotion-labelled music\ndatasets without lyric [13], [15].",
                "The largest human\nannotated dataset for text sentiment classification is GoEmo-\ntions",
                "Table I shows illustrative samples of GoEmotions dataset,\neach sample text has one or more corresponding labels.",
                "The advantage of GoEmotions dataset is its large scale, but\nthe disadvantage is that there\u2019s a domain gap between Reddit\ncomments and song lyrics.",
                "(a) Pitch distribution of the whole dataset.",
                "(b) Duration distribution of the whole dataset.",
                "(c) Rest distribution of the whole dataset.",
                "Melody distribution of the collected dataset.",
                "(a), (b) and (c) show the distribution of pitch, duration and rest of the whole dataset respectively.",
                "There\u2019s some relatively small-scale lyric datasets manually\nlabelled according to human emotions.",
                "Recently, Edmonds et\nal.constructed Edmonds Dance dataset [17], which consists of\nlyrics retrieved from 524 English songs.",
                "As shown in Table II,\nthere\u2019s 8 emotion categories in the Edmonds Dance Dataset\nand each song has one or more corresponding labels.",
                "Same\nas GoEmotions, the 8 categories are grouped into positive,\nnegative or ambiguous:\n\u2022positive: anticipation, joy, trust\n\u2022negative: anger, disgust, fear, sadness\n\u2022ambiguous: surprise\nIn order to have a common model for emotion classifi-\ncation, we train Bert-base [16] models on GoEmotions and\nEdmonds Dance Dataset.",
                "Bert stands for Bidirectional Encoder\nRepresentations from Transformers [36], which has been pre-\ntrained on large-scale natural language datasets and given\nstate-of-the-art results on a wide variety of natural language\nprocessing tasks.",
                "Because there\u2019s no domain gap between\nEdmonds Dance Dataset and our dataset, we randomly select\n1/10 data from the Edmonds Dance Dataset as test data.",
                "The\nexperimental results are shown in Table III, to our surprise, the\nBert model trained on GoEmotions dataset has relatively worse\nperformance for lyric emotion classification.",
                "However, the Bert\nmodel directly trained on Edmonds Dance Dataset achieves\nbetter performance, despite the in-domain dataset is magnitude\nsmaller than out-of-domain dataset.",
                "In addition, pre-training\nthe Bert model on GoEmotions dataset and then fine-tuningthe model on Edmonds Dance Dataset can slightly improve\nthe classification accuracy of song lyrics.",
                "In order to answer this question, we\ntrain deep learning models on the EMOPIA dataset [15] and\nevaluate if they can be used on our dataset.",
                "The EMOPIA\ndataset consists of 1,078 clips from 387 piano solo perfor-\nmances.",
                "We train a bidirectional LSTM with self-attention to classify\nthe music clips according to their valence, and achieves 83.3%\ntest accuracy on EMOPIA dataset.",
                "Then, this model are used\nto classify the melodies of our dataset, the results are shown\nin Table IV, we can see that the classification results are\ncatastrophically unbalanced, even though the training data in\nEMOPIA dataset is balanced.",
                "Therefore, we think the deep learning\nmodel trained on EMOPIA cannot be used to annatate our\ndataset because of the following reasons: 1) There\u2019s a domain\ngap between piano solo performances and pop songs\u2019 melodies\nin our dataset.",
                "2) EMOPIA is a small-scale dataset.",
                "14, NO. 8, AUGUST 2021 5\nTABLE IV\nANNOTATION RESULTS OF THE BI-LSTM TRAINED ON\nEMOPIA FOR OUR DATASET ,ALL SEGMENTS ARE\nLABELLED TO HIGH-VALENCE OR LOW-VALENCE .",
                "LengthAnnotationsTotal\nHigh-valence Low-valence\n20 25,743 77,797 103,540\n50 7,069 36,833 43,902\n100 2,699 20,163 22,862\nTABLE V\nANNOTATION RESULTS FOR OUR DATASET ,ALL SEGMENTS\nARE LABELLED TO POSITIVE ,NEGATIVE OR UNLABELLED .",
                "Then, the Bert model trained on GoEmotions dataset and then\nfine-tuned on Edmonds Dance Dataset is used to annotate\nthese music segments.",
                "Table V shows the annotation results for our dataset, from\nwhich we can see that about 64% are labelled, and the number\nof positive segments is larger than the number of negative\nsegments.",
                "Examples form the annotated dataset are shown\nin Table VI.",
                "Detailed quantitative comparison of melody\ndistributions is shown in Table VII, it shows that the pitch,\nduration and rest distributions of positive and negative samples\nare pretty similar to the whole dataset.",
                "Firstly, syllable-level and word-level\nskip-gram models are trained on the whole dataset, which aim\nat mapping each English word and syllable to a vector [39].",
                "VI\nEXAMPLES FROM ANNOTATED DATASET .",
                "INCLUDE THE WHOLE DATASET (WD),\nPOSITIVE AND NEGATIVE .",
                "[39] trained on the whole lyrics dataset, we keep\nmost of the hyper-parameter settings in [28] for training the\nskip-gram models: tokens context window c= 7, negative\nsampling distribution parameter \u03b1= 0.75, and the learning\nrate is set to 0.03 with a gradually decay.",
                "Algorithm 1 Emotional Lyric and Melody Generator\nRequire: labelled and unlabelled dataset XlandXu, required\nemotion e, piece of seed lyric m\n1:Initialize word embedding Ew\n2:Initialize syllable embedding Es\n3:forx\u2208Xl\u222aXudo\n4: Update EwandEs\n5:end for\n6:Initialize lyric and melody generator G\n7:forx\u2208Xl\u222aXudo\n8: Update G\n9:end for\n10:Initialize music sentiment classifier C\n11:forx\u2208Xldo\n12: Update C\n13:end for\n14:y\u2190EBS(G, C, m, e )\n15:return y,Ew,Es,G,C\ndenoted as Ew(\u00b7)andEs(\u00b7)respectively.",
                "where Eis the number of emotions in the dataset and ||\nrepresents the concatenation operation.",
                "TABLE VIII\nEMOTION CLASSIFICATION ACCURACY (%) OFLSTM AND\nTRANSFORMER ON DIFFERENT DATASETS WITH DIFFERENT\nLENGTH .",
                "datasetsLength\n20 50 100\nBidirectional LSTM 99.8 99.9 99.9\nSelf-attention Transformer 100.0 99.9 99.9\nA. Emotion Classifier",
                "As shown in Table V, the number of positive\nsamples is larger than the number of negative samples, so over-\nsampling method is used to overcome the imbalance problem\nof the dataset, which means repeatedly using negative samples\nin every epoch, so that the ratio of positive and negative\nsamples in the training data is close to 1:1.",
                "Table VIII shows the emotion\nclassification accuracy of all datasets created in Section III-C,\nfrom it we can see that both the LSTM and Transformer based\nmodels can successfully classify the datasets.",
                "Therefore we\ncan use the classifier trained on labelled data of the datasets\nin EBS algorithm.",
                "B. Music Generation\nThe lyric-melody generator is an encoder-decoder model\ntrained end-to-end on the unlabelled datasets.",
                "The\nbatch size is set to 64, 32, 16 for datasets with length 20, 50,\n100 respectively.",
                "We\nalso implement AutoNLMC on our dataset for comparison,\nwhich is a sequence to sequence model consists of one encoder\nand multiple decoders proposed in [30].",
                "It demonstrates the Transformer has\nstronger learning ability and can better fit the dataset.",
                "Then, we generate lyrics and melodies by using the EBS\nalgorithm introduced in Section IV-C. We use 5 different seed\nlyrics: \u201cI give you my\u201d, \u201cbut when you told me\u201d, \u201cif I was\nyour man\u201d, \u201cI have a dream\u201d, \u201cwhen I got the\u201d and different\ngenerators trained on various datasets (length = 20, 50, 100)\nwith various skip-gram models (dimension = 10, 50, 100, 128).",
                "Distributions of ground-truth melody and generated melody on the testing dataset.",
                "We invited volunteers to evaluate the music data selected from\nthe ground-truth dataset, music segments generated by GRU\nbased model and Transformer based model.",
                "This demonstrates that emotions in our dataset are mainly\nconveyed by lyrics and the ELMG system proposed by us\nsuccessfully learned to generate lyrics to represent the required\nemotion.",
                "We can see that both GRU based generator and Transformer\nbased generator can successfully generate music segments of\nalmost the same high quality as the training dataset.",
                "We think\nthat the quality of the dataset is the bottleneck of our ELMG\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "The ELMG system has the potential to generate music\nwith higher quality if a better dataset is given.\nVI.",
                "In this paper, we construct a large-scale paired lyric-\nmelody dataset with emotion labels and propose Emotional\nLyric and Melody Generator (ELMG) system for emotion-\nconditioned music generation.",
                "Firstly, we find that dataset\nannotators trained on in-domain data are more reliable than\nmodels trained on out-of-domain data.",
                "Then, both GRU and\nTransformer based encoder-decoder network trained on our\ndataset successfully learned to compose lyric and melody.",
                "The new dataset created in this work only has single track\nin the melody and the emotion annotator only focus on the\nlyric.",
                "The quality of the dataset limits the effectiveness of our\nproposed model.",
                "Collect large-scale polyphonic music dataset\nwith emotion labels is a valuable further work for us.",
                "[5] https://colinraffel.com/projects/lmd/.\n[6] https://www.reddit.com/r/datasets/.\n[7] H.-W. Dong, W.-Y .",
                "Yang,\n\u201cEmopia: A multi-modal pop piano dataset for emotion recognition\nand emotion-based music generation,\u201d arXiv preprint arXiv:2108.01374,\n2021.",
                "[32] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen, G. Nemade,\nand S. Ravi, \u201cGoemotions: A dataset of fine-grained emotions,\u201d arXiv\npreprint arXiv:2005.00547, 2020.",
                "[33] E. C \u00b8 ano and M. Morisio, \u201cMoodylyrics: A sentiment annotated lyrics\ndataset,\u201d in Proceedings of the 2017 International Conference on Intelli-\ngent Systems, Metaheuristics & Swarm Intelligence, 2017, pp."
            ]
        },
        {
            "title": "Generating Music with Generative Adversarial Networks and Long Short-Term Memory",
            "architecture": [
                "D. Potential reasons and further improvements \nWith the increase of training times, the generated WAV files \nare broken, all of these clues point in one direction: there ma y \nbe some problems with the GAN model architecture."
            ],
            "dataset": [
                "Here, the sample rate of the music in our dataset is 44. 1 \nkHz.",
                "With these pieces of music, the dataset can be generated by \nsplitting the music into small segments of n samples.",
                "Figure 6 Workflow  \nDataset size and the number of the training epoch are two \nkey elements that played a decisive role in the success or fail ure \nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "With the same dataset, the training epoch had been \nincreased from 3 epochs to 4, 5, 6, 7, 8, 40 and 50 epochs ( Table \n3)."
            ]
        },
        {
            "title": "Generation of Music With Dynamics Using Deep Convolutional Generative Adversarial Network",
            "architecture": [
                "DCGAN was chosen to be the deep \nlearning architecture used in this paper.",
                "There are many deep learning architectures used for music \ngeneration system.",
                "[1, 2] is the most \ncommonly used architecture in music generation system.",
                "Generator Architecture \nC. Discriminator \nFor the discriminator, it was designed in the reversed \norder of the generator.",
                "Discriminator Architecture \nD. Model Training"
            ],
            "dataset": [
                "With the piano-roll data \nrepresentation, Deep Convolutional Generative Adversarial \nNetwork (DCGAN) learned the data distribution from the \ngiven dataset and generated new data derived from the same \ndistribution.",
                "It can capture and \nlearn the data distribution given a dataset and generating a \nsample from the same distribution.",
                "Piano-roll Representation \n B. Dataset \nThe Oracle Hip Hop Sample Pack from Cymatics"
            ]
        },
        {
            "title": "Monophonic Music Generation With a Given Emotion Using Conditional Variational Autoencoder",
            "architecture": [
                "In [35], Valenti et al. presented the architecture for\nmusic generation that is based on an adversarial autoencoder.",
                "Available:\nhttp://arxiv.org/abs/1704.01444\n[34] G. Hadjeres, F. Nielsen, and F. Pachet, ``GLSR-VAE: Geodesic latent space\nregularization for variational autoencoder architectures,'' in Proc."
            ],
            "dataset": [
                "The training dataset was extracted\nfrom video game soundtracks in MIDI format, a part of which\nwas annotated according to a two-dimensional model that\nrepresents emotion using valence-arousal.",
                "Section III\ndescribes the phases of building a music dataset and the\nemotion model used in the experiments.",
                "MUSIC DATASET\nA. PREPARING OF SYMBOLIC MUSIC DATASET\nThe \u001crst phase of building a music generating system is build-\ning or selecting a database with musical compositions.",
                "In [39], Dong et al. studied\nkey mode distributions of different music datasets, among\nothers (Lakh MIDI Dataset, Wikifonia Lead Sheet Dataset,\nHymnal Dataset, J. S. Bach music21 Dataset).",
                "They found\nthat key mode distributions (minor, major) in most databases\nwere rather imbalanced, with the exception of the J. S. Bach\nmusic21 Dataset, where the occurrence of major compo-\nsitions is equal to 56% in relation to the whole.",
                "A fairly\neven key mode distribution of compositions is important\nwhen creating a database in which emotions will be assessed,\ntherefore the J. S. Bach music21 Dataset was selected as the\nstarting database for building the training set.",
                "The music generation system created in this work should\ngenerate monophonic sequences, therefore the original\nJ. S. Bach music21 Dataset underwent several transforma-\ntions (Fig. 1).",
                "Transformations of J. S. Bach music21 dataset.",
                "Another transformation is the limitation of the music exam-\nple length to four bars and the selection of pieces only\nin a 4/4 time signature, which prevail in the J. S. Bach\nmusic21 Dataset, but which resulted in a reduction in the\nnumber of examples in the dataset.",
                "Another transformation concerned the keys of the exam-\nples, which vary greatly in the J. S. Bach music21 Dataset.",
                "B. DATASET ANNOTATION",
                "Since the music generation system will learn using mono-\nphonic melodies, all MIDI \u001cles from the dataset have been\nencoded into pitch-based representation using the MusPy\nToolkit.",
                "The length of each example from the dataset corresponds\nto four bars in a 4/4 time signature, which is four quarter\nnotes per bar, making a total of 16 quarter notes.",
                "The shortest\nnote value in the dataset is sixteenth notes, and therefore\nexamples with sixteen notes were discretized.",
                "Thus, each\nMIDI \u001cle from the dataset was encoded into a pitch-based\nrepresentation with 64 time steps.",
                "After processing the MIDI dataset, the number of different\npitch notes was reduced to 29, which after adding rest and\nhold tokens gives a total of 31 different tokens in a sequence,\nwhich were additionally one-hot encoded.",
                "C. EVALUATION OF RESULTS USING EXPERT OPINIONS\nThe same method that was used to label the training dataset\n(Section III-B), i.e. asking the same three music experts\nwith a university music education to annotate the emotion of\nthe generated music \u001cles, was used as a second method of\nevaluating the generated music sequences.",
                "[14] M. Khateeb, S. M. Anwar, and M. Alnowami, ``Multi-domain feature\nfusion for emotion classi\u001ccation using DEAP dataset,'' IEEE Access ,\nvol."
            ]
        },
        {
            "title": "Music Deep Learning: A Survey on Deep Learning Methods for Music Processing",
            "architecture": [
                "For this purpose,\nvaluable information is extracted using MIR techniques and\nthen different DL architectures are usually tested [6].\nFig.",
                "DL METHODS FOR MIR\nThe DL architectures that are most frequently employed for\nMIR tasks are: i) Recurrent Neural Networks (RNNs), and\nii)Convolutional Neural Networks (CNNs).",
                "In Table I, the\nmost common used DL architectures applied on MIR tasks\nare summarized.",
                "I\nDL METHODS FOR MIR\nDL Architectures Applications Research Paper\nRNNs Feature extraction [11] - [14]\nLSTMs Emotion prediction [10]\nCNNs Feature extraction [16] - [25], [27]\nUnsupervised Learning Sound representations",
                "[14] different variants of RNN\narchitectures are employed in order to tackle such problems.",
                "In [27] attention augmented CNNs were trained to\nrecognize musical instruments, outperforming the classical\nCNN architectures.",
                "In Table II, the most common used DL\narchitectures applied on MG tasks are summarized.",
                "TABLE II\nDL METHODS FOR MG\nDL Architectures Applications Research Paper\nRNNs Music generation",
                "Classical\nRNN architectures have been tested on various MG tasks [30]\n- [35].",
                "Although\nattention mechanisms seem to promise better results in both\nMIR and MG, it is likely that standalone architectures will\nnot outperform the current ones.",
                "On the contrary, combined\narchitectures which leverage the individual characteristics of\neach model are going to dominate the field in the near future.",
                "The different\nDeep Learning models and a review of the state-of-the-art\narchitectures were thoroughly surveyed, while future research\ndirections were highlighted.",
                "\u201cCNN architectures for large-scale audio classifica-\ntion.\u201d"
            ],
            "dataset": [
                "Singal and S. Aggarwal, \u201dLSTM based Music\nGeneration with Dataset Preprocessing and Reconstruction Techniques,\u201d\n2018 IEEE Symposium Series on Computational Intelligence (SSCI),\n2018, pp."
            ]
        },
        {
            "title": "Music Generation using Deep Generative Modelling",
            "architecture": [
                "But this \narchitecture is trained  on a fixed number of chords and is \nthus unable t o create a more diverse combination of notes.",
                "2. Representation of musical waveforms  \nHere, we have used random interpolation to generate music on \na scaled  GAN  architecture."
            ],
            "dataset": [
                "The following results, as seen in Figures 2,3 and 4, were \nachieved when a GAN based model was trained on a dataset \nconsisting of Bach\u2019s musical symphonies.",
                "Pitch vs Time representation of dataset used for random interpolation  \nFollowing the training of the model using this data, the time \ninterval for random interpolation between instruments was set \nas 5 (sec/instrum ent) and constant Q -spectrogram of the \ngenerated music was obtained.  \nFig.",
                "Constant -Q spectrogram of generated music  \nThus, from our dataset, we were able to randomly subsample, \ninterpolate and generate 656 samples."
            ]
        },
        {
            "title": "Music Generation using Deep Learning with Spectrogram Analysis",
            "architecture": [],
            "dataset": []
        },
        {
            "title": "Music Generation with AI technology: Is It Possible?",
            "architecture": [
                "[0 0\n1 10 0\n0 0\n0\n10\n10\n00\n0]  \nAlso, while holding a note is not the same as replaying a note, \nwe need to distinguish these two events, so \ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66  is also \nneeded which looks similar to \ud835\udc61\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66. \n2) Architecture \nBiaxial LSTM generates polyphonic music by modeling \nevery note in every time step as a probability, using all previous \ntime steps and all notes already generated in the current time step \nas the condition.",
                "\ud835\udc73\ud835\udc85\ud835\udc9a\ud835\udc8f\ud835\udc82\ud835\udc8e\ud835\udc8a\ud835\udc84\ud835\udc94 =\u2211\ud835\udc95\ud835\udc91\ud835\udc8d\ud835\udc82\ud835\udc9a(\ud835\udc95\ud835\udc85\ud835\udc9a\ud835\udc8f\ud835\udc82\ud835\udc8e\ud835\udc8a\ud835\udc84\ud835\udc94 \u2212\ud835\udc9a\ud835\udc85\ud835\udc9a\ud835\udc8f\ud835\udc82\ud835\udc8e\ud835\udc8a\ud835\udc84\ud835\udc94 )\ud835\udfd0 \n3) Architecture",
                "The main difference of the architecture of DeepJ model \ncompared with Biaxial LSTM is that we use style conditioning  \nat each layer.",
                "Architecture of DeepJ  \n1267\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "7. Flow chart of pre -processing  \n6) Architecture  \nGANs  implements adversarial learning mainly by \nconstructing two sub -network generators and discriminators.",
                "For the three models, although they are all cutting -edge \nresearch in the field of AI -generated music, they all have very \ndifferent model architectures, which results in different music \nbeing generated by each of them."
            ],
            "dataset": [
                "In DeepJ model, we use a dataset of 23 different composers \nfrom different classical music periods.",
                "In the process of data pre -processing, authors found that the \ntracks in original dataset tend to play only a few notes in the \nentire songs.",
                "The authors stored the pre -processed data in Lakh Piano \nroll Dataset (LPD), and the subsequent LPD -5-matched dataset \nsaved data with higher confidence so that to improve the training \nof the model.",
                "In more technical terms, the goal of the generator is \nto generate samples that match the data distribution of the \ntraining dataset.",
                "Therefore, the features considered in the \ndatasets are also different.",
                "the research objectives, features in datasets and the \nevaluation methods taken by DeepJ model and the MuseGAN \nmodel.",
                "C. Features  in Datasets  \n1) DeepJ model  \nThe dataset of DeepJ includes MIDI files of pieces \ncomposed by 23 well -known composers in the three major \nperiods of class ical music (Baroque period, classical period, and \nromantic period).",
                "Besides, in this dataset, the temporal \nresolution was set as 24 time steps for one beat, thus, the concept \nof velocity was ignored."
            ]
        },
        {
            "title": "Music Generation with Bi-Directional Long Short Term Memory Neural Networks",
            "architecture": [
                "This paper is organized as follows: Section II describes, in \nbrief, the literature survey; Section III explains the network \narchitecture which includes the deep learning models \nimplemented; the methodology of implementation is \ndiscussed in section IV; Section V shows the results of the \nsurvey conducted to compare the computer -generated music \nwith human compositions; Section VI finally concludes the \npaper.",
                "Bretan et al. have used autoencoder s for predicting the \nnext four beats in a musical sequence by comparing a variety \nof these architectures and their objective functions.",
                "As mentioned in the methodology section, the \narchitecture of the suggested model is based on two \nbidirectional LSTM layers functioning as an encoder -decoder \nsystem with a self -attention layer between them to effectively \nutilize long sequences from the first b i-directional LSTM and \nformulate a weighted input for the second one.",
                "This approach to composition renders a simple LSTM \nbased architecture inadequate, and hence  our proposed system \nimplements bi -directional LSTM .",
                "1.   Representation of Bi -Directional LSTM  \n \nBidirectional LSTM layers are stacked in the proposed \nsystem forming a model where the output of one layer serves \nas the input for another, which significantly increases the \nperformance of this architecture by providing a cascading \ncumulative effect while processing the data [14].",
                "To integrate this approach, \nattention -based architecture is used.",
                "The training step consists of the architecture specified \nearlier in the form of a deep learning network using bi -lstm \nand self -attention layers.",
                "Further, the proposed \nmodel is able to overcome the drawbacks of the other \narchitectures such as GANs and Auto -Encoders by not being \nsusceptible to mode -collapse or generating overly repetitive \nmusic respectively."
            ],
            "dataset": [
                "It involves \ntraining  the model  on a large  dataset  of musical  input  and \nusing it to produce similar music with slight variations.",
                "In this paper,  the training  is done  using  the MIDI  and \nAudio  Edited  for Synchronous  Tracks  and Organization \n(MAESTRO) dataset  [15].",
                "It  consists  of audio recordings in \nMIDI  format  and has a duration  of 172 hours  which  is \nsignificantly  more  than other  datasets  having  similar  music.",
                "Two separate LSTM models were developed in \n[11] for training dynamics and tempo separately over a dataset \nof Chopin\u2019s mazurkas.",
                "Despite a smaller dataset, the model \nsuccessfully recognized the real -world dependencies .",
                "A recurrent \nneural network -based model is trained on the MAESTRO \ndataset [20].",
                "The dataset used to train the network consists of 200 piano \nsongs from the MAESTRO dataset provided by Magenta \n[17].",
                "In addition, \nthe length of the output layer is equal to the total number  of \nthe unique combinations of notes present in the dataset.",
                "[17] Mateusz Modrzejewski, Mateusz Dorobek & Przemys\u0142aw Rokita, \n\u201cApplication of Deep Neural Networks to Music Composition Based \non MIDI Datasets and Graphical Representation,\u201dArtificial \nIntelligence and Soft Computing, 2019, Volume 11508, ISBN : 978 -3-\n030-20911 -7 \n[18] Zhang, Z., Luo, C., and Yu, J. Towards the gradient vanishing, \ndivergence mismatching and mode collapse of generative adve rsarial \nnets.",
                "[20] Curtis Hawthorne , Andriy Stasyuk , Adam Roberts , Ian Simon , Cheng -\nZhi Anna Huang , Sander Dieleman , Erich Elsen , Jesse Engel , Douglas \nEck, \u201cEnabling Factorized Piano Music Modeling and Generation with \nthe MAESTRO Dataset,\u201d 2019 International Conference on Learning \nRepresentations(ICLR) , 2019, arXiv:1810.12247"
            ]
        },
        {
            "title": "RE-RLTuner: A topic-based music generation method",
            "architecture": [
                "As Long Short-Term Memory (LSTM) networkarchitectures excel in modeling sequential information indata, LSTM based music generation method is proposed\n1CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems,\nShenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\nShenzhen 518055, China.",
                "We use the deep Q learning(DQN) reinforcement learning\narchitecture."
            ],
            "dataset": [
                "MODEL DETAIL\nA. Dataset\nIn the \ufb01eld of neural network based music generation,\nmusic data format can be divided into audio and symbolrepresentations.",
                "We use the Nottingham dataset, a dataset offolk songs, pitch range from 53 to 88, which we quanti\ufb01edinto 4/4 beats.",
                "A. Objective Evaluation\nTo verify the validity of the model, we adopted the\nfollowing indexes for quantitative analysis as in [5], [12].\nPC/bar PI IOI Auto-lag1 Auto-lag2 Auto-lag3\ndataset 1.51 25.7 1.49 0.88 0.77 0.67\nLSTM 2.73 20.0 13 -0.41 0.44 -0.37\nRLTuner 3.6 28.02 18 -0.03 -0.03 -0.03\nRE-RLTuner 2.9 17.5 11 \u22120 .10 0.003 \u22120",
                "We found that the improved methodis better than RLTuner in PI, IOI, and correlation indexescompared to those of the dataset.",
                "We randomly chose four music\nsegments from each of the LSTM, RLTuner, Re-RLTuner,and a piece of original music from the dataset.",
                "We askedvolunteers to listen to the original music from the dataset andbased on that to rate extracted music samples individually.",
                "In addition, they were also asked to evaluate how closeeach of the three types of music samples to the music fromthe dataset."
            ]
        },
        {
            "title": "Some Reflections on the Potential and Limitations of Deep Learning for Automated Music Generation",
            "architecture": [
                "We are going\nto overview the models and techniques behind thelatest works in this \ufb01eld, highlighting the strengths\nand weaknesses of their proposed architectures in\nSection II.",
                "This similarity\nis the reason Convolutional Neural Networks, the\nmost common and successful deep architecture for\nimages, can be applied to music.",
                "This architecture has demon-\nstrated it\u2019s effectiveness in language modeling [8],\ntext generation and translation [9], as well as with\nimage captioning [10] and generation [11].",
                "The autoencoder architecture consists of two\nspecular networks called encoder and decoder.",
                "V AEs have been used for music generation in [19]\nwhere the authors also experimented with a stacked\narchitecture.",
                "Both models are based on CNNs,\nbut while MidiNet uses a fairly classical CNN\narchitecture, MuseGan uses an unprecedented model\nwith a strong hierarchy that is based not on the\nsingle not but on the bar as a unit, featuring a bar\ngenerator controlled by a phrase generator.",
                "It has to be noted that\nGANs are still affected by the issues of LSTM and\nCNN if those architecture are part of the generator\nand discriminator networks."
            ],
            "dataset": [
                "One way to overcome this is to impose a\nconstraint on the structure of the music, both at the\narchitectural level and in the dataset.",
                "Nonetheless this shows how, with the right\ndataset and constraints, these kind of models can\n28\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ]
        },
        {
            "title": "The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation",
            "architecture": [
                "It makes more\nsense to embed the notion of music structure into the model\narchitecture and generative procedure.",
                "As far as we know,\nthis is the \ufb01rst attempt in applying WaveNet to symbolic music\ngeneration (The name of WaveNet implies its usage on audio\napplications, but in theory the temporal-CNN architecture can\nalso be used for symbolic generation).",
                "An important variation is the bidirectional architecture.",
                "C. LSTM Architecture",
                "D. W aveNet Architecture\nWaveNet proposed to use a stack of dilated temporal convo-\nlution layers",
                "The WaveNet architecture, reproduced from [15].",
                "The overall architecture and a\nstack of dilated convolutions is shown in Fig. 4 and Fig.",
                "Also, unfortunately,\nthe current WaveNet architecture is restricted to one-way\nmusic generation."
            ],
            "dataset": [
                "E XPERIMENTS\nA. Dataset",
                "Bene\ufb01t from the short-term memory structure\nand the explicit input, it is natural for the LSTM model to\ncapture innate structures in the dataset."
            ]
        },
        {
            "title": "PopMNet: Generating structured pop music melodies using neural networks",
            "architecture": [
                "3.Architecture of SGN.",
                "4.Architecture  of MGN.",
                "The architecture  of MGN is illustrated  in Fig.4."
            ],
            "dataset": [
                "Unfortunately,  an analysis  on our dataset  shows  that only 3.35% of pairs have the tonal sequence  relation  \u2014 too few to learn \nan accurate  model.",
                "Fig.5a shows  \nthe adjacency  matrices  of some melody  structures  in our dataset.",
                "Statistics  on our dataset  shows  that about  99.83%  \nof pitches  are between  C2 to C5.",
                "Dataset\nMIDI \ufb01les typically  represent  chords  with notes and it is di\ufb03cult  to distinguish  between  chord  progressions  and melodies.",
                "The dataset  is public  available.1\n5.2.",
                "Graph  visualization\nFig.5visualizes  the structures  of several  melodies  in the dataset  and the structures  generated  by SGN.",
                "We extracted  the structure  \nof the \ufb01rst 32 bars of each lead sheet in the dataset  using Algorithm 1, which  resulted  in 3,859 structures  in total.",
                "\u2022PopMNet-Real: same as PopMNet  but the melody  structures  used in the melody  generation  are extracted  from the \ndataset.",
                "The AttentionRNN,  LookbackRNN,  and MidiNet  were trained  on our dataset  with the same setting  in their original  papers.",
                "The Music  Transformer  was also trained  on our dataset  but the setting  of the model  is a little bit different  from the original  J. Wu et al. /",
                "Despite  efforts  in tuning  its \nhyperparameters,  we failed to obtain  satisfactory  results  with MusicVAE  after training  it on our dataset.",
                "To obtain  good \nresults,  MusicVAE  may require  a large dataset  for training.",
                "In the original  work [3], the training  dataset  consisted  of 1.5 \nmillion  MIDI \ufb01les.",
                "The chord  progressions  and primer  melodies  were randomly  sampled  from lead sheets  in the \ndataset.",
                "Let udenote  the distribution  of melodies  generated  by a model  and vdenote  the distribution  of the dataset.",
                "Then l(u, v)\ndenotes  the structure  metric  of the model  on the dataset.",
                "Among  the four existing  models,  Music  Transformer  per-\nformed  the best in capturing  repetitions  in the dataset.",
                "Table 1\nThe percentages  of relations  in the melodies  in the dataset  and the melodies  \ngenerated  by different  models.",
                "Besides  melodies  generated  by PopMNet  and Music  Transformer,  10 melodies  sampled  from the dataset  were also evaluated.",
                "One reason  is that almost  all structures  of generated  melodies  contained  many rhythmic  sequences  (Figs.7, 9) and \ntheir distributions  had small wasserstein  distances  to the distribution  of the rhythmic  sequence  of the dataset  (Table2).J. Wu et al. /"
            ]
        },
        {
            "title": "Singability-enhanced lyric generator with music style transfer",
            "architecture": [
                "Section 3 describes\nthe lyrics style transfer generation modules, system architecture, model\nconstruct, transfer algorithm, etc. Section 4 describe the datasets and\nevaluation methodology used in experiments and discusses experimen-\ntal design and results.",
                "[4] showed that these architectures can be applied and\nadapted for natural language generation, comparing a number of ar-\nchitectural and training schemes.\n2.2.",
                "Generative pretrained transformer\nThe GPT-2 architecture is very similar to the Transformer model de-\ncoder structure.",
                "The Google Brain\nteam introduced the Transformer [5], which incorporates an encoder\u2013\ndecoder architecture to create a sequence to sequence (Seq2Seq) model\nwithout using convolutional (CNN) or recurrent (RNN) neural net-\nworks.",
                "Fig. 2-2 shows\nthe Transformer encoder and decoder architecture.",
                "2-3 shows GPT-2 architecture.",
                "The framework applies GPT-2 with several conditions, known as a\nconditional GPT-2, with architecture as shown in Fig. 2-4.",
                "Architecture of Transformer\u2019s encoder and decoder",
                "Typical GPT-2 architecture",
                "[10] offers a range of tools to process human natural\nlanguage and simplify text analysis, collated from research by the\nStanfordNLP Group, including part of speech tagger, named entity\nrecognizer, dependency parsing, etc. StanfordNLP is built using highly\naccurate neural networks, allowing efficient training and evaluation\nusing your own marker data, and these modules are based on the\nPyTorch architecture.",
                "This paper used a multilayer RNN encoder and\nmultilayer RNN with attention for decoding, creating a style transfer\nspecific architecture.",
                "2-9 shows the overall architecture.",
                "[15] proposed a simple AWD-\nLSTM neural language model architecture and pretrained weights for\nstyle-specific text generation.",
                "Overall architecture for Jhamtani et al.",
                "Jin et al. proposed a multitask\nframework that adopts Seq2Seq based on Transformer architecture to\nsummarize styles and introduces style-guided encoder attention into the\nmulti-head attention module.",
                "Fig. 2-12\nshows the proposed model architecture.",
                "System architecture of lyrics style transfer generation\nThis study demonstrates the effectiveness of the GPT-2 model in\ngenerating stylized lyrics.",
                "3-1 shows the overall architecture for the proposed method,\nwith more detailed description in subsequent sections.",
                "Model architecture proposed by Gao et al.",
                "Model architecture proposed by Jin et al.",
                "The model architecture of Syed et al."
            ],
            "dataset": [
                "Then the rock lyrics dataset\nis trained for model migration and the lyric text is modified using a\npost-processing module to ensure that every line and word in the lyrics\nmatches the audio.",
                "Section 3 describes\nthe lyrics style transfer generation modules, system architecture, model\nconstruct, transfer algorithm, etc. Section 4 describe the datasets and\nevaluation methodology used in experiments and discusses experimen-\ntal design and results.",
                "However, GPT-2 is a very large Transformer based lan-\nguage model that requires training on a large dataset.",
                "The pre-training dataset\ncontains 8 million web pages collected by crawling qualified outbound\nlinks from Reddit.",
                "They divided the data into three style\ndatasets and split style data into sentences for training examples.",
                "[20] proposed an author stylized rewriting language\nmodel (StyleLM) with two parts: unsupervised pretraining with a Trans-\nformer based language model on a large English dataset cascaded\nthem into an encoder\u2013decoder like framework; and author specific\nfine-tuning with denoising Auto-Encoder loss (DAE loss), allowing the\ndecoder to push the target author\u2019s style while rewriting the encoder\ninput text, as shown in Fig.",
                "Experiments were conducted on an English music\ndataset containing pop and rock music.",
                "Model training and fine-tuning\nThe proposed model was trained on a lyrics dataset collected by\nscraping outbound links on the Genius lyrics website, which is the\nworld\u2019s largest collection of song lyrics and crowdsourced musical\n39J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "The dataset included 2024, 1204, 2159, 2299, and\n266 songs in each genre, respectively.",
                "Basically,\nwe take a pretrained model and train it with a specific song style lyricsdataset, leading the model towards generating the specific lyric style.",
                "We had some\nlimitations song lyric dataset size, so we focused on pop and rock styles,\ncombining pop and rock lyric data to form a text file with over 8000\n40J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "Process to re-train GPT-2.\nlines as the training dataset.",
                "The next step was to encode the dataset,\nencoding generates the file.",
                "Abbreviation Name Sentence Representation\n\ud835\udc5b\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc57 Nominal subject Tim defeated Amy nsubj (defeated, Tim)\n\ud835\udc5b\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc57\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60 Passive nominal subject Amy was defeated by Tim nsubjpass (defeated, Amy)\n\ud835\udc4e\ud835\udc5a\ud835\udc5c\ud835\udc51 Adjectival modifier Sam eats red meat amod (meat, red)\n\ud835\udc5b\ud835\udc5a\ud835\udc5c\ud835\udc51 Noun modifier Filled up with water nmod (filled, water)\n\ud835\udc4e\ud835\udc51\ud835\udc63\ud835\udc5a\ud835\udc5c\ud835\udc51 Adverb modifier Genetically modified food advmod (modified, genetically)\n\ud835\udc51\ud835\udc5c\ud835\udc4f\ud835\udc57 Direct object She gave me a raise dobj (gave, raise)\n\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d\ud835\udc62\ud835\udc5b\ud835\udc51 Compound nouns Wait at this bus stop compound (bus, stop)\ndataset including many unique songs accompanied by metadata, i.e.,\ngenre, artist, year, album, and song title.",
                "Each style was fine-tuned to the pretrained model using the\nlyric dataset, and trained according to the original lyrics.",
                "Suppose we have two lyric text datasets X = {x(1),x(2),\u2026,x(m)}\n(original lyrics) and Y = {y(1),y(2),\u2026,y(n)}(output lyrics after GPT-2\nprocessing), with styles SxandSy, respectively (e.g. Sxis pop style and\nSyis rock style).",
                "The dataset is non-parallel, i.e., data does not contain\npairs (x(m),y(n))that describe the same content.",
                "Dataset\nSong lyrics are widely available across the internet in the form\nof user-generated content.",
                "Two large-scale datasets are used in our\nexperiments.",
                "Two large-scale datasets are used in this experiment.",
                "The\nfirst dataset is a large corpus of 7952 English lyrics crawled from the\nGenius lyrics website and used to fine-tune the lyric style conversion\ngeneration model.",
                "The second dataset is the audio of the original lyrics,\nand the DALI dataset [24] was chosen for this study, which is a large\ndataset of audio full sections synchronized with audio, lyrics and notes,\nwith lyrics and notes (of the vocal melody) aligned in time.",
                "Due to its\ndata integrity, the dataset for this study is composed of 5358 songs with\nEnglish lyrics, each of which contains audio, lyrics and midi notes.",
                "G. Meseguer-Brocal, A. Cohen-Hadria, G. Peeters, Dali: A large dataset of\nsynchronized audio, lyrics and notes, automatically created using teacher-student\nmachine learning paradigm, in: Proceedings of 19th International Society for\nMusic Information Retrieval Conference, ISMIR, 2018, September."
            ]
        },
        {
            "title": "Human, I wrote a song for you: An experiment testing the influence of machines\u2019 attributes on the AI-composed music evaluation",
            "architecture": [],
            "dataset": []
        },
        {
            "title": "Rethinking musicality in dementia as embodied and relational",
            "architecture": [],
            "dataset": []
        },
        {
            "title": "deepsing: Generating sentiment-aware visual stories using cross-modal music translation",
            "architecture": [],
            "dataset": [
                "Instead of directly training GANs for music visualization,\nwhich would be very challenging given the lack of appropriate datasets,\nwe propose employing an efficient cross-modal translation approach\nthat allows for translating the audio sentiment space into the visual\nsentiment space, through any pre-trained GAN model.",
                "These estimators can be trivially fit-\nted using annotated datasets for the corresponding domains.",
                "Please refer to Section 4 for more details on the\ndatasets and models used for training these estimators.",
                "It is worth noting that this choice was\nlargely dictated by the current availability of open datasets for training\nsentiment-related attribute extractors.",
                "The visual attribute estimator was pretrained on the\nImagenet dataset and then fine-tuned (after replacing the last regression\nlayer) for 50+10 epochs using the same learning rates ( 10\u22124\u221510\u22125).",
                "For the visual space we\nemployed the statistics of the corresponding training dataset, while\nfor the audio space we use song-level z-score normalization, except\nfrom the third song used for the conducted qualitative experiments,\nwhere using the statistics of the whole dataset led to a wider variety of\ngenerated concepts.",
                "Experimental results\nFirst, we provide quantitative results using the GTZAN dataset\n(Sturm, 2012), along with the results of the conducted user study.",
                "The GTZAN dataset, which contains music segments of 10\ndifferent music genres was used.",
                "The proposed\nmethod was applied on all data provided by the GTZAN dataset and the\nagreement between the audio sentiment and the sentiment expressed\nby the generated images was measured.",
                "Among the most important limitations is the lack of large-scale\nsentiment datasets that can be used for training the aforementionedsentiment extractors.",
                "Using EEG-based methods ( Gauba, Kumar, Roy,\nSingh, Dogra, & Raman , 2017 ) is a very promising future research\ndirections for easily collecting such large sentiment datasets with rel-\natively low effort.",
                "Exploring MIDI datasets.",
                "An analysis of the GTZAN music genre dataset."
            ]
        },
        {
            "title": "ComposeInStyle: Music composition with and without Style Transfer",
            "architecture": [
                "The GAN architecture of the style transfer step\nis built out of the GAN architecture of the second step.",
                "The compositions generated from each architecture\nis finally evaluated by the common pre-trained classifier of the first step.",
                "Using the latest state of the art architectures, a hybrid model is\ndeveloped which capture the entire objective of this paper in a nutshell.",
                "Since\nthere is no exactly relevant work on music composer style transfer,\ncomparison has been done by implementing the model architecture in\na contextually similar work on genre style transfer.",
                "The entire setup\nis same except the model architecture of the comparison paper.",
                "This\ngives an opportunity to compare and contrast the advantages of our\nproposed hybrid style transfer architecture as compared to another style\ntransfer model in a recent work by Brunner, Wang, Wattenhofer, and\nZhao (2018b ).",
                "The architecture has been\ndescribed later in the paper in details.",
                "Another\npaper (Zhu, Park, Isola, & Efros, 2017) introduces the architecture\nfor cycle GAN which is also a significant advancement in the field\nof generative networks.",
                "The models vary in\narchitecture and the authors also extend it to generate accompanying\ntracks for human composed music.",
                "Encoder/Decoder architecture has\nbeen used along with adversarial training to learn music represen-\ntations and skip connections have been used for pitch information.",
                "CNN in Siamese architecture have been used as the\nmodel which has been evaluated using KNN classifiers.",
                "Architecture of the hybrid model\nThe hybrid model in Fig. 2 visualizes the architectures of the\nresearch objectives in a nutshell.",
                "The style transfer GAN architecture focuses on keeping the\nFig.",
                "Architecture of using Vanilla GAN for generating compositions from noise in\nStep 2.\ncontent of composition by composer A and outputs the melody in the\nstyle of the preferred composer B of choice.",
                "Architecture of the vanilla GAN\nThe audio melodies thus generated are in MIDI format which is then\nconverted into raw audio format (wav).",
                "Description of GAN architectures \ud835\udc3a\ud835\udc34,\ud835\udc3a\ud835\udc35,\ud835\udc37\ud835\udc34and\ud835\udc37\ud835\udc35\nIn step 2, paired vanilla GAN has been trained to generate multi-\ntrack polyphonic music.",
                "In this step, composer specific melody isExpert Systems With Applications 191 (2022) 116195\n6S. Mukherjee and M. Mulimani\nFig. 4. High level generator architecture as used in the proposed hybrid model.",
                "5. High level discriminator architecture as used in the proposed hybrid model.\ngenerated from noise.",
                "The overall architecture of a single GAN consists\nmainly of 2 parts: Generator and Discriminator 4.",
                "The architecture of each of the\nindividual components is described in detail below.",
                "Individual Architecture of a single GAN:\n1.Generator: The generator which is used here for step 2 com-\nprises of the following high-level components:",
                "The main parts of the architecture are detailed below.",
                "(1)Generator: Generator works similar to the encoding\u2013decoding\narchitecture.",
                "Full style transfer architecture",
                "6. Low level generator architecture as used in the proposed hybrid model.",
                "Residual network architecture as used in the proposed hybrid model.",
                "8. Low level discriminator architecture as used in the proposed hybrid model.",
                "Complete style transfer model architecture.",
                "The configurations used for the training of the style transfer model\narchitecture are:\n1.",
                "One can note that since there is no recent works on music composer\nspecific composition generation and style transfer, comparison has\nbeen done strictly on the model architectures.",
                "Key differences between the 2 model architectures\nThe main differences between the proposed hybrid style transfer\nmodel and the comparison model are given in Table 8.",
                "Since here\ncomparison is done architecture wise (due to unavailability of work\non composer style transfer), Table 8 captures in detail the major\ndifferences between the 2 models in the next sections.",
                "The style transfer architecture has been trained with only 100\nsongs from each composer.",
                "Vol. 3807 , In Advanced signal processing algorithms, architectures, and\nimplementations (pp. 637\u2013645)."
            ],
            "dataset": [
                "The paper focuses on 3 composer maestros Liszt, Chopin and Schubert taken from\nthe Maestro dataset.",
                "The highest accuracy obtained is\n80% for composer classification using the maestro dataset, 77.27% for the classification of the generated style\ntransferred version of a composition into the target composer class using the pre-trained classifiers.",
                "116195\n3S. Mukherjee and M. Mulimani\nNow the dataset consisting of 100 songs of each of the 3 composers,\nare classified using various well-known ML models like:\n\u2022Basic Models: Decision Tree, K-Nearest Neighbors (KNN), Gaus-\nsian Naive Bayes (GNB), Support Vector Machine (SVM) using\nboth linear and radial kernels, Random Forest.",
                "The GAN is\ntrained by divide and conquer by dividing the dataset with overlap and\nthen concatenating the generated output again with overlap in order\nto minimize information loss.",
                "Dataset\nThe dataset used here consists of 3 composers named Liszt, Chopin\nand Schubert from the Maestro dataset ( Hawthorne et al. , 2019 ).",
                "The\nMIDI files from Maestro v2 dataset is taken, converted to wav format,\npreprocessed and features are extracted from them.",
                "Step 1: Audio data in the MIDI format is taken from the Maestro\ndataset ( Hawthorne et al. , 2019 ) and features are extracted from them.",
                "The dataset: The dataset of 3 composers (Liszt, Chopin and\nSchubert) taken from the Maestro dataset (Hawthorne et al.,\n2019) are used to train both the networks.",
                "(2018b) performs on the dataset after\nevaluation using the classifiers trained in step 1.\n4.7.4.",
                "The dataset used here is the Maestro dataset (Hawthorne\net al., 2019).",
                "Enabling factorized piano music modeling and generation with the\nMAESTRO dataset."
            ]
        },
        {
            "title": "The algorithmic composition for music copyright protection under deep learning and blockchain",
            "architecture": [
                "Theyalsoproposedagenerativearchitecture\nthatcombinedthesemodelswiththepredictionsofacousticclas-\nsifiers."
            ],
            "dataset": [
                "Algorithm performance test\nToverifythesuperiorityoftheMCNNalgorithm,experiments\nareconductedbasedontheCPMGdataset,andtheresultsare\ncompared with three music generation algorithms: VAE\u2013GAN,\nSeqGAN,andRL-RNN.VAE\u2013GANaddstheencodinganddecoding\nprocesstotheGANnetwork."
            ]
        },
        {
            "title": "Towards a Deep Improviser: a prototype deep learning post-tonal free music generator",
            "architecture": [],
            "dataset": []
        },
        {
            "title": "From artificial neural networks to deep learning for music generation: history, concepts and trends",
            "architecture": [
                "Since a few years, there are a large number of\nscienti\ufb01c papers about deep learning architectures andexperiments to generate music, as witnessed in [ 3].",
                "In\n[18], Graves analyzes the application of recurrent neural\nnetworks architectures to generate sequences (text and\nmusic).",
                "Then, we analyze successively: basic types of\narchitectures and strategies in Sect. 7, various ways to\nconstruct compound architectures in Sect.",
                "8and some\nmore re\ufb01ned architectures and strategies in Sect. 9.",
                "An example is the experiment\nconducted by the YACHT dance music band with theMusicVAE architecture\n8from the Google Magenta Project\n[39].",
                "The\nunderlying architecture, named Coconet [ 27], has been\ntrained on a dataset of 306 Bach chorales.",
                "9.5, but, in this section, we will at \ufb01rst consider a\nmore straightforward architecture, named MiniBach\n9[3,\nSection 6.2.2].",
                "The architecture, shown in Fig. 2, is feedforward (the\nmost basic type of arti\ufb01cial neural network architecture) for\na multiple classi\ufb01cation task: to \ufb01nd out the most likely\nnote for each time slice of the three counterpoint melodies.",
                "4.1 Todd\u2019s Time-Windowed and conditioned\nrecurrent architectures\nThe experiments by Todd in [ 61] were one of the very \ufb01rst\nattempts at exploring how to use arti\ufb01cial neural networks\nto generate music.",
                "Although the architectures he proposed\nare not directly used nowadays, his experiments and dis-cussion were pioneering and are still an important source of\ninformation.",
                "He named his \ufb01rst design the Time-\nWindowed architecture, shown in Fig. 4, where a sliding\nwindow of successive time periods of \ufb01xed size is con-sidered (in practice, one measure long).",
                "2 MiniBach architecture\nand encoding\n14In that respect, the Time-Windowed model is analog to an order 1\nMarkov model (considering only the previous state) at the level of a\nmelody measure.",
                "7.2.1 ), in which the output\nis explicitly reentered (recursively) into the input of the\narchitecture, in Todd\u2019s Sequential architecture the reen-trance is implicit because of the speci\ufb01c nature of the\nrecurrent connexions: the output is reentered into the\ncontext units while the input\u2014the plan melody\u2014isconstant.",
                "6.\n4.1.1 Influence\nTodd\u2019s Sequential architecture is one of the \ufb01rst examples\nof using a recurrent architecture and an iterative strategy\n20\nfor music generation.",
                "4 Time-Windowed architecture.",
                "5 Sequential architecture.",
                "Adapted from [ 61]\n17This is a peculiar characteristic of this architecture, as in recent\nstandard recurrent network architecture recurrent connexions are\nencapsulated within the hidden layer (as shown in Sect. 7.2).",
                "19Actually, as an optimization, Todd proposes in the following of his\ndescription to pass back the target (training) values and not the outputvalues.20These and other types of architectures and generation strategies are\nmore systematically analyzed in Sects.",
                "5and7.Neural Computing and Applications (2021) 33:39\u201365 43\n123extra input, named plan, which represents a melody that the\nnetwork has learnt, could be seen as a precursor of con-\nditioning architectures, where a speci\ufb01c additional input is\nused to condition (parametrize) the training of the\narchitecture.21",
                "\u2019\u2019\nThus, these early designs may be seen as precursors of\nsome recent proposals:\n\u2013 Hierarchical architectures, such as MusicVAE [ 53]\n(shown in Fig.",
                "7and described in Sect. 9.3); and\n\u2013 Architectures with multiple time/clocks, such as Clock-\nwork RNN [ 32] (shown in Fig. 8) and SampleRNN",
                "his described initial experiment [ 35], the architecture\nis a conventional feedforward neural network architecture\nused for binary classi\ufb01cation, to classify \u2018\u2018well-formed\u2019\u2019\nmelodies.",
                "Adapted from\n[61]\n21An example is to condition the generation of a melody on a chord\nprogression, in the MidiNet architecture [ 67] to be described in\nSect.",
                "\u2019\u2019\nThe idea of an attention mechanism , although not yet\nvery developed, may be seen as a precursor of attentionmechanisms in deep learning architectures: at \ufb01rst as an\nadditional mechanism to focus on elements of an input\nsequence during the training phase [ 15, Section 12.4.5.1],\nnotably for translation applications, until being proposed as\nthefundamental and unique mechanism (as a full alterna-\ntive to recurrence or convolution) in the Transformerarchitecture [ 64], with its application to music generation,\nnamed MusicTransformer [ 28].\nFig. 7 MusicVAE architecture.",
                "Reproduced from [ 53] with\npermission of the authors\nFig. 8 Clockwork RNN\narchitecture.",
                "But, as dis-\ncussed in Sect. 9, novel types of architectures have also\nbeen proposed.",
                "Before that, we will introduce a conceptual\nframework in order to help at organizing, analyzing and\nclassifying various types of architectures, as well as varioususages of arti\ufb01cial neural networks for music generation.",
                "\u2013Architecture the nature of the assemblage of processing\nunits (the arti\ufb01cial neurons) and their connexions.",
                "\u2013Strategy : the way the architecture will process repre-\nsentations in order to generate\n24the objective while\nmatching desired requirements.",
                "10 Creation by\nre\ufb01nement\u2014Architecture andstrategy\n23Systems refers to various proposals (architectures, systems and\nexperiments) about deep learning-based music generation surveyedfrom the literature.24It is important to highlight that, in this conceptual framework, by\nstrategy we only consider the generation strategy , i.e., the strategy to\ngenerate musical content.",
                "A strategy for training an architecture couldbe quite different and is out of direct concern in this classi\ufb01cation.46 Neural Computing and Applications (2021) 33:39\u201365\n123Note that these \ufb01ve dimensions are not com-\npletely orthogonal (unrelated).",
                "Select a type(s) of architecture andcon\ufb01gurate it;\n4.Train thearchitecture with the examples ;\n5.",
                "6 Representation\nThe choice of representation and its encoding is tightly\nconnected to the con\ufb01guration of the input and the output\nof the architecture, i.e., the number of input and output\nnodes (variables).",
                "Although a deep learning architecturecan automatically extract signi\ufb01cant features from the data,\nthe choice of representation may be signi\ufb01cant for the\naccuracy of the learning and for the quality of the gener-\nated content.",
                "6.1 Phases and types of data\nBefore getting into the choices of representation to beprocessed by a deep learning architecture, it is important to\nidentify the main types of data to be considered, dependingon the phase (training or generation):\n\u2013Training data , the examples used as input for the\ntraining;\n\u2013Generation (input) data , used as input for the gener-\nation (e.g., a melody for which an accompaniment willbe generated, as in the \ufb01rst example in Sect. 3); and\n\u2013Generated (output) data , produced by the generation\n(e.g., the accompaniment generated), as speci\ufb01ed by theobjective.",
                "Architectures that process the raw signal are sometimes\nnamed end-to-end architectures.",
                "The WaveNet\narchitecture [ 47], used for speech generation for the Google\nassistants, was the \ufb01rst to prove the feasibility of sucharchitectures.",
                "26Indeed, at the level of processing by a deep network architecture,\nthe initial distinction between audio and symbolic representation boils\ndown, as only numerical values and operations are considered.",
                "The encoding of a representation (of a musical content)\nconsists in the mapping of the representation (composed of\na set of variables , e.g., pitch or dynamics) into a set of\ninputs (also named input nodes orinput variables ) for the\nneural network architecture.",
                "The advantage of one-hot\nencoding is its robustness against numerical operations\napproximations (discrete versus analog), at the cost of ahigh cardinality and therefore a potentially large number of\nnodes for the architecture.",
                "33The \ufb01gure also illustrates that a piano roll could be straightfor-\nwardly encoded as a sequence of one-hot vectors to construct theinput representation of an architecture, as, e.g., shown in Fig.",
                "2.48 Neural Computing and Applications (2021) 33:39\u201365\n1237 Main basic architectures and strategies\nFor reasons of space limitation, we will now jointly introduce\narchitectures and strategies.34For an alternative analysis\nguided by requirements (challenges), please see [ 4].\n7.1 Feedforward architecture\nThe feedforward architecture35is the most basic and\ncommon type of arti\ufb01cial neural network architecture.",
                "In Todd\u2019s Time-Windowed architecture in Sect.",
                "7.2 Recurrent architecture\nArecurrent neural network (RNN) is a feedforward neural\nnetwork extended with recurrent connexions in order to\nlearn series of items (e.g., a melody as a sequence of notes).",
                "Todd\u2019s Sequential architecture in Sect.",
                "As pointed out in\nSect. 4.1, in modern recurrent architectures, recurrent\nconnexions are encapsulated within the hidden layer, whichallows an arbitrary number of recurrent layers (as shown in\nFig. 13).",
                "7.2.1 Recursive strategy\nThe \ufb01rst music generation experiment using current state of\nthe art of recurrent architectures, the LSTM (Long Short-\nTerm Memory [ 25]) architecture, is the generation of blues\nchord (and melody) sequences by Eck and Schmidhuber in[8].",
                "Another interesting example is the architecture by\nSturm et al. to generate Celtic melodies [ 59].",
                "36The feedforward architecture and the feedforward strategy are\nnaturally associated, although, as we will see in some of the nextsections, other associations are possible.",
                "By sampling a pitch followingthe distribution generated recursively by the architecture,\n41\nwe introduce stochasticity in the process of generation andthus content variability in the generation.\n8 Compound architectures\nFor more sophisticated objectives and requirements, com-\npound architectures may be used.",
                "We will see that, from an\narchitectural point of view, various types of combination42\nmay be used:\n8.1 Composition\nSeveral architectures, of the same type or of different types,\nare composed, e.g.:\n\u2013 A bidirectional RNN, composing two RNNs, forward\nand backward in time, e.g., as used in the C-RNN-GAN\n[44] (see Fig. 16) and the MusicVAE [ 53] (see Fig. 7\nand Sect. 9.3) architectures; and\n\u2013 The RNN-RBM architecture [ 1], composing an RNN\narchitecture and an RBM architecture.",
                "8.2 Refinement\nOne architecture is re\ufb01ned and specialized through someadditional constraint(s), e.g.:\n\u2013 An autoencoder architecture (to be introduced in\nSect. 9.1), which is a feedforward architecture with\nFig.",
                "15,G ]has around one\nchance in two of being selected and A ]one chance in four.\n42We are taking inspiration from concepts and terminology in\nprogramming languages and software architectures [ 57], such as\nre\ufb01nement ,instantiation ,nesting andpattern [13].50 Neural Computing and Applications (2021)",
                "33:39\u201365\n123one hidden layer with the same cardinality (number of\nnodes) for the input layer and the output layer; and\n\u2013 A variational autoencoder (VAE) architecture, which is\nan autoencoder with an additional constraint on the\ndistribution of the variables of the hidden layer (see\nSect. 9.2), e.g., the GLSR-VAE architecture [ 20].\n8.3 Nesting\nAn architecture is nested into the other one, e.g.:\n\u2013 A stacked autoencoder architecture,43e.g., the Dee-\npHear architecture [ 60]; and\u2013 A recurrent autoencoder architecture (Sect. 9.3), where\nan RNN architecture is nested within an autoencoder,44\ne.g., the MusicVAE architecture [ 53] (see Sect. 9.3).",
                "8.4 Pattern\nAn architectural pattern is instantiated onto a given archi-\ntecture(s),45e.g.:\n\u2013 The anticipation-RNN architecture [ 19] that instantiates\ntheconditioning pattern46onto an RNN with the output\nof another RNN as the conditioning input; and\n\u2013 The C-RNN-GAN architecture [ 44], where the GAN\n(Generative Adversarial Networks) pattern (to be\nintroduced in Sect. 9.4) is instantiated onto two RNN\narchitectures, the second one (discriminator) beingbidirectional (see Fig. 16); and\n\u2013 The MidiNet architecture [ 67] (see Sect. 9.4), where\ntheGAN pattern is instantiated onto two convolu-\ntional\n47feedforward architectures, on which a condi-\ntional pattern is instantiated.",
                "Therefore, it is also named an RNN\nEncoder\u2013Decoder architecture.",
                "45Note that we limit here the scope of a pattern to the external\nenfolding of an existing architecture.",
                "46Such as introduced by Todd in his Sequential architecture\nconditioned by a plan in Sect.",
                "47Convolutional architectures are actually an important component\nof the current success of deep learning and they recently emerged as\nan alternative, more ef\ufb01cient to train, to recurrent architectures [ 3,\nSection 8.2].",
                "A convolutional architecture is composed of a succes-\nsion of feature maps and pooling layers [ 15, Section 9][ 3, Sec-\ntion 5.9].",
                "However, we do not detail convolutional architectureshere, because of space limitation and of nonspeci\ufb01city regarding mu-sic generation applications.",
                "Neural Computing and Applications (2021) 33:39\u201365 51\n1238.5 Examples\nFigure 17illustrates various examples of compound\narchitectures and of actual music generation systems.",
                "8.6 Combined strategies\nNote that the strategies for generation can be combined too,\nalthough not in the same way as the architectures: they are\nactually used simultaneously on different components of\nthe architecture.",
                "7.2.2 ,\ntherecursive strategy is used by recursively feedforward-\ning current note into the architecture in order to produce\nnext note and so on, while the sampling strategy is used at\nthe output of the architecture to sample the actual note\n(pitch) from the possible notes with their respectiveprobabilities.",
                "9 Examples of refined architectures\nand strategies\n9.1 Autoencoder architecture\nAnautoencoder is a re\ufb01nement of a feedforward neural\nnetwork with two constraints: (exactly) one hidden layer\nand the number of output nodes are equal to the number of\ninput nodes.",
                "An early example of this strategy is the use of the Dee-\npHear nested (stacked) autoencoder architecture to gener-\nate ragtime music according to the style learnt [ 60].",
                "9.2 Variational autoencoder architecture\nAlthough producing interesting results, an autoencoder\nsuffers from some discontinuity in the generation when\nexploring the latent space.49Avariational autoencoder\n(VAE)",
                "16 C-RNN-GAN architecture with the D(iscriminator)",
                "17 A tentative illustration\nof various examples andcombination types (in color\nfonts) of compound\narchitectures (in black boldfont) and systems (in black\nitalics font) (color \ufb01gure online)\nFig. 18 (left) Autoencoder\narchitecture.",
                "(right) Stacked\nautoencoder (order-2)\narchitecture\nFig.",
                "Another issue is that the semantics (meaning) of the\ndimensions captured by the latent variables is automati-\ncally \u2018\u2018chosen\u2019\u2019 by the VAE architecture in function of thetraining examples and the con\ufb01guration and thus can only\nbe interpreted a posteriori .",
                "[ 68].9.3 Variational recurrent autoencoder (VRAE)\narchitecture\nAn interesting example of nested architecture (see\nSect. 8.3) is a variational recurrent autoencoder (VRAE).",
                "The motivation is to combine:\n\u2013 The variational property of the VAE architecture for\ncontrolling the generation; and\n\u2013 The arbitrary length property of the RNN architecture\nused with the recursive strategy.54\nAn example (also hierarchical) is the MusicVAE archi-tecture [ 53] (shown in Fig. 7, with an example of con-\ntrolled generation in Fig. 21).",
                "9.4 Generative adversarial networks (GAN)\narchitecture\nAn interesting example of architectural pattern is the\nconcept of Generative Adversarial Networks (GAN) [ 16],\nas illustrated in Fig.",
                "The architecture, illus-\ntrated in Fig. 23, follows two patterns: adversarial (GAN)\nand conditional (on history and on chords to condition\nmelody generation).",
                "22 Generative adversarial networks (GAN) architecture.",
                "55Please refer to [ 67]o r[ 3, Section 6.10.3.3] for more details about\nthis sophisticated architecture.",
                "This incremental instantiation strategy has been used in\nthe DeepBach architecture [ 21] for generation of Bach\nchorales.",
                "The compound architecture,56shown in Fig. 25,\ncombines two recurrent and two feedforward networks.",
                "As\nopposed to standard use of recurrent networks, where asingle time direction is considered, DeepBach architecture\nconsiders the two directions forward in time and back-\nwards in time.",
                "Coconet [ 27], the architecture used for implementing the\nBach Doodle (introduced in Sect. 3), is another example of\nthis approach.",
                "It uses a Block Gibbs sampling algorithm forgeneration and a different architecture (using masks to\nindicate for each time slice whether the pitch for that voice\nis known, see Fig. 27).",
                "MidiNet architecture.",
                "Reproduced from [ 67] with permission of the authors\n56Actually this architecture is replicated 4 times, one for each voice\n(4 in a chorale).57The two bottom lines correspond to metadata (fermata and beat\ninformation), not detailed here.56 Neural Computing and Applications (2021) 33:39\u201365\n123An example of counterpoint accompaniment generation is\nshown in Fig.",
                "An example of application to music is the generation\nalgorithm for the C-RBM architecture [ 34].",
                "9.7 Other architectures and strategies\nResearchers in the domain of deep learning techniques for\nmusic generation are designing and experimenting with\nvarious architectures and strategies,61in most cases com-\nbinations or re\ufb01nements of existing ones, or sometimes\nFig.",
                "25 DeepBach architecture for the soprano voice prediction.",
                "Reproduced from [ 21] with\npermission of the authors\n58The architecture is convolutional (only) on the time dimension, in\norder to model temporally invariant motives, but not pitch invariantmotives which would break the notion of tonality.\n59Because of space limitation, and the fact that RBMs are not\nmainstream, we do not detail here the characteristics of RBM (see,e.g., [ 15, Section 20.2] or [ 3, Section 5.7] for details).",
                "However, there is no guarantee that combining a\nmaximal variety of types will make a sound and accurate\narchitecture.62Therefore, it is important to continue to\ndeepen our understanding and to explore solutions as well\nas their possible articulations and combinations.",
                "Control is necessary to inject constraints (e.g., tonality,\nrhythm) in the generation, as witnessed by the C-RBMarchitecture (see Sect. 9.6).",
                "Some challenge is that a deep\nlearning architecture is a kind of black box; therefore, some\ncontrol entry points (hooks) need to be identi\ufb01ed, such as:\nFig.",
                "27 Coconet Architecture.",
                "28 C-RBM Architecture generation algorithm.",
                "Music generated by a\ndeep learning architecture may be very pleasing for less\nthan a minute but usually starts to be boring after a little\nwhile because of the absence of a clear sense of direction.",
                "Structure imposition is a \ufb01rst direction, as in C-RBM, or by\nusing hierarchical architectures as in Music-VAE.",
                "A notable attempt has been proposed for creating\npaintings in [ 9], by extending a GAN architecture to favor\nthe generation of content dif\ufb01cult to classify within exist-ing styles and therefore favoring the emergence of new\nstyles.",
                "Meanwhile, as discussed in Sect. 2.2, we believe\nthat is more interesting to use deep learning architectures toassist human musicians to create and construct music, than\npursuing purely autonomous music generating systems.",
                "9.5.11 Conclusion\nThe use of arti\ufb01cial neural networks and deep learning\narchitectures and techniques for the generation of music (as\nwell as other artistic contents) is a very active area of\nresearch.",
                "Architecture An (arti\ufb01cial neural network) architecture\nis the structure of the organization of computational units\n(neurons), usually grouped in layers, and their weightedconnexions.",
                "Examples of types of architecture are:\nfeedforward (aka multilayer perceptron), recurrent\n(RNN), autoencoder and generative adversarial networks(GAN).",
                "Architectures process encoded representations\n(in our case of a musical content) which have been\nencoded.",
                "Autoencoder A speci\ufb01c case of arti\ufb01cial neural network\narchitecture with an output layer mirroring the input\nlayer and with one hidden layer.",
                "Compound architecture An arti\ufb01cial neural network\narchitecture which is the result of some combination of\nsome architectures.",
                "Conditioning architecture The parametrization of an\narti\ufb01cial neural network architecture by some condition-ing information (e.g., a bass line, a chord progression...)\nrepresented via a speci\ufb01c extra input, in order to guide\nthe generation.",
                "The function used\nfor measuring the distance between the prediction by an\narti\ufb01cial neural network architecture (y \u02c6) and the actual\ntarget (true value y).",
                "Creation by re\ufb01nement strategy A strategy for gener-\nating content based on the incremental modi\ufb01cation of a\nrepresentation to be processed by an arti\ufb01cial neural\nnetwork architecture.",
                "It is used as a cost(loss) function for a classi\ufb01cation task to measure the60 Neural Computing and Applications (2021) 33:39\u201365\n123difference between the prediction by an arti\ufb01cial neural\nnetwork architecture (y \u02c6) and the actual target (true value\ny).",
                "Dataset The set of examples used for training an\narti\ufb01cial neural network architecture.",
                "Decoder feedforward strategy A strategy for generat-\ning content based on an autoencoder architecture in\nwhich values are assigned onto the latent variables of the\nhidden layer and forwarded into the decoder componentof the architecture in order to generate a musical content\ncorresponding to the abstract description inserted.",
                "An arti\ufb01cial\nneural network architecture with a signi\ufb01cant number of\nsuccessive layers.",
                "Encoding The encoding of a representation consists in\nthe mapping of the representation (composed of a set of\nvariables, e.g., pitch or dynamics) into a set of inputs\n(also named input nodes or input variables) for the neuralnetwork architecture.",
                "End-to-end architecture An arti\ufb01cial neural network\narchitecture that processes the raw unprocessed data\u2014\nwithout any pre-processing, transformation of represen-\ntation or extraction of features\u2014to produce a \ufb01nal\noutput.",
                "Feedforward The basic way for a neural network\narchitecture to process an input by feedforwarding theinput data into the successive layers of neurons of the\narchitecture until producing the output.",
                "Feedforward architecture It is the most basic and\ncommon type of arti\ufb01cial neural network architecture.",
                "Generative adversarial networks (GAN) A compound\narchitecture composed of two component architectures,the generator and the discriminator, who are trained\nsimultaneously with opposed objectives.",
                "Hidden layer Any neuron layer located between the\ninput layer and the output layer of a neural networkarchitecture.",
                "The \ufb01rst layer of a neural network\narchitecture.",
                "In deep learning architectures, vari-\nables within a hidden layer.",
                "Layer A component of a neural network architecture\ncomposed of a set of neurons.",
                "A type of recurrent\nneural network architecture with capacity for learning\nlong-term correlations and not suffering from the\nvanishing or exploding gradient problem during thetraining phase.",
                "Multilayer perceptron (MLP) A feedforward neural\narchitecture composed of successive layers, with at leastone hidden layer.",
                "Also named Feedforward architecture.",
                "Neuron The atomic processing element (unit) of an\narti\ufb01cial neural network architecture, inspired by thebiological model of a neuron.",
                "Weights\nwill be adjusted during the training phase of the neuralnetwork architecture.",
                "Node The atomic structural element of an arti\ufb01cial\nneural network architecture.",
                "Objective The nature and the destination of the musical\ncontent to be generated by a neural network architecture.",
                "The name comes from digital circuits, one-hot referringto a group of bits among which the only legal (possible)\ncombinations of values are those with a single high (hot)\n(1) bit, all the others being low (0).Output layer The last layer of a neural network\narchitecture.",
                "Parameter The parameters of an arti\ufb01cial neural\nnetwork architecture are the weights associated witheach connexion between neurons as well as the biases\nassociated with each layer.",
                "Perceptron One of the \ufb01rst arti\ufb01cial neural network\narchitectures, created by Rosenblatt in 1957.",
                "Pooling For a convolutional architecture, a data dimen-\nsionality reduction operation (by max, average or sum)for each feature map produced by a convolutional stage,\nwhile retaining signi\ufb01cant information.",
                "This is the basis of a recurrent\nneural network (RNN) architecture.62 Neural Computing and Applications (2021) 33:39\u201365\n123Recurrent neural network (RNN)",
                "A type of arti\ufb01cial\nneural network architecture with recurrent connex-ions and memory.",
                "Single-step feedforward strategy A strategy for gener-\nating content where a feedforward architecture processes\nin a single processing step a global temporal scoperepresentation which includes all time slices.",
                "Strategy The way the architecture will process repre-\nsentations in order to generate the objective whilematching desired requirements.",
                "Time slice The time interval considered as an atomic\nportion (grain) of the temporal representation used by anarti\ufb01cial neural network architecture.",
                "Time step The atomic increment of time considered by\nan arti\ufb01cial neural network architecture.",
                "The long short-term memory (LSTM) architecture solved the problem.",
                "Software architecture: perspectives on\nan emerging discipline."
            ],
            "dataset": [
                "The\nunderlying architecture, named Coconet [ 27], has been\ntrained on a dataset of 306 Bach chorales.",
                "Therefore, the dataset is\nconstructed by extracting all possible 4 measures long\nexcerpts from the original 352 chorales, also transposed inall possible keys.",
                "Once trained on this dataset, the systemmay be used to generate three counterpoint voices corre-\nsponding to an arbitrary 4 measures long melody provided\nas an input.",
                "Dataset The set of examples used for training an\narti\ufb01cial neural network architecture."
            ]
        },
        {
            "title": "Conditional hybrid GAN for melody generation from lyrics",
            "architecture": [
                "InMaskGAN, [ 18] proposes the actor-critic GAN\narchitecture that uses reinforcement learning to\ntrain the generator, where the in-\ufb01lling techniquemay alleviate mode collapse.",
                "1 Architecture of\nconditional hybrid GAN3194 Neural Computing and Applications (2023) 35:3191\u20133202\n123yp\nt\u00fe1/C24softmax \u00f0ot\u00de: \u00f03\u00de\nHere, softmax \u00f0ot\u00derepresents the multinomial distribution\non the set of all possible MIDI numbers.",
                "Therefore, to evaluate our proposed architecture, we\nuse Self-BLEU in [ 30] to measure the diversity of gener-\nated samples and maximum mean discrepancy (MMD) in\n[31] to measure the quality of generated samples.",
                "During the adversarial training,\nSelf-BLEU values of our C-Hybrid-GAN architecturereach the peak around 45 epochs, decrease until 100\nepochs, and then approach to the stability."
            ],
            "dataset": [
                "Large-scale Chinese language lyrics-melody\ndataset was built to evaluate the proposed learning model.",
                "In our initial work by [ 1], we not only built a large dataset\nconsisting of 12,197 MIDI songs each with paired lyricsand melody, but also have veri\ufb01ed the feasibility of melody\ngeneration from lyrics by LSTM-based deep generative\nmodel [ 8].",
                "In our dataset, the number of distinct MIDI numbersis 100.",
                "The melody-lyrics dataset in [ 1] is utilized in\nour experiment, which contains 13,251 sequences, eachconsisting of 20 syllables aligned with the triplet of music\nattributes { y\np\nt;yd\nt;yr\nt}.",
                "The dataset is split into training,\nvalidation and test sets with the ratio of 8:1:1.",
                "2 Training curves of self-\nBLEU scores on testing dataset\nFig.",
                "Average note duration distance between generatedsequences and sequences from ground truth dataset is\ncalculated and shown in Fig.",
                "Average rest duration\ndistance between generated sequences and sequences from\nground truth dataset is shown in Fig. 7.",
                "4 Training curves of MMD\nscores on testing dataset3198 Neural Computing and Applications (2023) 35:3191\u20133202\n123melodies generated by our model C-Hybrid-GAN,\nC-Hybrid-MLE, and C-LSTM-GAN.",
                "Data Availability The datasets generated during and/or analyzed\nduring the current study are available in [ 1] repository, https://github."
            ]
        },
        {
            "title": "Scene2Wav: a deep convolutional sequence-to-conditional SampleRNN for emotional scene musicalization",
            "architecture": [
                "The proposed architecture, shown in Fig. 2, consists of three modules.",
                "Dataset distribution:\ntrain and test, positive and\nnegative emotion scoresPositive Negative Total\nTrain 1,778 1,404 3,182\nTest 369 772 1,141\nTotal 2,147 2,176 4,3231798 MultimediaToolsandApplications(2021)80: 1793\u20131812Fig.2 Proposed model\narchitecture for Scene2Wav.",
                "This model is also\ntrained by end-to-end manner with architecture differing only in the decoder module.",
                "6 Baseline model architecture for ConvSeq2Seq consisting of two modules: emotional visual feature\nextraction with CNN, and sequence encoding and decoding with an Encoder-Decoder Deep RNN framework\n5.2 Examplesofgeneratedsamples\nExperimental results are showcased in Figs."
            ],
            "dataset": [
                "Others have used the IAPS dataset to train a Sup-\nport Vector Classifier to classify 1D emotion in paintings, successfully demonstrating the\npotential of machines deriving emotion from images [ 40], or used it to collect \u201cdescriptive\nemotional category data\u201d with the aim of identifying images that evoke one emotion more\nthan others [ 16].",
                "Researchers and developers can have access to our code,1\nincluding data pre-processing and proposed model, and replicate it with their own dataset.",
                "Section 3describes the dataset and pre-preocessing steps.",
                "On top of excelling in the mentioned\ntasks, another advantage of using this network is the fact that it takes as input minimally\nprocessed images, abolishing the need of hand-crafted features and making it general for a\nplethora of datasets.",
                "This model hierarchically combines sample-level modules as multilayer perceptrons\nand frame-level modules as RNNs in order to capture long-term dependencies in the\ntemporal sequences, and it does so on three different datasets.",
                "3 Datasetandpre-processing\nOne of the challenges in this work is finding an appropriate dataset with all the required\ninformation needed for our task.",
                "The dataset is divided in such a way as to guarantee disjoint\ntrain and test datasets, presented in the top and bottom half of the table respectively, so as\nto prevent overfitting of the model.",
                "The pre-processing of the dataset2consists of splicing the data (scene, audio, and\nemotion scores) into chunks of 3 seconds, duration that was kept short due to the high-\ndimensional characteristic of audio signals.",
                "Dataset distribution:\ntrain and test, positive and\nnegative emotion scoresPositive Negative Total\nTrain 1,778 1,404 3,182\nTest 369 772 1,141\nTotal 2,147 2,176 4,3231798 MultimediaToolsandApplications(2021)80: 1793\u20131812Fig.2 Proposed model\narchitecture for Scene2Wav.",
                "6.\n5 Resultsanddiscussion\n5.1 Trainingcon\ufb01guration\nBoth the proposed and baseline models are trained on the previously discussed 3 second\nspliced dataset (see Section 3) and both use the same CNN and Encoder RNN with heuris-\ntically determined configurations.",
                "5.5.1 Extendedemotionevaluation\nIn this section, we extend the emotion analysis on longer samples obtained with the same\ndataset and also with short samples obtained with the additional DEAP dataset [ 11].",
                "We first show emotion evaluation on samples of\n10 seconds duration obtained with the same dataset used in this paper, the COGNIMUSE\ndataset [ 46].",
                "Emotion evaluation on short and long samples,\nand even samples obtained from a different dataset, show that Scene2Wav is better able to\nproduce music with the same emotion as the scene.",
                "Future works include increasing the size of\nthe dataset, improving longer-term dependency modeling for longer audios, and including\nmore emotion classes, such as considering arousal and adding a neutral class."
            ]
        },
        {
            "title": "Attentional networks for music generation",
            "architecture": [
                "Regularly used architecture of LSTM units have a cell andthree regulators.",
                "81:5179\u201351893 Proposedmethodology\nFigure 2outlines the proposed architecture for music generation.",
                "81:5179\u20135189Fig.2 Architecture diagram\nrequired to add 12 to its pitch value.",
                "Browne CB (2001) System and method for automatic music generation using a neural network\narchitecture.",
                "Framewise phoneme classification with bidirectional lstm and other\nneural network architectures."
            ],
            "dataset": [
                "Table 1shows the batch construction for the used dataset.",
                "1 a) Sheet Music of the song: \u201cThe Last Farewell\u201d by Roger Whittaker b) Musical Notes (Extracted\nfrom MIDI file) of the song: \u201cThe Last Farewell\u201d5181 Multimedia Tools and Applications (2022) 81:5179\u20135189Table1 Batch construction for the JAZZ ML ready MIDI dataset: Batch size 64 characters\nBatch-1",
                "Thetypical length of each MIDI file in the dataset ranges from 1 to 4774.",
                "4 Experimentationandresults\n4.1 Datasetdiscription\nWe used Jazz ML ready MIDI dataset2to train our model.",
                "The dataset comprises of 818\ndiverse Jazz music melodies.",
                "The dataset comprises of:\n\u2013 The list of notes extracted from the midi file,\n\u2013 Number of notes and\u2013 List of unique notes for each midi file.",
                "We have divided the dataset into training/validation and test splits in the ratio 80:20."
            ]
        },
        {
            "title": "Monophonic music composition using genetic algorithm and Bresenham\u2019s line algorithm",
            "architecture": [],
            "dataset": [
                "[ 2] Generative RNN model for sheet music Uses dataset in ABC music notation26488 Multimedia Tools and Applications (2022) 81:26483\u201326503recurrent unit (GRU) to generate convincing monophonic melodies.",
                "[ 2] proposed generative RNN models to build a music gener-\nator using Seq2Seq and Character RNN, which is trained by Abc formatted music datasetwith approximately 34,000 songs of different genres.",
                "These methods generate music that is akin to the music in the dataset."
            ]
        },
        {
            "title": "Polyphonic music generation generative adversarial network with Markov decision process",
            "architecture": [
                "These algorithms rely\non the experimental enumeration of the discriminator/generator architectures to find betternetwork architecture settings, but they do not completely solve the training difficulties and thelack of diversity among the generated samples."
            ],
            "dataset": [
                "For example, it is difficult for a classic GAN to create a new melody outside thetraining dataset, and it is difficult to break through the shackles of melody and tone in that\ndataset.",
                "By learning the characteristics of and mapping between various independentmelody sequences in a polyphonic music dataset, this model can generate polyphonic musicthat is closer to real-world music, effectively resolving the issues concerning the continuity ofpolyphonic music sequences and ensuring the diversity of generated samples \u2014making the\nmodel more powerful for unsupervised music sequence processing.",
                "Thus, the generatedpolyphonic music will not be destroyed with the growth of the sequence; better quality musicwill be generated; the shackles of the dataset will be broken; and melodies outside the datasetcan be created.",
                "3 Experimental results and analysis\n3.1 Dataset composition and preprocessing\nThe research team have constructed 6,947 original datasets composed of polyphonic music\n[19] to serve as the dataset for the polyphonic music generation model.",
                "The file format for the\ndataset is music instrument digital interface (MIDI)",
                "After the team loaded a MIDI file in the dataset, each note in the filewas parsed into a list of start time, duration, tone, and speed.",
                "Original music\n(MIDI files)\nPreprocessing\nVector \nsequencesToken \nsequencesVector \nsequencesPreprocessingMIDI files\nMelody\nchord\nToken Token\nTrain Valid\nFig. 5 Complete processing flow chart of original music datasetMultimedia Tools and Applications (2022)",
                "Figure 10shows the various styles of polyphonic\nmusic output by the model using the dataset created for this paper.",
                "macro/C0P\u00bc1\nnXn\ni\u00bc1Pi \u00f017\u00de\nmacro/C0R\u00bc1\nnXn\ni\u00bc1Ri \u00f018\u00de\nmacro/C0F1\u00bc2/C2macro/C0P/C2macro/C0R\nmacro/C0P\u00femacro/C0R\u00f019\u00de\nIn this paper, all polyphonic music sequences in the original dataset are \u201ctrue\u201dsamples, while\nnoise sequences are \u201cfalse\u201dsamples.",
                "During the experiments, the research team input the original polyphonic music dataset into\nthe proposed music generation model to obtain the polyphonic music generated by thealgorithm in this paper.",
                "After completing the above steps, the team used the polyphonic music sequence in\nthe original dataset and the formulas for precision and recall to obtain the comparison testresults of the two models, as shown in Table 2.",
                "It should be noted that, after the neural network, the output music sequence will produce a\nvariety of new notes, so it is not possible to determine the one-to-one correspondence betweenthe music sequence in the original dataset and the generated music sequence.",
                "81:29865 \u201329885 29881the innovation of neural networks for music generation and will not be limited to the melody or\ntone in the original music dataset.",
                "This study also relies on changing the dataset of the input model for the style change of musicproduction.",
                "Adding MDP and MCTS to the model preventsthe generated music from being constrained to the melody and tone in the original dataset,making the generated music more original.",
                "At the same time, the research team has built a new\ndataset, in which all data files are in MIDI format.",
                "LSTM based music generation with dataset prepro-\ncessing and reconstruction techniques\n2."
            ]
        },
        {
            "title": "A combination of multi-objective genetic algorithm and deep learning for music harmony generation",
            "architecture": [
                "However recently, the neural networks, and evolutionary algorithms have been more\nwidely used in the AMG literature, such as conditional rhythms generation of drum sequences\nwith neural networks [ 21], and using a Hierarchical Recurrent Neural Network (HRNN) for\nmelody generation [ 35] or combining two types of music generation models, namely symbol-\nic, and raw audio models based on the WaveNet architecture [ 22].",
                "The architecture of theLSTM network is shown in Fig.",
                "The architecture of the Bi-LSTM network2426 Multimedia Tools and Applications (2023) 82:2419\u20132435trained neural network model with opinions of music experts, and the trained Bi-LSTM neural\nnetwork model with opinions of regular listeners.",
                "Long short-term memory recurrent neural network\narchitectures for melody generation."
            ],
            "dataset": [
                "Lstm based music generation with dataset preprocess-\ning and reconstruction techniques."
            ]
        },
        {
            "title": "A Style-Specific Music Composition Neural Network",
            "architecture": [
                "The architecture consists of a generator, adiscriminator and a conditional network with four convolutional layers to de\ufb01ne the har-mony direction.",
                "Therefore, the neural network architecture with the best performance in the trainingdata has been the MCNN model, evaluated in the iteration from 0 to 3500 time steps.",
                "Sak H, Senior A, Beaufays F (2014) Long short-term memory recurrent neural network architectures for\nlarge scale acoustic modeling."
            ],
            "dataset": [
                "According to the different characteristics of intelligent composition, it can be divided into:\nrandom generation composition, rule-based knowledge system composition, mathematic-based composition, music grammar composition, and genetic algorithm composition [ 5].T o\na certain extent, these methods can meet the basic requirements of automatic composition,but as for the melody\u2019s structure, can not meet the constant changes of music datasets.",
                "123A Style-Speci\ufb01c Music Composition Neural Network 1895\n2.2 Symbol Model Based on Neural Networks\nNowadays, existing intelligent music composition method can be broadly classi\ufb01ed into two\ntypes according to format variety of training datasets: an original audio model and a symbolmodel (MIDI).",
                "The parameters of generator are trained by the dataset, and actor\u2013critic (AC) network is usedto make \ufb01ne-tuning.",
                "We employed the matched subset of the Lakh MIDI dataset (LMD) and Classical Piano MidiPage dataset as the training dataset, Lakh MIDI dataset [ 47] provides a rich collection of real-\nworld MIDI \ufb01les and some associated meta-data.",
                "Classical Piano Midi Page dataset collectedclassical music sets of composers from the eighteenth century to the nineteenth century,including works of Bach, Beethoven, Chopin and other 25 composers.",
                "In the experiment, weselect 2000 music samples of Classical Piano Midi Page dataset with speci\ufb01c composers\u2019styles in MIDI format [ 48] as test dataset, and each sample was a single orbital with an average\ntime of 2\u20134 min. To meet the requirements of the experiment, each sample was divided into20 s to obtain more than 20,000 classical music samples.",
                "As is shown in Fig. 6, midi \ufb01les in the dataset were transformed into sequences.",
                "All the models adopt 1024 neurons and have trained the same datasets."
            ]
        },
        {
            "title": "Self-Supervised Music Motion Synchronization Learning for Music-Driven Conducting Motion Generation",
            "architecture": [
                "An overview of the two-\nstage training procedure can be found in Algorithm 1.\n4.3 Network Architectures\nAs shown in Fig.3, four neural networks are involved\nin our approach: a music encoder Emusic, a motion en-\ncoderEmotion , a generator G, and a discriminator D.",
                "Although\nthe architectures of these studies[57{60]appear similarto that of our proposed approach, there are three fun-\ndamental di\u000berences between them."
            ],
            "dataset": [
                "To verify the e\u000bectiveness of our method,\nwe construct a large-scale dataset, named ConductorMotion100, which consists of unprecedented 100 hours of conducting\nmotion data.",
                "In addition, we \fnd that existing conducting mo-\ntion datasets are too small to train a generative deep\nlearning model.",
                "Thus, we collect and construct a large-\nscale conducting motion dataset.",
                "The constructed dataset, named\nConductorMotion100, has 100 hours of conducting mo-\ntion data and aligned Mel spectrograms.",
                "Its scale\nsigni\fcantly exceeds that of existing conducting mo-\ntion datasets.",
                "2) We collect and construct a large-scale conduct-\ning motion dataset, ConductorMotion100, based on\nadvanced object detection and pose estimation tech-\nniques.",
                "ConductorMotion100 contains 100 hours of\nconducting motion and aligned music data; its scale\nsigni\fcantly exceeds that of existing conducting motion\ndatasets.",
                "Both the dataset ConductorMotion100 and the ex-\nperimental codes are open-sourced1\u25cb.",
                "3 Data Preparation\nDue to the scale of existing conducting motion\ndatasets being insu\u000ecient to train a deep genera-\ntive model, we construct a large-scale conducting mo-\ntion dataset, named ConductorMotion100, by deploy-\ning pose estimation on conductor view videos of con-\ncert performance recordings collected from online video\nplatforms.",
                "As\nshown in Table 1, its scale far exceeds that of exist-\ning conducting motion datasets.",
                "To facilitate related\nresearch, the dataset is made public4\u25cb.",
                "Comparison on the Scale of Conducting Motion Datasets\nYear Dataset Length (min)\n2013 Saras\u0013 ua et al.[48]120.0\n2013 Dansereau et al.[25]0.5\n2014 Saras\u0013 ua and Guaus[49]250.0\n2017 Karipidou et al.[50]36.0\n2019 Huang et al.[51]180.0\n2019 Lemouton et al.[52](IDEA dataset) 56.0\n2021 Ours (ConductorMotion100) 6 000.0\n4\u25cbhttps://github.com/ChenDelong1999/VirtualConductor, Mar. 2022.Fan",
                "Therefore, we \frst annotate a small object detec-\ntion dataset, Concert300, and \fne-tune a pre-trained\nYOLO-V3[20]to recognize which human is the conduc-\ntor.",
                "Formally, the ConductorMotion100 dataset can be\ndescribed asD=f(Xi;Yi)gN\ni=1, where Xi=fxtgTx\ni\nt=1\nandYi=fytgTy\ni\nt=1are thei-th Mel spectrogram and\nconducting motion sequence respectively.",
                "In Fig.2 ,\nwe present the visualization of a sample ( X;Y) in the\nConductorMotion100 dataset; the sample corresponds\nto the \fnal part of Tchaikovsky's 1812 Overture.",
                "Visualization of a sample in the ConductorMotion100\ndataset.",
                "4.2 Overview of Proposed Approach\nFormally, given the ConductorMotion100 dataset D,\nour goal is to learn a music encoder Emusic and a gener-\natorGto predict a motion sequence from a given Mel\nspectrogram Xand random zsampled from a normal\ndistribution, i.e., ^Y=G(Emusic(X);z).",
                "Training Procedure of M2S-Net and M2S-GAN\nInput : datasetD=f(Xi;Yi)gN\ni=1; loss function weights \u0015adv,\u0015sync,wGP;\nOutput : trained music encoder Emusic , generator G;\n/",
                "During training,\nthe positive and negative pairs are automatically sam-\npled from the dataset.",
                "In addition, their method is computationally ine\u000ecient,\nespecially when facing our large-scale ConductorMo-\ntion100 dataset.",
                "knowledge of the music motion relationship is learned\nfrom the large dataset by the model itself.",
                "In addition, we construct a large-scale conducting\nmotion dataset, ConductorMotion100, which contains\nan unprecedented 100 hours of conducting motion and\ncorresponding music data.",
                "The ConductorMotion100\ndataset enables M2S-GAN to learn rich music seman-\ntics.",
                "Since the scale of ConductorMotion100 is also\nlarger than many datasets for music information re-\ntrieval (MIR) tasks, in future, we will also validate the\ne\u000bectiveness of using it as a pretraining dataset for MIR\ntasks, such as beat tracking and tempo estimation.",
                "On our collected ConductorMo-\ntion100 dataset, the proposed method achieved 0.049\nRDE and 2.046 SCE, outperforming all the compared\nmethods.",
                "[52] Lemouton S, Borghesi R, Haapam\u007f aki S, Bevilacqua F, Fl\u0013 ety\nE. Following orchestra conductors: The IDEA open move-\nment dataset."
            ]
        },
        {
            "title": "Integration of a music generator and a song lyrics generator to create Spanish popular songs",
            "architecture": [],
            "dataset": []
        },
        {
            "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
            "architecture": [
                "ReghunathandRajan EURASIPJournalonAudio,Speech,andMusic\nProcessing         (2022) 2022:11 \nhttps://doi.org/10.1186/s13636-022-00245-8\nEMPIRICAL RESEARCH OpenAccess\nTransformer-basedensemblemethod\nformultiplepredominantinstruments\nrecognitioninpolyphonicmusic\nLekshmiChandrikaReghunath*and RajeevRajan\nAbstract\nMultiplepredominantinstrumentrecognitioninpolyphonicmusicisaddressedusingdecisionlevelfusionofthree\ntransformer-basedarchitecturesonanensembleofvisualrepresentations.",
                "Weexperimentedwith\ntwotransformerarchitectureslikeVisiontransformer(Vi-T)andShiftedwindowtransformer(Swin-T)fortheproposed\ntask.",
                "Awavegenerative\nadversarialnetwork(WaveGAN)architectureisalsoemployedtogenerateaudiofilesfordataaugmentation.",
                "[ 10]\nanalyzed the architecture of Han et al. in order to for-\nmulateanefficientdesignstrategytocapturetherelevant\ninformation about timbre.",
                "Ourmodelisderivedfrom\n[29]withsomesignificantchangesasdescribedin\nSection4,anditoutperformstheexistingmodels,\nincluding[ 1].Theefficacyoftransformermodelsand\nattentionmechanismsaredemonstratedby\ncomparisonwithCNNandDNNarchitectures.",
                "Theprincipleofautocorrelationis\nusedtoestimatethetempoateverysegmentinthenovelty\nfunction[ 37].Autocorrelationtempogramsarecomputed\nwithlibrosa.feature.tempogram using a 2048 point FFT\nwindowandahopsizeof512.\n4 Modelarchitectures\n4.1 DNN\nA DNN framework on musical texture features (MTF)\nis experimented with to examine the performance of\ndeeplearningmethodologyonhandcraftedfeatures.",
                "4.2 CNN\nCNN uses a deep architecture with repeated convolu-\ntions followed by max-pooling.",
                "The last convolutional layers used 3 \u00d73\nfilters as later layers reveal more specific and complex\n1https://librosa.org/doc/latest/tutorial.htmlReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page5of14\nFig.2(a)BlockdiagramoftheproposedmethodofVisiontransformer,(b)Internalarchitectureoftransformerencoder\npatterns and final layers activations help to recognize the\npredominant instruments from accompaniments.",
                "Figure 2shows the architecture of our\nproposedmethod.",
                "Unlike other\ntransformers Swin-T [ 29] has a hierarchical architecture\nandhaslinearcomputationalcomplexitythroughthepro-\nposedshiftedwindow-basedself-attentionapproach.",
                "Figure 3shows the architecture of our\nproposed method.",
                "Figure3(b) shows the internal architecture of the Swin-T\nblock.",
                "All audio files in the IRMAS dataset are\nFig.3(a)BlockdiagramoftheproposedmethodofSwintransformer,(b)InternalarchitectureofSwin-TblockReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page7of14\nin a 16-bit stereo .wav format with a sampling rate of\n44,100 Hz.",
                "In WaveGAN architecture, the transposed convolution\noperation is modified to widen its receptive field.",
                "Thediscriminatorisalsomodifiedsimilarly,usinglength-\n25 filters in one dimension and increasing stride from\ntwo to four which results in WaveGAN architecture [ 43].The transposed convolution in the generator produces\ncheckerboard artifacts [ 43].",
                "6.3 Effectoftransformerarchitectureandattention\nThe instrument-wise F1 scores for all the Mel-\nspectrogram experiments are shown in Fig.",
                "Ontheotherhand,experiments\nwith transformer architecture showed improved perfor-\nmance for all the instruments.",
                "This is mainly because\nthe transformer architecture with a multi-head attention\nmechanism helps to focus or attend to specific regions of\nthe visual representation for predominant instruments\nrecognition.",
                "Wealsoconductedanablationstudyofthearchitecture\nin order to gain a better understanding of the network\u2019s\nbehavior.",
                "The results are tabulated\ninTable5.TheoptimalparametersobtainedthroughMel-\nspectrogram analysis are applied to the modgdgram andtempogramarchitecturesthroughasimilarablationstudy.",
                "To summarize, the results show the potential of Swin-T\narchitectureandthepromiseofalternatevisualrepresen-\ntationsotherthantheconventionalMel-spectrogramsfor\npredominantinstrumentsrecognitiontasks.",
                "Thisshows\nthe importance of phase information in the proposed\nTable5AblationstudyoftheMel-spectrogramarchitectureshowingtheeffectofnumberofheads,patchsize,projectiondimension,\nandnumberofMLPnodes.",
                "Highestvaluesarehighlighted\nSL.No ArchitectureSpec.",
                "[10] customized the architecture of Han et al. and intro-\nduced two models, namely, single-layer and multi-layer\napproaches.",
                "We experimented with Vi-T and the\nrecentSwin-Tarchitectureswithadetailedablationstudy\nand our proposed experiments using Swin-T outperform\nexistingalgorithmswithverylesstrainableparameters.",
                "Musicalinstrumentrecognitioninuser-generatedvideosusing\namultimodalconvolutionalneuralnetworkarchitecture,(2017),\npp.226\u2013232."
            ],
            "dataset": [
                "The\nproposedsystemissystematicallyevaluatedusingtheIRMASdatasetwithelevenclasses.",
                "Both approaches were trained\nand validated by the IRMAS dataset of polyphonic music\nexcerpts.",
                "It was found that\nbothsourceseparationandtransferlearningcouldsignif-\nicantly improve the recognition performance, especially\nfor a small dataset composed of highly similar musical\ninstruments.",
                "The pro-\nposedworkin[ 15]employedanattentionmechanismand\nmultiple-instance learning (MIL) framework to address\nthe challengeof weakly labeled instrument recognition in\ntheOpenMICdataset.",
                "All audio files in the IRMAS dataset are\nin a 16-bit stereo .wav format with a sampling rate of\n44,100 Hz.",
                "5 Performanceevaluation\n5.1 Dataset\nThe performance of the proposed system is evaluated\nusing the IRMAS (Instrument Recognition in Musical\nAudio Signals) dataset, developed by the Music Technol-\nogy Group (MTG) of Universitat Pompeu Fabra (UPF).",
                "All audio files in the IRMAS dataset are\nFig.3(a)BlockdiagramoftheproposedmethodofSwintransformer,(b)InternalarchitectureofSwin-TblockReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page7of14\nin a 16-bit stereo .wav format with a sampling rate of\n44,100 Hz.",
                "IRMAS dataset [ 2] contains separate train-\ning and testing set of eleven classes.",
                "Thisdatasethastwodisadvan-\ntages when training models.",
                "Second, the dataset is not\nwell balanced in terms of either musical genre or instru-\nmentation.",
                "However, this may not be a problem if the\ndatasets were larger and the distribution represented the\nrealworld.",
                "The problem\nwith small datasets is that models trained with them\ndo not generalize well from the validation and test set\n[47].",
                "It reports a micro and macro F1 score of\n0.50 and 0.43 respectively, and it is evident that the pro-\nposedensembleframeworksoutperformthehand-craftedTable7PerformancecomparisononIRMASdataset.",
                "In [15], the usage of an attention layer was\nshown to improve classification results in the OpenMIC\ndataset when applied to a set of Mel-spectrogram fea-\ntures extracted from a pre-trained VGG net.",
                "Our proposed ensemble voting\ntechniqueoutperformedexistingalgorithmsandtheMTF\nDNNandSVMframeworkontheIRMASdatasetforboth\nthemicroandthemacroF1measure.\n7C o n c l u s i o n\nWe presented a transformer-based predominant instru-\nment recognition system using multiple visual repre-\nsentations.",
                "The proposed method is evaluated using the IRMAS\ndataset.",
                "Acknowledgements\nTheauthorswouldliketoacknowledgeJuanJ.Bosch,FerdinandFuhrmann,\nandPerfectoHerrera(MusicTechnologyGroup-UniversitatPompeuFabra)\nfordevelopingtheIRMASdatasetandmakingitpubliclyavailable.",
                "Availabilityofdataandmaterials\nThedatasetsgeneratedand/oranalyzedduringthecurrentstudyareavailable\ninthezenodorepository( https://www.upf.edu/web/mtg/irmas )andare\npubliclyavailable."
            ]
        },
        {
            "title": "Genre Recognition from Symbolic Music with CNNs: Performance and Explainability",
            "architecture": [
                "We propose an architecture for handling MIDI data that makes use \nof multiple resolutions of the input, called Multiple Sequence Resolution Network (MuSeReNet).",
                "These \ndifficulties, among others, have led to the development of \nnew approaches for processing music in both the audio \nand the symbolic domain, many of which take the form of \nspecialized neural network architectures such as WaveNets This article is part of the topical collection \u201cEvolutionary Art \nand Music\u201d guest edited by Aniko Ekart, Juan Romero and Tiago \nMartins.\n *",
                "However, finding an \noptimal architecture and set of hyper-parameters, such as \nnumber of layers, kernel size, number of kernels, etc. for \na given task remains a difficult problem that is still being \ntackled mostly experimentally.",
                "In addition, multiple network architecture search (NAS) \nalgorithms have been proposed in recent years, especially \nfor computer vision, such as Amoebanets [32], but these are \nbeyond the scope of this paper.",
                "The rest of the paper is structured as follows: Sec-\ntion\u00a0\u201c Related Work \u201d presents relevant literature for genre \nrecognition, in Section\u00a0\u201c Preliminaries \u201d we expose the \nreader to important concepts for 1D Convolutional Neural \nNetworks, and for music genres, in Section\u00a0\u201c Architecture\u201d \nwe show the proposed neural architecture, in Sect.",
                "The most up-to-date techniques for the \nclassification of music use neural networks on MFCCs or \nspectrograms [29, 47], some of them focus on feature selec-\ntion [39, 44], or introducing new architectures [22, 26, 51, \n52].",
                "Recently in multiple domains, there is a tendency to forgo \nfeature extraction stages of an information retrieval pipeline, \ninstead using a more complex neural network architecture, in \nwhich the first layers act as feature extractors.",
                "Architecture\nTrees are an effective representation of music since they are \nable to capture information, patterns, and structures at multi -\nple different time scales.",
                "Such an architecture is similar to U-nets [38] which \nis used for image segmentation.",
                "Experiments\nTo explore the effectiveness of our architecture for informa-\ntion retrieval from symbolic music and to check the com -\npatibility of 1D CNNs for the task, along with the effect of \nallocating resources to network depth or to kernel size we \nconducted a set of experiments.1\nData\nFor our experiments, we use the Lakh Pianoroll Dataset \nas presented by Dong et\u00a0al.",
                "The first case represents a \ntraditional CNN architecture, while the second represents \nMuSeReNets in which information flows from the leaves \nto the root (Fig.\u00a0 2).",
                "In general, all CNNs which were trained on sequences \nlonger than 256 samples surpassed Ferraro and Lemstr\u00f6m\u2019s \npattern recognition approach, as well as Liang et\u00a0al. model \nFig. 7  a Sequence architecture and b MuSeRe architecture used in \nexperiments SN Computer Science (2023) 4:106\n 106 Page 8 of 18\nSN Computer Science\nwith regards to F1 metric",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "We plan to further explore this idea, using \nnetwork architecture search to find a good baseline network \nand similarly to [45] find an efficient way to scale up neural \nnetworks for MIDI classification tasks.",
                "Which neural net architectures give rise to exploding \nand vanishing gradients?",
                "Regularized evolution for \nimage classifier architecture search."
            ],
            "dataset": [
                "Through our \nexperiments, we outperform the state-of-the-art for MIDI genre recognition on the topMAGD and MASD datasets.",
                "[4 ] used a machine learning approach, including \na simple neural network, on a custom dataset for successful \ngenre recognition in the symbolic domain.",
                "in [16] compute a \nsequence similarity between all pairs of channels of two \nMIDI files and then use a k-NN classifier for genre recog-\nnition, on a dataset of 100 songs and four genres.",
                "[53] extract features related to the melody and the bass \nthrough a musicological perspective, incorporating text clas -\nsification techniques and using Multinomial Naive Bayes \nas the principal probabilistic classifier, in a self-collected \ndataset of 273 records.",
                "These approaches were experimentally validated on rela -\ntively small datasets compared to, for example, the openly \navailable Lakh MIDI dataset",
                "Given a large enough dataset of n  such sequences, along \nwith their ground-truth labels D= {(X1,Y1),\u2026,(Xn,Yn)} , \nin the context of machine learning, the goal is to train an \nalgorithm F(X;/u1D703) to model the conditional distribution of \nlabels with respect to input sequences.",
                "Each classifier\u2019s performance is then \nmeasured on a test dataset which does not overlap with D.\n1D Convnets\nThe state-of-the-art approach for MIDI genre classification \npresented by Ferraro and Lemstr\u00f6m in [7 ] uses an algorithm \nfor recognizing patterns of notes in an input sequence and \nthen performs classification based on recognized patterns.",
                "Experiments\nTo explore the effectiveness of our architecture for informa-\ntion retrieval from symbolic music and to check the com -\npatibility of 1D CNNs for the task, along with the effect of \nallocating resources to network depth or to kernel size we \nconducted a set of experiments.1\nData\nFor our experiments, we use the Lakh Pianoroll Dataset \nas presented by Dong et\u00a0al.",
                "This dataset consists of pianoroll represen-\ntations of MIDI files in the Lakh MIDI Dataset presented \nby Raffel in [31].",
                "The LMD-matched subset \ncontains pianorolls that have been linked with the Million \nSong Dataset (MSD)",
                "The Million Song Dataset is the \nlargest currently available collection of audio features and \nmetadata for a million contemporary popular music tracks.",
                "We use labels acquired by MSD to construct the MASD and \ntopMAGD datasets presented by Schindler et\u00a0al. in [42], so \nwe can compare our results with existing work.",
                "At the time \nof writing, Ferraro and Lemstr\u00f6m in [7 ] have achieved the \nbest results with regards to genre classification of symbolic \nmusic for the MASD and topMAGD datasets.",
                "Finally, we \nrandomly split each dataset into a train and test set (.75/.25), \nwe use the train set for training our models and the test set \nfor evaluating them.",
                "Both datasets are imbalanced with regard to the number \nof files corresponding to each label (Figs.\u00a0 4 and 5 ).",
                "ndeep=nshallow ,\n1153 fo1+9fo1fo2+fo2+1153fo2+128=nshallow ,\nFig. 4  Number of files in the LPD dataset per label of the MASD \ndataset\nFig. 5  Number of files in the LPD dataset per label of the topMAGD \ndataset\nFig.",
                "In the \ncontext of our dataset, these lengths represent musical time \nfrom approximately 5 quarter notes to 170 quarter notes, or \n42 bars for a 4\n4 time signature (around one\u2013two minutes for \ntypical values of a song\u2019s tempo).",
                "Then, each network will \nconsist of log2l blocks stacked depth-wise, followed by a \nfully connected layer at the output, with as many sigmoid-\nactivated units as there are different labels in each dataset.",
                "Evaluation and\u00a0Post Processing\nWe evaluate our models on the held-out test set for each of \nthe MASD and topMAGD datasets by computing precision, \nrecall, and micro f1 metric.",
                "If no labels have a \nprobability greater than 0.5, then we assign as a single label \nthe element of the vector which has the maximum prob-\nability, since there are no unlabeled samples in the dataset.",
                "In addition, we present precision, recall, and F1 scores for \neach label in the topMAGD dataset for the best performing \nmodel (Table\u00a0 2).",
                "On the one hand, the effect of the imbal-\nanced dataset is apparent in the network\u2019s performance for \nthe most common label (Pop-Rock) when compared to those \nwith fewer files in the dataset such as Blues, Reggae, and \nFolk.",
                "It is interesting that genres such as Jazz which have lit-\ntle representation in the dataset are better classified than gen-\nres such as Electronic which has almost double the support.",
                "Furthermore, since genres \nthemselves are not well-defined terms, and their characteris-\ntics can vastly change over time, explanations of predictions \ncould be valuable for understanding both the model and the \ndataset.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "The results concern four hand-picked samples from the \nreddit MIDI dataset.2",
                "We chose \nMoonlight Sonata, since it is out of domain as its genre is not \nincluded in the dataset\u2019s labels.",
                "In music-theoretical terms, B\u266d appears as a \nDorian substitute and it would be interesting to explore the \ndistribution of the Dorian mode within our dataset of Inter -\nnational music, and determine if this is a bias learned by the classifier, or if it is an actual feature of the genre.",
                "For Here \nComes the Sun, we attribute the failure of GPX to the bias \nof the classifier towards the Pop-Rock genre which is a result \nof dataset imbalance.",
                "[14]\nMMD-critic is a methodology for analyzing the distribution \nof a dataset to find specific samples which are prototypes, \nand others which are criticisms.",
                "Regarding the results on the test set (Table\u00a0 5), we can \ngain some insight about the dataset and by extension the \nperformance of the CNNs.",
                "A second issue raised by Table\u00a0 5 is that of dataset \nimbalance.",
                "Finally, for those genres with very low \nsupport, we cannot expect MMD-critic to produce meaning-\nful explanations since it is a statistics-based approach that \nrequires a sufficiently large dataset.",
                "Furthermore, increasing input sequence length effectively \nreduces the number of (non-overlapping) sequences in the \ndataset while network size increases, which makes models \nmore prone to overfitting.",
                "In addition, even though we used one of the largest avail-\nable MIDI genre annotated datasets for training and evaluat-\ning our models, the dataset is by no means representative of \nall available music and suffers from poor class balance.",
                "For \nfuture work, we plan to augment our dataset by including files from other sources, such as the reddit4 MIDI dataset and \nautomatically acquire additional labels and annotations from \nonline sources such as The Echo Nest5 and Spotify APIs.",
                "Bertin-Mahieux T, Ellis DP, Whitman B, Lamere P. The million \nsong dataset 2011.",
                "Facilitating comprehensive \nbenchmarking experiments on the million song dataset."
            ]
        },
        {
            "title": "CycleDRUMS: automatic drum arrangement for bass lines using CycleGAN",
            "architecture": [
                "In addition, more solutions based on deep learning \ntechniques, such as RL-Duet [4 ]\u2014a deep reinforcement learning algorithm for online accompaniment generation\u2014or \nPopMAG, a transformer-based architecture which relies on a multi-track MIDI representation of music [5], continue to be \nstudied.",
                "In particular, we \ntrained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset [8 ] and the musdb18 dataset [9 ].",
                "To sum up, our main contributions are the following:\n\u2022 we trained a CycleGAN architecture on bass and drum mel-spectrograms in order to automatically generate drums \nthat follow the beat and sound credible for any given bass line;\n\u2022 our approach can generate drum arrangements with low computational resources and limited inference time, if \ncompared to other popular solutions for automatic music generation [12];\n\u2022 we developed a metric\u2014partially based on or correlated to human (and expert) judgment\u2014to automatically evaluate \nthe obtained results and the creativity of the proposed system, given the challenges of a quantitative assessment of \nmusic;\n\u2022 we compared our method to Pix2Pix, another popular image transfer network, showing that the music arrangement \nproblem can be better tackled with an unpaired approach and adding a cycle-consistency loss.",
                "The following contributions used MIDI, piano rolls, chord \nand note names to feed several deep learning architectures and tackle different aspects of the music generation problem.",
                "In [21], symbolic sequences of polyphonic music are modeled in an entirely general piano-roll representation, while the \nauthors of [22] propose a novel architecture to generate melodies satisfying positional constraints in the style of the \nsoprano parts of the J.S. Bach chorale harmonizations encoded in MIDI.",
                "[38] tested a model for unconditional audio synthesis based on generating one audio sample at a time, and \n[39] applied Restricted Boltzmann Machine and LSTM architectures to raw audio files in the frequency domain in order \nto generate music.",
                "The authors of [40] present a raw audio music generation model based on the WaveNet architecture, \nwhich takes the composition notes as a secondary input.",
                "Nonetheless, due to the \ncomputational resources required to model long-range dependencies in the time domain directly, either short samples \nof music can be generated or complex and large architectures and long inference time are required.",
                "As to \nthe arrangement generation task, the large majority of approaches proposed in the literature is based on a symbolic \nrepresentation of music: in [5 ], a novel multi-track MIDI representation (MuMIDI) is presented, which enables simultane -\nous multi-track generation in a single sequence and explicitly models the dependency of the notes from different tracks \nutilizing a Transformer-based architecture; in [4 ], a deep reinforcement learning algorithm for online accompaniment \ngeneration is described.",
                "It features a U-NET encoder\u2013decoder architecture with a bidirectional LSTM as hidden layer.",
                "The architecture assumes some underlying relationship between \ndomains and tries to learn it.",
                "In particular, we trained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset",
                "[7] to obtain:\nWe adopt the architecture from [56] for our generative networks, which have shown impressive neural style transfer \nand super-resolution results.",
                "Figure\u00a0 2 shows a schema summarizing the entire architecture.\n3.4  Automatic bass to\u00a0drums arrangement\nCycleDRUMS takes as input a set of N  music songs in the waveform domain X={xi}N\ni=1 , where /u1D431/u1D422 is a waveform whose \nnumber of samples depends on the sampling rate and the audio length.",
                "In the final stage of our pipeline, we fed CycleGAN architecture with the obtained dataset.",
                "The architecture assumes \nsome underlying relationship between domains and tries to learn it.",
                "For this reason, our training strategy is to pre-train the architecture with the artificially source-\nseparated FMA dataset and then fine-tune it with musdb18.",
                "Ultimately, instead of forcing a pre-\nexisting method to work in our specific scenario, we decided to replicate our experiments using the Pix2Pix architecture \n[11], another image-to-image translation network.",
                "At this website7 a private Sound Cloud playlist of some of the most exciting results is available, \nwhile at this link8 we uploaded some samples obtained with the Pix2Pix baseline architecture.",
                "Even with the promising results, some critical issues \nmust be addressed before a more compelling architecture can be developed.",
                "Moreover, the model architecture should be further improved to focus on longer dependencies and consider \nthe actual degradation of high frequencies."
            ],
            "dataset": [
                "In particular, we \ntrained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset [8 ] and the musdb18 dataset [9 ].",
                "Coming to the most relevant issues in the development of music generation systems, both the training and evalu-\nation of such systems have proven challenging, mainly because of the following reasons: (i) the available datasets for \nmusic generation tasks are challenging due to their inherent high-entropy",
                "After the source separation task is carried out on our song dataset, both the bass and drum waveforms are turned \ninto the corresponding mel-spectrograms using PyTorch Audio.1 PyTorch works very fast and is optimized to perform \nrobust GPU-accelerated conversion.",
                "In particular, we trained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset",
                "[8] and the musdb18 dataset [9].",
                "After the source separation task on \nour song dataset, the bass and drum waveforms are turned into the corresponding mel-spectrograms.",
                "In the final stage of our pipeline, we fed CycleGAN architecture with the obtained dataset.",
                "4  Experiments\n4.1  Dataset\nIt is important to carefully pick the dataset for the quality of the generated music samples.",
                "To train and test our model, \nwe decided to use the Free Music Archive2 (FMA), and the musdb183 dataset [9 ] that were both released in 2017.",
                "The \nFree Music Archive (FMA) is the largest publicly available dataset suitable for music information retrieval tasks [8 ].",
                "Finally, to better validate and fine-tune our model, we decided also to use the full musdb18 dataset.",
                "This rather small \ndataset comprises 100 tracks taken from the DSD100 dataset, 46 tracks from the MedleyDB, two tracks kindly provided by \nNative Instruments, and two tracks from the Canadian rock band The Easton Ellises.",
                "We used the 100 tracks taken from the DSD100 dataset to fine-tune \nthe model ( \u22486.5 h) and the remaining 50 songs to test it ( \u22483.5 h).",
                "For this reason, our training strategy is to pre-train the architecture with the artificially source-\nseparated FMA dataset and then fine-tune it with musdb18.",
                "A large, clean dataset of separated raw-audio sources remains a research objective.",
                "We trained our model on 2 Tesla V100 SXM2 GPUs with 32 GB of RAM for 12 epochs (FMA dataset) and fine-tuned it for \n20 more epochs (musdb18 dataset).",
                "Moreover, the cost and time required to manually \nannotate the dataset could become prohibitive even for relatively few samples (over 1000).",
                "At training time, we relied on the default network provided by the original \nauthors,6 we ran it on 2 Tesla V100 SXM2 GPUs with 32 GB of RAM for 50 epochs (FMA dataset), and we fine-tuned it for \n30 more epochs (musdb18 dataset).",
                "First and foremost, a more extensive and \ncleaner dataset of source-separated songs should be created.",
                "Data availability  The datasets generated by the survey research and analyzed during the current study are publicly available at the following \naddresses: https:// freem usica rchive.",
                "Hawthorne C, Stasyuk A, Roberts A, Simon I, Huang C-ZA, Dieleman S, Elsen E, Engel J, Eck D. Enabling factorized piano music modeling \nand generation with the maestro dataset."
            ]
        }
    ]
}