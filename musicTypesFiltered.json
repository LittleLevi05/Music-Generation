{
    "data_collection": [
        {
            "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "Hadjeres, G., Pachet, F., and Nielsen, F. Deepbach: a\nsteerable model for bach chorales generation, 2017."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [
                "Brock et al. (2019)\nexplore these issues and give an extensive hyper-parameter\nscan for hinge-loss based model such as SAGAN.\nPart of the research into audio generation has focus on sym-\nbolic music generation such as in Hadjeres et al.",
                "Brock, A., Donahue, J., and Simonyan, K. Large scale gan\ntraining for high \ufb01delity natural image synthesis, 2019."
            ],
            "pop": [],
            "classical": [
                "This dataset contains over\n200h of classical piano music, recorded over nine years of\nthe International Piano-e-Competition.",
                "The harmonic progressions in many of the generated sam-\nples follow the patterns common for western classical mu-\nsic.",
                "S AMPLE DIVERSITY\nThe MAESTRO dataset consists of music from the Baroque\nera over the Classical, Romantic and Impressionist styles,\nto the Expressionist period."
            ],
            "electronic": [
                "On the other hand, audio is an integral part of consumer\nelectronic devices, which have much less computing power."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "Alternatively\nit can be used to reduce the dimensionality of the problem.",
                "Alternatively, we can double the sampling rate of x(t)by\ndoubling the size of the blocks N!2N:\nx\u0012\n(m2N+ ~n)t0\n2\u0013\nwith\u001a\n~n= 0;:::; 2N\u00001\nm= 0;:::;M\u00001(20)"
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [],
            "dubstep": [],
            "opera": [
                "This projection opera-\ntor projected both real and generated samples onto a lower\ndimensional manifold by attenuating inaudible frequencies."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Hierarchical Recurrent Neural Networks for Conditional Melody Generation with Long-term Structure",
            "homophonic": [],
            "polyphonic": [
                "Looking at \u201cpure\u201d neural networks that do not leverage\nslower optimization techniques, [11] use a novel Tonnetz\nrepresentation to train an LSTM that is better able to gen-\nerate polyphonic music with repeated patterns than a similar\nnetwork with a more traditional piano roll representation.",
                "2.[11] C.-H. Chuan and D. Herremans, \u201cModeling temporal tonal relations\nin polyphonic music through deep networks with a novel image-based\nrepresentation,\u201d in Proc.",
                "[19] D. Meredith, K. Lemstr \u00a8om, and G. A. Wiggins, \u201cAlgorithms for\ndiscovering repeated patterns in multidimensional representations of\npolyphonic music,\u201d J. New Music Res. , vol.",
                "[25] S. Lattner, M. Grachten, and G. Widmer, \u201cImposing higher-level struc-\nture in polyphonic music generation using convolutional restricted\nboltzmann machines and constraints,\u201d J. of Creative Music Syst. , vol. 2,\np. 1, 2018."
            ],
            "monophonic": [
                "Two\nrecent RNN-based systems, LookbackRNN and AttentionRNN\n[1], aim to generate monophonic melodies with long term\nstructure in an autoagressive way.",
                "By combining the generated pitch and rhythmic pattern, the\ngenerated monophonic melodies are qualitatively evaluated\nthrough multiple listening tests and are shown to outperform\nthe LookbackRNN",
                "For simplicity, we included only monophonic melodies with\na time signature of 4/4, along with their chords."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [
                "Composing melodies from a given set of chords is a task\nfaced by many musicians in the real world, e.g. in pop music\ncomposition or jazz improvisation.",
                "For example,a melodic motif may be repeated several times in a jazz\npiece with slight rhythmic variations."
            ],
            "rock": [],
            "pop": [
                "Popular data encoding schemes for symbolic music include\npiano-roll representation",
                "Popular data encoding schemes for symbolic music include\npiano-roll representation",
                "Composing melodies from a given set of chords is a task\nfaced by many musicians in the real world, e.g. in pop music\ncomposition or jazz improvisation.",
                "Composing melodies from a given set of chords is a task\nfaced by many musicians in the real world, e.g. in pop music\ncomposition or jazz improvisation."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "[6] J. A. Burgoyne, D. Bountouridis, J. van Balen, and H. Honing, \u201cHooked:\na game for discovering what makes music catchy,\u201d in Proc.",
                "[19] D. Meredith, K. Lemstr \u00a8om, and G. A. Wiggins, \u201cAlgorithms for\ndiscovering repeated patterns in multidimensional representations of\npolyphonic music,\u201d J. New Music Res. , vol."
            ],
            "techno": [
                "Hierarchical Recurrent Neural Networks for\nConditional Melody Generation with Long-term\nStructure\nGuo Zixun\nInformation Systems,\nTechnology, and Design\nSingapore University\nof Technology and Design\nSingapore\nnicolas guo@sutd.edu.sgDimos Makris\nInformation Systems,\nTechnology, and Design\nSingapore University\nof Technology and Design\nSingapore\ndimosthenis makris@sutd.edu.sgDorien Herremans\nInformation Systems,\nTechnology, and Design\nSingapore University\nof Technology and Design\nSingapore\ndorien herremans@sutd.edu.sg\nAbstract \u2014The rise of deep learning technologies has quickly\nadvanced many \ufb01elds, including generative music systems.",
                "The \ufb01eld of computer generated music\nhas grown ever since [4], with great strides being made in\nrecent years due to deep learning technologies [5].",
                "on Technol."
            ],
            "dubstep": [],
            "opera": [
                "At the dawn of computing, the idea of generating music\nwas \ufb01rst conceived by Lady Ada Lovelace, when she wrote:\n\u2018[The Engine\u2019s] operating mechanism might act upon other\nthings besides numbers",
                "Instead of manipulating the RNN weights update rate to\ncapture the temporal and long-term structure of sequences of\naudio samples, it uses multiple stacks of RNN with upper tier\nRNN layers operating on more grouped audio samples per\nstep and lower tier RNN layers operating on fewer grouped\nsamples per step.",
                "Inspired by [7], the CM-HRNN\u2019s bottom\ntier uses a conv1d operation to process overlapping sliding\nwindows of ntier 1events ( 1indicates the bottom tier) and\ngenerate the next event.",
                "The sliding window in\nthe bottom tier is implemented as a conv1d operation with\na stride of 1."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "LEARNING TO GENERATE MUSIC WITH SENTIMENT",
            "homophonic": [],
            "polyphonic": [
                "LEARNING TO GENERATE MUSIC WITH SENTIMENT\nLucas N. Ferreira\nUniversity of California, Santa Cruz\nDepartment of Computational MediaJim Whitehead\nUniversity of California, Santa Cruz\nDepartment of Computational Media\nABSTRACT\nDeep Learning models have shown very promising re-\nsults in automatically composing polyphonic music pieces."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "[5] use a\ndependency network and a Gibbs-like sampling procedure\nto generate high-quality four-part chorales in the style of\nBach.",
                "Deepbach: a steerable model for bach chorales gen-\neration."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "Each\nindividual in the population of this GA has 161real-valued\ngenes representing a small noise to be added to the weights\nof the 161 L1 neurons.",
                "Each\nindividual in the population of this GA has 161real-valued\ngenes representing a small noise to be added to the weights\nof the 161 L1 neurons.",
                "The GA starts with a random population of size 100\nwhere each gene of each individual is an uniformly sam-\npled random number \u00002\u0014r\u00142.",
                "The GA starts with a random population of size 100\nwhere each gene of each individual is an uniformly sam-\npled random number \u00002\u0014r\u00142.",
                "For each generation,\nthe GA (i) evaluates the current population, (ii) selects 100\nparents via a roulette wheel with elitism, (iii) recombines\nthe parents (crossover) taking the average of their genes\nand (iv) mutates each new recombined individual (new\noffspring) by randomly setting each gene to an uniformly\nsampled random number \u00002\u0014r\u00142.",
                "For each generation,\nthe GA (i) evaluates the current population, (ii) selects 100\nparents via a roulette wheel with elitism, (iii) recombines\nthe parents (crossover) taking the average of their genes\nand (iv) mutates each new recombined individual (new\noffspring) by randomly setting each gene to an uniformly\nsampled random number \u00002\u0014r\u00142.",
                "This means that if we add the genes of the\nbest individual of the \ufb01nal population to the weights of the\ngenerative mLSTM, we generate positive pieces with 84%\naccuracy and negative pieces with 67% accuracy.",
                "This means that if we add the genes of the\nbest individual of the \ufb01nal population to the weights of the\ngenerative mLSTM, we generate positive pieces with 84%\naccuracy and negative pieces with 67% accuracy."
            ],
            "classical": [
                "For example, one can\u2019t easily con-\ntrol a model trained on classical piano pieces to compose\na tense piece for a horror scene of a movie."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "[13] discovered a single\nunit within the mLSTM that directly corresponded to sen-\ntiment.",
                "Learning to generate reviews and discovering senti-\nment."
            ],
            "techno": [],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Personalized Popular Music Generation Using Imitation and Structure",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "Part I Music Background\n\u2022Music listening and Practice (select one from the following)\n{I am an expert, e.g. more than 5 years of vocal or instrumental training and\npractice."
            ],
            "vocal": [
                "Part I Music Background\n\u2022Music listening and Practice (select one from the following)\n{I am an expert, e.g. more than 5 years of vocal or instrumental training and\npractice."
            ],
            "choral": [
                "Deepbach: a steerable model for bach chorales\ngeneration.",
                "Automatic composition in the style of bach chorales."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?"
            ],
            "blues": [
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?"
            ],
            "jazz": [
                "Deep neural models for\npersonalized jazz improvisations.",
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?"
            ],
            "rock": [
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?"
            ],
            "pop": [
                "Personalized Popular Music Generation Using Imitation\nand Structure\nShuqi Dai\nCarnegie Mellon University\nshuqid@cs.cmu.eduXichu Ma\nNational University of Singapore\nmaxichu@u.nus.edu\nYe Wang\nNational University of Singapore\nwangye@comp.nus.edu.sgRoger B. Dannenberg\nCarnegie Mellon University\nrbd@cs.cmu.edu\nMay 8, 2021\nAbstract\nMany practices have been presented in music generation recently.",
                "Personalized Popular Music Generation Using Imitation\nand Structure\nShuqi Dai\nCarnegie Mellon University\nshuqid@cs.cmu.eduXichu Ma\nNational University of Singapore\nmaxichu@u.nus.edu\nYe Wang\nNational University of Singapore\nwangye@comp.nus.edu.sgRoger B. Dannenberg\nCarnegie Mellon University\nrbd@cs.cmu.edu\nMay 8, 2021\nAbstract\nMany practices have been presented in music generation recently.",
                "An evaluation using 10 pop songs shows\nthat our new representations and methods are able to create high-quality stylistic mu-\nsic that is similar to a given input song.",
                "An evaluation using 10 pop songs shows\nthat our new representations and methods are able to create high-quality stylistic mu-\nsic that is similar to a given input song.",
                "[cs.SD]  10 May 2021Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\ngoal is to create personalized music automatically by imitating one or more songs that are\nselected by a user.",
                "[cs.SD]  10 May 2021Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\ngoal is to create personalized music automatically by imitating one or more songs that are\nselected by a user.",
                "In addition, automatic generation of\nmusic for music therapy can avoid the copyright issues and access to large corpora of popular\nmusic, especially in communities with limited resources and budgets.",
                "In addition, automatic generation of\nmusic for music therapy can avoid the copyright issues and access to large corpora of popular\nmusic, especially in communities with limited resources and budgets.",
                "Another signi\fcant issue in music generation is music structure, particularly at the level\n2Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nFigure 1: Inverted-U relationship: the Wundt Curve originally suggested by Wundt and\nlater adapted by Berlyne (1971), and the linking of favorability to familiarity/time curve by\nSluckin et al.",
                "Another signi\fcant issue in music generation is music structure, particularly at the level\n2Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nFigure 1: Inverted-U relationship: the Wundt Curve originally suggested by Wundt and\nlater adapted by Berlyne (1971), and the linking of favorability to familiarity/time curve by\nSluckin et al.",
                "For example a common pattern in popular music songs is ABABB , where letters\nstand for sections and repeated letters represent an approximate repetition of a section.",
                "For example a common pattern in popular music songs is ABABB , where letters\nstand for sections and repeated letters represent an approximate repetition of a section.",
                "We introduce a stylistic music generation model that is able to capture melody, chord\nand bass style from a single pop song and imitate them with structure information in a new\ncomplete piece.",
                "We introduce a stylistic music generation model that is able to capture melody, chord\nand bass style from a single pop song and imitate them with structure information in a new\ncomplete piece.",
                "We note that imitation of performance, orchestration,\nand production (especially in pop music) are also important aspects of style and perceived\nsimilarity, but we leave these to future work.",
                "We note that imitation of performance, orchestration,\nand production (especially in pop music) are also important aspects of style and perceived\nsimilarity, but we leave these to future work.",
                "Our work focuses on popular music for multiple reasons.",
                "Our work focuses on popular music for multiple reasons.",
                "The second reason\nfor studying popular music is the practical consideration that most people have familiarity\n3Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nand knowledge of popular music styles.",
                "The second reason\nfor studying popular music is the practical consideration that most people have familiarity\n3Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nand knowledge of popular music styles.",
                "Therefore, it is easier to compare, discuss, and\nevaluate popular music than other music.",
                "Therefore, it is easier to compare, discuss, and\nevaluate popular music than other music.",
                "It is also the case that popular music has many\nconventions that seem to simplify the music generation problem.",
                "It is also the case that popular music has many\nconventions that seem to simplify the music generation problem.",
                "This is not to say that\ntruly great popular music is easy to make, but at least we can formalize approaches that\ngenerate serviceable popular music.",
                "This is not to say that\ntruly great popular music is easy to make, but at least we can formalize approaches that\ngenerate serviceable popular music.",
                "Thus, we are able to learn enough from\na single input song to form an imitation, even when seeds vary from Chinese Pop to Western\nPop songs.",
                "Thus, we are able to learn enough from\na single input song to form an imitation, even when seeds vary from Chinese Pop to Western\nPop songs.",
                "Recently,\ndeep generative models have become popular in this \feld (Briot, Hadjeres, & Pachet, 2019).",
                "Recently,\ndeep generative models have become popular in this \feld (Briot, Hadjeres, & Pachet, 2019).",
                "Also, the quality\n4Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nof the model largely depends on the style/rule design.",
                "Also, the quality\n4Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nof the model largely depends on the style/rule design.",
                "While the results from this ap-\nproach are impressive, they often lack common popular hierarchical music structure such as\n4-or-8-bar phrases or repetition of melodic and rhythmic patterns.",
                "While the results from this ap-\nproach are impressive, they often lack common popular hierarchical music structure such as\n4-or-8-bar phrases or repetition of melodic and rhythmic patterns.",
                "Elowsson and Friberg (2012) created an algorithmic composition system for popular\nmusic using probabilistic methods guided by music theory.",
                "Elowsson and Friberg (2012) created an algorithmic composition system for popular\nmusic using probabilistic methods guided by music theory.",
                "This system generates a generic popular music style, but\ncannot represent style variations, personalize music, or model bass style.",
                "This system generates a generic popular music style, but\ncannot represent style variations, personalize music, or model bass style.",
                "5Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n3 Data Representation and Pre-processing\nThe input/output data is represented in quantized MIDI format.",
                "5Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n3 Data Representation and Pre-processing\nThe input/output data is represented in quantized MIDI format.",
                "6Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n4.1",
                "6Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n4.1",
                "We developed three ways to generate new structures: (1) copy the seed song structure; (2)\ngenerate according to a speci\fcation string such as `AABABC' from the user and treat each\n1Seed songs for di\u000berent modules can be di\u000berent, e.g. taking the melody from seed A and bass from\nseed B.\n7Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nsection as 8 bars; (3) generate random structures, which includes selecting from a collection\nof typical structures.",
                "We developed three ways to generate new structures: (1) copy the seed song structure; (2)\ngenerate according to a speci\fcation string such as `AABABC' from the user and treat each\n1Seed songs for di\u000berent modules can be di\u000berent, e.g. taking the melody from seed A and bass from\nseed B.\n7Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nsection as 8 bars; (3) generate random structures, which includes selecting from a collection\nof typical structures.",
                "4.3 Stylistic Chord Generation\nTo generate convincing chord progressions while imitating the harmonic style of the seed\nsong, we combine statistical features from a general popular music data set2, seed song\nstatistics, and distinctive sequences from the seed song.",
                "4.3 Stylistic Chord Generation\nTo generate convincing chord progressions while imitating the harmonic style of the seed\nsong, we combine statistical features from a general popular music data set2, seed song\nstatistics, and distinctive sequences from the seed song.",
                "2https://github.com/tmc323/Chord-Annotations\n8Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nBlending Transition Matrix Ignoring distinctive sequences for the moment, we create\n\frst-order Markov chain chord transition matrices from both general statistics and the seed\nsong, then blend them together using Ptrans = (1\u0000\u000b)",
                "2https://github.com/tmc323/Chord-Annotations\n8Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nBlending Transition Matrix Ignoring distinctive sequences for the moment, we create\n\frst-order Markov chain chord transition matrices from both general statistics and the seed\nsong, then blend them together using Ptrans = (1\u0000\u000b)",
                "We identify distinc-\ntive chord sequences in the seed song by comparing statistical features between the seed song\nand a general dataset consisting of 300 annotated pop songs.",
                "We identify distinc-\ntive chord sequences in the seed song by comparing statistical features between the seed song\nand a general dataset consisting of 300 annotated pop songs.",
                "Stylistic Cadence Cadences are commonly used in popular music to end a phrase or sec-\ntion.",
                "Stylistic Cadence Cadences are commonly used in popular music to end a phrase or sec-\ntion.",
                "Again,\n9Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nthe cadence transition matrix combines both general statistics and seed song statistics, i.e.\nspeci\fc cadences in the seed song.",
                "Again,\n9Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nthe cadence transition matrix combines both general statistics and seed song statistics, i.e.\nspeci\fc cadences in the seed song.",
                "The stylistic rating functions used to extract and represent melody style feature are\ninspired by Elowsson and Friberg (2012), who use similar methods to generate popular\nmusic.",
                "The stylistic rating functions used to extract and represent melody style feature are\ninspired by Elowsson and Friberg (2012), who use similar methods to generate popular\nmusic.",
                "10Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nPitch frequency Frequency of pitch poccurring in both the seed song statistics and\ngeneral popular music statistics:\nPfreq(p)",
                "10Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nPitch frequency Frequency of pitch poccurring in both the seed song statistics and\ngeneral popular music statistics:\nPfreq(p)",
                "Pitch harmony with chord progression Given chord c, probability of pitch poccurring\nin both the seed song statistics and general popular music statistics:\nPhar(pjc) =\u000bPhar;seed (pjc) + (1\u0000\u000b)Phar;general (pjc);\nwherePhar;seed (pjc) =jnotes with pitch pand chordcin seed songj\njnotes with chord cin seed songj(4)\nPitch interval frequency Frequency of consecutive pitch interval p\u0000pprevoccurring in\nboth the seed song statistics and general popular music statistics.",
                "Pitch harmony with chord progression Given chord c, probability of pitch poccurring\nin both the seed song statistics and general popular music statistics:\nPhar(pjc) =\u000bPhar;seed (pjc) + (1\u0000\u000b)Phar;general (pjc);\nwherePhar;seed (pjc) =jnotes with pitch pand chordcin seed songj\njnotes with chord cin seed songj(4)\nPitch interval frequency Frequency of consecutive pitch interval p\u0000pprevoccurring in\nboth the seed song statistics and general popular music statistics.",
                "Pinterval (p\u0000pprev) =Pinterval (\u0001p) =\u000bPinterval;seed (\u0001p) + (1\u0000\u000b)Pinterval;general (\u0001p);\nwherePinterval;seed (\u0001p) =jinterval \u0001 pin seed songj\njintervals in seed song j(5)\nPitch interval with harmony Given chords c1andc2, probability of the pitch interval\np\u0000pprevoccurring in both the seed song statistics and general popular music statistics.",
                "Pinterval (p\u0000pprev) =Pinterval (\u0001p) =\u000bPinterval;seed (\u0001p) + (1\u0000\u000b)Pinterval;general (\u0001p);\nwherePinterval;seed (\u0001p) =jinterval \u0001 pin seed songj\njintervals in seed song j(5)\nPitch interval with harmony Given chords c1andc2, probability of the pitch interval\np\u0000pprevoccurring in both the seed song statistics and general popular music statistics.",
                "11Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nNote duration frequency Frequency of note duration (length) dappearing in both the\nseed song statistics and general popular music statistics,\nPdurfreq (d) =\u000bPdurfreq;seed (d) + (1\u0000\u000b)Pdurfreq;general (djtempo );\nwherePdurfreq;seed (d) =jnotes with duration din seed songj\njnotes in seed song j(7)",
                "11Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nNote duration frequency Frequency of note duration (length) dappearing in both the\nseed song statistics and general popular music statistics,\nPdurfreq (d) =\u000bPdurfreq;seed (d) + (1\u0000\u000b)Pdurfreq;general (djtempo );\nwherePdurfreq;seed (d) =jnotes with duration din seed songj\njnotes in seed song j(7)",
                "Note duration transition Probability of a note duration given previous consecutive note\ndurationdin both the seed song statistics and general popular music statistics,\nPdurtrans (d1jd2) =\u000bPdurtrans;seed (d1jd2) + (1\u0000\u000b)Pdurtrans;general (d1jd2);\nwherePdurtrans;seed (d1jd2) =jnote transitions with duration d2tod1in seed songj\njnote transitions in seed song j(8)\nRest note duration Rest notes are more common to see in the last bar of a phrase/section.",
                "Note duration transition Probability of a note duration given previous consecutive note\ndurationdin both the seed song statistics and general popular music statistics,\nPdurtrans (d1jd2) =\u000bPdurtrans;seed (d1jd2) + (1\u0000\u000b)Pdurtrans;general (d1jd2);\nwherePdurtrans;seed (d1jd2) =jnote transitions with duration d2tod1in seed songj\njnote transitions in seed song j(8)\nRest note duration Rest notes are more common to see in the last bar of a phrase/section.",
                "If we use Phar,\n12Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nbased on song statistics, to capture the notion of chord tones, the following rather obscure\nequation captures this heuristic:\nP(p;djc) = ((Phar(pjc)\u00000:5)\u0003log2(d=4) +",
                "If we use Phar,\n12Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nbased on song statistics, to capture the notion of chord tones, the following rather obscure\nequation captures this heuristic:\nP(p;djc) = ((Phar(pjc)\u00000:5)\u0003log2(d=4) +",
                "Last bar and the tonic Sections ending with a strong perfect authentic cadence almost\nalways end on the tonic (i.e. the pitch C) in popular music.",
                "Last bar and the tonic Sections ending with a strong perfect authentic cadence almost\nalways end on the tonic (i.e. the pitch C) in popular music.",
                "+w2\u0001dist dir(prev 1\u0000p1;prev 2\u0000p2);\nwherew1= 1:0 andw2= 2:0 are weights\nandprev iis the pitch before pi: (13)\n13Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\ndist pit(p1;p2)",
                "+w2\u0001dist dir(prev 1\u0000p1;prev 2\u0000p2);\nwherew1= 1:0 andw2= 2:0 are weights\nandprev iis the pitch before pi: (13)\n13Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\ndist pit(p1;p2)",
                "14Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "14Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "15Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n5 Evaluation\nWe conducted both objective and subjective evaluations of our stylistic music generation\nsystem.",
                "15Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n5 Evaluation\nWe conducted both objective and subjective evaluations of our stylistic music generation\nsystem.",
                "The other ten songs, \fve Western pop songs and\n\fve Chinese pop songs, are used as seed songs for evaluation.",
                "The other ten songs, \fve Western pop songs and\n\fve Chinese pop songs, are used as seed songs for evaluation.",
                "These test songs are quite\npopular and recognizable.",
                "These test songs are quite\npopular and recognizable.",
                "One application of our system is to imitate Parkinson patients'\nfavorite pop songs, so we wanted to imitate songs that are already popular and familiar.",
                "One application of our system is to imitate Parkinson patients'\nfavorite pop songs, so we wanted to imitate songs that are already popular and familiar.",
                "Then, the listeners answered four parts of the evaluation (melody, chord,\n16Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nbass and combination) in a random order.",
                "Then, the listeners answered four parts of the evaluation (melody, chord,\n16Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nbass and combination) in a random order.",
                "We also adjust song settings such as pitch\n17Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n(a) Gender\n (b) Age.",
                "We also adjust song settings such as pitch\n17Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n(a) Gender\n (b) Age.",
                "A problem with measuring preference is that familiar music is generally preferred over\n18Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "A problem with measuring preference is that familiar music is generally preferred over\n18Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "19Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nFigure 12: Correlation between preference-of-A \u0000preference-of-B and preference-of-A' \u0000\npreference-of-B' (all four values from the same listener), where A and B are seed songs, A'\nand B' are imitations.",
                "19Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nFigure 12: Correlation between preference-of-A \u0000preference-of-B and preference-of-A' \u0000\npreference-of-B' (all four values from the same listener), where A and B are seed songs, A'\nand B' are imitations.",
                "Perhaps when listeners are more familiar with the seed song, they form strong\nexpectations and are therefore more attentive to di\u000berences between the imitation and seed\nsong.\n20Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "Perhaps when listeners are more familiar with the seed song, they form strong\nexpectations and are therefore more attentive to di\u000berences between the imitation and seed\nsong.\n20Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "6 Conclusions\nWe have described techniques to automatically generate stylistic melody, harmony and bass\nlines for pop songs with a logical and hierarchical music structure.",
                "6 Conclusions\nWe have described techniques to automatically generate stylistic melody, harmony and bass\nlines for pop songs with a logical and hierarchical music structure.",
                "Our original motivation\n21Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nwas to enable the creation of therapeutic music where repeated listening might call for\nsome variety, but where therapists might not have the time, resources or permission to use\nexisting popular music.",
                "Our original motivation\n21Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nwas to enable the creation of therapeutic music where repeated listening might call for\nsome variety, but where therapists might not have the time, resources or permission to use\nexisting popular music.",
                "AAAI Press.\n22Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nBriot, J.-P., Hadjeres, G., & Pachet, F. (2019).",
                "AAAI Press.\n22Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nBriot, J.-P., Hadjeres, G., & Pachet, F. (2019).",
                "Automatic analysis and in\ruence of\nhierarchical structure on melody, rhythm and harmony in popular music.",
                "Automatic analysis and in\ruence of\nhierarchical structure on melody, rhythm and harmony in popular music.",
                "Algorithmic composition of popular music.",
                "Algorithmic composition of popular music.",
                "University\nof Cambridge ,8, 19{48.\n23Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nLo, M., & Lucas, S. M. (2006).",
                "University\nof Cambridge ,8, 19{48.\n23Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nLo, M., & Lucas, S. M. (2006).",
                "24Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "24Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?",
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?",
                "25Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "25Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)"
            ],
            "classical": [
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?"
            ],
            "electronic": [],
            "hip-hop": [
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?"
            ],
            "reggae": [],
            "country": [
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?"
            ],
            "r&b": [
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?"
            ],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "In Proceedings of the 3rd international conference on knowledge discovery and data\nmining (p. 359{370)."
            ],
            "techno": [],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network",
            "homophonic": [],
            "polyphonic": [
                "Mogren [42] proposed an RNN GAN for generating single voice polyphonic music.",
                "[43] introduced a novel multi-track polyphonic symbolic music generator using GANs.",
                "[39] Generate melodies based on lyrics Conditional GAN Cross entropy LSTM generators and discriminators No evaluation was provided  [41] Generate melodies based on lyrics Conditional LSTM GAN Cross entropy Dense layer followed by 2 LSTM followed by a dense layer for generator, 2 LSTM followed by dense for discriminator BLEU-2 score of 0.735, scores of about 3.8, 3.5, 4.1 respectively out of 5 for lyrics, rhythm, and melody by evaluators [42] Generate single voice polyphonic music RNN GAN Cross entropy and Squared error loss 2 LSTM layers for generator, 2 Bi-directional LSTM layers followed by a dense for discriminator No evaluation was provided  [43] Generate multi-track, polyphonic music Conditional GAN Wasserstein Generators contain 1D transposed CONV, discriminators 5 contain 1D Conv layers followed by one dense layer The highest score for conditional generation was 3.1 and non-conditional was 3.16 out of 5 by \u2018non-pro\u2019 evaluators."
            ],
            "monophonic": [
                "The authors in [37] presented an adversarial and convolutional based architecture known as MidiNet for generating pop music monophonic melodies using 1022 pop music from an online MIDI database called TheoryTab [38].",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds"
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "In this work, five genres were considered, namely folk music, Arabic, jazz, metal rock, and classical.",
                "For intra-track metrics, jamming model performed best [44] Generate folk music RL GAN Cross entropy and policy gradient RNN generators, CNN discriminators BLEU score of 0.94 and MSE of 20.6 outperformed baseline maximum likelihood estimation D. Poetry and Literary Text Generation using GANs Researchers have also focused on literary text generation using GANs."
            ],
            "blues": [],
            "jazz": [
                "For instance, conditional GANs can be used to generate various genres of music including jazz, rock, and classic.   2) Deep Convolutional GAN: Commonly known as DCGAN",
                "In this work, five genres were considered, namely folk music, Arabic, jazz, metal rock, and classical."
            ],
            "rock": [
                "For instance, conditional GANs can be used to generate various genres of music including jazz, rock, and classic.   2) Deep Convolutional GAN: Commonly known as DCGAN",
                "In this work, five genres were considered, namely folk music, Arabic, jazz, metal rock, and classical."
            ],
            "pop": [
                "The other popular loss is the mean squared error (MSE) loss.",
                "The other popular loss is the mean squared error (MSE) loss.",
                "The authors in [37] presented an adversarial and convolutional based architecture known as MidiNet for generating pop music monophonic melodies using 1022 pop music from an online MIDI database called TheoryTab [38].",
                "The authors in [37] presented an adversarial and convolutional based architecture known as MidiNet for generating pop music monophonic melodies using 1022 pop music from an online MIDI database called TheoryTab [38].",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds"
            ],
            "classical": [
                "In this work, five genres were considered, namely folk music, Arabic, jazz, metal rock, and classical.",
                "The dataset used consists of 3697 classical music from 160 different composers.",
                "The dataset used for poetry generation consists of 740 classical and contemporary English poems.",
                "740 classical and contemporary English poems and 1500 song lyrics across various genres Perplexity score of 42.5 for poetry and 9.02 for lyrics generations IV."
            ],
            "electronic": [
                "[25] B. Kuriakose, T. Thomas, N. E. Thomas, S. J. Varghese, and V. A. Kumar, \u201cSynthesizing Images from Hand-Drawn Sketches using Conditional Generative Adversarial Networks,\u201d in 2020 International Conference on Electronics and Sustainable Communication Systems (ICESC), 2020, pp."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "The advancement in technology and deep learning in particular, has caught the attention of many researchers trying to investigate whether art generation is possible by computers and algorithms.",
                "Also, due to the advancement of technology and digitization, the two main requirements of GANs, datasets and computing power, have become widely available.",
                "Fortunately, due to the advancement in the field of deep learning, GANs emerged as a promising technology for computer generated arts.",
                "[31] C. Philip and L. H. Jong, \u201cFace sketch synthesis using conditional adversarial networks,\u201d in 2017 International Conference on Information and Communication Technology Convergence (ICTC), 2017, pp.",
                "A Hybrid Approach to Intelligent Musical Composition Based on Generative Adversarial Networks with a Variational Autoencoder,\u201d in Proceedings of the Future Technologies Conference, 2020, pp."
            ],
            "dubstep": [],
            "opera": [
                "Strided and fractionally strided convolutions are used to learn the spatial up sampling and down sampling operators.",
                "Similarly, [48] handled the discrete output space issue by forcing the discriminator to operate on continuous-valued output distributions."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "CONTROLLABLE DEEP MELODY GENERATION VIA HIERARCHICAL MUSIC STRUCTURE REPRESENTATION",
            "homophonic": [],
            "polyphonic": [
                "[3], polyphonic performance generation [4] and drum\npattern generation",
                "In the future, we hope to develop more intelligent\nways to analyze music and music frameworks supporting\na richer musical vocabulary, generation of harmony and\npolyphonic generation.",
                "Bengio, and P. Vin-\ncent, \u201cModeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic\nmusic generation and transcription,\u201d arXiv preprint\narXiv:1206.6392 , 2012.[15] J.-P.",
                "Zhang, and G. Xia, \u201cLearning in-\nterpretable representation for controllable polyphonic\nmusic generation,\u201d in Proc. of 21st International Con-\nference on Music Information Retrieval, ISMIR , 2020."
            ],
            "monophonic": [
                "Research has tackled this question\nfrom many angles, including monophonic melody genera-\n\u00a9 S. Dai, Z. Jin, C. Gomes and R. Dannenberg."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "[16] F. Liang, \u201cBachbot: Automatic composition in the\nstyle of bach chorales,\u201d University of Cambridge ,\nvol. 8, pp.",
                "[17] G. Hadjeres, F. Pachet, and F. Nielsen, \u201cDeepbach:\na steerable model for bach chorales generation,\u201d in\nProc. of the 34th International Conference on Machine\nLearning-Volume 70 ."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [
                "[7] S. H. Hakimi, N. Bhonker, and R. El-Yaniv, \u201cBebop-\nnet: Deep neural models for personalized jazz impro-\nvisations,\u201d in Proc. of the 21st International Society for\nMusic Information Retrieval Conference, ISMIR ."
            ],
            "rock": [],
            "pop": [
                "A listening test\nreveals that melodies generated by our method are rated\nas good as or better than human-composed music in the\nPOP909 dataset about half the time.",
                "A listening test\nreveals that melodies generated by our method are rated\nas good as or better than human-composed music in the\nPOP909 dataset about half the time.",
                "However, there are three problems that are dif\ufb01cult\nto address: (1) Modeling larger scale music structure and\nmultiple levels of repetition as seen in popular songs, (2)\nControllability to match music to video or create desired\ntempo, styles, and mood, and (3) Scarcity of training data\ndue to limited curated and machine-readable compositions,\nespecially in a given style.",
                "However, there are three problems that are dif\ufb01cult\nto address: (1) Modeling larger scale music structure and\nmultiple levels of repetition as seen in popular songs, (2)\nControllability to match music to video or create desired\ntempo, styles, and mood, and (3) Scarcity of training data\ndue to limited curated and machine-readable compositions,\nespecially in a given style.",
                "There are a few\nmodels using rule-based and statistical methods to con-\nstruct long-term repetitive structure in classical music [19]\nand pop music",
                "There are a few\nmodels using rule-based and statistical methods to con-\nstruct long-term repetitive structure in classical music [19]\nand pop music",
                "Machine learning models with\nmemory and the ability to associate context have also been\npopular in this area and include LSTMs and Transformers\n[4, 6, 22, 23], which operate by generating music one or a\nfew notes at a time, based on information from previously\ngenerated notes.",
                "Machine learning models with\nmemory and the ability to associate context have also been\npopular in this area and include LSTMs and Transformers\n[4, 6, 22, 23], which operate by generating music one or a\nfew notes at a time, based on information from previously\ngenerated notes.",
                "StructureNet [3], PopMNet",
                "StructureNet [3], PopMNet",
                "The most popular models are Vari-\national Auto-Encoders (V AE) and their variants",
                "The most popular models are Vari-\national Auto-Encoders (V AE) and their variants",
                "We describe a controllable melody generation system that\nuses hierarchical music representation to generate full-\nlength pop song melodies with multi-level repetition and\nstructure.",
                "We describe a controllable melody generation system that\nuses hierarchical music representation to generate full-\nlength pop song melodies with multi-level repetition and\nstructure.",
                "Our work is with pop music because structures are rel-\natively simple and listeners are generally familiar with the\nstyle and thus able to evaluate compositions.",
                "Our work is with pop music because structures are rel-\natively simple and listeners are generally familiar with the\nstyle and thus able to evaluate compositions.",
                "We use a Chi-\nnese pop song dataset, POP909",
                "We use a Chi-\nnese pop song dataset, POP909",
                "Sections contain multi-\nple phrases, e.g., the illustrated song has an intro, a main\ntheme section (phrase A as verse and phrase B as chorus),\na bridge section followed by a repeat of the theme, and an\noutro section, which is a typical pop song structure.",
                "Sections contain multi-\nple phrases, e.g., the illustrated song has an intro, a main\ntheme section (phrase A as verse and phrase B as chorus),\na bridge section followed by a repeat of the theme, and an\noutro section, which is a typical pop song structure.",
                "With the analysis algorithm, we can process a music\ndataset such as POP909 for subsequent machine learning\nand music generation.",
                "With the analysis algorithm, we can process a music\ndataset such as POP909 for subsequent machine learning\nand music generation.",
                "We used 4188 phrases from 528 songs in major mode\nfrom the POP909 dataset, using 90% of them as training\ndata and the other 10% for validation.",
                "We used 4188 phrases from 528 songs in major mode\nfrom the POP909 dataset, using 90% of them as training\ndata and the other 10% for validation.",
                "We con\ufb01rmed that our generation exhibits sim-\nilar structure-related distributions to that of the POP909\ndataset.",
                "We con\ufb01rmed that our generation exhibits sim-\nilar structure-related distributions to that of the POP909\ndataset.",
                "The demographics information about the\nlisteners are as follows:\nGender male: 120, female: 75, other: 1;\nAge distribution 0-10: 0, 11-20: 17, 21-30: 149, 31-40:\n28, 41-50: 0, 51-60: 2, >60: 0;\nMusic pro\ufb01ciency levels lowest (listen to music <1\nhour/week): 16, low (listen to music 1\u201315 hours/week):\n62, medium (listen to music >15 hours/week): 21, high\n(studied music for 1\u20135 years): 52, expert ( >5 years of\nmusic practice): 44;\nNationality Chinese: 180, Others: 16 (note that the\nPOP909 dataset is primarily Chinese pop songs, and lis-\nteners who are more familiar with this style are likely to be\nmore reliable and discriminating raters.)",
                "The demographics information about the\nlisteners are as follows:\nGender male: 120, female: 75, other: 1;\nAge distribution 0-10: 0, 11-20: 17, 21-30: 149, 31-40:\n28, 41-50: 0, 51-60: 2, >60: 0;\nMusic pro\ufb01ciency levels lowest (listen to music <1\nhour/week): 16, low (listen to music 1\u201315 hours/week):\n62, medium (listen to music >15 hours/week): 21, high\n(studied music for 1\u20135 years): 52, expert ( >5 years of\nmusic practice): 44;\nNationality Chinese: 180, Others: 16 (note that the\nPOP909 dataset is primarily Chinese pop songs, and lis-\nteners who are more familiar with this style are likely to be\nmore reliable and discriminating raters.)",
                "The last two pairs show the ratings of our\nmethod compared to music in the POP909 dataset.",
                "The last two pairs show the ratings of our\nmethod compared to music in the POP909 dataset.",
                "The human-composed songs used in this study are from\nthe most popular ones in Chinese pop history.",
                "The human-composed songs used in this study are from\nthe most popular ones in Chinese pop history.",
                "Yang, \u201cPop music transformer:\nGenerating music with rhythm and harmony,\u201d arXiv\npreprint arXiv:2002.00212 , 2020.",
                "Yang, \u201cPop music transformer:\nGenerating music with rhythm and harmony,\u201d arXiv\npreprint arXiv:2002.00212 , 2020.",
                "[20] A. Elowsson and A. Friberg, \u201cAlgorithmic composition\nof popular music,\u201d in the 12th International Confer-\nence on Music Perception and Cognition and the 8th\nTriennial Conference of the European Society for the\nCognitive Sciences of Music , 2012, pp.",
                "[20] A. Elowsson and A. Friberg, \u201cAlgorithmic composition\nof popular music,\u201d in the 12th International Confer-\nence on Music Perception and Cognition and the 8th\nTriennial Conference of the European Society for the\nCognitive Sciences of Music , 2012, pp.",
                "Wang, and R. B. Dannenberg, \u201cPer-\nsonalized popular music generation using imitation and\nstructure,\u201d arXiv preprint arXiv:2105.04709 , 2021.",
                "Wang, and R. B. Dannenberg, \u201cPer-\nsonalized popular music generation using imitation and\nstructure,\u201d arXiv preprint arXiv:2105.04709 , 2021.",
                "[24] J. Wu, X. Liu, X. Hu, and J. Zhu, \u201cPopmnet: Gener-\nating structured pop music melodies using neural net-\nworks,\u201d Arti\ufb01cial Intelligence , vol.",
                "[24] J. Wu, X. Liu, X. Hu, and J. Zhu, \u201cPopmnet: Gener-\nating structured pop music melodies using neural net-\nworks,\u201d Arti\ufb01cial Intelligence , vol.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset for\nmusic arrangement generation,\u201d in Proc. of 21st Inter-\nnational Conference on Music Information Retrieval,\nISMIR , 2020.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset for\nmusic arrangement generation,\u201d in Proc. of 21st Inter-\nnational Conference on Music Information Retrieval,\nISMIR , 2020.",
                "[36] S. Dai, H. Zhang, and R. B. Dannenberg, \u201cAuto-\nmatic analysis and in\ufb02uence of hierarchical structure\non melody, rhythm and harmony in popular music,\u201d in\nProc. of the 2020 Joint Conference on AI Music Cre-\nativity (CSMC-MuMe) , 2020.",
                "[36] S. Dai, H. Zhang, and R. B. Dannenberg, \u201cAuto-\nmatic analysis and in\ufb02uence of hierarchical structure\non melody, rhythm and harmony in popular music,\u201d in\nProc. of the 2020 Joint Conference on AI Music Cre-\nativity (CSMC-MuMe) , 2020.",
                "[38] S. Dai, H. Zhang, and R. B. Dannenberg, \u201cAuto-\nmatic analysis and in\ufb02uence of hierarchical structure\non melody, rhythm and harmony in popular music,\u201d in\nin Proc. of the 2020 Joint Conference on AI Music Cre-\nativity (CSMC-MuMe 2020) , 2020.",
                "[38] S. Dai, H. Zhang, and R. B. Dannenberg, \u201cAuto-\nmatic analysis and in\ufb02uence of hierarchical structure\non melody, rhythm and harmony in popular music,\u201d in\nin Proc. of the 2020 Joint Conference on AI Music Cre-\nativity (CSMC-MuMe 2020) , 2020."
            ],
            "classical": [
                "There are a few\nmodels using rule-based and statistical methods to con-\nstruct long-term repetitive structure in classical music [19]\nand pop music"
            ],
            "electronic": [
                "[9] L. Hiller and L. Isaacson, Experimental Music: Com-\nposition with an Electronic Computer ."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "We compared this model with several alternatives: the\nsecond model is a bi-directional LSTM followed by a uni-\ndirectional LSTM (model size is 64 in both).",
                "For this \u201csanity check,\u201d we randomly picked 20 test\nsongs and generated 500 alternative basic melodies and\nrhythm forms."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [],
            "dubstep": [],
            "opera": [
                "Machine learning models with\nmemory and the ability to associate context have also been\npopular in this area and include LSTMs and Transformers\n[4, 6, 22, 23], which operate by generating music one or a\nfew notes at a time, based on information from previously\ngenerated notes."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer",
            "homophonic": [],
            "polyphonic": [
                "We report on objective and subjective evaluations of variants of\nthe proposed Theme Transformer and the conventional prompt-\nbased baseline, showing that our best model can generate, to\nsome extent, polyphonic pop piano music with repetition and\nplausible variations of a given condition.",
                "We present an empirical performance comparison between\nthe proposed theme-conditioned Transformer and the con-\nventional prompt-conditioned Transformer [13], [17] for gen-\nerating polyphonic piano music.",
                "[13] greatly improved\nupon this by presenting the \ufb01rst Transformer decoder model\nfor symbolic music generation, showing that a Transformer\nmodel can generate coherent minute-long polyphonic piano\nmusic with local repetitions and variations.",
                "In the objective evaluation, we let each model generate 64-\nbar polyphonic piano music using the thematic conditions\nretrieved from each of the 29 testing songs of POP909.",
                "Zhang, and G. Xia, \u201cLearning interpretable\nrepresentation for controllable polyphonic music generation,\u201d in Proc.\nInt.",
                "[51] D. Meredith, K. Lemstrm, and G. A. Wiggins, \u201cAlgorithms for discover-\ning repeated patterns in multidimensional representations of polyphonic\nmusic,\u201d Journal of New Music Research , vol."
            ],
            "monophonic": [
                "In this way, the ending a note would be exactly\nthe beginning of a new note (including REST), as the melody\nis monophonic.",
                "[10] J. Grekow and T. Dimitrova-Grekow, \u201cMonophonic music generation\nwith a given emotion using conditional variational autoencoder,\u201d IEEE\nAccess , vol. 9, pp."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "E. Li, G. W. Cottrell, and J. McAuley,\n\u201cLakhNES: Improving multi-instrumental music generation with cross-\ndomain pre-training,\u201d in Proc."
            ],
            "vocal": [
                "Each\narrangement is stored in three separate MIDI tracks: MELODY\n(transcription of the lead melody, which corresponds to the\nvocal), BRIDGE (the secondary melodies or lead instruments),\nand PIANO (the main body of the accompaniment), making\n8At inference time, this implies that there is already a T HEME -START token\nbut not yet a T HEME -ENDtoken in the recent past."
            ],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "Ren, \u201cClosed patterns in folk music and other genres,\u201d in Proc.",
                "Workshop on Folk Music Analysis , 2016."
            ],
            "blues": [],
            "jazz": [
                "Yang, \u201cThe Jazz Transformer on the front line:\nExploring the shortcomings of AI-composed music through quantitative\nmeasures,\u201d in Proc."
            ],
            "rock": [],
            "pop": [
                "To condition\nthe generation process of such a model with a user-speci\ufb01ed\nsequence, a popular approach is to take that conditioning se-\nquence as a priming sequence and ask a Transformer decoder to\ngenerate a continuation.",
                "To condition\nthe generation process of such a model with a user-speci\ufb01ed\nsequence, a popular approach is to take that conditioning se-\nquence as a priming sequence and ask a Transformer decoder to\ngenerate a continuation.",
                "We report on objective and subjective evaluations of variants of\nthe proposed Theme Transformer and the conventional prompt-\nbased baseline, showing that our best model can generate, to\nsome extent, polyphonic pop piano music with repetition and\nplausible variations of a given condition.",
                "We report on objective and subjective evaluations of variants of\nthe proposed Theme Transformer and the conventional prompt-\nbased baseline, showing that our best model can generate, to\nsome extent, polyphonic pop piano music with repetition and\nplausible variations of a given condition.",
                "Among such efforts, the adoption of the Transformer decoder-\nbased neural network [12] as the backbone generative model\nhas become popular [13]\u2013[24].",
                "Among such efforts, the adoption of the Transformer decoder-\nbased neural network [12] as the backbone generative model\nhas become popular [13]\u2013[24].",
                "The original piece here is an excerpt of the\nsong \u2018907.mid\u2019 from the test split of the POP909 dataset\n[25], starting from its thematic fragment.",
                "The original piece here is an excerpt of the\nsong \u2018907.mid\u2019 from the test split of the POP909 dataset\n[25], starting from its thematic fragment.",
                "Note that the embedding distance as the\naverage of the whole POP909 dataset is 0.895.",
                "Note that the embedding distance as the\naverage of the whole POP909 dataset is 0.895.",
                "Similar to the popular \u201cprompt-based\u201d approach,\nthe proposed approach also uses a music fragment (e.g., a\nmusical theme) as input to condition the generative model.",
                "Similar to the popular \u201cprompt-based\u201d approach,\nthe proposed approach also uses a music fragment (e.g., a\nmusical theme) as input to condition the generative model.",
                "2: The piano roll of the \ufb01rst 24 bars of a composition by Theme Transformer, conditioned on the theme of an unseen\ntesting song \u2018899.mid\u2019 from POP909.",
                "2: The piano roll of the \ufb01rst 24 bars of a composition by Theme Transformer, conditioned on the theme of an unseen\ntesting song \u2018899.mid\u2019 from POP909.",
                "Speci\ufb01cally, we train the\nmodels using the training split of the POP909 dataset [25],\nand conduct objective and subjective evaluations on its test\nsplit.",
                "Speci\ufb01cally, we train the\nmodels using the training split of the POP909 dataset [25],\nand conduct objective and subjective evaluations on its test\nsplit.",
                "The subjective evaluation en-\ntails an online listening test that involves listeners familiar\nand unfamiliar with the music pieces in the POP909 dataset.",
                "The subjective evaluation en-\ntails an online listening test that involves listeners familiar\nand unfamiliar with the music pieces in the POP909 dataset.",
                "Our evaluation shows that the proposed Theme Transformer\nexhibits similar perceptual theme controllability and theme\nvariation as original pieces in the test split of POP909.",
                "Our evaluation shows that the proposed Theme Transformer\nexhibits similar perceptual theme controllability and theme\nvariation as original pieces in the test split of POP909.",
                "Prompt-based conditioning is arguably to date the most\npopular approach to condition the generation process of the\nTransformer decoder via a conditioning sequence [42]\u2013[45].",
                "Prompt-based conditioning is arguably to date the most\npopular approach to condition the generation process of the\nTransformer decoder via a conditioning sequence [42]\u2013[45].",
                "We identify the melody notes for each fragment using\nthe annotations of POP909 dataset.",
                "We identify the melody notes for each fragment using\nthe annotations of POP909 dataset.",
                "Speci\ufb01cally, we adopt the\nfollowing three rules, motivated by common techniques in pop\nsong writing:\n\u000fPitch shift on scale : Keep the same contour of a melody\nbut shift them according to their positions in the musical\nscale.",
                "Speci\ufb01cally, we adopt the\nfollowing three rules, motivated by common techniques in pop\nsong writing:\n\u000fPitch shift on scale : Keep the same contour of a melody\nbut shift them according to their positions in the musical\nscale.",
                "5According to our own listening of the original pieces in POP909, a music\nphrase in POP909 usually lasts for two bars, possibly due to the comfortable\nbreath length for human singing.",
                "5According to our own listening of the original pieces in POP909, a music\nphrase in POP909 usually lasts for two bars, possibly due to the comfortable\nbreath length for human singing.",
                "I MPLEMENTATION DETAILS\nA. Dataset\nWe train our models using the piano covers of Mandarin\npop music from the POP909 dataset [25], which is composed\nof the piano covers of 909 Mandarin pop songs originally\ncomposed by 462 artists, released from the earliest in 1950s\nto the latest around 2010",
                "I MPLEMENTATION DETAILS\nA. Dataset\nWe train our models using the piano covers of Mandarin\npop music from the POP909 dataset [25], which is composed\nof the piano covers of 909 Mandarin pop songs originally\ncomposed by 462 artists, released from the earliest in 1950s\nto the latest around 2010",
                "Following [22], we pick only the songs with a 4/4 time\nsignature and quantize the note onset times and duration to be\nmultiples of 1/4 beat (ignoring triplets) for simplicity, using the\nbeat annotations provided by POP909.",
                "Following [22], we pick only the songs with a 4/4 time\nsignature and quantize the note onset times and duration to be\nmultiples of 1/4 beat (ignoring triplets) for simplicity, using the\nbeat annotations provided by POP909.",
                "B. Token representation\nWe consider three types of tokens to represent each song\nfrom POP909 by a token sequence.",
                "B. Token representation\nWe consider three types of tokens to represent each song\nfrom POP909 by a token sequence.",
                "In the objective evaluation, we let each model generate 64-\nbar polyphonic piano music using the thematic conditions\nretrieved from each of the 29 testing songs of POP909.",
                "In the objective evaluation, we let each model generate 64-\nbar polyphonic piano music using the thematic conditions\nretrieved from each of the 29 testing songs of POP909.",
                "The scores are in general closer to those of the original pieces (i.e., test split of\nPOP909) the better.",
                "The scores are in general closer to those of the original pieces (i.e., test split of\nPOP909) the better.",
                "The \ufb01rst user group\nis familiar with Mandarin pop music, while the second group does not.",
                "The \ufb01rst user group\nis familiar with Mandarin pop music, while the second group does not.",
                "Therefore, we require\nour subjects to self-report whether they are familiar with\nMandarin pop music, and only ask those who are unfamiliar\nto listen to the original pieces.",
                "Therefore, we require\nour subjects to self-report whether they are familiar with\nMandarin pop music, and only ask those who are unfamiliar\nto listen to the original pieces.",
                "We ask the\nsubjects to indicate (yes or no) whether they are familiar\nwith Mandarin pop music by the question: \u201cDo you listen\nto Mandarin pop songs?\u201d",
                "We ask the\nsubjects to indicate (yes or no) whether they are familiar\nwith Mandarin pop music by the question: \u201cDo you listen\nto Mandarin pop songs?\u201d",
                "11Please note that, the \u201coriginal pieces\u201d used in the objective and subjective\nevaluations are actually not the original MIDI \ufb01les provided by POP909, but\ninstead MIDI \ufb01les converted from our token sequences.",
                "11Please note that, the \u201coriginal pieces\u201d used in the objective and subjective\nevaluations are actually not the original MIDI \ufb01les provided by POP909, but\ninstead MIDI \ufb01les converted from our token sequences.",
                "We conjecture that this can be attributed to\nthe short sequence length N= 512 , which amounts to about\n9 bars of music only in POP909.",
                "We conjecture that this can be attributed to\nthe short sequence length N= 512 , which amounts to about\n9 bars of music only in POP909.",
                "On the Quality of Theme Retrieval\nTo have an idea of the performance and limit of the pro-\nposed \u201ccontrastive learning +clustering\u201d based theme retrieval\nmethod (denoted as CLhereafter), we invite three annotators\nwith some musical training to annotate the theme and its occur-\nrences of six random songs from the test set of POP909.12We\nhave three annotators to account for the possible subjectivity\nof the theme labeling.",
                "On the Quality of Theme Retrieval\nTo have an idea of the performance and limit of the pro-\nposed \u201ccontrastive learning +clustering\u201d based theme retrieval\nmethod (denoted as CLhereafter), we invite three annotators\nwith some musical training to annotate the theme and its occur-\nrences of six random songs from the test set of POP909.12We\nhave three annotators to account for the possible subjectivity\nof the theme labeling.",
                "However, as\nmentioned in Section IX-B, a music piece (e.g., a sonata, or\na pop song in verse-chorus form) can often contain multiple\nthemes, which may be similar or in stark contrast to each\nother.",
                "However, as\nmentioned in Section IX-B, a music piece (e.g., a sonata, or\na pop song in verse-chorus form) can often contain multiple\nthemes, which may be similar or in stark contrast to each\nother.",
                "[2] A. Elowsson and A. Friberg, \u201cAlgorithmic composition of popular\nmusic,\u201d in Proc.",
                "[2] A. Elowsson and A. Friberg, \u201cAlgorithmic composition of popular\nmusic,\u201d in Proc.",
                "Yang, \u201cPop Music Transformer: Beat-based\nmodeling and generation of expressive pop piano compositions,\u201d in Proc.",
                "Yang, \u201cPop Music Transformer: Beat-based\nmodeling and generation of expressive pop piano compositions,\u201d in Proc.",
                "Liu, \u201cPopMAG:\nPop music accompaniment generation,\u201d in Proc.",
                "Liu, \u201cPopMAG:\nPop music accompaniment generation,\u201d in Proc.",
                "Zhang, M. Xu, S. Dai, X. Gu, and G. Xia,\n\u201cPOP909: A pop-song dataset for music arrangement generation,\u201d in\nProc.",
                "Zhang, M. Xu, S. Dai, X. Gu, and G. Xia,\n\u201cPOP909: A pop-song dataset for music arrangement generation,\u201d in\nProc.",
                "6: The piano roll of the \ufb01rst 24 bars of a composition by Theme Transformer, conditioned on the theme of unseen testing\nsongs (from top to bottom : \u2018875.mid\u2019 \u2018888.mid\u2019 \u2018890.mid\u2019 \u2018893.mid\u2019 \u2018894.mid\u2019 \u2018900.mid\u2019 \u2018901.mid\u2019 \u2018904.mid\u2019) from POP909.\n(Melody in magenta, accompaniment in grey, generated theme regions shaded in pink).",
                "6: The piano roll of the \ufb01rst 24 bars of a composition by Theme Transformer, conditioned on the theme of unseen testing\nsongs (from top to bottom : \u2018875.mid\u2019 \u2018888.mid\u2019 \u2018890.mid\u2019 \u2018893.mid\u2019 \u2018894.mid\u2019 \u2018900.mid\u2019 \u2018901.mid\u2019 \u2018904.mid\u2019) from POP909.\n(Melody in magenta, accompaniment in grey, generated theme regions shaded in pink)."
            ],
            "classical": [
                "A Transformer decoder can take a random seed\nand generate a piece from scratch, or take any user-provided\nmusic fragment as the \u201cprompt\u201d and generate a continuation\n1In Western classical music, both themes and motifs can be considered\nshort, salient recurring patterns.",
                "[3],\nis only for Western classical music and does not specify all\npositions where the themes occur in compositions.",
                "Moreover,\nboth of them consist only of classical music."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "In this\npaper, we propose an alternative conditioning approach, called\ntheme-based conditioning , that explicitly trains the Transformer\nto treat the conditioning sequence as a thematic material that\nhas to manifest itself multiple times in its generation result.",
                "[50]\u2013[52] as\na plausible alternative thematic material."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "[29] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu, \u201cA density-based algorithm\nfor discovering clusters in large spatial databases with noise,\u201d ser.\nKDD\u201996.",
                "Machine Learning for\nMedia Discovery Workshop, extended abstract , 2019.",
                "[39] M. Shan and S. Chiu, \u201cAlgorithmic compositions based on discovered\nmusical patterns,\u201d Multimedia Tools and Applications , vol.",
                "[46] A. Pinto, \u201cRelational motif discovery via graph spectral ranking,\u201d in\nProc.",
                "Eglin, and M. Pardoen, \u201cDiscovering motifs\nwith variants in music databases,\u201d in Proc.",
                "[51] D. Meredith, K. Lemstrm, and G. A. Wiggins, \u201cAlgorithms for discover-\ning repeated patterns in multidimensional representations of polyphonic\nmusic,\u201d Journal of New Music Research , vol.",
                "[53] D. Meredith, \u201cRECURSIA-RRT: recursive translatable point-set pattern\ndiscovery with removal of redundant translators,\u201d in Proc.",
                "Machine\nLearning and Knowledge Discovery in Databases , 2019, pp.",
                "[70] J.-L. Hsu, C.-C. Liu, and A. Chen, \u201cDiscovering nontrivial repeating\npatterns in music data,\u201d IEEE Transactions on Multimedia , vol. 3, no. 3,\npp.",
                "For fragments with no melody notes, D(S1;Si)would\nreturn NaN, leading to discontinuities in the curves."
            ],
            "techno": [],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "homophonic": [],
            "polyphonic": [
                "Here we explore\nits ability to reconstruct polyphonic music .",
                "Fig-\nure 2 indicates that DDSP generalised best to all classes, im-\nplying that polyphonic music, reconstructed by DDSP, main-\ntains much of its meaningful information."
            ],
            "monophonic": [
                "While the former two have been shownarXiv:2201.00052v1  [cs.SD]  31 Dec 2021to generate commercial music [1], [3], [8], the latter was\nproposed in the context of monophonic audio.",
                "Although DDSP was\ndesigned for monophonic music, it can also reconstruct poly-\nphonic music since it is trained to reconstruct a spectrogram."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [
                "[8] C. Carr and Z. Zukowski, \u201cGenerating albums with samplernn to imitate\nmetal, rock, and punk bands,\u201d arXiv preprint arXiv:1811.06633 , 2018."
            ],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [
                "[8] C. Carr and Z. Zukowski, \u201cGenerating albums with samplernn to imitate\nmetal, rock, and punk bands,\u201d arXiv preprint arXiv:1811.06633 , 2018."
            ],
            "alternative": [
                "Alternatively, evalu-\nation has been approached by monitoring quantities related\nThis work is funded by the UK Economic & Social Research Council (UK-\nESRC) through the research Grant No. HJ-253479 (ACLEW), the Engineering\nand Physical Sciences Research Council (EPSRC) Grant No. 2021037, and\nthe the DFG\u2019s Reinhart Koselleckproject No. 442218748 (AUDI0NOMOUS)."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "[21] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra,\n\u201cThe mtg-jamendo dataset for automatic music tagging,\u201d in Machine\nLearning for Music Discovery Workshop, International Conference on\nMachine Learning (ICML 2019) , Long Beach, CA, United States,\n2019."
            ],
            "techno": [],
            "dubstep": [],
            "opera": [
                "We measure classi\ufb01er performance according to the three\nmetrics that were used in the MediaEval 2019 competition:\nF1-score, area under the precision-recall curve (PR-AUC),\nand the area under the Receiver Operator Characteristics\ncurve (ROC-AUC)."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [
                "Brunner et al. (2018) propose MIDI-\nV AE as a method for conditional generation as well as genre\ntransfer between pop and jazz music."
            ],
            "rock": [],
            "pop": [
                "Especially the Transformer architecture (Vaswani\net al., 2017), popularized in the context of Natural Language\nprocessing (Brown et al., 2020) and then successfully ap-\n1Department of Computer Science, ETH Z \u00a8urich, Z \u00a8urich,\nSwitzerland.",
                "Especially the Transformer architecture (Vaswani\net al., 2017), popularized in the context of Natural Language\nprocessing (Brown et al., 2020) and then successfully ap-\n1Department of Computer Science, ETH Z \u00a8urich, Z \u00a8urich,\nSwitzerland.",
                "Brunner et al. (2018) propose MIDI-\nV AE as a method for conditional generation as well as genre\ntransfer between pop and jazz music.",
                "Brunner et al. (2018) propose MIDI-\nV AE as a method for conditional generation as well as genre\ntransfer between pop and jazz music.",
                "Pop Music Transformer: Beat-\nbased Modeling and Generation of Expressive Pop Piano\nCompositions.",
                "Pop Music Transformer: Beat-\nbased Modeling and Generation of Expressive Pop Piano\nCompositions."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Music Generation Using an LSTM",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "1  \n  \n \nMusic Generation Using an LSTM  \n \nMichael Conner, Lucas Gral,  Kevin Adams  \nDavid Hunger, Reagan Strelow, and Alexander Neuwirth  \nDepartment of Electrical Engineering and Computer Science  \nMilwaukee School of Engineering  \n{connerm, grall, adamsk, hungerd, strelowr, neuwirtha}@msoe.edu  \n \n \nAbstract  \nOver the past several years, deep learning for sequence modeling has grown in popularity.",
                "1  \n  \n \nMusic Generation Using an LSTM  \n \nMichael Conner, Lucas Gral,  Kevin Adams  \nDavid Hunger, Reagan Strelow, and Alexander Neuwirth  \nDepartment of Electrical Engineering and Computer Science  \nMilwaukee School of Engineering  \n{connerm, grall, adamsk, hungerd, strelowr, neuwirtha}@msoe.edu  \n \n \nAbstract  \nOver the past several years, deep learning for sequence modeling has grown in popularity."
            ],
            "classical": [
                "The songs in our data are a mix of classical music and video game soundtracks consisting \nof single -track piano music."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "In 2015 a \ngroup of researchers at Google tried to  improve the generally excepted  structure even \nfurther and set out to discover any possible  improvements that could be made."
            ],
            "techno": [
                "[7] Dillon Ranwala \u201cThe Evolution of Music and AI Technology\u201d https://watt -\nai.github.io/blog/music_ai_evolution (2020)"
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Prote\u00e7\u00e3o intelectual de obras produzidas por sistemas baseados em intelig\u00eancia artificial: uma vis\u00e3o tecnicista sobre o tema",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [
                "[Silver et al.,\n2018], a aplica\u00e7\u00e3o em mundo real de tal abordagem ainda parece incipiente - dado a natureza\ndoambientedosagentesinteligentes-quen\u00e3opossuiregrasest\u00e1ticaseambientecontroladotal\ncomo um tabuleiro de xadrez ou de go.",
                "Ent\u00e3o,\nquestiono,comopodemosdesenvolvedoreseanalistasseremdeixadosdeladododireito\nautoral advindo de uma obra produzida por IA?\n\u2022Plataforma de desenvolvimento ( software):uso aqui o termo plataforma ao inv\u00e9s de\nambiente,poisestetermo\u00e9maisamplo,abarcandon\u00e3osomenteasferramentasutilizadas\npara desenvolver a aplica\u00e7\u00e3o como as Integrated Development Environment (IDE), mas\ntamb\u00e9m os frameworks que encapsulam os algoritmos de IA."
            ],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Destaco aqui o processo de\ndescoberta de conhecimentos em bases de dados, por seu termo em l\u00edngua inglesa Knowledge\nDiscovery in Databases (KDD) proposto no trabalho seminal de Fayyad, Piatesky-Shapiro e\nSmythem1997[Fayyadetal.",
                "Fayyad,U.,Piatetsky-Shapiro,G.,andSmyth,P.(1996).Fromdatamining\nto knowledge discovery in databases.",
                "In Proceedings of the 4th international conference on the practical\napplicationsofknowledgediscoveryanddatamining ,volume1,pages29\u201339.Springer-Verlag\nLondon, UK.\n14"
            ],
            "techno": [],
            "dubstep": [],
            "opera": [
                "Emsua\nmais recente obra, Stuart Russel a\ufb01rma:\n\u201c(...) ele(Turing)prop\u00f4sumtesteoperacionalparaintelig\u00eancia,chamadodejogo\ndaimita\u00e7\u00e3o,quemaistarde(numaformasimpli\ufb01cada\ufb01cariaconhecidocomoteste\nde Turing.",
                "Evidenciei que operacionaliza\u00e7\u00e3o de uma\nIA consciente e senciente ainda est\u00e1 restrita ao universo \ufb01ccional, e que o processo de cria\u00e7\u00e3o\nde modelos-padr\u00e3o - isto \u00e9, que atendam \u00e0 um prop\u00f3sito espec\u00ed\ufb01co (e.g.: cria\u00e7\u00e3o de v\u00eddeos\nreal\u00edsticos como deepfake ou gera\u00e7\u00e3o de imagens estilizadas como DeepArt) dependem da\n8mobiliza\u00e7\u00e3o de uma grande quantidade de especialistas, como analistas de banco de dados,\nestat\u00edsticos, programadores, analistas de desempenho etc."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "An adaptive music generation architecture for games based on the deep learning Transformer model",
            "homophonic": [],
            "polyphonic": [
                "For the polyphonic\ntranscription to midi \fles, we used the Onsets and Frames transcription system [10], developed by the Magenta\nproject."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "And, perhaps even more importantly,\nwe should seek a model to support instrumental layers used in video game composition (such as the practice\nof \\striping\", which is to record orchestral sections separately for future mixing according to the whims of the\ncomposer).",
                "Some musical features (e.g., adding or removing instrumental layers (such as for striping),\nchanging the tempo, adding or removing processing, changing the pitch content. . . ) are linked to game\nplay variables."
            ],
            "vocal": [],
            "choral": [],
            "orchestral": [
                "And, perhaps even more importantly,\nwe should seek a model to support instrumental layers used in video game composition (such as the practice\nof \\striping\", which is to record orchestral sections separately for future mixing according to the whims of the\ncomposer).",
                "4.3 Layering\nWe consider layers of music, analogous to the production of orchestral music for games [37], with currently up\nto 4 layers:\n\u20221st layer, the most conservative and neutral;\n\u20222nd layer, to add more excitement, e.g., though some additional instrument;\n\u20223rd and 4th layers, to intensify the immersion and the tension.",
                "It allows de\fning some kind of\n\\orchestral blueprint\" (actually, some cartography of possible paths) for activating various musical components\nof a piece of music.",
                "Fig. 7 shows an example of visual orchestral blueprint (musical \row) in Skini.\nFigure 7: Example of orchestral \row in Skini (Opus1 Piece by Bertrand Petit)."
            ],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [
                "3(as for a musician improvising in some jazz context, balancing between constructing and following some musical\ndiscourse and \ftting into the dynamic context, in the \frst place, harmonic modulations).",
                "4.2 Training Examples\nThe user may select a corpus of music of its preference (e.g., classical, jazz, techno, ambient. ."
            ],
            "rock": [],
            "pop": [
                "It recently became popular for such applications\nas: translation, text generation (e.g., the Generative Pre-trained Transformer 3 aka GPT-3 model), biological\nsequence analysis and music generation [12].",
                "It recently became popular for such applications\nas: translation, text generation (e.g., the Generative Pre-trained Transformer 3 aka GPT-3 model), biological\nsequence analysis and music generation [12].",
                "From Pac-Man to Pop Music Interactive Audio in Games and New Media .",
                "From Pac-Man to Pop Music Interactive Audio in Games and New Media ."
            ],
            "classical": [
                "Its main novelty is a self-attention\nmechanism (as a full alternative to more classical mechanisms such as recurrence or convolution), to focus on\ncontributing elements of an input sequence.",
                "4.2 Training Examples\nThe user may select a corpus of music of its preference (e.g., classical, jazz, techno, ambient. ."
            ],
            "electronic": [
                "Experimental Music: Composition with an Electronic Computer ."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "In this context, instead of looking for alternatives or improvements in the few existing complete models\nfor game music generation (such as the excellent work by Hutchings and McCormack",
                "Its main novelty is a self-attention\nmechanism (as a full alternative to more classical mechanisms such as recurrence or convolution), to focus on\ncontributing elements of an input sequence.",
                "Plain arrows represent \fxed\npaths and bold arrows represent alternative paths which may be decided by the public."
            ],
            "k-pop": [],
            "ambient": [
                "The proposal by Je\u000bries for ambient music generation based on the Transformer [17] has also been a source\nof inspiration.",
                "4.2 Training Examples\nThe user may select a corpus of music of its preference (e.g., classical, jazz, techno, ambient. .",
                "In the experiment described\nin this paper, we have chosen a corpus of ambient music, more precisely a Spotify playlist named \\Ambient\nsongs for creativity and calm\", curated by Je\u000bries, and containing approximately 20 hours and 165 titles [18].",
                "4.5 Strategy and Control Model\nWhile planning for the future some more advanced state machine for mapping the emotions into generation\ncontrol strategies (as will be detailed in Section 5.1), in current prototype we have implemented 9 pre-de\fned\nstrategies (corresponding to the 9 emotions shown in Fig. 4 with for each one di\u000berent values corresponding to\nthe parameters for the generation: which layers are activated, which instruments (sampled or synthetic sounds,\ncurrently selected from some instruments library for ambient music within Ableton Live) are used and which\n5Figure 3: The 4 Layers (each one with a di\u000berent color) in the Ableton Live session view window\nFigure 4: Strategy/Layer/Emotion model, with the 9 pre-de\fned emotions based on the arousal/valence emotion\nmodel\n6Figure 5: Adding a new strategy named angry.",
                "4.8 Evaluation\nCurrent architecture has been tested with an emulated game model and with music generated from a corpus\nof ambient music.",
                "It has\nbeen tested with an emulated game model and with music generated from a corpus of ambient music.",
                "Ambient songs for creativity and calm."
            ],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "3(as for a musician improvising in some jazz context, balancing between constructing and following some musical\ndiscourse and \ftting into the dynamic context, in the \frst place, harmonic modulations)."
            ],
            "techno": [
                "4.2 Training Examples\nThe user may select a corpus of music of its preference (e.g., classical, jazz, techno, ambient. ."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "WHAT IS MISSING IN DEEP MUSIC GENERATION? A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC",
            "homophonic": [],
            "polyphonic": [
                "[31] C.-H. Chuan and D. Herremans, \u201cModeling tempo-\nral tonal relations in polyphonic music through deep\nnetworks with a novel image-based representation,\u201d in\nProc. of the AAAI Conference on Arti\ufb01cial Intelligence ,\nvol."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "[1] W. E. Caplan, Classical Form: A Theory of For-\nmal Functions for the Instrumental Music of Haydn,\nMozart, and Beethoven, Revised Edition ."
            ],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC\nShuqi Dai *\nCarnegie Mellon University\nshuqid@cs.cmu.eduHuiran",
                "A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC\nShuqi Dai *\nCarnegie Mellon University\nshuqid@cs.cmu.eduHuiran",
                "Analyses of two\npopular music datasets (Chinese and American) illustrate\nimportant music construction principles: (1) structure ex-\nists at multiple hierarchical levels, (2) songs use repetition\nand limited vocabulary so that individual songs do not fol-\nlow general statistics of song collections, (3) structure in-\nteracts with rhythm, melody, harmony and predictability,\nand (4) over the course of a song, repetition is not random,\nbut follows a general trend as revealed by cross-entropy.",
                "Analyses of two\npopular music datasets (Chinese and American) illustrate\nimportant music construction principles: (1) structure ex-\nists at multiple hierarchical levels, (2) songs use repetition\nand limited vocabulary so that individual songs do not fol-\nlow general statistics of song collections, (3) structure in-\nteracts with rhythm, melody, harmony and predictability,\nand (4) over the course of a song, repetition is not random,\nbut follows a general trend as revealed by cross-entropy.",
                "A study of repetition and structure in popular\nmusic\u201d, in Proc. of the 23rd Int.",
                "A study of repetition and structure in popular\nmusic\u201d, in Proc. of the 23rd Int.",
                "We use objective data analysis to support the existence and\nsigni\ufb01cance of multiple levels of hierarchy in popular mu-\nsic.",
                "We use objective data analysis to support the existence and\nsigni\ufb01cance of multiple levels of hierarchy in popular mu-\nsic.",
                "The main contribution of this work is a better under-\nstanding of the nature of repetition in popular music.",
                "The main contribution of this work is a better under-\nstanding of the nature of repetition in popular music.",
                "For ex-\nample, listening experiments with reordered Classical and\nPopular music have shown that listeners are rather insensi-\ntive to restructuring, but these results are subtle and some-\nwhat ambiguous [17].",
                "For ex-\nample, listening experiments with reordered Classical and\nPopular music have shown that listeners are rather insensi-\ntive to restructuring, but these results are subtle and some-\nwhat ambiguous [17].",
                "Another popular trend is to use sequential models\nsuch as LSTMs and Transformers [11, 22, 27] to generate\nlonger music sequences, but they still struggle to generate\nrepetition and coherent structure on long-term time scales.",
                "Another popular trend is to use sequential models\nsuch as LSTMs and Transformers [11, 22, 27] to generate\nlonger music sequences, but they still struggle to generate\nrepetition and coherent structure on long-term time scales.",
                "Figure 1 : Structure hierarchy in pop music.",
                "Figure 1 : Structure hierarchy in pop music.",
                "For training and testing, we use a Chinese pop\nsong dataset POP909",
                "For training and testing, we use a Chinese pop\nsong dataset POP909",
                "[36], which has 909 pop song per-\nformances in MIDI, and an American pop song dataset\nPDSA",
                "[36], which has 909 pop song per-\nformances in MIDI, and an American pop song dataset\nPDSA",
                "[37], in MusicXML, which has 348 American pop\nsongs originating from 1580 to 1924.",
                "[37], in MusicXML, which has 348 American pop\nsongs originating from 1580 to 1924.",
                "3.1.1 Phrases and Sections\nResearchers [13] found two levels of structure in POP909:\nsections andphrases .",
                "3.1.1 Phrases and Sections\nResearchers [13] found two levels of structure in POP909:\nsections andphrases .",
                "For example, Figure 1 has an intro, two sections connected\nby a bridge transition, and an outro, which is a typical pop\nsong structure.",
                "For example, Figure 1 has an intro, two sections connected\nby a bridge transition, and an outro, which is a typical pop\nsong structure.",
                "Figure 4 : Average cross-entropy of diatonic pitch predic-\ntions over time within a phrase in POP909, using variable-\norder Markov models.",
                "Figure 4 : Average cross-entropy of diatonic pitch predic-\ntions over time within a phrase in POP909, using variable-\norder Markov models.",
                "We extract the phrase-level structure in PDSA using the\nalgorithm in [13], and use human-labeled structure in [13]\nfor POP909.",
                "We extract the phrase-level structure in PDSA using the\nalgorithm in [13], and use human-labeled structure in [13]\nfor POP909.",
                "PDSA has\n54 different half-note patterns, while POP909 has all 128\npossible patterns (onsets are quantized to 16th notes, and\nwe assume an initial onset).",
                "PDSA has\n54 different half-note patterns, while POP909 has all 128\npossible patterns (onsets are quantized to 16th notes, and\nwe assume an initial onset).",
                "For POP909\nsongs, Figure 3(a) shows the average number of different\nonset patterns in phrases of different lengths (solid lines)\nand also the average number of patterns that occur only\nonce in the phrase (dashed lines).",
                "For POP909\nsongs, Figure 3(a) shows the average number of different\nonset patterns in phrases of different lengths (solid lines)\nand also the average number of patterns that occur only\nonce in the phrase (dashed lines).",
                "In summary, we \ufb01nd abundant evidence for repetition\nwithin phrases:\n\u2022 Compared to sampling onset patterns at random from the\nPOP909 distribution, real phrases have fewer distinct on-\nset patterns, and more onset patterns are repeated in the\nphrase (Fig 3(a)).",
                "In summary, we \ufb01nd abundant evidence for repetition\nwithin phrases:\n\u2022 Compared to sampling onset patterns at random from the\nPOP909 distribution, real phrases have fewer distinct on-\nset patterns, and more onset patterns are repeated in the\nphrase (Fig 3(a)).",
                "Predictions are a linear combination of the back-(a) Including duplicated phrases in\nforeground training\n(b) Hold out repeated phrases in\nforeground training\nFigure 5 : Average entropy, cross-entropy and accuracy of\npredicting the \ufb01rst eight diatonic pitches in each phrase,\ntuning between foreground and background in POP909.\nground and foreground models.",
                "Predictions are a linear combination of the back-(a) Including duplicated phrases in\nforeground training\n(b) Hold out repeated phrases in\nforeground training\nFigure 5 : Average entropy, cross-entropy and accuracy of\npredicting the \ufb01rst eight diatonic pitches in each phrase,\ntuning between foreground and background in POP909.\nground and foreground models.",
                "In Figure 6, we\ntrained on the entire dataset (background model) of melody\nFigure 6 : Average cross-entropy on diatonic pitches at dif-\nferent structure level positions in POP909 dataset predicted\nby background and foreground variable Markov models.\npitch sequences, holding out test songs; and also trained\non single songs (foreground model), holding out all phrase\nrepetitions to eliminate prediction by memorizing phrases,\nand holding out eight notes at a time for testing.",
                "In Figure 6, we\ntrained on the entire dataset (background model) of melody\nFigure 6 : Average cross-entropy on diatonic pitches at dif-\nferent structure level positions in POP909 dataset predicted\nby background and foreground variable Markov models.\npitch sequences, holding out test songs; and also trained\non single songs (foreground model), holding out all phrase\nrepetitions to eliminate prediction by memorizing phrases,\nand holding out eight notes at a time for testing.",
                "POP909 supports this hypothesis:\nMore than 50% of the phrases repeat immediately, and al-\nmost all phrases repeat within a quarter of the song.",
                "POP909 supports this hypothesis:\nMore than 50% of the phrases repeat immediately, and al-\nmost all phrases repeat within a quarter of the song.",
                "At the song\nlevel as well, using the phrase repetition labels in POP909,\nwe found that for 79% of songs, 15% to 35% of their du-\nration consists of new material and the rest is repetition.",
                "At the song\nlevel as well, using the phrase repetition labels in POP909,\nwe found that for 79% of songs, 15% to 35% of their du-\nration consists of new material and the rest is repetition.",
                "There is a notice-Figure 7 :Left (green): Percentage of POP909 songs that\nhave phrase repetitions at different song locations.",
                "There is a notice-Figure 7 :Left (green): Percentage of POP909 songs that\nhave phrase repetitions at different song locations.",
                "-entropy using variable-\norder Markov models on diatonic pitches in POP909 songs\nover time, and note the y-axis is inverted.",
                "-entropy using variable-\norder Markov models on diatonic pitches in POP909 songs\nover time, and note the y-axis is inverted.",
                "8-\nbar phrases from V AE with real phrases in POP909.\nre\ufb02ected in pitch and rhythm.",
                "8-\nbar phrases from V AE with real phrases in POP909.\nre\ufb02ected in pitch and rhythm.",
                "These differences from popular songs are striking.",
                "These differences from popular songs are striking.",
                "Figure 8 shows that there is no sig-\nni\ufb01cant differences between the entropy of pitch scale de-\ngree distributions at the start of phrases, middle of phrases\nand end of phrases, but we see signi\ufb01cant differences in\nthe real POP909 phrases.",
                "Figure 8 shows that there is no sig-\nni\ufb01cant differences between the entropy of pitch scale de-\ngree distributions at the start of phrases, middle of phrases\nand end of phrases, but we see signi\ufb01cant differences in\nthe real POP909 phrases.",
                "For example, the probabil-\nity of seeing the tonic at phrase end is 35% in real POP909\nphrases, while at the other positions it is around 20%.",
                "For example, the probabil-\nity of seeing the tonic at phrase end is 35% in real POP909\nphrases, while at the other positions it is around 20%.",
                "Following the logic in Figure 3, we count the rhythm\n(melody note onset) patterns for whole songs from Mu-\nsic Transformer and from POP909 itself.",
                "Following the logic in Figure 3, we count the rhythm\n(melody note onset) patterns for whole songs from Mu-\nsic Transformer and from POP909 itself.",
                "We trained the\nMusic Transformer on 80% of songs in POP909, using\n10% for validation and 10% for testing.",
                "We trained the\nMusic Transformer on 80% of songs in POP909, using\n10% for validation and 10% for testing.",
                "We also analyzed half-note pitch patterns in 4-bar and\n8-bar phrases from the PDSA, from random pitch patterns\ndrawn from the entire PDSA distribution, and for the V AEFigure 9 : Analysis of note onset pattern vocabulary\nfrom randomly sampled POP909 patterns (yellow), Mu-\nsic Transformer songs (red), and POP909 songs (blue) as\na function of song length.",
                "We also analyzed half-note pitch patterns in 4-bar and\n8-bar phrases from the PDSA, from random pitch patterns\ndrawn from the entire PDSA distribution, and for the V AEFigure 9 : Analysis of note onset pattern vocabulary\nfrom randomly sampled POP909 patterns (yellow), Mu-\nsic Transformer songs (red), and POP909 songs (blue) as\na function of song length.",
                "We also compared trends in cross-entropy of the pitch se-\nquence over the course of songs from POP909 (Figure 7\nblue) and Music Transformer (Figure 11).",
                "We also compared trends in cross-entropy of the pitch se-\nquence over the course of songs from POP909 (Figure 7\nblue) and Music Transformer (Figure 11).",
                "POP909\ncross-entropy increases at around 20% of the song length,\nprobably due to the introduction of novel and contrasting\npatterns, and also around the end.",
                "POP909\ncross-entropy increases at around 20% of the song length,\nprobably due to the introduction of novel and contrasting\npatterns, and also around the end.",
                "POP909\u2019s average cross-\nentropy is greater than one bit per note except for a small\nregion around 90% of song length, while Music Trans-\nformer is below one on average for the last 30% of the\nsong, suggesting overly predictable sequences.",
                "POP909\u2019s average cross-\nentropy is greater than one bit per note except for a small\nregion around 90% of song length, while Music Trans-\nformer is below one on average for the last 30% of the\nsong, suggesting overly predictable sequences.",
                "Although we have focused on popular music, repetition\nand hierarchical structure seem to be ubiquitous in music.",
                "Although we have focused on popular music, repetition\nand hierarchical structure seem to be ubiquitous in music.",
                "Pop music, with its nearly exact repetitions, seems easier\nto study than Classical music where we might expect more\nvariation, development and modulation, which make repe-\ntition less obvious.",
                "Pop music, with its nearly exact repetitions, seems easier\nto study than Classical music where we might expect more\nvariation, development and modulation, which make repe-\ntition less obvious.",
                "[13] S. Dai, H. Zhang, and R. B. Dannenberg, \u201cAuto-\nmatic analysis and in\ufb02uence of hierarchical structure\non melody, rhythm and harmony in popular music,\u201d in\nin Proc. of the 2020 Joint Conference on AI Music Cre-\nativity (CSMC-MuMe 2020) , 2020.",
                "[13] S. Dai, H. Zhang, and R. B. Dannenberg, \u201cAuto-\nmatic analysis and in\ufb02uence of hierarchical structure\non melody, rhythm and harmony in popular music,\u201d in\nin Proc. of the 2020 Joint Conference on AI Music Cre-\nativity (CSMC-MuMe 2020) , 2020.",
                "[17] J. J. Rolison and J. Edworthy, \u201cThe role of formal struc-\nture in liking for popular music,\u201d Music Perception: An\nInterdisciplinary Journal , vol.",
                "[17] J. J. Rolison and J. Edworthy, \u201cThe role of formal struc-\nture in liking for popular music,\u201d Music Perception: An\nInterdisciplinary Journal , vol.",
                "[18] O. Julian and C. Levaux, Eds., Over and Over: Explor-\ning Repetition in Popular Music .",
                "[18] O. Julian and C. Levaux, Eds., Over and Over: Explor-\ning Repetition in Popular Music .",
                "[33] A. Elowsson and A. Friberg, \u201cAlgorithmic composition\nof popular music,\u201d in Proc. of the 12th Int.",
                "[33] A. Elowsson and A. Friberg, \u201cAlgorithmic composition\nof popular music,\u201d in Proc. of the 12th Int.",
                "Wang, and R. B. Dannenberg, \u201cPer-\nsonalized popular music generation using imitation and\nstructure,\u201d arXiv preprint arXiv:2105.04709 , 2021.",
                "Wang, and R. B. Dannenberg, \u201cPer-\nsonalized popular music generation using imitation and\nstructure,\u201d arXiv preprint arXiv:2105.04709 , 2021.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset\nfor music arrangement generation,\u201d in Proc. of 21st\nInt.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset\nfor music arrangement generation,\u201d in Proc. of 21st\nInt."
            ],
            "classical": [
                "For ex-\nample, listening experiments with reordered Classical and\nPopular music have shown that listeners are rather insensi-\ntive to restructuring, but these results are subtle and some-\nwhat ambiguous [17].",
                "Pop music, with its nearly exact repetitions, seems easier\nto study than Classical music where we might expect more\nvariation, development and modulation, which make repe-\ntition less obvious.",
                "Perhaps\nthis approach will be of use in Classical and other music.",
                "[1] W. E. Caplan, Classical Form: A Theory of For-\nmal Functions for the Instrumental Music of Haydn,\nMozart, and Beethoven, Revised Edition ."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "By characterizing structural information\nin music, we can discover new principles of music organi-\nzation and propose new challenges and evaluation strate-\ngies for music information retrieval and music generation."
            ],
            "techno": [],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "VIS2MUS: EXPLORING MULTIMODAL REPRESENTATION MAPPING FOR CONTROLLABLE MUSIC GENERATION",
            "homophonic": [],
            "polyphonic": [
                "In order to generate music from the latent space, we use a pre-\ntrained polyphonic music representation learning model (the\nbottom half of Figure 2)",
                "An illustration of the polyphonic disentanglement\nmodel and how to apply cross-modal transformation gto the\nlatent texture feature.\nof such design is that the intermediate representation of the\nmusic texture ztxt;mis a two-dimensional feature map, which\nmakes it easy to perform various image-like operations on it,\nsuch as changing the \u201dbrightness\u201d and \u201dcontrast\u201d of the fea-\nture map.",
                "Correspondingly, we regard melody contour as content and\npolyphonic texture as style in the music domain.",
                "An illustration of applying cross-modal style trans-\nformationgon the images (v1;v2)and the music pieces\n(m1;m2), respectively, in which (v1;m1)and(v2;m2)are\ntwo matched image-music pairs.\nis a sketched contour, v2is a styled image (with colored tex-\nture),m1is a melody contour, and m2is a styled accompa-\nniment (with polyphonic texture).",
                "[16] Ziyu Wang, Dingsu Wang, Yixiao Zhang, and Gus\nXia, \u201cLearning interpretable representation for con-\ntrollable polyphonic music generation,\u201d arXiv preprint\narXiv:2008.07122 , 2020.",
                "[17] Ziyu Wang, Yiyi Zhang, Yixiao Zhang, Junyan Jiang,\nRuihan Yang, Junbo Zhao, and Gus Xia, \u201cPianoTree\nV AE: Structured representation learning for polyphonic\nmusic,\u201d arXiv preprint arXiv:2008.07118 , 2020."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Such an approach enables us to\ndiscover interpretable representation mapping without a huge\namount of paired data.",
                "In particular, we discover that visual-\nto-music mapping has a nice property similar to equivariant .",
                "We discovered that visual-to-music map-\nping in the latent space has a nice property analogous to\nequivariant with respect to three transformations: changing\nbrightness, changing contrast, and style transfer.",
                "Inspired by the discoveries, we\nhave also developed Vis2Mus, a controllable music genera-\ntion interface using images and image transformations as the\ncontrol handlers.",
                "We also plan to conduct a much\nlarger scale user study, from which more interesting multi-\nmodal patterns can potentially be discovered.5."
            ],
            "techno": [],
            "dubstep": [],
            "opera": [
                "An illustration of the polyphonic disentanglement\nmodel and how to apply cross-modal transformation gto the\nlatent texture feature.\nof such design is that the intermediate representation of the\nmusic texture ztxt;mis a two-dimensional feature map, which\nmakes it easy to perform various image-like operations on it,\nsuch as changing the \u201dbrightness\u201d and \u201dcontrast\u201d of the fea-\nture map.",
                "The cross-modal\nstyle transfer operation gsgenerates the latent representation\nof a new image or music:"
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "GENERATING MUSIC WITH SENTIMENT USING TRANSFORMER-GANS",
            "homophonic": [],
            "polyphonic": [
                "In [31], Biax-\nial LSTM networks are used to produce polyphonic music,and the generation can be conditioned by emotion via 4 pa-\nrameters originating from the valence and arousal dimen-\nsions."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "Here, we use\nthe same schedule as in [13], that is, 1=\u001c= (1=\u001cmin)n=N,\nwherenis the index of the current global optimization, N\nis the total number of steps and \u001cminis a hyperparameter\nwhich we chose to be 10\u00002.\n2.2 Transformers\nShortly after its presentation in 2017, the Transformer [14]\nbecame the most popular architecture in the \ufb01eld of Natural\nLanguage Processing (NLP), and it has also been success-\nfully applied to other areas, such as image recognition",
                "Here, we use\nthe same schedule as in [13], that is, 1=\u001c= (1=\u001cmin)n=N,\nwherenis the index of the current global optimization, N\nis the total number of steps and \u001cminis a hyperparameter\nwhich we chose to be 10\u00002.\n2.2 Transformers\nShortly after its presentation in 2017, the Transformer [14]\nbecame the most popular architecture in the \ufb01eld of Natural\nLanguage Processing (NLP), and it has also been success-\nfully applied to other areas, such as image recognition",
                "The excerpts on\nthe dataset are from piano transcriptions of pop songs that\nwere labeled by its authors.",
                "The excerpts on\nthe dataset are from piano transcriptions of pop songs that\nwere labeled by its authors.",
                "For contemporary styles,\nlike Pop or Hip-Hop, where a rigid metrical grid is often\nfollowed, it is desirable to incorporate data about the rhyth-\nmic structure of the songs into the representation.",
                "For contemporary styles,\nlike Pop or Hip-Hop, where a rigid metrical grid is often\nfollowed, it is desirable to incorporate data about the rhyth-\nmic structure of the songs into the representation.",
                "The\nAILABS17k dataset [38] contains over 108hours of piano\ncovers of pop songs automatically transcribed by a state-of\nthe art piano transcription model",
                "The\nAILABS17k dataset [38] contains over 108hours of piano\ncovers of pop songs automatically transcribed by a state-of\nthe art piano transcription model",
                "H. Yang, \u201cEMOPIA: A multi-modal pop piano dataset\nfor emotion recognition and emotion-based music gen-\neration,\u201d in Proc.",
                "H. Yang, \u201cEMOPIA: A multi-modal pop piano dataset\nfor emotion recognition and emotion-based music gen-\neration,\u201d in Proc.",
                "Yang, \u201cPop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions,\u201d in Proceedings of the 28th ACM\nInternational Conference on Multimedia , ser.",
                "Yang, \u201cPop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions,\u201d in Proceedings of the 28th ACM\nInternational Conference on Multimedia , ser."
            ],
            "classical": [],
            "electronic": [
                "[31] K. Zhao, S. Li, J. Cai, H. Wang, and J. Wang, \u201cAn emo-\ntional symbolic music generation system based on lstm\nnetworks,\u201d in 2019 IEEE 3rd Information Technology,\nNetworking, Electronic and Automation Control Con-\nference (ITNEC) ."
            ],
            "hip-hop": [
                "For contemporary styles,\nlike Pop or Hip-Hop, where a rigid metrical grid is often\nfollowed, it is desirable to incorporate data about the rhyth-\nmic structure of the songs into the representation."
            ],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "In this same work, the authors also discover an\nanalogy between the Transformer and the RNN."
            ],
            "techno": [
                "Available: https:\n//openreview.net/forum?id=rJe4ShAcF7\n[23] P. Shaw, J. Uszkoreit, and A. Vaswani, \u201cSelf-attention\nwith relative position representations,\u201d in Proceedings\nof the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short\nPapers) .",
                "[31] K. Zhao, S. Li, J. Cai, H. Wang, and J. Wang, \u201cAn emo-\ntional symbolic music generation system based on lstm\nnetworks,\u201d in 2019 IEEE 3rd Information Technology,\nNetworking, Electronic and Automation Control Con-\nference (ITNEC) ."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [
                "4.2 Symbolic melody representation\nIn this work, we adopted a modi\ufb01ed version of the \u201cMuMIDI\u201d symbolic music representation (18)\nto encode a piece of monophonic melody into discrete musical event sequences."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "1 Introduction\nAutomatic music generation is one of the popular multidisciplinary research topics in generative\nart and computational creativity (1), which has achieved revolutionary advances in various arti\ufb01cial\nintelligence-generated content applications by utilizing deep learning techniques (2, 3), including\ninteractive music production collaboration tools (4, 5), video background music generation (6), music\neducation (7), and music therapy (8).",
                "1 Introduction\nAutomatic music generation is one of the popular multidisciplinary research topics in generative\nart and computational creativity (1), which has achieved revolutionary advances in various arti\ufb01cial\nintelligence-generated content applications by utilizing deep learning techniques (2, 3), including\ninteractive music production collaboration tools (4, 5), video background music generation (6), music\neducation (7), and music therapy (8).",
                "We\narrived at a similar conclusion as PopMNet (32) that a better result of objective evaluation does not\nmean better structure and musicality of generated music.",
                "We\narrived at a similar conclusion as PopMNet (32) that a better result of objective evaluation does not\nmean better structure and musicality of generated music.",
                "2.6 Comparisons with other melody generation methods\nTo prove the effectiveness of the proposed hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, we compared WuYun-RS (i.e., using the rhythmic\nskeleton setting) to \ufb01ve public SOTA Transformer-based melody generation models, namely, Music\nTransformer (15), Pop Music Transformer (17), Compound Word Transformer (19), Melons (33)\nand MeMIDI, that follow an end-to-end left-to-right note-by-note generative paradigm and treat\neach note equally.",
                "2.6 Comparisons with other melody generation methods\nTo prove the effectiveness of the proposed hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, we compared WuYun-RS (i.e., using the rhythmic\nskeleton setting) to \ufb01ve public SOTA Transformer-based melody generation models, namely, Music\nTransformer (15), Pop Music Transformer (17), Compound Word Transformer (19), Melons (33)\nand MeMIDI, that follow an end-to-end left-to-right note-by-note generative paradigm and treat\neach note equally.",
                "Additionally, the MIDI quantization level of the Pop MusicTransformer, Compound\nWord Transformer, and Melons only considered the 16th note time grid.",
                "Additionally, the MIDI quantization level of the Pop MusicTransformer, Compound\nWord Transformer, and Melons only considered the 16th note time grid.",
                "Overall, WuYun-RS (No. 6) and WuYun-RRS (No. 7) outperformed the\nother \ufb01ve current SOTA end-to-end left-to-right note-by-note melody generation models on all metrics,\nincluding MusicTransformer (No. 1), Pop Music Transformer (No. 2), Compound Word Transformer\n(No. 4), Melons (No. 5), and MeMIDI (No. 3).",
                "Overall, WuYun-RS (No. 6) and WuYun-RRS (No. 7) outperformed the\nother \ufb01ve current SOTA end-to-end left-to-right note-by-note melody generation models on all metrics,\nincluding MusicTransformer (No. 1), Pop Music Transformer (No. 2), Compound Word Transformer\n(No. 4), Melons (No. 5), and MeMIDI (No. 3).",
                "2.84\u00060.89 2.68\u00060.95 2.75\u00060.87 2.67\u00060.87 2.71\u00060.88\n6 WuYun-RS 3.13\u00060.88 3.07\u00060.87 3.13\u00060.86 3.02\u00060.92 3.00\u00060.87\n7 WuYun-RRS 3.20\u00060.81 3.11\u00060.85 3.15\u00060.88 3.00\u00060.96 3.02\u00060.88\n8 Human 3.54\u00060.82 3.65\u00060.76 3.68\u00060.89 3.55\u00060.92 3.57\u00060.84\nMT, PMT, and CWT stand for Music Transformer, Pop Music Transformer, and Compound Word Trans-\nformer, respectively.",
                "2.84\u00060.89 2.68\u00060.95 2.75\u00060.87 2.67\u00060.87 2.71\u00060.88\n6 WuYun-RS 3.13\u00060.88 3.07\u00060.87 3.13\u00060.86 3.02\u00060.92 3.00\u00060.87\n7 WuYun-RRS 3.20\u00060.81 3.11\u00060.85 3.15\u00060.88 3.00\u00060.96 3.02\u00060.88\n8 Human 3.54\u00060.82 3.65\u00060.76 3.68\u00060.89 3.55\u00060.92 3.57\u00060.84\nMT, PMT, and CWT stand for Music Transformer, Pop Music Transformer, and Compound Word Trans-\nformer, respectively.",
                "Yang, Pop music transformer: Beat-based modeling and generation of expressive pop piano\ncompositions, in Proceedings of the 28th ACM International Conference on Multimedia (MM, 2020), pp.",
                "Yang, Pop music transformer: Beat-based modeling and generation of expressive pop piano\ncompositions, in Proceedings of the 28th ACM International Conference on Multimedia (MM, 2020), pp.",
                "Ren, J. He, X. Tan, T. Qin, Z. Zhao, T. Liu, Popmag: Pop music accompaniment generation, in\nProceedings of the 28th ACM International Conference on Multimedia (MM, 2020), pp.",
                "Ren, J. He, X. Tan, T. Qin, Z. Zhao, T. Liu, Popmag: Pop music accompaniment generation, in\nProceedings of the 28th ACM International Conference on Multimedia (MM, 2020), pp.",
                "[32] J. Wu, X. Liu, X. Hu, J. Zhu, Popmnet:",
                "[32] J. Wu, X. Liu, X. Hu, J. Zhu, Popmnet:",
                "Generating structured pop music melodies using neural networks.",
                "Generating structured pop music melodies using neural networks.",
                "3:43\u000210\u000075:37\u000210\u000092:44\u000210\u0000126:50\u000210\u000062:21\u000210\u000010\nPMT 2:18\u000210\u000081:21\u000210\u000091:20\u000210\u000062:55\u000210\u000061:13\u000210\u00007\nMeMIDI 1:88\u000210\u000063:09\u000210\u000066:86\u000210\u000072:31\u000210\u000056:29\u000210\u00007\nCWT 3:31\u000210\u000048:47\u000210\u000043:02\u000210\u000042:03\u000210\u000031:69\u000210\u00003\nMelons 4:60\u000210\u000031:54\u000210\u000038:02\u000210\u000042:12\u000210\u000037:41\u000210\u00003\nWuYun-RRS 0.26 0.36 0.42 0.42 0.41\nMT, PMT, and CWT stand for Music Transformer, Pop Music Transformer, and Compound Word Trans-\nformer, respectively.\n19",
                "3:43\u000210\u000075:37\u000210\u000092:44\u000210\u0000126:50\u000210\u000062:21\u000210\u000010\nPMT 2:18\u000210\u000081:21\u000210\u000091:20\u000210\u000062:55\u000210\u000061:13\u000210\u00007\nMeMIDI 1:88\u000210\u000063:09\u000210\u000066:86\u000210\u000072:31\u000210\u000056:29\u000210\u00007\nCWT 3:31\u000210\u000048:47\u000210\u000043:02\u000210\u000042:03\u000210\u000031:69\u000210\u00003\nMelons 4:60\u000210\u000031:54\u000210\u000038:02\u000210\u000042:12\u000210\u000037:41\u000210\u00003\nWuYun-RRS 0.26 0.36 0.42 0.42 0.41\nMT, PMT, and CWT stand for Music Transformer, Pop Music Transformer, and Compound Word Trans-\nformer, respectively.\n19"
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "So far, however, there is still\nan insuf\ufb01cient investigation into an alternative order of melody generation and the difference in the\nrelative structural importance among musical notes."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Hence, the complex long-distance dependencies make it dif\ufb01cult for neural networks to discover and\nlearn the hierarchical structure relationships among these musical elements and generate long-term\nstructured melodies.",
                "[56] Nattiez, J. J., Music and Discourse: Toward a Semiology of Music (Princeton University Press, 1990)."
            ],
            "techno": [
                "WuYun: Exploring hierarchical skeleton-guided\nmelody generation using knowledge-enhanced deep\nlearning\nKejun Zhang1;2;3;\u0003, Xinda Wu1;\u0003, Tieyao Zhang1, Zhijie Huang1, Xu Tan4,\nQihao Liang1, Songruoyao Wu1, Lingyun Sun1;2;y\n1College of Computer Science and Technology, Zhejiang University, China.\n2Alibaba-Zhejiang University Joint Institute of Frontier Technologies, China.\n3Innovation Center of Yangtze River Delta, China.",
                "Acknowledgements\nThanks to Huawei Technologies Co., Ltd for the help in dataset collection and comments.",
                "Funding: This work is supported by the National Natural Science Foundation of China\n(No.62272409), the Key R&D Program of Zhejiang Province (No.2022C03126), the Project of\nKey Laboratory of Intelligent Processing Technology for Digital Music (Zhejiang Conservatory of\nMusic), and the Ministry of Culture and Tourism (No.2022DMKLB001).",
                "Biotechnol.",
                "Technol.",
                "Quantifying and visualising tonal tension, in Proceedings of the\nSecond International Conference on Technologies for Music Notation and Representation (TENSOR, 2016), pp."
            ],
            "dubstep": [],
            "opera": [
                "Therefore, the extraction of the hierarchical dependency structural relationship between\nmusical elements is affected by multiple musical dimensions; it will not be like a simple addition or\nsubtraction operation but a complex organic combination (63)."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "An investigation of the reconstruction capacity of stacked convolutional autoencoders for log-mel-spectrograms",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [
                "In an unsupervised manner, the\nnetwork is able to reconstruct a monophonic and harmonic sound\nbased on latent representations.",
                "The network attempts to\nproject the log-mel-spectrogram of monophonic and harmonic\nsounds to a lower dimensional space.",
                "More-\nover, we presented an evaluation method for calculating the\naccuracy of predicted frequencies in monophonic and har-\nmonic musical sounds."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "These representations can be used to\nmanipulate the timbre and in\ufb02uence the synthesis of creative\ninstrumental notes.",
                "The design of the network is\nillustrated in Fig. 1, where an encoder based on convolu-\ntional networks aims to compress instrumental sounds to a\nlower dimensional space and a mirrored decoder attempts to\nreconstruct the samples from this high-level representation.",
                "Based on the results above,\nwe can conclude that stacked convolutional autoencoders with\na rough pooling approach, such as max pooling, can generate\na more accurate audio time-frequency representation from a\ncompressed low dimensional space of instrumental pitched\nsound.",
                "Furthermore, using Wavenet [24] as a vocoder may not pre-\nserve the phase continuity in the waveform and therefore\ngenerate more noisy instrumental sounds."
            ],
            "vocal": [
                "The subsample is composed of 3750 samples\nfrom a variety of instruments: guitar, bass, brass, synth,\nkeyboard, \ufb02ute, organ, mallet, vocal, reed, and string for a\nsingle pitch."
            ],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [
                "The sounds were acoustic, electronic, or synthetic\nand could belong in different categories as per their velocity\nor acoustic quality.",
                "513\u2013516,\nInstitute of Electrical and Electronics Engineers, 1985."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "An investigation of the reconstruction capacity of\nstacked convolutional autoencoders for\nlog-mel-spectrograms\nAnastasia Natsiou\nTechnological University of Dublin\nDublin, Ireland\nanastasia.natsiou@tudublin.ieLuca Longo\nTechnological University of Dublin\nDublin, Ireland\nluca.longo@tudublin.ieSe\u00b4an O\u2019Leary\nTechnological University of Dublin\nDublin, Ireland\nsean.oleary@tudublin.ie\nAbstract \u2014In audio processing applications, the generation of\nexpressive sounds based on high-level representations demon-\nstrates a high demand."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Byte Pair Encoding for Symbolic Music",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "E., Cottrell, G. W., and\nMcAuley, J. J. Lakhnes: Improving multi-instrumental\nmusic generation with cross-domain pre-training."
            ],
            "vocal": [],
            "choral": [
                "Hadjeres, G., Pachet, F., and Nielsen, F. DeepBach:\na steerable model for Bach chorales generation.",
                "Liang, F. T., Gotham, M., Johnson, M., and Shotton,\nJ. Automatic stylistic composition of bach chorales\nwith deep LSTM."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "Early research in-\ntroduced representations speci\ufb01cally tied to the training\ndata being used, such as DeepBach (Hadjeres et al., 2017),\nFolkRNN (Sturm et al., 2015) or BachBot (Liang et al.,\n2017).",
                "Sturm, B. L., Santos, J. F., and Korshunova, I. Folk mu-\nsic style modelling by recurrent neural networks with\nlong short-term memory units."
            ],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "strategies can be split in two categories: 1) embedding pool-\ning strategies such as Compound Word (Hsiao et al., 2021)\n(CPWord ),Octuple (Zeng et al., 2021) or PopMag (Ren\net al., 2020); 2) token combination strategies such as in\nMuseNet (Payne, 2019) or LakhNES (Donahue et al., 2019).",
                "strategies can be split in two categories: 1) embedding pool-\ning strategies such as Compound Word (Hsiao et al., 2021)\n(CPWord ),Octuple (Zeng et al., 2021) or PopMag (Ren\net al., 2020); 2) token combination strategies such as in\nMuseNet (Payne, 2019) or LakhNES (Donahue et al., 2019).",
                "Datasets\nWe experiment with two datasets: POP909 (Wang et al.,\n2020b) and GiantMIDI (Kong et al., 2021).",
                "Datasets\nWe experiment with two datasets: POP909 (Wang et al.,\n2020b) and GiantMIDI (Kong et al., 2021).",
                "The POP909 dataset (Wang et al., 2020b) is composed of\n909 piano tracks of Pop musics, with aligned MIDI and\naudio versions.",
                "The POP909 dataset (Wang et al., 2020b) is composed of\n909 piano tracks of Pop musics, with aligned MIDI and\naudio versions.",
                "Normalized distributions of the token types of the BPE\ntokens, per BPE factor for the POP909 dataset.\n0",
                "Normalized distributions of the token types of the BPE\ntokens, per BPE factor for the POP909 dataset.\n0",
                "2k 4k 6k 8k 10k 12k 14k\nVocabulary size2.02.53.03.54.0Avg. token combinationsPOP909 TSD\nPOP909 REMI\nGiantMIDI TSD",
                "2k 4k 6k 8k 10k 12k 14k\nVocabulary size2.02.53.03.54.0Avg. token combinationsPOP909 TSD\nPOP909 REMI\nGiantMIDI TSD",
                "The\nPOP909 dataset being smaller than GiantMIDI, it naturally\nleads to a higher maximum number of combinations as the\nlatter is more diverse.",
                "The\nPOP909 dataset being smaller than GiantMIDI, it naturally\nleads to a higher maximum number of combinations as the\nlatter is more diverse.",
                "Overall (\")\nPOP909 TSD 0.66 \u00b1 0.13 0.84 \u00b1 0.12 0.69 \u00b1 0.14",
                "Overall (\")\nPOP909 TSD 0.66 \u00b1 0.13 0.84 \u00b1 0.12 0.69 \u00b1 0.14",
                "No BPE 1.0 \u00b1 1.8 13.6 \u00b1 8.0 - 0.59 \u00b1 0.08 0.82 \u00b1 0.10 0.64 \u00b1 0.09 0.00 0.00\nBPE\u00024 0.2 \u00b1 0.9 21.9 \u00b1 19.9 - 0.65 \u00b1 0.07 0.82 \u00b1 0.10 0.74 \u00b1 0.08 0.24 0.19\nBPE\u000210 0.5 \u00b1 2.2 13.4 \u00b1 14.6 - 0.64 \u00b1 0.07 0.78 \u00b1 0.12 0.74 \u00b1 0.07 0.53 0.42\nBPE\u000220 0.8 \u00b1 2.1 12.8 \u00b1 11.0 - 0.62 \u00b1 0.07 0.79 \u00b1 0.11 0.70 \u00b1 0.09 0.20 0.31\nBPE\u000250 22.4 \u00b1 24.0 4.4 \u00b1 5.3 - 0.56 \u00b1 0.07 0.70 \u00b1 0.12 0.62 \u00b1 0.11 0.02 0.02\nBPE\u0002100 21.5 \u00b1 40.2 35.6 \u00b1 56.0 - 0.54 \u00b1 0.08 0.66 \u00b1 0.14 0.63 \u00b1 0.10 0.00 0.00\nPVm 6.1 \u00b1 6.6 6.9 \u00b1 9.3 - 0.59 \u00b1 0.08 0.78 \u00b1 0.12 0.73 \u00b1 0.08 0.01 0.06\nPVDm 23.6 \u00b1 19.3 0.2 \u00b1 0.7 - 0.43 \u00b1 0.09 0.57 \u00b1 0.19 0.54 \u00b1 0.12 0.00 0.00\nPOP909 REMI 0.66 \u00b1 0.13 0.84 \u00b1 0.12 0.69 \u00b1 0.14",
                "No BPE 1.0 \u00b1 1.8 13.6 \u00b1 8.0 - 0.59 \u00b1 0.08 0.82 \u00b1 0.10 0.64 \u00b1 0.09 0.00 0.00\nBPE\u00024 0.2 \u00b1 0.9 21.9 \u00b1 19.9 - 0.65 \u00b1 0.07 0.82 \u00b1 0.10 0.74 \u00b1 0.08 0.24 0.19\nBPE\u000210 0.5 \u00b1 2.2 13.4 \u00b1 14.6 - 0.64 \u00b1 0.07 0.78 \u00b1 0.12 0.74 \u00b1 0.07 0.53 0.42\nBPE\u000220 0.8 \u00b1 2.1 12.8 \u00b1 11.0 - 0.62 \u00b1 0.07 0.79 \u00b1 0.11 0.70 \u00b1 0.09 0.20 0.31\nBPE\u000250 22.4 \u00b1 24.0 4.4 \u00b1 5.3 - 0.56 \u00b1 0.07 0.70 \u00b1 0.12 0.62 \u00b1 0.11 0.02 0.02\nBPE\u0002100 21.5 \u00b1 40.2 35.6 \u00b1 56.0 - 0.54 \u00b1 0.08 0.66 \u00b1 0.14 0.63 \u00b1 0.10 0.00 0.00\nPVm 6.1 \u00b1 6.6 6.9 \u00b1 9.3 - 0.59 \u00b1 0.08 0.78 \u00b1 0.12 0.73 \u00b1 0.08 0.01 0.06\nPVDm 23.6 \u00b1 19.3 0.2 \u00b1 0.7 - 0.43 \u00b1 0.09 0.57 \u00b1 0.19 0.54 \u00b1 0.12 0.00 0.00\nPOP909 REMI 0.66 \u00b1 0.13 0.84 \u00b1 0.12 0.69 \u00b1 0.14",
                "In particular, big jumps\nof maximum number of combinations, e.g. from 14 to 27\nfor POP909 Remi , indicate that two already big BPE tokens\nrepresent the most recurrent succession.",
                "In particular, big jumps\nof maximum number of combinations, e.g. from 14 to 27\nfor POP909 Remi , indicate that two already big BPE tokens\nrepresent the most recurrent succession.",
                "Number of tokens sampled and not sampled by generative\nmodels, respectively right and left separated by j.\nStrategy POP909 TSD POP909 Remi GiantMIDI",
                "Number of tokens sampled and not sampled by generative\nmodels, respectively right and left separated by j.\nStrategy POP909 TSD POP909 Remi GiantMIDI",
                "Generator POP909 TSD POP909 Remi GiantMIDI TSD GiantMIDI Remi\nNo BPE 0.09 0.14 0.08 0.09\nBPE\u00024 0.02 0.04 0.02 0.02\nBPE\u000210 0.12 0.11 0.02 0.07\nBPE\u000220 0.13 0.05 0.02 0.02\nBPE\u000250 0.02 0.01 0.01 0.01\nBPE\u0002100 0.01 0.01 0.01 0.00\nPVm 0.02 0.02 0.01 0.02\nPVDm 0.00 0.00 0.00 0.00\nCPWord - 0.04 - 0.08\nOctuple - 0.04 - 0.02\nClassi\ufb01er TSD (\") Remi (\") TSD Large ( \")",
                "Generator POP909 TSD POP909 Remi GiantMIDI TSD GiantMIDI Remi\nNo BPE 0.09 0.14 0.08 0.09\nBPE\u00024 0.02 0.04 0.02 0.02\nBPE\u000210 0.12 0.11 0.02 0.07\nBPE\u000220 0.13 0.05 0.02 0.02\nBPE\u000250 0.02 0.01 0.01 0.01\nBPE\u0002100 0.01 0.01 0.01 0.00\nPVm 0.02 0.02 0.01 0.02\nPVDm 0.00 0.00 0.00 0.00\nCPWord - 0.04 - 0.08\nOctuple - 0.04 - 0.02\nClassi\ufb01er TSD (\") Remi (\") TSD Large ( \")",
                "The estimations are\nmore accurate when N\u001dd, as more samples populate allByte Pair Encoding for Symbolic Music\nGen. POP909 TSD\nnoBPEbpe4bpe10 bpe20 bpe50bpe100PVmPVDm020406080DimensionlPCA\nMLE\nMOM\nTLE\nT woNN",
                "The estimations are\nmore accurate when N\u001dd, as more samples populate allByte Pair Encoding for Symbolic Music\nGen. POP909 TSD\nnoBPEbpe4bpe10 bpe20 bpe50bpe100PVmPVDm020406080DimensionlPCA\nMLE\nMOM\nTLE\nT woNN",
                "FisherS Gen. POP909 Remi\nnoBPEbpe4bpe10bpe20bpe50bpe100PVmPVDm\nCPWordOctuple0204060lPCA\nMLE\nMOM\nTLE\nT woNN\nFisherS Gen. GiantMIDI TSD\nnoBPEbpe4bpe10 bpe20 bpe50bpe100PVmPVDm0510152025lPCA\nMLE\nMOM\nTLE\nT woNN\nFisherS Gen. GiantMIDI Remi\nnoBPEbpe4bpe10bpe20bpe50bpe100PVmPVDm\nCPWordOctuple020406080\nlPCA\nMLE\nMOM\nTLE\nT woNN\nFisherS\nClasmall TSD\nnoBPEbpe4bpe10 bpe20 bpe50bpe100PVmPVDm020406080Dimension\n200300400500600700lPCA\nMLE\nMOM\nTLE\nT woNN",
                "FisherS Gen. POP909 Remi\nnoBPEbpe4bpe10bpe20bpe50bpe100PVmPVDm\nCPWordOctuple0204060lPCA\nMLE\nMOM\nTLE\nT woNN\nFisherS Gen. GiantMIDI TSD\nnoBPEbpe4bpe10 bpe20 bpe50bpe100PVmPVDm0510152025lPCA\nMLE\nMOM\nTLE\nT woNN\nFisherS Gen. GiantMIDI Remi\nnoBPEbpe4bpe10bpe20bpe50bpe100PVmPVDm\nCPWordOctuple020406080\nlPCA\nMLE\nMOM\nTLE\nT woNN\nFisherS\nClasmall TSD\nnoBPEbpe4bpe10 bpe20 bpe50bpe100PVmPVDm020406080Dimension\n200300400500600700lPCA\nMLE\nMOM\nTLE\nT woNN",
                "Gen. POP909 no BPE\n Gen. POP909 BPE\u000220\nClasmall GiantMIDI",
                "Gen. POP909 no BPE\n Gen. POP909 BPE\u000220\nClasmall GiantMIDI",
                "Pop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions.",
                "Pop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions.",
                "Popmag:",
                "Popmag:",
                "Pop music accompaniment generation.",
                "Pop music accompaniment generation.",
                "URL https://openreview.\nnet/forum?id=ByxY8CNtvr .\nWang, Z., Chen, K., Jiang, J., Zhang, Y ., Xu, M., Dai,\nS., Bin, G., and Xia, G. Pop909: A pop-song dataset\nfor music arrangement generation.",
                "URL https://openreview.\nnet/forum?id=ByxY8CNtvr .\nWang, Z., Chen, K., Jiang, J., Zhang, Y ., Xu, M., Dai,\nS., Bin, G., and Xia, G. Pop909: A pop-song dataset\nfor music arrangement generation.",
                "B. Data downsampling\n0 1 2 3 4 5 6 7\nduration0.00.20.40.60.81.01.21.4densityDataset\nPOP909\nGiantMIDI\n0 20 40 60 80 100 120\nvelocity0.0000.0050.0100.0150.0200.0250.030densityDataset\nPOP909\nGiantMIDI\nFigure 7.",
                "B. Data downsampling\n0 1 2 3 4 5 6 7\nduration0.00.20.40.60.81.01.21.4densityDataset\nPOP909\nGiantMIDI\n0 20 40 60 80 100 120\nvelocity0.0000.0050.0100.0150.0200.0250.030densityDataset\nPOP909\nGiantMIDI\nFigure 7.",
                "Distributions of the note durations and velocities of the POP909 and GiantMIDI datasets.",
                "Distributions of the note durations and velocities of the POP909 and GiantMIDI datasets.",
                "time (sec)\nPOP909 TSD\nNo BPE 139 17.81 \u00b1 4.12 - 0.04 \u00b1 0.02 0.01 \u00b1 0.02\nBPE\u00024 556 9.71 \u00b1 2.12 -45.50 0.20 \u00b1 0.05 0.02 \u00b1 0.02\nBPE\u000210 1390 8.05 \u00b1 1.75 -54.80 0.44 \u00b1 0.10 0.02 \u00b1 0.02\nBPE\u000220 2780 6.95 \u00b1 1.53 -60.99 0.77 \u00b1 0.18 0.02 \u00b1 0.02\nBPE\u000250 6950 5.84 \u00b1 1.28 -67.20 1.59 \u00b1 0.37 0.02 \u00b1 0.02\nBPE\u0002100 13.9k 5.33 \u00b1 1.16 -70.10 2.72 \u00b1 0.63 0.02 \u00b1 0.02\nPVm 747 12.72 \u00b1 2.92 -28.59 0.03 \u00b1 0.01 0.01 \u00b1 0.01\nPVDm 14.1k 7.63 \u00b1",
                "time (sec)\nPOP909 TSD\nNo BPE 139 17.81 \u00b1 4.12 - 0.04 \u00b1 0.02 0.01 \u00b1 0.02\nBPE\u00024 556 9.71 \u00b1 2.12 -45.50 0.20 \u00b1 0.05 0.02 \u00b1 0.02\nBPE\u000210 1390 8.05 \u00b1 1.75 -54.80 0.44 \u00b1 0.10 0.02 \u00b1 0.02\nBPE\u000220 2780 6.95 \u00b1 1.53 -60.99 0.77 \u00b1 0.18 0.02 \u00b1 0.02\nBPE\u000250 6950 5.84 \u00b1 1.28 -67.20 1.59 \u00b1 0.37 0.02 \u00b1 0.02\nBPE\u0002100 13.9k 5.33 \u00b1 1.16 -70.10 2.72 \u00b1 0.63 0.02 \u00b1 0.02\nPVm 747 12.72 \u00b1 2.92 -28.59 0.03 \u00b1 0.01 0.01 \u00b1 0.01\nPVDm 14.1k 7.63 \u00b1",
                "0.02 \u00b1 0.01 0.01 \u00b1 0.01\nPOP909 Remi\nNo BPE 152 18.06 \u00b1 4.12 - 0.03 \u00b1 0.02 0.01 \u00b1 0.01\nBPE\u00024 608 10.55 \u00b1 2.26 -41.61 0.21 \u00b1 0.05 0.02 \u00b1 0.02\nBPE\u000210 1520 8.85 \u00b1 1.90 -51.00 0.47 \u00b1 0.11 0.02 \u00b1 0.02\nBPE\u000220 3040 8.01 \u00b1 1.74 -55.64 0.86 \u00b1 0.19 0.02 \u00b1 0.02\nBPE\u000250 7600 7.32 \u00b1 1.58 -59.46 1.97 \u00b1 0.43 0.02 \u00b1 0.02\nBPE\u0002100 15.2k 6.70 \u00b1 1.43 -62.92",
                "0.02 \u00b1 0.01 0.01 \u00b1 0.01\nPOP909 Remi\nNo BPE 152 18.06 \u00b1 4.12 - 0.03 \u00b1 0.02 0.01 \u00b1 0.01\nBPE\u00024 608 10.55 \u00b1 2.26 -41.61 0.21 \u00b1 0.05 0.02 \u00b1 0.02\nBPE\u000210 1520 8.85 \u00b1 1.90 -51.00 0.47 \u00b1 0.11 0.02 \u00b1 0.02\nBPE\u000220 3040 8.01 \u00b1 1.74 -55.64 0.86 \u00b1 0.19 0.02 \u00b1 0.02\nBPE\u000250 7600 7.32 \u00b1 1.58 -59.46 1.97 \u00b1 0.43 0.02 \u00b1 0.02\nBPE\u0002100 15.2k 6.70 \u00b1 1.43 -62.92",
                "Figure 10 shows the pairwise cosine similarity of the learned embedding vectors, for the TSD andRemi representation on\nthe POP909 dataset.",
                "Figure 10 shows the pairwise cosine similarity of the learned embedding vectors, for the TSD andRemi representation on\nthe POP909 dataset.",
                "POP909\n100101102\nDimension0.00.20.40.60.81.0Singular valuenoBPE\nbpe4\nbpe10\nbpe20\nbpe50\nbpe100\nPVm\nPVDm\nnoBPE adj.",
                "POP909\n100101102\nDimension0.00.20.40.60.81.0Singular valuenoBPE\nbpe4\nbpe10\nbpe20\nbpe50\nbpe100\nPVm\nPVDm\nnoBPE adj.",
                "Pairwise cosine similarity matrix of learned embedding of the generative models, on the POP909 dataset.",
                "Pairwise cosine similarity matrix of learned embedding of the generative models, on the POP909 dataset.",
                "UMAP 3d representations of the embeddings of generative models with the POP909 dataset.",
                "UMAP 3d representations of the embeddings of generative models with the POP909 dataset."
            ],
            "classical": [
                "Giantmidi-\npiano: A large-scale midi dataset for classical\npiano music."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies , pp.",
                "In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pp."
            ],
            "dubstep": [],
            "opera": [
                "Embedding pooling consists in merging the embeddings of\nseveral distinct tokens with a pooling operation.",
                "Embedding pooling: 1) requires a more complex training\nprocedure; 2) for generation, inferring from such model\ncan be seen as sampling from a multivariate distribution,\nwhich can be a delicate operation; 3) the results can easily\ndegenerate if the pooling does not yield semantically rich\nembeddings that represent the underlying tokens."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "A Symbolic-domain Music Generation Method Based on Leak-GAN",
            "homophonic": [],
            "polyphonic": [
                "[10] proposed three \ndifferent methods combined with GAN to handle the \ninteraction among tracks and generate polyphonic music with \nharmonic and rhythmic structure."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "Based on the understanding  \nof how pop music is composed, Dong et al.",
                "Based on the understanding  \nof how pop music is composed, Dong et al.",
                "Dataset \nAs we mainly focus on melody generation, the POP909[19] \npop music dataset is the best choice at our knowledge, as its music melody track can be explicitly distinguished and directly  \nderived.",
                "Dataset \nAs we mainly focus on melody generation, the POP909[19] \npop music dataset is the best choice at our knowledge, as its music melody track can be explicitly distinguished and directly  \nderived.",
                "The reason we choose the length of 150 is that in our representation of pop song melodies, the verse and chorus of a song are approximately 150-word sequence.",
                "The reason we choose the length of 150 is that in our representation of pop song melodies, the verse and chorus of a song are approximately 150-word sequence.",
                "Pop909:",
                "Pop909:",
                "A pop-song dataset for music arrangement genera tion.",
                "A pop-song dataset for music arrangement genera tion."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "2021 3rd International Academic E xchange Conference on Science and Technology Innovation (IAECST) \n978-1-6654-0267-5/21/$31.00 \u00a92021 IEEE \n549 \n A Symbolic-domain Music Generation Method Based \non Leak-GAN \n \nZihan Li \nXihua Honor College,  \nXihua University,   \nChengdu, China,  \nkkli19@outlook.com \n \nYangcheng Liu \nSchool of Electrical and El ectronic Information, \nXihua University,  \nChengdu, China, \nluming9704@163.com Yi Guo * \nSchool of Electrical and El ectronic Information, \nXihua University,  \nChengdu, China, \nlpngy@vip.163.com \n \nQianxue Zhang \nSchool of Electrical and El ectronic Information, \nXihua University,  \nChengdu, China, \nlouxzqx@163.com",
                "3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST) | 978-1-6654-0267-5/21/$31.00 \u00a92021 IEEE | DOI: 10.1109/IAECST54258.2021.9695533\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "Computer Music Technology[M].",
                "Research on Deep Learning Automatic Composit ion \nBased on MIDI Music (Master's Thesis, South China University of  \nTechnology."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "AI Music Therapist: A Study on Generating Specific Therapeutic Music based on Deep Generative Adversarial Network Approach",
            "homophonic": [],
            "polyphonic": [
                "Finally, in polyphonic \nmusic, notes are usually divided into chords or melodies."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "Wang Yuhua et al. played pop songs or \nlight music to conscious patients during surgery, and \nobserved the changes of their heart rate and blood pressure, \nand scored their anxiety.",
                "Wang Yuhua et al. played pop songs or \nlight music to conscious patients during surgery, and \nobserved the changes of their heart rate and blood pressure, \nand scored their anxiety."
            ],
            "classical": [],
            "electronic": [
                "2022 IEEE 2nd International Conference on Electronic Technology, Communication and Information (ICETCI) \nAI Music Therapist: A Study on Generating \nSpecific Therapeutic Music based on Deep \nGenerative Adversarial Network Approach \nYurui Hou \nWalnut Hill School for The Arts \nNatick, MA 01760, United States \nyurui.hou2023 @walnuthillarts.org",
                "International Conference on Electronic Technology, Communication and Information (ICETCI)"
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "2022 IEEE 2nd International Conference on Electronic Technology, Communication and Information (ICETCI) \nAI Music Therapist: A Study on Generating \nSpecific Therapeutic Music based on Deep \nGenerative Adversarial Network Approach \nYurui Hou \nWalnut Hill School for The Arts \nNatick, MA 01760, United States \nyurui.hou2023 @walnuthillarts.org",
                "International Conference on Electronic Technology, Communication and Information (ICETCI)",
                "The possibility \nof using computer-aided technology to replace manual music \ngeneration was analyzed."
            ],
            "dubstep": [],
            "opera": [
                "It was concluded that the application of background music in \nthe operating room is one of the effective measures to reduce \nthe psychological stress of surgical patients.",
                "The convolutional layer of the CNN is \nmainly convolutional operation, the input data is a matrix, \nand the features are extracted by multiple shared weight \nkernels/filter, and nonlinear steganography is performed."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "An intelligent music generation based on Variational Autoencoder",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [
                "[10] established a \nbacktracking Backtracking specification language (BSL), \nwhich is used to implement CHORAL, a rule-based expert \nsystem that can construct a four-part chorus with Bach style \nfor the monophonic main melody, and has certain practical \nvalue."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "[10] established a \nbacktracking Backtracking specification language (BSL), \nwhich is used to implement CHORAL, a rule-based expert \nsystem that can construct a four-part chorus with Bach style \nfor the monophonic main melody, and has certain practical \nvalue.",
                "Au expert system for harmonizing chorales in the style of J.S. \nBach."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [
                "Or swing music in jazz",
                "V. EXPERIMENTAL ANALYSIS  \nCombining the theories of melody rule model, harmony \nrule model and rhythm rule model, different styles of music \nhave been successfully realized, such as pop, classical, jazz, \nmelody + chord, etc.",
                "Jazz style music generation \n \nFigure5.",
                "Microtiming Deviations \nand Swing Feel in Jazz."
            ],
            "rock": [],
            "pop": [
                "V. EXPERIMENTAL ANALYSIS  \nCombining the theories of melody rule model, harmony \nrule model and rhythm rule model, different styles of music \nhave been successfully realized, such as pop, classical, jazz, \nmelody + chord, etc.",
                "V. EXPERIMENTAL ANALYSIS  \nCombining the theories of melody rule model, harmony \nrule model and rhythm rule model, different styles of music \nhave been successfully realized, such as pop, classical, jazz, \nmelody + chord, etc.",
                "Pop style music generation \n \nFigure4.",
                "Pop style music generation \n \nFigure4."
            ],
            "classical": [
                "Waltzes in classical music, \nfor example, are characterized by its triple meter style, in \nwhich the dance steps rise and fall with the beat, giving one \na magnificent and elegant enjoyment.",
                "Experimental data \nThe data used in this paper is the classical piano MIDI \ndataset, which contains music files of different formats for \nall classical music.",
                "V. EXPERIMENTAL ANALYSIS  \nCombining the theories of melody rule model, harmony \nrule model and rhythm rule model, different styles of music \nhave been successfully realized, such as pop, classical, jazz, \nmelody + chord, etc.",
                "Classical Bach Style Music Generation \n \nFigure6."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "Then Mozer constructed CONCERT using recursive \nneural network technology [11], and used back propagation \nlearning algorithm to train CONCERT to create melody in \none sound and one tone.",
                "3942020 International Conference on Culture-oriented Science & Technology (ICCST)\n978-1-7281-8138-7/20/$31.00 \u00a92020 IEEE\nDOI 10.1109/ICCST50977.2020.00082\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ],
            "dubstep": [],
            "opera": [
                "Calculating the gradient by ELBO is not feasible due to the \nsampling operation used to acquire z."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "APE-GAN: A Novel Active Learning Based Music Generation Model With Pre-Embedding",
            "homophonic": [],
            "polyphonic": [
                "Notably this\nmodel can generate multi-track polyphonic musics and reason-\nable accompaniments.",
                "Polyphonic\nmusic generation by modeling temporal dependencies using a rnn-dbn.",
                "Convolutional generative adversarial\nnetworks with binary neurons for polyphonic music generation."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "This method\nuses BERT to obtain embedding vector, thus provides prior and\nartistic thoughts from humans so that our model can achieve\na higher level of creativity than most of the popular methods.",
                "This method\nuses BERT to obtain embedding vector, thus provides prior and\nartistic thoughts from humans so that our model can achieve\na higher level of creativity than most of the popular methods.",
                "Before Transformers, Long-\nShort-Term-Memory (LSTM)-based models are very popular\nin constructing Seq2Seq models because the LSTM model can\nmap sequences of data to vectors while selectively remember-\ning the parts of the sequences it \ufb01nds important.",
                "Before Transformers, Long-\nShort-Term-Memory (LSTM)-based models are very popular\nin constructing Seq2Seq models because the LSTM model can\nmap sequences of data to vectors while selectively remember-\ning the parts of the sequences it \ufb01nds important.",
                "A mu-\nsically plausible network for pop music generation.",
                "A mu-\nsically plausible network for pop music generation."
            ],
            "classical": [
                "Classical Music Generation in Distinct Dastgahs with AlimNet\nACGAN. arXiv preprint arXiv:1901.04696 ."
            ],
            "electronic": [
                "\u000fWe use active learning technology to ef\ufb01ciently train\nour APE-GAN to achieve high performance under the\n123\nICEICT 2021 \nIEEE 4th International Conference on Electronic Information and Communication Technology\n978-1-6654-3203-0/21/$31.00 \u00a92021 IEEE\nXi'an, China \u2022",
                "August 18-20, 20212021 IEEE 4th International Conference on Electronic Information and Communication Technology (ICEICT) | 978-1-6654-3203-0/21/$31.00",
                "Can GAN originate new electronic dance music\ngenres?\u2013Generating novel rhythm patterns using GAN with Genre\nAmbiguity Loss."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "\u000fWe use active learning technology to ef\ufb01ciently train\nour APE-GAN to achieve high performance under the\n123\nICEICT 2021 \nIEEE 4th International Conference on Electronic Information and Communication Technology\n978-1-6654-3203-0/21/$31.00 \u00a92021 IEEE\nXi'an, China \u2022",
                "August 18-20, 20212021 IEEE 4th International Conference on Electronic Information and Communication Technology (ICEICT) | 978-1-6654-3203-0/21/$31.00"
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Automatic Music Generation System based on RNN Architecture",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [
                "Dataset A GAN-based model that was trained on a dataset of Bach's \norchestral symphonies produced the desired outcomes."
            ],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [
                "The \nmost classical track becomes composed of sophisticated \nshapes and arrangements to deliver ideas and feelings",
                "11 Tianyu Jiang et al., \n2019 Bidirectional LSTM \nnetwork  Classical \nPiano \nDataset Bidirectional LSTM network was presented to produce harmonic \nmusic."
            ],
            "electronic": [
                "[10] Chen, Hongyu, Qinyin Xiao, and Xueyuan Yin, \u201cGenerating music \nalgorithm with deep convolutional generative adversarial networks,\u201d \nIn 2nd IEEE International Conference on Electronics Technology \n(ICET), pp.",
                "[11] Jiang, Tianyu, Qinyin Xiao, and Xueyuan Yin, \u201cMusic generation \nusing the recurrent bidirectional network,\u201d In 2nd IEEE International \nConference on Electronics Technology (ICET), pp.",
                "[15] Lang, Runnan, Songsong Wu, Songhao Zhu, and Zuoyong Li, \n\u201cSSCL: Music Generation in Long-term with Cluster Learning,\u201d In \n4th IEEE Information Technology, Networking, Electronic and \nAutomation Control Conference (ITNEC), pp. 77-81, 2020.",
                "[29] Misra, N. R., Kumar, S., & Jain, A, \u201cA review on E-waste: Fostering \nthe need for green electronics,\u201d In International Conference on \nComputing, Communication, and Intelligent Systems (ICCCIS), pp. \n1032-1036, 2021."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n294\n \n978-1-6654-7657-7/22/$31.00 \u00a92022 IEEE  Auto\nmatic Music Generation System based on RNN \nArchitecture \n \nSandeep Kumar \nDepartment of Computer Science & \nEngineering \nKoneruLakshmaiah Educational \nFoundation  \nVaddeswaram, Andhra Pradesh, India \ner.sandeepsahratia@gmail.com \n \nShilpa Rani \nDepartment of Computer Science & \nEngineering \nNeil Gogte Institute of Technology \nHyderabad, India \nshilpachoudhary1987@gmail.com \n \n KeerthiGudiseva \nDepartment of Computer Science & \nEngineering \nKoneruLakshmaiah Educational \nFoundation  \nVaddeswaram, Andhra Pradesh, India \nkeerthigudiseva0611@gmail.com",
                "KMVV Prasad \nDepartment of ECE \nSymbiosis Institute of Technology \nSymbiosis International Deemed \nUniversity  \nPune, India \nmvvprasad.kantipudi@gmail.com \n AallaIswarya \nDepartment of Computer Science & \nEngineering \nKoneruLakshmaiah Educational \nFoundation  \nVaddeswaram, Andhra Pradesh, India \naallaiswarya@gmail.com",
                "Recurrent neural networks, like feedforward and \nconvolutional neural networks learn from training data 2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS) | 978-1-6654-7657-7/22/$31.00 \u00a92022 IEEE | DOI: 10.1109/ICTACS56270.2022.9988652\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n295 \n (CNNs).",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n296 \n 9 Sarthak Agarwal et al., \n2018 PRECON-LSTM  Nottingham \nMus\nic \nDatabase  Sco\nreMeanMean Realness \nArtif3.120 2.747 \nGen \nHum3.613 3.516 \nComp  \n10 AdvaitMaduskar et al. \n2020 Autoregressive GAN \nmodel Bach\u2019s \nmusical \nsymphonies \ndataset Outlined a generation model for note sequence generation using the \nGAN framework.",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n297 \n visualise the musical composition (i.e., Piano roll).",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n298 \n  \nFigure 6 .",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n299",
                "[2] Sakurai, Keigo, Ren Togo, Takahiro Ogawa, and Miki Haseyama, \n\u201cMusic playlist generation based on graph exploration using \nreinforcement learning,\u201d In 3rd IEEE Global Conference on Life \nSciences and Technologies (LifeTech), pp. 53-54, 2021.",
                "[7] Wang, Tao, Junzhe Liu, Cong Jin, Jianguang Li, and Shihao Ma, \u201cAn \nintelligent music generation based on VariationalAutoencoder,\u201d In \nInternational Conference on Culture-oriented Science & Technology \n(ICCST), pp. 394-398, 2020.",
                "[10] Chen, Hongyu, Qinyin Xiao, and Xueyuan Yin, \u201cGenerating music \nalgorithm with deep convolutional generative adversarial networks,\u201d \nIn 2nd IEEE International Conference on Electronics Technology \n(ICET), pp.",
                "[11] Jiang, Tianyu, Qinyin Xiao, and Xueyuan Yin, \u201cMusic generation \nusing the recurrent bidirectional network,\u201d In 2nd IEEE International \nConference on Electronics Technology (ICET), pp.",
                "[15] Lang, Runnan, Songsong Wu, Songhao Zhu, and Zuoyong Li, \n\u201cSSCL: Music Generation in Long-term with Cluster Learning,\u201d In \n4th IEEE Information Technology, Networking, Electronic and \nAutomation Control Conference (ITNEC), pp. 77-81, 2020.",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n300\n \n Diseases, \" Mathematical Problems in Engineering, Hindawi Journal \nPublication, vol. 21, no. 1, pp. 1-18, 2021.",
                "[22] Sandeep Kumar, Shailu, Arpit Jain, \u201cEnhanced Method of Object \nTracing Using Extended Kalman Filter via Binary Search Algorithm\u201d  \nin Journal of Information Technology and Management, vol.",
                "[26] Shukla A. P, \u201cTraining cellular automata for image edge detection,\u201d \nRomanian Journal of Information Science and Technology, vol. 19, \nno. 4, pp. 338-359, 2016.",
                "In 2021 International Conference on Advance \nComputing and Innovative Technologies in Engineering (ICACITE), \npp.",
                "In 2021 International Conference \non Advance Computing and Innovative Technologies in Engineering \n(ICACITE), pp."
            ],
            "dubstep": [],
            "opera": [
                "According to the findings, \nadaptive operator configuration only offers slight advantages in \nunique circumstances.",
                "[4] Sulyok, Csaba, \u201cGenetic Operators in Evolutionary Music \nComposition,\u201d In 20th IEEE International Symposium on Symbolic \nand Numeric Algorithms for Scientific Computing (SYNASC), pp."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Development of Application Software for Generating Music Composition Inspired by Nature Using Deep Learning",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "C , E.S.Gopi , \"Music composition inspired by sea wave patterns ob served from \nbeaches\", Proceedings of the 2nd International Conf erence on Data Engineering and \nCommunication Technology (ICDECT 2017), Springer,20 17.",
                "\"Signal processing  approach for music synthesis using bird\u2019s \nSounds\", Elsevier journal on Procedia Technology, V olume 10, 2013, Pages 287-294 \n13."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "homophonic": [],
            "polyphonic": [
                "Here we explore\nits ability to reconstruct polyphonic music .",
                "Fig-\nure 2 indicates that DDSP generalised best to all classes, im-\nplying that polyphonic music, reconstructed by DDSP, main-\ntains much of its meaningful information."
            ],
            "monophonic": [
                "to generate commercial music [1], [3], [8], the latter was\nproposed in the context of monophonic audio.",
                "Although DDSP was\ndesigned for monophonic music, it can also reconstruct poly-\nphonic music since it is trained to reconstruct a spectrogram."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [
                "[8] C. Carr and Z. Zukowski, \u201cGenerating albums with samplernn to imitate\nmetal, rock, and punk bands,\u201d arXiv preprint arXiv:1811.06633 , 2018."
            ],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [
                "[8] C. Carr and Z. Zukowski, \u201cGenerating albums with samplernn to imitate\nmetal, rock, and punk bands,\u201d arXiv preprint arXiv:1811.06633 , 2018."
            ],
            "alternative": [
                "Alternatively, evalu-\nation has been approached by monitoring quantities related\nThis work is funded by the UK Economic Social Research Council (UK-\nESRC) through the research Grant No. HJ-253479 (ACLEW), the Engineering\nand Physical Sciences Research Council (EPSRC) Grant No. 2021037, and\nthe the DFG\u2019s Reinhart Koselleckproject No. 442218748 (AUDI0NOMOUS)."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "[21] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra,\n\u201cThe mtg-jamendo dataset for automatic music tagging,\u201d in Machine\nLearning for Music Discovery Workshop, International Conference on\nMachine Learning (ICML 2019) , Long Beach, CA, United States,\n2019."
            ],
            "techno": [],
            "dubstep": [],
            "opera": [
                "We measure classi\ufb01er performance according to the three\nmetrics that were used in the MediaEval 2019 competition:\nF1-score, area under the precision-recall curve (PR-AUC),\nand the area under the Receiver Operator Characteristics\ncurve (ROC-AUC)."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Generating Music Algorithm with Deep Convolutional Generative Adversarial Networks",
            "homophonic": [],
            "polyphonic": [
                "Music can \nbe separated into main tone music and polyphonic music according to the tone classification.",
                "In order to solve a problem that can solve polyphonic music and multi-track orbit generation, this paper proposes a scheme consistent \nwith this algorithm."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "First, considering that we are expected to generate some \nPop music.",
                "First, considering that we are expected to generate some \nPop music.",
                "Popular music is mainly composed of the above instruments, and other less common instruments are classified as others.",
                "Popular music is mainly composed of the above instruments, and other less common instruments are classified as others."
            ],
            "classical": [],
            "electronic": [
                "* Corresponding author: Qinyin Xiao(xiaoqinyin@msn.com) \n/g24/g26/g252019 2nd International Conference on Electronics Technology\n978-1-7281-1618-1/19/$31.00 \u00a92019 IEEE\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "A complete music according to the performance mode, the instrument can be generally separated into: keyboard, wind, string, percussion, electronic."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "* Corresponding author: Qinyin Xiao(xiaoqinyin@msn.com) \n/g24/g26/g252019 2nd International Conference on Electronics Technology\n978-1-7281-1618-1/19/$31.00 \u00a92019 IEEE\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Generating Music with Emotions",
            "homophonic": [],
            "polyphonic": [
                "[21] model to\nreproduce polyphonic music sequences.",
                "[7] is a convolutional\nneural network (CNN) based GAN to compose polyphonic\nmusic with 5 sound-tracks.",
                "Similarly, recurrent neural network\n(RNN) based GAN is proposed in C-RNN-GAN [23], which\ncan generate polyphonic continuous music sequence.",
                "Collect large-scale polyphonic music dataset\nwith emotion labels is a valuable further work for us."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "DeepBach [20]\nis proposed by Hadjeres et al., which used a dependency\nneural network and a Gibbs-like sampling procedure to gen-\nerate Bach\u2019s four parts chorales.",
                "[20] G. Hadjeres, F. Pachet, and F. Nielsen, \u201cDeepbach: a steerable model\nfor bach chorales generation,\u201d in International Conference on Machine\nLearning."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "In human life, the contem-\nporary pop music is often used to express and share\nemotions.",
                "In human life, the contem-\nporary pop music is often used to express and share\nemotions.",
                "Therefore, we think the deep learning\nmodel trained on EMOPIA cannot be used to annatate our\ndataset because of the following reasons: 1) There\u2019s a domain\ngap between piano solo performances and pop songs\u2019 melodies\nin our dataset.",
                "Therefore, we think the deep learning\nmodel trained on EMOPIA cannot be used to annatate our\ndataset because of the following reasons: 1) There\u2019s a domain\ngap between piano solo performances and pop songs\u2019 melodies\nin our dataset.",
                "Yang,\n\u201cEmopia: A multi-modal pop piano dataset for emotion recognition\nand emotion-based music generation,\u201d arXiv preprint arXiv:2108.01374,\n2021.",
                "Yang,\n\u201cEmopia: A multi-modal pop piano dataset for emotion recognition\nand emotion-based music generation,\u201d arXiv preprint arXiv:2108.01374,\n2021."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "This paper was produced by the IEEE Publication Technology Group.",
                "Chunhui Bao received the B.S. degree in Computer\nScience and Technology from Sichuan University,\nChengdu, China, in 2019."
            ],
            "dubstep": [],
            "opera": [
                "MIDI is the abbreviation of musical\ninstrument digital interface, which is an industry standard\nthat describes the interoperability protocol of digital music\nrepresentation.",
                "where Eis the number of emotions in the dataset and ||\nrepresents the concatenation operation."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Generating Music with Generative Adversarial Networks and Long Short-Term Memory",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            "jazz": [
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            "rock": [
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            "pop": [
                "However, due to the long cycle, high cost and complicated process, the traditional manual creation method has been unable to meet the society\u2019s increasing demand for music, and the use of computers for music generation will become a popular composition trend in the future.",
                "However, due to the long cycle, high cost and complicated process, the traditional manual creation method has been unable to meet the society\u2019s increasing demand for music, and the use of computers for music generation will become a popular composition trend in the future.",
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles.",
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            "classical": [
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            "electronic": [
                "The author found that the melodies generated by this model were  2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI) | 978-1-6654-3881-0/21/$31.00 \u00a92021 IEEE | DOI: 10.1109/CEI52496.2021.9574491\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "MIDI is a technical standard that describes a communications protocol, \ndigital interface, and electrical connectors that connect a wid e \nvariety of electronic musical instruments, computers, and \nrelated audio devices for playing, editing, and recording music  \n[11]."
            ],
            "hip-hop": [],
            "reggae": [
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            "country": [
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            "techno": [
                "2021 IEEE International Conferen ce on Computer Science, Electro nic Information Engineering and Intelligent Control Technology (CEI) \n978-0-7381-4649-2/21/$31.00 \u00a92021 IEEE \n268 \n Generating Music with Generative Adversarial \nNetworks and Long Short-Term Memory \n \nYuhan Dai1,  *, \u2020 \n1College of Letters and Science, \nEconomics Department \nUniversity of California, Davis \nDavis, United States \n*Corresponding author\u2019s e-mail: \ndyhdai@ucdavis.edu \n Tong Xin2, *, \u2020 \n2School of Computer  Engineering and \nScience \nShanghai University \nShanghai, China \n*Corresponding author\u2019s e-mail:",
                "In the early development of artificial intelligence technology, \nscholars began to study how to apply artificial intelligence technology to music generation.",
                "The author found that the melodies generated by this model were  2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI) | 978-1-6654-3881-0/21/$31.00 \u00a92021 IEEE | DOI: 10.1109/CEI52496.2021.9574491\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Generation of Music With Dynamics Using Deep Convolutional Generative Adversarial Network",
            "homophonic": [],
            "polyphonic": [
                "PROPOSED METHOD  \nIn this paper, we designed and implemented a music \ngeneration system that can generate polyphonic music with \ndynamic using Deep Convolutional Generative Adversarial \nNetwork (DCGAN)."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "Generation of Music With Dynamics Using \nDeep Convolutional Generative Adversarial Network \n \nRaymond Kwan How Toh \nSchool of Computer Science and Engineering \nNanyang Technological University \nSingapore \nrtoh004@ntu.edu.sg Alexei Sourin \nSchool of Computer Science and Engineering \nNanyang Technological University \nSingapore \nassourin@ntu.edu.sg\n \n \nAbstract \u2014Following the rapid advancement of Artificial \nIntelligence and transition into the era of Big Data, researchers \nhave started to explore the possibility of using machine \nlearning in creative domains such as music generation."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Monophonic Music Generation With a Given Emotion Using Conditional Variational Autoencoder",
            "homophonic": [],
            "polyphonic": [
                "In the future, the generating system should be broadened\nto the possibility of working with polyphonic, four-voice\nmusic."
            ],
            "monophonic": [
                "Digital Object Identifier 10.1 109/ACCESS.2021.31 13829\nMonophonic Music Generation With\na Given Emotion Using Conditional\nVariational Autoencoder\nJACEK GREKOW\n AND TEODORA DIMITROVA-GREKOW\nFaculty of Computer Science, Bialystok University of Technology, 15-351 Bialystok, Poland\nCorresponding author: Jacek Grekow (j.grekow@pb.edu.pl)",
                "For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nsystems",
                "The aim of this paper was to build a model generating\nmonophonic music sequences with one of four basic emo-\ntions: happy, angry, sad, relaxed.",
                "In [34], Hadjeres et al. proposed geodesic latent space reg-\nularization for the variational autoencoder, which enhances\nVOLUME 9, 2021 129089J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nlatent space navigation with the change of the attributes of the\ndecoded sequences.",
                "The paper presents a music generation\nsystem using the proposed regulation that controls the num-\nber of notes generated by variations of a given monophonic\nmelody.",
                "The music generation system created in this work should\ngenerate monophonic sequences, therefore the original\nJ. S. Bach music21 Dataset underwent several transforma-\ntions (Fig. 1).",
                "Happy, angry, sad, relaxed, these\n129090 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 2.",
                "There are four\n3https://github.com/grekowj/musgenvae\nVOLUME 9, 2021 129091J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nsixteenth notes for each quarter note, dividing the segment\nwith the shortest note (sixteenth note) we get 64 time steps,\n4(bar)\u00024(quarter note )\u00024(sixteenth note ).",
                "Latent\nloss is calculated using the Kullback-Leibler divergence,\nwhich calculates the distance between the target distribution\n(the Gaussian distribution) and the actual distribution in latent\nvector z:\nLLD\u00001\n2KX\niD1(1Clog\u001b2\ni\u0000\u001b2\ni\u0000\u00162\ni) (1)\nwhere Kis the dimensionality of latent vector z,\u0016iand\u001bi\nare mean and standard deviation of idimension of latent\nvector z.\nB. TRAINING OF THE NETWORK\nFor our classi\u001ccation task, which is the prediction of one\ncategory (one pitch of note), the softmax function was used as\n4https://keras.io\n5https://www.tensor\u001dow.org\n129092 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 5.",
                "Testing how the use of\nthe baseline model (CVAE CDense) and the proposed model\n(CVAECGRU) affects the obtained metrics for the generated\nVOLUME 9, 2021 129093J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 6.",
                "A set of generated MIDI examples can be found\n129094 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 8.",
                "The sequences with emotions happy (e1)\nVOLUME 9, 2021 129095J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 10.",
                "Table 8 shows which of the training sets are closest to\n129096 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 12.",
                "The diagonal values are\nVOLUME 9, 2021 129097J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nTABLE 4.",
                "The\nannotated examples were mixed up so that their order was\n129098 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nnot grouped by emotion.",
                "This article presents the stages of creating a system gen-\nerating monophonic musical sequences with one of four\nbasic emotions.",
                "The limitations of this study include the emotional model\nwe adopt, the musical area used in the training set, and the\nlength of the monophonic pieces.",
                "Another potential application of the system is music therapy,\nVOLUME 9, 2021 129099J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nwhere the generated melodies with a speci\u001cc emotion could\nbe used to change or enhance the emotional state of the\npatient.",
                "13\u001524.\n129100 VOLUME 9, 2021J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\n[42] A. P. Oliveira and A. Cardoso, ``Towards affective-psychophysiological\nfoundations for music production,'' in Affective Computing and Intelligent\nInteraction ."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "JACEK GREKOW received the M.S. degree in\ncomputer systems from the Technical University,\nSo\u001ca, Bulgaria, in 1994, the B.S. degree in music\nfrom Vienna Conservatoire (Konservatorium der\nStadt Wien), Austria, in 1996, the M.S. degree\nin music from the Department of Instrumental\nand Educational Studies, Fryderyk Chopin Univer-\nsity of Music, Warsaw, Poland, in 2006, and the\nPh.D. degree in computer sciences from the Fac-\nulty of Information Technology, Polish-Japanese\nAcademy of Information Technology, Warsaw, in 2009."
            ],
            "vocal": [],
            "choral": [
                "This collection mostly\nincludes chorales (382) as well as several other composi-\ntions, 410 pieces in total.",
                "[4] G. Hadjeres, F. Pachet, and F. Nielsen, ``DeepBach: A steerable model for\nBach chorales generation,'' in Proc."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "Being an important basis for\nhuman-machine interaction success, emotion recognition is\nalso a popular \u001celd of exploration.",
                "Being an important basis for\nhuman-machine interaction success, emotion recognition is\nalso a popular \u001celd of exploration."
            ],
            "classical": [],
            "electronic": [
                "TEODORA DIMITROVA-GREKOW received the\nPh.D. degree in electronics and automation from\nVienna University of Technology, Austria, in 1997."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "[33] A. Radford, R. Jozefowicz, and I. Sutskever, ``Learning to generate reviews\nand discovering sentiment,'' 2017, arXiv:1704.01444 ."
            ],
            "techno": [
                "Digital Object Identifier 10.1 109/ACCESS.2021.31 13829\nMonophonic Music Generation With\na Given Emotion Using Conditional\nVariational Autoencoder\nJACEK GREKOW\n AND TEODORA DIMITROVA-GREKOW\nFaculty of Computer Science, Bialystok University of Technology, 15-351 Bialystok, Poland\nCorresponding author: Jacek Grekow (j.grekow@pb.edu.pl)",
                "This work was supported by the Ministry of Science and Higher Education through Bialystok University of Technology\nunder Grant WZ/WI-IIT/2/2020.",
                "Due to this simi-\nlarity, also technological solutions to problems such as text\ngeneration and music generation have similar approaches.",
                "IEEE 3rd\nInf. Technol., Netw., Electron.",
                "[13] M. Gui and X. Xu, ``Technology forecasting using deep learning neural net-\nwork: Taking the case of robotics,'' IEEE Access , vol.",
                "Technol. , vol.",
                "JACEK GREKOW received the M.S. degree in\ncomputer systems from the Technical University,\nSo\u001ca, Bulgaria, in 1994, the B.S. degree in music\nfrom Vienna Conservatoire (Konservatorium der\nStadt Wien), Austria, in 1996, the M.S. degree\nin music from the Department of Instrumental\nand Educational Studies, Fryderyk Chopin Univer-\nsity of Music, Warsaw, Poland, in 2006, and the\nPh.D. degree in computer sciences from the Fac-\nulty of Information Technology, Polish-Japanese\nAcademy of Information Technology, Warsaw, in 2009.",
                "He is currently\nan Associate Professor with the Faculty of Computer Science, Bialystok\nUniversity of Technology, Poland.",
                "TEODORA DIMITROVA-GREKOW received the\nPh.D. degree in electronics and automation from\nVienna University of Technology, Austria, in 1997.",
                "She is currently an Assistant Professor with\nthe Department of Computer Science, Bialystok\nUniversity of Technology, Poland."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Music Deep Learning: A Survey on Deep Learning Methods for Music Processing",
            "homophonic": [],
            "polyphonic": [
                "In [33] a novel RNN model, DeepBach, is proposed aimed\nat modeling polyphonic music and specifically hymn-like\npieces, while in [34] the model produces only drums\u2019 sounds.",
                "Polyphonic Music Instrument Detection on\nWeakly Labelled Data using Sequence Learning Models (Dissertation).",
                "Recurrent neural\nnetworks for polyphonic sound event detection in real life recordings.",
                "Automatic Instrument Recogni-\ntion in Polyphonic Music Using Convolutional Neural Networks.",
                "Polyphonic Music Mod-\nelling with LSTM-RTRBM.",
                "Generating Polyphonic Music Using Tied\nParallel Networks.",
                "Polyphonic Music\nGeneration with Sequence Generative Adversarial Networks."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "DeepBach: a steerable\nmodel for bach chorales generation."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "Attention mechanism [26] has gained much popularity re-\ncently.",
                "Attention mechanism [26] has gained much popularity re-\ncently.",
                "B. GANs\nAnother popular approach in the field of MG is the use of\nGANs.",
                "B. GANs\nAnother popular approach in the field of MG is the use of\nGANs.",
                "The authors of [54]\npropose Pop Music Transformer to compose pop piano music,\nachieving better rhythmic structure than other models.",
                "The authors of [54]\npropose Pop Music Transformer to compose pop piano music,\nachieving better rhythmic structure than other models.",
                "Pop Music Transformer:\nBeat-based Modeling and Generation of Expressive Pop Piano Com-\npositions.",
                "Pop Music Transformer:\nBeat-based Modeling and Generation of Expressive Pop Piano Com-\npositions."
            ],
            "classical": [
                "Finally, in [8] classical ML and DL methods\nare reviewed for the task of music genre classification.",
                "In [27] attention augmented CNNs were trained to\nrecognize musical instruments, outperforming the classical\nCNN architectures.",
                "Classical\nRNN architectures have been tested on various MG tasks [30]\n- [35].",
                "Variants of classical transformer\nare designed in [52], [53] reducing the required memory, A\nsparse factorization of the attention matrix was proposed in\n[54], reducing the computation time and producing longer\nsequences of data, including music.",
                "The\nraw audio data were first compressed into compressed codes\nusing Vector Quantization - Variational Autoencoders (VQ-\nV AE), a variant of classical V AE which produces discrete data."
            ],
            "electronic": [
                "[8] N. Ndou, R. Ajoodha and A. Jadhav, \u201dMusic Genre Classification:\nA Review of Deep-Learning and Traditional Machine-Learning Ap-\nproaches,\u201d 2021 IEEE International IOT, Electronics and Mechatronics\nConference (IEMTRONICS), 2021, pp. 1-6, doi: 10.1109/IEMTRON-\nICS52119.2021.9422487."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "C. Alternative approaches\nSome other alternative approaches have been utilized\nthrough the years in various MIR tasks, providing new ways\nto extract useful information."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "The rest of this paper is structured as follows: The DL\nmethods applied on MIR are discussed in section II, while2022 11th International Conference on Modern Circuits and Systems Technologies (MOCAST)\n978-1-6654-6717-9/22/ $31.00 \u00a92022 IEEE2022 11th International Conference on Modern Circuits and Systems Technologies (MOCAST) | 978-1-6654-6717-9/22/$31.00 \u00a92022 IEEE | DOI: 10.1109/MOCAST54814.2022.9837541\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "PhD\nthesis, Music Technology Group (MTG), Universitat Pompeu Fabra,\nBarcelona, 2019."
            ],
            "dubstep": [],
            "opera": [
                "CNNs make use\nof the convolution operation instead of matrix multiplication in\nat least one of their layers [1]."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Music Generation using Deep Generative Modelling",
            "homophonic": [],
            "polyphonic": [
                "[8] J.  A.   Henning,  A.  Umakantha   and   R.  C.   Williamson,  \u201cA classifying \nvariational autoencoder with the application to polyphonic music generation,\u201d\narXiv preprint arXiv:1711.07050, 2017."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "[3] A. Shin and L. Crestel, \u201cMelody Generation for Pop Music via word\nrepresentation of musical properties,\u201d arXiv, vol.",
                "[3] A. Shin and L. Crestel, \u201cMelody Generation for Pop Music via word\nrepresentation of musical properties,\u201d arXiv, vol."
            ],
            "classical": [
                "VI.METHODOLOGY\nWe are focusing on classical music for training of the model \nbecause of the availability of MIDI files for classical music as \nwell as classical music is relatively more standardized in \ncontrast t o other genres."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [
                "In \nspite  of their  limitations,  or perhaps  because  of them, \nsynthesizers have had a profound e\ufb00ect on the course of music \nand culture in the past half century (Punk, 2014)"
            ],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "Music Generation using Deep Generative Modelling \nAdvait Maduskar \nDepartment of Information Technology \nThakur College of Engineering and \nTechnology, \nMumbai, India",
                "advaitmaduskar.1@gmail.com Shubhankar Gore \nDepartment of Information Technology \nThakur College of Engineering and \nTechnology\nMumbai, India \nshubhankar27gore@gmail.",
                "com Aniket Ladukar \nDepartment of Information Technology \nThakur College of Engineering and \nTechnology\nMumbai, India \nladukaraniket@gmail.com",
                "Neha Patwari \nDepartment of Information Technology\nThakur College of Engineering and \nTechnology\nMumbai, India \nneha.patwari@thakureducation.org \nAbstract - Efficient synthesis of musical sequences is a challenging \ntask from a machine learning perspective, as human perception is \naware  of the global context to shorter sequences as  well of audio \nwaveforms  on a smaller  scale."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Music Generation using Deep Learning with Spectrogram Analysis",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "SURSRVHG\u0003 WKH\u0003 'DXEHFKLHV\u0003 :DYHOHW\u0003\n&RHIILFLHQW\u0003 +LVWRJUDPV\u0003 IHDWXUH\u0003 H[WUDFWLRQ\u0003 PHWKRG\u000f\u0003 ZKLFK\u0003\nFDSWXUHG\u0003 PXVLF\u0003 VLJQDOV\n\u0003 ORFDO\u0003 DQG\u0003 JOREDO\u0003 LQIRUPDWLRQ\u0003\nVLPXOWDQHRXVO\\\u0003E\\\u0003FRPSXWLQJ\u0003KLVWRJUDPV\u0003RQ\u0003WKHLU\u0003'DXEHFKLHV\u0003\nZDYHOHW\u0003 FRHIILFLHQWV\u0003 DFKLHYLQJ\u0003 VLJQLILFDQW\u0003 LPSURYHPHQWV\u0003 LQ\u0003\nDFFXUDF\\\u0003>\u0014\u0013@\u0011\u0003/L\u0003DQG\u00032JLKDUD\u0003DOVR\u0003LQYHVWLJDWHG\u0003KLHUDUFKLFDO\u0003\n5892021 2nd International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)\n978-1-6654-1296-4/21/$31.00 \u00a92021 IEEE\nDOI 10.1109/AINIT54228.2021.001192021 2nd International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT) | 978-1-6654-1296-4/21/$31.00 \u00a92021 IEEE | DOI: 10.1109/AINIT54228.2021.00119\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Music Generation with AI technology: Is It Possible?",
            "homophonic": [],
            "polyphonic": [
                "The Biaxial-LSTM can \ngenerate polyphonic music, and the model evaluation mainly \nuses the Turing test.",
                "DeepJ can be seen as an improvement based \non Biaxial-LSTM, generating specific-style polyphonic music.",
                "While LSTM \nmodel can only be used to generate monophonic music, Biaxial \nLSTM can be used to generate polyphonic music which is a kind \nof music texture whe re multiple voices play at the same time.",
                "[0 0\n1 10 0\n0 0\n0\n10\n10\n00\n0]  \nAlso, while holding a note is not the same as replaying a note, \nwe need to distinguish these two events, so \ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66  is also \nneeded which looks similar to \ud835\udc61\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66. \n2) Architecture \nBiaxial LSTM generates polyphonic music by modeling \nevery note in every time step as a probability, using all previous \ntime steps and all notes already generated in the current time step \nas the condition.",
                "LSTM model performs weak \nwhen generating polyphonic music.",
                "As this model is \ndeveloped based on Biaxial LSTM, it has retained the features \nof genera ting polyphonic music texture and transposition \ninvariance.",
                "Biaxial LSTM can \ndistinguish relative pitches and therefore can handle polyphonic \nmusic, which is the biggest improvement over the LSTM model.",
                "This model is related to musical style, but \nis more suitable for polyphonic music generation with multiple \ntracks.",
                "The soundtracks are all polyphonic piano \npieces.",
                "Polyphonicity (PP): the r atio of time steps with more than \ntwo notes playing together to the total counts of time steps.",
                "This model \nhas the ability to generate polyphonic music, but since the \nalgorithm still does not have the ability to filter music styles in \norder to customize music generated, two cutting -edge researches \non AI music gen eration  were  further explored, which are the \nDeepJ and MuseGAN models.  \n \nFig. 12."
            ],
            "monophonic": [
                "While LSTM \nmodel can only be used to generate monophonic music, Biaxial \nLSTM can be used to generate polyphonic music which is a kind \nof music texture whe re multiple voices play at the same time.",
                "While for the simple LSTM  model, it can only handle \nmonophonic music."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [
                "MuseGAN \nis an innovational approach based on Generative Adversarial \nNetwork that can be used to generate pop music with multiple \ntracks.",
                "MuseGAN \nis an innovational approach based on Generative Adversarial \nNetwork that can be used to generate pop music with multiple \ntracks.",
                "For instance, in pop music, there is more \nthan one instrument playing at the same time and have their own \nmelody and rhythmic patterns.",
                "For instance, in pop music, there is more \nthan one instrument playing at the same time and have their own \nmelody and rhythmic patterns.",
                "Both of these two models are related to \nmusic style, while the main difference is, the DeepJ model is \napplicable to all musi c styles, but the MuseGAN model is mostly \nused to generate pop music which has multiple instruments and tracks.",
                "Both of these two models are related to \nmusic style, while the main difference is, the DeepJ model is \napplicable to all musi c styles, but the MuseGAN model is mostly \nused to generate pop music which has multiple instruments and tracks.",
                "4) Apply GAN into pop music generation  \nPop music is usually composed of multiple \ninstruments/tracks.",
                "4) Apply GAN into pop music generation  \nPop music is usually composed of multiple \ninstruments/tracks.",
                "Due to the existence of a long -term independent  structure of \nmultitrack pop music, the authors used bars as the most basi c \nunit for model training.",
                "Due to the existence of a long -term independent  structure of \nmultitrack pop music, the authors used bars as the most basi c \nunit for model training.",
                "In the pre -processing process for multiple tracks of popular \nmusic, the authors used the musPy tool previously developed by \ntheir team to binarize the music tracks into a binarized image \nmatrix.",
                "In the pre -processing process for multiple tracks of popular \nmusic, the authors used the musPy tool previously developed by \ntheir team to binarize the music tracks into a binarized image \nmatrix.",
                "The MuseGAN model, on the other \nhand, is mainly targeted at modern pop music and usually \ncontains a variety of instruments such as drums, bass, guitar, \norchestra and piano.",
                "The MuseGAN model, on the other \nhand, is mainly targeted at modern pop music and usually \ncontains a variety of instruments such as drums, bass, guitar, \norchestra and piano.",
                "2) MuseGAN model  \nThe goal of Mu seGAN is to generate pop music of multiple \ntracks in piano -roll format.",
                "2) MuseGAN model  \nThe goal of Mu seGAN is to generate pop music of multiple \ntracks in piano -roll format.",
                "The \nMuseGAN model is another style -related model that is used to \ngenerate music with multiple instrument tracks, normally used \nto generate modern pop music.",
                "The \nMuseGAN model is another style -related model that is used to \ngenerate music with multiple instrument tracks, normally used \nto generate modern pop music.",
                "Compared with DeepJ model, the MuseGAN model is \nmore suitable for generating pop music w ith multiple \ninstruments and tracks, and can be used in the future for game \nsoundtracks, etc.",
                "Compared with DeepJ model, the MuseGAN model is \nmore suitable for generating pop music w ith multiple \ninstruments and tracks, and can be used in the future for game \nsoundtracks, etc."
            ],
            "classical": [
                "In DeepJ model, we use a dataset of 23 different composers \nfrom different classical music periods.",
                "In this paper, the DeepJ model is mainly used for the \ngeneration of classical music, and the generated music style also \nfocus on classical music.",
                "C. Features  in Datasets  \n1) DeepJ model  \nThe dataset of DeepJ includes MIDI files of pieces \ncomposed by 23 well -known composers in the three major \nperiods of class ical music (Baroque period, classical period, and \nromantic period).",
                "20 people with musical background are \nselected to classify the classical period for the specific piece they \nheard.",
                "10 of them is tested by the control samples, which are real \nclassical music composed by human, another 10 is tested by the \ngenerated samples.",
                "Beethoven, who is the yellow point right next to the red point, is \na representation for the transition between classical and \nromantic periods, which proved our observation.",
                "We also made \nanother 3 -D visualization with x -axis, y -axis, and z -axis \nrepresenting baroque, classical and romantic period, \nrespectively.  \n \nFig.",
                "In theory, the DeepJ model can be generalized to other types \nof music, but modific ations to the model are required, so the \nmusic generated here is classified into three categories of \nclassical music which are Baroque, Classical and Romantic \nperiods."
            ],
            "electronic": [
                ", \ud835\udc18 \ud835\udfcf,\ud835\udfd0, \ud835\udc18\ud835\udfcf,\ud835\udfcf) \n12652022 IEEE5th InternationalConference onElectronics Technology (ICET)",
                "IEEE 5th International Conference on Electronics Technology (ICET) | 978-1-6654-8508-1/22/$31.00 \u00a92022"
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "Music Generation with AI technology: Is It Possible?",
                "Haowen Tang \nDivision of Science and Technology  \nBNU-HKBU United International Colloge \nZhuhai , China \nn830026106@gmail.com Yikun Gu \nDivision of Science and Technology  \nBNU-HKBU United International Colloge \nZhuhai , China \nsherly28gyk@gmail.com Xinyu Yang \nDivision of Science and Technology  \nBNU-HKBU United International Colloge \nZhuhai , China \nxinyu_yang0113@163.com",
                "Thus, automatic composition using artificial intelligence \ntechnology and collaborative composition with musicians will \nbecome mainstream.",
                ", \ud835\udc18 \ud835\udfcf,\ud835\udfd0, \ud835\udc18\ud835\udfcf,\ud835\udfcf) \n12652022 IEEE5th InternationalConference onElectronics Technology (ICET)",
                "IEEE 5th International Conference on Electronics Technology (ICET) | 978-1-6654-8508-1/22/$31.00 \u00a92022"
            ],
            "dubstep": [],
            "opera": [
                "We would also like to thank the team members for the \nwonderful cooperation."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Music Generation with Bi-Directional Long Short Term Memory Neural Networks",
            "homophonic": [
                "This paper works with the generation of homophonic \nmusic for which the piano channel of the input MI DI file is \nconverted to a piano roll format and is read over two axes."
            ],
            "polyphonic": [
                "Mao et al. have bui lt a Biaxial LSTM model to generate \npolyphonic music based on a mixture of different composition \nstyles.",
                "\u201cModelling \nhigh-dimensional sequences with LSTM -RTRBM: application to \npolyphonic music generation\u201d 24th International Conference on \nArtificial Intelligence (IJCAI'15)."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "Firstly, the file \nname is passed as the pa rameter to load the song in the \nvariable followed by extracting the data of the first channel, \ni.e., the piano instrumental of the song."
            ],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [
                "International Conference on \nElectronics Technology (ICET), 2019, pp."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "Music Generation with Bi -Directional Long Short \nTerm Memory Neural Networks  \nMadhur Rajadhyaksha   \nDepartment of Information Technology  \nSardar Patel Institute of Technology  \nMumbai, India  \nmadhur.rajadhyaksha@spit.ac.in  \nDr. Prasenjit Bhavathankar  \nDepartment of Information Technology  \nSardar Patel Institute of Technology  \nMumbai, India  \np_bhavathankar@spit.ac.in  Neha Lakhani  \nDepar tment of Information Technology  \nSardar Patel Institute of Technolgy  \nMumbai, India  \nneha.lakhani@spit.ac.in  Mohammad Anas Mudassir  \nDepartment of Information Technology  \nSardar Patel Institute of Technology  \nMumbai, India  \nmohammadanas.mudassir@spit.ac.in  \nAbstract\u2014 Deep  Learning  has been  utilized  in music \ngeneration where long term dependencies and patterns need to \nbe learned  by the network.",
                "International Conference on Computing Communication and Networking Technologies (ICCCNT) | 978-1-6654-5262-5/22/$31.00 \u00a92022 IEEE | DOI: 10.1109/ICCCNT54827.2022.9984228\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "[1] Eva P.S. Rani, S.V. Praneeth, V.R.K. Reddy,  M.J. Sathish, \"MUSIC \nGENERATION USING DEEP LEARNING\", International Research \nJournal of Engineering and Technology, vol.",
                "[3] A. Pal, S. Saha, R. Anita,   \"Musenet : Music Generation using \nAbstractive and Generative Methods\", International Journal of \nInnovative Technology and Exploring Engineerin g, vol.",
                "International Conference on \nElectronics Technology (ICET), 2019, pp.",
                "[13] Syeda Sarah Azmi, Shreekara C, Shwetha Baliga, \u201cMusic generation \nusing Bidirectional Recurrent Neural Nets,\u201d International Research \nJournal of Engineering and Technology, Volume: 07 Issue: 05, May \n2020, e -ISSN: 2395 -0056, p -ISSN: 23 95-0072.",
                "[23] Akanksha Dawande , Uday Chourasia , Priyanka Dixit, \u201cMusic \nGeneration and Composition Using Machine Learning \u201d, International \nJournal Of Engineering Research & Technology (IJERT) Volume 10, \nIssue 12 (December 2021)"
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "RE-RLTuner: A topic-based music generation method",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "We use the Nottingham dataset, a dataset offolk songs, pitch range from 53 to 88, which we quanti\ufb01edinto 4/4 beats.",
                "[8] Sturm B, Santos J F, Korshunova I. Folk music style modelling by\nrecurrent neural networks with long short term memory units[C]//16thInternational Society for Music Information Retrieval Conference.2015."
            ],
            "blues": [
                "Finding temporal structure in music: Blues\nimprovisation with LSTM recurrent"
            ],
            "jazz": [],
            "rock": [],
            "pop": [
                "Carrying the duration and pitch in-formation of music, symbolic representation has simplercomputation characteristics and therefor it is popular amongresearchers[12].",
                "Carrying the duration and pitch in-formation of music, symbolic representation has simplercomputation characteristics and therefor it is popular amongresearchers[12]."
            ],
            "classical": [
                "Inspiredby RLTuner, we treated the music generation problem asa sequential decision problem using the classical DQNalgorithm."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "In the training of RL, the learning rate of the Q network is0.01, the discount rate of reward is 0.95, and the update rateof the target Q network is 0.01."
            ],
            "techno": [
                "To meet the increasing needs of music products, re-searchers both from computer science and digital music tech-nology attempt to realize computational music generationtechnology.",
                "As Long Short-Term Memory (LSTM) networkarchitectures excel in modeling sequential information indata, LSTM based music generation method is proposed\n1CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems,\nShenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\nShenzhen 518055, China.",
                "The current technology often pursues music data[2], [3],\n[4] and rarely considers it from the music compositionperspective.",
                "This work was supported by the National Key R&D\nProgram of China [grant number: 2020YFC2004100]; theNational Natural Science Foundation of China [grantnumbers: NSFCU1736202, NSFC61771461]; the Shen-zhen Fundamental Research Program [grant numbers:JCYJ20170413161611534], and the Science and TechnologyPlanning Project of Guangdong Province [grant number:2019B090915002]."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Some Reflections on the Potential and Limitations of Deep Learning for Automated Music Generation",
            "homophonic": [],
            "polyphonic": [
                "Bengio, and P. Vincent,\n\u201cModeling temporal dependencies in high-dimensional se-\nquences: Application to polyphonic music generation and\ntranscription,\u201d arXiv preprint arXiv:1206.6392, 2012."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "Bach-styled\nChorales generated in [14] and Irish folk music\ngenerated in [15] are an example of how a having\na corpus with rigid structural rules makes it easier\nfor the model to generate convincing results, even\nthough expert ears can still identify them as arti\ufb01cial\ndue to some dubious style decision or technical\nerror.",
                "[14] G. Hadjeres, F. Pachet, and F. Nielsen, \u201cDeepBach: a Steer-\nable Model for Bach chorales generation,\u201d arXiv preprint\narXiv:1612.01010, 2016."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "This notation was introduced\nin the early 90s to share Irish folk tunes on the\nInternet and has since become the most popular\nformat alternative to midi (it even has a MIME type).",
                "Bach-styled\nChorales generated in [14] and Irish folk music\ngenerated in [15] are an example of how a having\na corpus with rigid structural rules makes it easier\nfor the model to generate convincing results, even\nthough expert ears can still identify them as arti\ufb01cial\ndue to some dubious style decision or technical\nerror."
            ],
            "blues": [
                "[12] D. Eck and J. Schmidhuber, \u201cFinding temporal structure\nin music: Blues improvisation with LSTM recurrent net-\nworks,\u201d in Neural Networks for Signal Processing, 2002."
            ],
            "jazz": [
                "Flow Harmonizer has the ability to\nperform style transfer on an existing piece, examples\nshow Ode to Joy in the style of Beatles, Bossa Nova,\nBach, Jazz, etc."
            ],
            "rock": [],
            "pop": [
                "This notation was introduced\nin the early 90s to share Irish folk tunes on the\nInternet and has since become the most popular\nformat alternative to midi (it even has a MIME type).",
                "This notation was introduced\nin the early 90s to share Irish folk tunes on the\nInternet and has since become the most popular\nformat alternative to midi (it even has a MIME type).",
                "An example of pianoroll notationAnother popular representation is the so-called\nPianoroll, taking its name from the cylinders in\nold automatic player pianos.",
                "An example of pianoroll notationAnother popular representation is the so-called\nPianoroll, taking its name from the cylinders in\nold automatic player pianos.",
                "They gained popularity in 2016\nwith the release of \u201dDaddy\u2019s Car\u201d a track in style\nof the Beatles generated by their software; while the\npress described as the rise of the AI composers the\ntruth is that the model generated the sheet music,\nin a style that sound like a mishmash of all the\nBeatles different styles, and this was then arranged\nand performed by the musician Beno \u02c6\u0131t Carr \u00b4e.",
                "They gained popularity in 2016\nwith the release of \u201dDaddy\u2019s Car\u201d a track in style\nof the Beatles generated by their software; while the\npress described as the rise of the AI composers the\ntruth is that the model generated the sheet music,\nin a style that sound like a mishmash of all the\nBeatles different styles, and this was then arranged\nand performed by the musician Beno \u02c6\u0131t Carr \u00b4e.",
                "Available:\nhttp://arxiv.org/abs/1704.01279\n[28] M. Marchini, F. Pachet, and B. Carr \u00b4e, \u201cRethinking Re\ufb02exive\nLooper for structured pop music,\u201d in Proceedings of the\nInternational Conference on New Interfaces for Musical\nExpression, 2017, pp.",
                "Available:\nhttp://arxiv.org/abs/1704.01279\n[28] M. Marchini, F. Pachet, and B. Carr \u00b4e, \u201cRethinking Re\ufb02exive\nLooper for structured pop music,\u201d in Proceedings of the\nInternational Conference on New Interfaces for Musical\nExpression, 2017, pp."
            ],
            "classical": [
                "Both models are based on CNNs,\nbut while MidiNet uses a fairly classical CNN\narchitecture, MuseGan uses an unprecedented model\nwith a strong hierarchy that is based not on the\nsingle not but on the bar as a unit, featuring a bar\ngenerator controlled by a phrase generator.",
                "E. Constrained Markov Models\nA completely different approach relies on more\nclassical machine learning."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "This notation was introduced\nin the early 90s to share Irish folk tunes on the\nInternet and has since become the most popular\nformat alternative to midi (it even has a MIME type)."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "With AI duet[26] they showcased their technology\ncreating an AI ensemble that responded to the user\ninput.",
                "Luxembourg based Aiva Technologies focused on\nthe creation an AI composer that produces scores to\nbe recorded by real orchestras but it is not clear\nhow this is achieved and to which degree human\nintervention is necessary."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation",
            "homophonic": [],
            "polyphonic": [
                "[6] S. Lattner, M. Grachten and G. Widmer, Imposing higher-level Structure\nin Polyphonic Music Generation using Convolutional Restricted Boltz-\nmann Machines and Constraints, Journal of Creative Music Systems,\nvol. 2, Issue 1, March 2018"
            ],
            "monophonic": [
                "B. LSTM for Music Generation\nMany music generation works by deep neural networks start\nwith unconditional (monophonic) symbolic melody genera-\ntion."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "[3] Liang and Feynman, BachBot: Automatic composition in the style of\nBach chorales, University of Cambridge, 2016.",
                "[4] G. Hadjeres and F. Pachet, B DeepBach: a Steerable Model for Bach\nchorales generation, Proceedings of the 34th International Conference\non Machine Learning, PMLR 70:1362-1371, 2017.",
                "[11] H. Hermann, F. Johannes and M. Wolfram, HARMONET: A Neural Net\nfor Harmonizing Chorales in the Style of J.S.Bach, Proceedings of the\n4th International Conference on Neural Information Processing Systems,\npp. 267287, 1991."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "[35], which consists of\n941 folk songs, and each contains both melody and chords."
            ],
            "blues": [
                "They\ntested the Blues improvisation performance of LSTM by\ninputting note slices in real time.",
                "[30] D. Eck and S. Jurgen, Learning the Long-Term Structure of the Blues,\nArti\ufb01cial Neural Networks ICANN 2002, Springer Berlin Heidelberg,\npp. 284289, 2002."
            ],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "Formally:\nH0:\u03bcA=\u03bcB=\u03bcC (11)\nthe alternative hypothesis is that:\nH1:\u2203i,j\u2208{A,B,C}:\u03bci/negationslash=\u03bcj (12)",
                "Formally:\nH0:\u03bci=\u03bcj (13)\nthe alternative hypothesis is that:\nH1:\u03bci/negationslash=\u03bcj (14)\nD. Survey Evaluation\nA total of n= 106 people (42 females and 64 males) have\ncompleted the survey."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "We conduct a survey for\nevaluation in our generations and implemented V ariable Markov\nOracle in music pattern discovery.",
                "In order to be\nable to see how well the neurally generated music is able\nto learn such structure, we applied an Information Dynamics\nanalysis developed by [13] for music pattern discovery.",
                "We discuss several important discoveries\nin Section V and \ufb01nally come to the conclusion in Section VI.\nII.",
                "E. Pattern Discovery by VMO\nWe implemented V ariable Markov Oracle from [13] to\nillustrate disparities of models in generating music patterns\nand repetition structures.",
                "Finally, the patterns discovered by VMO in one samples\ngroup are shown in Fig. 10.",
                "Patterns discovered in each sample by VMO.",
                "[13] C. Wang, J. Hsu and S. Dubnov, Music Pattern Discovery with Variable\nMarkov Oracle:"
            ],
            "techno": [],
            "dubstep": [],
            "opera": [
                "The unconditioned model with the input melody vector\nm, and the activation function in dilation layer kis:\nz=tanh(Wf,k\u2217m)\u2299\u03c3(Wg,k\u2217m) (9)\nwhere\u2217represents a dilated convolution operator, Wf,kand\nWg,kare the learnable parameters in the convolution layer,\nand\u2299is a piecewise multiplication operator.",
                "The activation function at layer k, with the embedded chord\ncondition vector cand the melody vector m:\nz=tanh(Wf,k\u2217m+Vf,k\u2217c)\u2299\u03c3(Wg,k\u2217m+Vg,k\u2217c)(10)\nwhere the \ufb01rst \u2217in both parentheses represents a dilated\nconvolution operator with Wf,kandWg,kas the learnable\nparameters."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "PopMNet: Generating structured pop music melodies using neural networks",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [
                "In a monophonic  melody,  all notes and \nrests are represented  by 128 \u201cnote-on\u201d  tokens  and one \u201cnote-off\u201d  token."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [
                "Over 10 years ago, it was used to learn a form of blues music [14]."
            ],
            "jazz": [
                "GenJam  is a genetic  algorithm-based  model  that pro-\nduces  jazz solos over a given chord  progression [ 12].",
                "[12]J.A. Biles, et al., GenJam:  a genetic  algorithm  for generating  jazz solos, in: International  Computer  Music Conference,  vol."
            ],
            "rock": [],
            "pop": [
                "Arti\ufb01cial Intelligence 286 (2020) 103303\nContents lists available at ScienceDirect\nArti\ufb01cial  Intelligence\nwww.elsevier.com/locate/artint\nPopMNet:  Generating  structured  pop  music  melodies  using  \nneural  networks\nJian Wua, Xiaoguang Liub, Xiaolin Hua,\u2217, Jun Zhua\naInstitute  for Arti\ufb01cial  Intelligence,  Beijing National  Research  Center for Information  Science and Technology  (BNRist),  the State Key Laboratory  \nof Intelligent  Technology  and Systems,  and Department  of Computer  Science and Technology,  Tsinghua  University,  Beijing 100084,  China\nbLingDongYin  Technoloy  Co., Ltd., Beijing, 100084,  China\na r t",
                "Arti\ufb01cial Intelligence 286 (2020) 103303\nContents lists available at ScienceDirect\nArti\ufb01cial  Intelligence\nwww.elsevier.com/locate/artint\nPopMNet:  Generating  structured  pop  music  melodies  using  \nneural  networks\nJian Wua, Xiaoguang Liub, Xiaolin Hua,\u2217, Jun Zhua\naInstitute  for Arti\ufb01cial  Intelligence,  Beijing National  Research  Center for Information  Science and Technology  (BNRist),  the State Key Laboratory  \nof Intelligent  Technology  and Systems,  and Department  of Computer  Science and Technology,  Tsinghua  University,  Beijing 100084,  China\nbLingDongYin  Technoloy  Co., Ltd., Beijing, 100084,  China\na r t",
                "However,  generating  pop music melodies  with well organized  structures  remains  to be \nchallenging.",
                "However,  generating  pop music melodies  with well organized  structures  remains  to be \nchallenging.",
                "In this paper, we present  a melody  structure-based  model  called PopMNet  \nto generate  structured  pop music melodies.",
                "In this paper, we present  a melody  structure-based  model  called PopMNet  \nto generate  structured  pop music melodies.",
                "PopMNet  \nconsists  of a Convolutional  Neural  Network (CNN)-based  Structure  Generation  Net (SGN) \nand a Recurrent  Neural  Network (RNN)-based  Melody  Generation  Net (MGN).",
                "PopMNet  \nconsists  of a Convolutional  Neural  Network (CNN)-based  Structure  Generation  Net (SGN) \nand a Recurrent  Neural  Network (RNN)-based  Melody  Generation  Net (MGN).",
                "Fig.1shows  the melody  of a pop song \u201cSimple  Love\u201d.",
                "Fig.1shows  the melody  of a pop song \u201cSimple  Love\u201d.",
                "Speci\ufb01cally,  we consider  two important  relations  \u2014 repetition and sequence , which  play critical  \nroles in the formation  of pop music  melody  structures",
                "Speci\ufb01cally,  we consider  two important  relations  \u2014 repetition and sequence , which  play critical  \nroles in the formation  of pop music  melody  structures",
                "1.A piece of melody of \u201cSimple Love\u201d, which is a pop song by Chinese singer Jay Chou, released on 14 September 2001.",
                "1.A piece of melody of \u201cSimple Love\u201d, which is a pop song by Chinese singer Jay Chou, released on 14 September 2001.",
                "We propose  a melody  structure-based  melody  generation  model,  called  PopMNet , where  the structure  is characterized  \nby repetition  and sequence  between  pairs of bars.",
                "We propose  a melody  structure-based  melody  generation  model,  called  PopMNet , where  the structure  is characterized  \nby repetition  and sequence  between  pairs of bars.",
                "Experiments  show that melodies  \ngenerated  by our PopMNet  have better  structures  compared  to many existing  models.",
                "Experiments  show that melodies  \ngenerated  by our PopMNet  have better  structures  compared  to many existing  models.",
                "Many types of music  such as pop music  have high-level  units such as phrases  and periods  \u2014 a \nphrase  consists  of several  bars and a period  consists  of several  phrases.",
                "Many types of music  such as pop music  have high-level  units such as phrases  and periods  \u2014 a \nphrase  consists  of several  bars and a period  consists  of several  phrases.",
                "PopMNet  shares  a similar  \nidea with the above  models:  generates  high-level  features  (melody  structure)",
                "PopMNet  shares  a similar  \nidea with the above  models:  generates  high-level  features  (melody  structure)",
                "Our models\nThe PopMNet  and its two variants  were used in the experiments:\n\u2022PopMNet: the model  proposed  in this paper.",
                "Our models\nThe PopMNet  and its two variants  were used in the experiments:\n\u2022PopMNet: the model  proposed  in this paper.",
                "\u2022PopMNet-Real: same as PopMNet  but the melody  structures  used in the melody  generation  are extracted  from the \ndataset.",
                "\u2022PopMNet-Real: same as PopMNet  but the melody  structures  used in the melody  generation  are extracted  from the \ndataset.",
                "\u2022PopMNet-NC: the PopMNet  without  chord  progressions  as a condition.",
                "\u2022PopMNet-NC: the PopMNet  without  chord  progressions  as a condition.",
                "while melodies  generated  by PopMNet  and Music  Transformer  contained  clear structures.",
                "while melodies  generated  by PopMNet  and Music  Transformer  contained  clear structures.",
                "One observation  \nis that the melodies  generated  by PopMNet  and Music  Transformer  often contain  long-term  relations,  which  is indicated  \nby red or black dots towards  the lower  left of the adjacency  matrices  (e.g., the \ufb01rst and fourth  column  in Figs.7d,7e).",
                "One observation  \nis that the melodies  generated  by PopMNet  and Music  Transformer  often contain  long-term  relations,  which  is indicated  \nby red or black dots towards  the lower  left of the adjacency  matrices  (e.g., the \ufb01rst and fourth  column  in Figs.7d,7e).",
                "The percents  of PopMNet  and Music  Transformer  are close to the real data.",
                "The percents  of PopMNet  and Music  Transformer  are close to the real data.",
                "The percents  of repetition  and \nrhythmic  sequence  of MidiNet  and LookbackRNN  are comparable  with those of PopMNet.",
                "The percents  of repetition  and \nrhythmic  sequence  of MidiNet  and LookbackRNN  are comparable  with those of PopMNet.",
                "The distributions  of PopMNet  and Music  Transformer  were \nsimilar  to the distributions  of real melody  structures  (Fig.8).",
                "The distributions  of PopMNet  and Music  Transformer  were \nsimilar  to the distributions  of real melody  structures  (Fig.8).",
                "Structure  metrics  of PopMNet  and PopMNet-NC  \nwere similar,  suggesting  that the structure  of the generated  melodies  was mainly  brought  by the guidance  of the generated  \nmelody  structure  instead  of the chord  progression.",
                "Structure  metrics  of PopMNet  and PopMNet-NC  \nwere similar,  suggesting  that the structure  of the generated  melodies  was mainly  brought  by the guidance  of the generated  \nmelody  structure  instead  of the chord  progression.",
                "In this experiment,  we evaluated  \ufb01ve models,  PopMNet,  PopMNet-real,  AttentionRNN,  LookbackRNN,  and \nMidiNet.",
                "In this experiment,  we evaluated  \ufb01ve models,  PopMNet,  PopMNet-real,  AttentionRNN,  LookbackRNN,  and \nMidiNet.",
                "Repetition Rhythmic sequence No relation\nReal data 29.06% 32.64% 38.30%\nAttentionRNN 0.42% 24.73% 74.85%\nLookbackRNN 9.13 % 37.78% 53.09%MidiNet 18.75% 18.752% 62.29%Music Transformer 35.64% 25.15% 39.21%\nPopMNet 27.05% 28.91% 44.04%\nPopMNet-Real 31.45% 31.33% 37.22%PopMNet-NC 26.28% 29.45% 44.27%\nin the revision  phase  and it would  be costly  to redo this experiment  by incorporating  a new model.",
                "Repetition Rhythmic sequence No relation\nReal data 29.06% 32.64% 38.30%\nAttentionRNN 0.42% 24.73% 74.85%\nLookbackRNN 9.13 % 37.78% 53.09%MidiNet 18.75% 18.752% 62.29%Music Transformer 35.64% 25.15% 39.21%\nPopMNet 27.05% 28.91% 44.04%\nPopMNet-Real 31.45% 31.33% 37.22%PopMNet-NC 26.28% 29.45% 44.27%\nin the revision  phase  and it would  be costly  to redo this experiment  by incorporating  a new model.",
                "Transformer 2.597 0.848\nPopMNet 1.743 0.484\nPopMNet-Real 1.565 0.665\nPopMNet-NC 1.738 0.562",
                "Transformer 2.597 0.848\nPopMNet 1.743 0.484\nPopMNet-Real 1.565 0.665\nPopMNet-NC 1.738 0.562",
                "PopMNet  outperformed  the four existing  models  on \nall metrics  by a large margin  (Pleasure:  p \u22640.0043;  Reality:  p \u22640.00054;  Smooth:  p \u22640.0011;  Integrity,  p \u22649.00 \u00d710\u22125; \none-tailed  t-test,  N=170).",
                "PopMNet  outperformed  the four existing  models  on \nall metrics  by a large margin  (Pleasure:  p \u22640.0043;  Reality:  p \u22640.00054;  Smooth:  p \u22640.0011;  Integrity,  p \u22649.00 \u00d710\u22125; \none-tailed  t-test,  N=170).",
                ".18\u00b11.16\nMidiNet 2 .70\u00b11.36 2 .57\u00b11.39 2 .81\u00b11.32 2 .93\u00b11.34\nPopMNet",
                ".18\u00b11.16\nMidiNet 2 .70\u00b11.36 2 .57\u00b11.39 2 .81\u00b11.32 2 .93\u00b11.34\nPopMNet",
                "3 .50\u00b11.00 3 .40\u00b11.02 3 .46\u00b11.07 3 .64\u00b11.03\nPopMNet-Real 3",
                "3 .50\u00b11.00 3 .40\u00b11.02 3 .46\u00b11.07 3 .64\u00b11.03\nPopMNet-Real 3",
                "MidiNet 2 .02\u00b11.10 1 .70\u00b10.97 2 .19\u00b11.16 2 .21\u00b11.20\nPopMNet",
                "MidiNet 2 .02\u00b11.10 1 .70\u00b10.97 2 .19\u00b11.16 2 .21\u00b11.20\nPopMNet",
                "3 .39\u00b10.83 3 .10\u00b11.02 3 .26\u00b11.05 3 .33\u00b11.03\nPopMNet-Real 3 .31\u00b11.08",
                "3 .39\u00b10.83 3 .10\u00b11.02 3 .26\u00b11.05 3 .33\u00b11.03\nPopMNet-Real 3 .31\u00b11.08",
                ".01\u00b11.16 3 .10\u00b11.97 3 .02\u00b11.23\nPopMNet",
                ".01\u00b11.16 3 .10\u00b11.97 3 .02\u00b11.23\nPopMNet",
                "3 .98\u00b10.89\nscores  of the PopMNet  and PopMNet-Real  were not signi\ufb01cantly  different  (p \u22650.34 on  all metrics;  two-tailed  t-test,  N=\n170).",
                "3 .98\u00b10.89\nscores  of the PopMNet  and PopMNet-Real  were not signi\ufb01cantly  different  (p \u22650.34 on  all metrics;  two-tailed  t-test,  N=\n170).",
                "PopMNet  outperformed  \nthe four existing  models  on all metrics  by a large margin  (Pleasure:  p \u22643.73 \u00d710\u22125; Reality:  p \u22649.86 \u00d710\u22126; Smooth:  \np \u22641.19 \u00d710\u22125; Integrity,  p \u22647.21 \u00d710\u22124; one-tailed  t-test,  N=80).",
                "PopMNet  outperformed  \nthe four existing  models  on all metrics  by a large margin  (Pleasure:  p \u22643.73 \u00d710\u22125; Reality:  p \u22649.86 \u00d710\u22126; Smooth:  \np \u22641.19 \u00d710\u22125; Integrity,  p \u22647.21 \u00d710\u22124; one-tailed  t-test,  N=80).",
                "The average  scores  of the PopMNet  and PopMNet-\nReal were not signi\ufb01cantly  different  (p \u22650.738 on  all metrics;  two-tailed  t-test,  N=80).",
                "The average  scores  of the PopMNet  and PopMNet-\nReal were not signi\ufb01cantly  different  (p \u22650.738 on  all metrics;  two-tailed  t-test,  N=80).",
                "In this experiment,  we evaluated  PopMNet  and Music  Transformer.",
                "In this experiment,  we evaluated  PopMNet  and Music  Transformer.",
                "Besides  melodies  generated  by PopMNet  and Music  Transformer,  10 melodies  sampled  from the dataset  were also evaluated.",
                "Besides  melodies  generated  by PopMNet  and Music  Transformer,  10 melodies  sampled  from the dataset  were also evaluated.",
                "PopMNet  performed  signi\ufb01cantly  better  than Music  Transformer  (Pleasure:  p =2.25 \u00d710\u22125; Reality:  p =0.0059;  Smooth:  \np =0.0038;  Integrity,  p =0.0070;  one-tailed  t-test,  N=180), but performed  worse  than human  (Pleasure:  p \u22642.66 \u00d710\u22128; \nReality:  p \u22641.87 \u00d710\u221210; Smooth:  p \u22641.34 \u00d710\u22126; Integrity,  p \u22644.91 \u00d710\u221212; one-tailed  t-test,  N=180) (Table 5).",
                "PopMNet  performed  signi\ufb01cantly  better  than Music  Transformer  (Pleasure:  p =2.25 \u00d710\u22125; Reality:  p =0.0059;  Smooth:  \np =0.0038;  Integrity,  p =0.0070;  one-tailed  t-test,  N=180), but performed  worse  than human  (Pleasure:  p \u22642.66 \u00d710\u22128; \nReality:  p \u22641.87 \u00d710\u221210; Smooth:  p \u22641.34 \u00d710\u22126; Integrity,  p \u22644.91 \u00d710\u221212; one-tailed  t-test,  N=180) (Table 5).",
                "Discussion\nWe present  the PopMNet  to generate  structured  pop music  melodies,  which  integrates  melody  structure  into the gen-\neration  process.",
                "Discussion\nWe present  the PopMNet  to generate  structured  pop music  melodies,  which  integrates  melody  structure  into the gen-\neration  process.",
                "In experiments,  \nwe compared  PopMNet  with four existing  models  AttentionRNN,  LookbackRNN,  MidiNet,  and Music  Transformer.",
                "In experiments,  \nwe compared  PopMNet  with four existing  models  AttentionRNN,  LookbackRNN,  MidiNet,  and Music  Transformer.",
                "The results  \nindicate  that PopMNet  can generate  higher  quality  melodies  with clear structures  than",
                "The results  \nindicate  that PopMNet  can generate  higher  quality  melodies  with clear structures  than",
                "This is only the \ufb01rst step towards  generating  rich and compelling  pop music  melodies.",
                "This is only the \ufb01rst step towards  generating  rich and compelling  pop music  melodies.",
                "First, only repetition  and rhythmic  \nsequence  between  bars were considered  as structures,  while real, human-composed  pop music  melodies  contain  much more \ncomplex  relations  between  melody  segments,  which  will certainly  be studied.",
                "First, only repetition  and rhythmic  \nsequence  between  bars were considered  as structures,  while real, human-composed  pop music  melodies  contain  much more \ncomplex  relations  between  melody  segments,  which  will certainly  be studied.",
                "[5]H. Zhu, Q. Liu, N.J. Yuan, C. Qin, J. Li, K. Zhang, G. Zhou, F. Wei, Y. Xu, E. Chen, Xiaoice  band: a melody  and arrangement  generation  framework  \nfor pop music, in: Proceedings  of the 24th ACM SIGKDD  International  Conference  on Knowledge  Discovery,  Data Mining,  New York, NY, USA, 2018, \npp.",
                "[5]H. Zhu, Q. Liu, N.J. Yuan, C. Qin, J. Li, K. Zhang, G. Zhou, F. Wei, Y. Xu, E. Chen, Xiaoice  band: a melody  and arrangement  generation  framework  \nfor pop music, in: Proceedings  of the 24th ACM SIGKDD  International  Conference  on Knowledge  Discovery,  Data Mining,  New York, NY, USA, 2018, \npp.",
                "[8]R. Middleton,  \u2018Play it again sam\u2019: some notes on the productivity  of repetition  in popular  music, Pop.",
                "[8]R. Middleton,  \u2018Play it again sam\u2019: some notes on the productivity  of repetition  in popular  music, Pop.",
                "[16]H. Chu, R. Urtasun,  S. Fidler, Song from pi: a musically  plausible  network  for pop music generation,  in: International  Conference  on Learning  Repre-\nsentations  (ICLR), 2017, workshop  track.",
                "[16]H. Chu, R. Urtasun,  S. Fidler, Song from pi: a musically  plausible  network  for pop music generation,  in: International  Conference  on Learning  Repre-\nsentations  (ICLR), 2017, workshop  track."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Graph  generation\nGraph  generation  has applications  in many domains,  such as discovering  new chemical  structures [ 29] and modeling  \nsocial interactions [ 30].",
                "[5]H. Zhu, Q. Liu, N.J. Yuan, C. Qin, J. Li, K. Zhang, G. Zhou, F. Wei, Y. Xu, E. Chen, Xiaoice  band: a melody  and arrangement  generation  framework  \nfor pop music, in: Proceedings  of the 24th ACM SIGKDD  International  Conference  on Knowledge  Discovery,  Data Mining,  New York, NY, USA, 2018, \npp.",
                "[22]S. Chiu, M. Shan, Computer  music composition  based on discovered  music patterns,  in: Proceedings  of the 2006 IEEE International  Conference  on \nSystems,  Man and Cybernetics,  IEEE, Taipei, 2006, pp.",
                "[24]H. Jhamtani,  T. Berg-Kirkpatrick,  Modeling  self-repetition  in music generation  using generative  adversarial  networks,  in: Machine  Learning  for Music \nDiscovery  Workshop,  ICML, 2019."
            ],
            "techno": [
                "Arti\ufb01cial Intelligence 286 (2020) 103303\nContents lists available at ScienceDirect\nArti\ufb01cial  Intelligence\nwww.elsevier.com/locate/artint\nPopMNet:  Generating  structured  pop  music  melodies  using  \nneural  networks\nJian Wua, Xiaoguang Liub, Xiaolin Hua,\u2217, Jun Zhua\naInstitute  for Arti\ufb01cial  Intelligence,  Beijing National  Research  Center for Information  Science and Technology  (BNRist),  the State Key Laboratory  \nof Intelligent  Technology  and Systems,  and Department  of Computer  Science and Technology,  Tsinghua  University,  Beijing 100084,  China\nbLingDongYin  Technoloy  Co., Ltd., Beijing, 100084,  China\na r t"
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Singability-enhanced lyric generator with music style transfer",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [
                "The second dataset is the audio of the original lyrics,\nand the DALI dataset [24] was chosen for this study, which is a large\ndataset of audio full sections synchronized with audio, lyrics and notes,\nwith lyrics and notes (of the vocal melody) aligned in time."
            ],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [
                "For\nexample, suppose the original lyrics have a \u2018\u2018pop\u2019\u2019 style and the goal\nis to generate lyrics with a \u2018\u2018rock\u2019\u2019 style.",
                "Then the rock lyrics dataset\nis trained for model migration and the lyric text is modified using a\npost-processing module to ensure that every line and word in the lyrics\nmatches the audio.",
                "Experiments were conducted on an English music\ndataset containing pop and rock music.",
                "This study collected lyrics from five genres: pop, country,\nrock, rap, and reggae; with the songs collected spanning different years.",
                "The different lyric styles\nuse different expressions and lyricism, e.g. pop lyrics tend to emphasize\nromantic love, whereas rock lyrics tend to emphasize social or political\naspects.",
                "The different styles often use different words, e.g. pop songs\nusually contain \u2018\u2018love\u2019\u2019, \u2018\u2018feel\u2019\u2019, \u2018\u2018live\u2019\u2019, \u2018\u2018heart\u2019\u2019, and \u2018\u2018tell\u2019\u2019; whereas rock\nsongs usually contain \u2018\u2018oh\u2019\u2019, \u2018\u2018never\u2019\u2019, \u2018\u2018burn\u2019\u2019, and \u2018\u2018rock\u2019\u2019.",
                "We had some\nlimitations song lyric dataset size, so we focused on pop and rock styles,\ncombining pop and rock lyric data to form a text file with over 8000\n40J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "For example, the lyrics could be constrained to be\nlike rock style, with the key sentence \u2018\u2018I\u2019ve seen your shadow in the dark\nI\u2019ve seen this struggle in your life\u2019\u2019 as a prefix.",
                "This study captured pop, rap, country, rock, and reggae lyric styles\nfrom the Genius website [22], which is non-parallel data, to overcome\nsparse data problems when generating different lyric styles.",
                "Thus, pop\nstyle and rock style models were generated by fine-tuning GPT-2.",
                "Suppose we have two lyric text datasets X = {x(1),x(2),\u2026,x(m)}\n(original lyrics) and Y = {y(1),y(2),\u2026,y(n)}(output lyrics after GPT-2\nprocessing), with styles SxandSy, respectively (e.g. Sxis pop style and\nSyis rock style).",
                "Suppose pop music style lyrics are transferred to rock music.",
                "Then\nX(structure template) is the provided pop music lyrics and Yis the\ntarget rock music lyrics.",
                "A few lines from the target lyrics are chosen\nas the key sentence Kso GPT-2 can include rock style elements in its\ngeneration ( G).",
                "Table 3-4 shows the lyrics change to \u2018\u2018You make river\nwas a black sea\u2019\u2019, which has same dependencies between words as in\nthe original lyrics and adds a rock element to the sentence.",
                "We chose a sample of 100 stylized\nlyrics texts generated by the proposed system for each transfer task\nfor users to cross-rate, with 50 each being pop to rock and rock to\npop conversion, respectively.",
                "Fig. 4-3 and Fig. 4-6 show stylized lyrics using\nGPT-2 from pop to rock and rock to pop, respectively.",
                "We set the key\nsentence as \u2018\u2018Bury every word I\u2019ve said in the city of the dead and drown\nthis masterpiece in red\u2019\u2019 (rock style) and conditionally transferred from\npop to rock style.",
                "The generated results are divided into two\nparts, one is Pop to Rock (Pop2Rock) and the other is Rock to Pop\n(Rock2Pop).",
                "A total of 100 songs in the experimental section, 50\nPop to Rock and 50 Rock to Pop, will be evaluated for each of\nthe three different approaches.",
                "Method Style transfer from rock\n(Pop2Rock)Style transfer from pop\n(Rock2Pop)\nAverage of 50 tracks\n(Mean \u00b1Std)Average of 50 tracks\n(Mean \u00b1Std)\nGPT-2 0.2893 \u00b10.0674 0.3039 \u00b10.0702\nGPT-2 + DP 0.6683 \u00b10.1351 0.6962 \u00b10.0996\nGPT-2 + DP + RM 0.5500 \u00b10.1137 0.5798 \u00b10.0774\nGPT-2 yields 0.2893 \u00b10.0674 in Pop2Rock and 0.3039 \u00b10.0702\nin Rock2Pop, which is the lowest overlap value among all methods,\nindicating that the GPT-2 model is capable of generating original song\ntexts.",
                "In Pop2Rock, GPT-2+DP has an overlap of 0.6683 \u00b10.1351, and\nin Rock2Pop, GPT-2+DP has an overlap of 0.6962 \u00b10.0996, which\nis even higher than GPT-2 alone because GPT-2+DP draws on the\nstructure of the original pop or rock lyrics, thus increasing the overlap\nscore, but the overlap is within acceptable limits and the content still\nretains the results produced by GPT-2.",
                "GPT-2+DP+RM was modified for each concluding\nphrase of the GPT-2+DP results, yielding 0.5500 \u00b10.1137 in Pop2Rock\nand 0.5798 \u00b10.0774 in Rock2Pop, with overlap scores even lower\nthan GPT-2+DP, indicating that some rhyming words can be effectively\nsubstituted.",
                "The result of the GPT-2 processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP+RM processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2 processing of the lyrics from Rock to Pop.",
                "The result of the GPT-2+DP processing of the lyrics from Rock to Pop.\nTable 4-3\nResults of human evaluation (Pop2Rock).",
                "Model Metrics Rater Mean \u00b1Std\n1 2 3\nGPT-2Thematic (T) 4.4 4.5 3.9 4.2667 \u00b10.2625\nStructural (S) 1.6 1.9 1.9 1.8000 \u00b10.1414\nOriginality (O) 4.5 4.0 4.1 4.2000 \u00b10.2160\nMeaningfulness (M) 3.1 3.0 3.8 3.3000 \u00b10.3559\nFitness (F) 1.4 1.9 1.8 1.7000 \u00b10.2160\nGPT-2+DPThematic (T) 3.5 4.0 4.0 3.8333 \u00b10.2357\nStructural (S) 3.9 4.1 3.8 3.9333 \u00b10.1247\nOriginality (O) 3.0 3.5 3.6 3.3667 \u00b10.2625\nMeaningfulness (M) 3.2 3.7 3.2 3.3667 \u00b10.2357\nFitness (F) 3.1 3.0 3.5 3.2000 \u00b10.2160\nGPT-2+DP+RMThematic (T) 3.8 3.6 3.9 3.7667 \u00b10.1247\nStructural (S) 4.3 4.4 3.7 4.1333 \u00b10.3091\nOriginality (O) 3.5 3.5 3.2 3.4000 \u00b10.1414\nMeaningfulness (M) 2.9 3.4 3.4 3.2333 \u00b10.2357\nFitness (F) 4.7 3.8 3.8 4.1000 \u00b10.4243Table 4-4\nResults of human evaluation (Rock2Pop).",
                "The result of the GPT-2+DP+RM processing of the lyrics from Rock to Pop.\nFig. 4-9. Results of human evaluation (Pop2Rock).",
                "Results of human evaluation (Rock2Pop).",
                "[13] K. Carlson, A. Riddell, D. Rockmore, Evaluating prose style transfer with the\nBible, R. Soc.",
                "[18] X. Gao, Y. Zhang, S. Lee, M. Galley, C. Brockett, J. Gao, B. Dolan, Structuring\nlatent spaces for stylized response generation, in: Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing (EMNLP) and\nthe 9th International Joint Conference on Natural Language Processing, IJCNLP,\n2019, pp. 1814\u20131823."
            ],
            "pop": [
                "For\nexample, suppose the original lyrics have a \u2018\u2018pop\u2019\u2019 style and the goal\nis to generate lyrics with a \u2018\u2018rock\u2019\u2019 style.",
                "For\nexample, suppose the original lyrics have a \u2018\u2018pop\u2019\u2019 style and the goal\nis to generate lyrics with a \u2018\u2018rock\u2019\u2019 style.",
                "Experiments were conducted on an English music\ndataset containing pop and rock music.",
                "Experiments were conducted on an English music\ndataset containing pop and rock music.",
                "This study collected lyrics from five genres: pop, country,\nrock, rap, and reggae; with the songs collected spanning different years.",
                "This study collected lyrics from five genres: pop, country,\nrock, rap, and reggae; with the songs collected spanning different years.",
                "For example,\nsuppose pop song lyrics were used as training data, then and all the data\nis combined into a single text, splitting each lyric with < |endoftext |>.",
                "For example,\nsuppose pop song lyrics were used as training data, then and all the data\nis combined into a single text, splitting each lyric with < |endoftext |>.",
                "The different lyric styles\nuse different expressions and lyricism, e.g. pop lyrics tend to emphasize\nromantic love, whereas rock lyrics tend to emphasize social or political\naspects.",
                "The different lyric styles\nuse different expressions and lyricism, e.g. pop lyrics tend to emphasize\nromantic love, whereas rock lyrics tend to emphasize social or political\naspects.",
                "The different styles often use different words, e.g. pop songs\nusually contain \u2018\u2018love\u2019\u2019, \u2018\u2018feel\u2019\u2019, \u2018\u2018live\u2019\u2019, \u2018\u2018heart\u2019\u2019, and \u2018\u2018tell\u2019\u2019; whereas rock\nsongs usually contain \u2018\u2018oh\u2019\u2019, \u2018\u2018never\u2019\u2019, \u2018\u2018burn\u2019\u2019, and \u2018\u2018rock\u2019\u2019.",
                "The different styles often use different words, e.g. pop songs\nusually contain \u2018\u2018love\u2019\u2019, \u2018\u2018feel\u2019\u2019, \u2018\u2018live\u2019\u2019, \u2018\u2018heart\u2019\u2019, and \u2018\u2018tell\u2019\u2019; whereas rock\nsongs usually contain \u2018\u2018oh\u2019\u2019, \u2018\u2018never\u2019\u2019, \u2018\u2018burn\u2019\u2019, and \u2018\u2018rock\u2019\u2019.",
                "We had some\nlimitations song lyric dataset size, so we focused on pop and rock styles,\ncombining pop and rock lyric data to form a text file with over 8000\n40J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "We had some\nlimitations song lyric dataset size, so we focused on pop and rock styles,\ncombining pop and rock lyric data to form a text file with over 8000\n40J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "This study captured pop, rap, country, rock, and reggae lyric styles\nfrom the Genius website [22], which is non-parallel data, to overcome\nsparse data problems when generating different lyric styles.",
                "This study captured pop, rap, country, rock, and reggae lyric styles\nfrom the Genius website [22], which is non-parallel data, to overcome\nsparse data problems when generating different lyric styles.",
                "The top\n60 songs were then captured by sorting them according to artist\u2019s song\npopularity.",
                "The top\n60 songs were then captured by sorting them according to artist\u2019s song\npopularity.",
                "Thus, pop\nstyle and rock style models were generated by fine-tuning GPT-2.",
                "Thus, pop\nstyle and rock style models were generated by fine-tuning GPT-2.",
                "Suppose we have two lyric text datasets X = {x(1),x(2),\u2026,x(m)}\n(original lyrics) and Y = {y(1),y(2),\u2026,y(n)}(output lyrics after GPT-2\nprocessing), with styles SxandSy, respectively (e.g. Sxis pop style and\nSyis rock style).",
                "Suppose we have two lyric text datasets X = {x(1),x(2),\u2026,x(m)}\n(original lyrics) and Y = {y(1),y(2),\u2026,y(n)}(output lyrics after GPT-2\nprocessing), with styles SxandSy, respectively (e.g. Sxis pop style and\nSyis rock style).",
                "Suppose pop music style lyrics are transferred to rock music.",
                "Suppose pop music style lyrics are transferred to rock music.",
                "Then\nX(structure template) is the provided pop music lyrics and Yis the\ntarget rock music lyrics.",
                "Then\nX(structure template) is the provided pop music lyrics and Yis the\ntarget rock music lyrics.",
                "We chose a sample of 100 stylized\nlyrics texts generated by the proposed system for each transfer task\nfor users to cross-rate, with 50 each being pop to rock and rock to\npop conversion, respectively.",
                "We chose a sample of 100 stylized\nlyrics texts generated by the proposed system for each transfer task\nfor users to cross-rate, with 50 each being pop to rock and rock to\npop conversion, respectively.",
                "Fig. 4-3 and Fig. 4-6 show stylized lyrics using\nGPT-2 from pop to rock and rock to pop, respectively.",
                "Fig. 4-3 and Fig. 4-6 show stylized lyrics using\nGPT-2 from pop to rock and rock to pop, respectively.",
                "We set the key\nsentence as \u2018\u2018Bury every word I\u2019ve said in the city of the dead and drown\nthis masterpiece in red\u2019\u2019 (rock style) and conditionally transferred from\npop to rock style.",
                "We set the key\nsentence as \u2018\u2018Bury every word I\u2019ve said in the city of the dead and drown\nthis masterpiece in red\u2019\u2019 (rock style) and conditionally transferred from\npop to rock style.",
                "The generated results are divided into two\nparts, one is Pop to Rock (Pop2Rock) and the other is Rock to Pop\n(Rock2Pop).",
                "The generated results are divided into two\nparts, one is Pop to Rock (Pop2Rock) and the other is Rock to Pop\n(Rock2Pop).",
                "A total of 100 songs in the experimental section, 50\nPop to Rock and 50 Rock to Pop, will be evaluated for each of\nthe three different approaches.",
                "A total of 100 songs in the experimental section, 50\nPop to Rock and 50 Rock to Pop, will be evaluated for each of\nthe three different approaches.",
                "Method Style transfer from rock\n(Pop2Rock)Style transfer from pop\n(Rock2Pop)\nAverage of 50 tracks\n(Mean \u00b1Std)Average of 50 tracks\n(Mean \u00b1Std)\nGPT-2 0.2893 \u00b10.0674 0.3039 \u00b10.0702\nGPT-2 + DP 0.6683 \u00b10.1351 0.6962 \u00b10.0996\nGPT-2 + DP + RM 0.5500 \u00b10.1137 0.5798 \u00b10.0774\nGPT-2 yields 0.2893 \u00b10.0674 in Pop2Rock and 0.3039 \u00b10.0702\nin Rock2Pop, which is the lowest overlap value among all methods,\nindicating that the GPT-2 model is capable of generating original song\ntexts.",
                "Method Style transfer from rock\n(Pop2Rock)Style transfer from pop\n(Rock2Pop)\nAverage of 50 tracks\n(Mean \u00b1Std)Average of 50 tracks\n(Mean \u00b1Std)\nGPT-2 0.2893 \u00b10.0674 0.3039 \u00b10.0702\nGPT-2 + DP 0.6683 \u00b10.1351 0.6962 \u00b10.0996\nGPT-2 + DP + RM 0.5500 \u00b10.1137 0.5798 \u00b10.0774\nGPT-2 yields 0.2893 \u00b10.0674 in Pop2Rock and 0.3039 \u00b10.0702\nin Rock2Pop, which is the lowest overlap value among all methods,\nindicating that the GPT-2 model is capable of generating original song\ntexts.",
                "In Pop2Rock, GPT-2+DP has an overlap of 0.6683 \u00b10.1351, and\nin Rock2Pop, GPT-2+DP has an overlap of 0.6962 \u00b10.0996, which\nis even higher than GPT-2 alone because GPT-2+DP draws on the\nstructure of the original pop or rock lyrics, thus increasing the overlap\nscore, but the overlap is within acceptable limits and the content still\nretains the results produced by GPT-2.",
                "In Pop2Rock, GPT-2+DP has an overlap of 0.6683 \u00b10.1351, and\nin Rock2Pop, GPT-2+DP has an overlap of 0.6962 \u00b10.0996, which\nis even higher than GPT-2 alone because GPT-2+DP draws on the\nstructure of the original pop or rock lyrics, thus increasing the overlap\nscore, but the overlap is within acceptable limits and the content still\nretains the results produced by GPT-2.",
                "GPT-2+DP+RM was modified for each concluding\nphrase of the GPT-2+DP results, yielding 0.5500 \u00b10.1137 in Pop2Rock\nand 0.5798 \u00b10.0774 in Rock2Pop, with overlap scores even lower\nthan GPT-2+DP, indicating that some rhyming words can be effectively\nsubstituted.",
                "GPT-2+DP+RM was modified for each concluding\nphrase of the GPT-2+DP results, yielding 0.5500 \u00b10.1137 in Pop2Rock\nand 0.5798 \u00b10.0774 in Rock2Pop, with overlap scores even lower\nthan GPT-2+DP, indicating that some rhyming words can be effectively\nsubstituted.",
                "The result of the GPT-2 processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2 processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP+RM processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP+RM processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2 processing of the lyrics from Rock to Pop.",
                "The result of the GPT-2 processing of the lyrics from Rock to Pop.",
                "The result of the GPT-2+DP processing of the lyrics from Rock to Pop.\nTable 4-3\nResults of human evaluation (Pop2Rock).",
                "The result of the GPT-2+DP processing of the lyrics from Rock to Pop.\nTable 4-3\nResults of human evaluation (Pop2Rock).",
                "Model Metrics Rater Mean \u00b1Std\n1 2 3\nGPT-2Thematic (T) 4.4 4.5 3.9 4.2667 \u00b10.2625\nStructural (S) 1.6 1.9 1.9 1.8000 \u00b10.1414\nOriginality (O) 4.5 4.0 4.1 4.2000 \u00b10.2160\nMeaningfulness (M) 3.1 3.0 3.8 3.3000 \u00b10.3559\nFitness (F) 1.4 1.9 1.8 1.7000 \u00b10.2160\nGPT-2+DPThematic (T) 3.5 4.0 4.0 3.8333 \u00b10.2357\nStructural (S) 3.9 4.1 3.8 3.9333 \u00b10.1247\nOriginality (O) 3.0 3.5 3.6 3.3667 \u00b10.2625\nMeaningfulness (M) 3.2 3.7 3.2 3.3667 \u00b10.2357\nFitness (F) 3.1 3.0 3.5 3.2000 \u00b10.2160\nGPT-2+DP+RMThematic (T) 3.8 3.6 3.9 3.7667 \u00b10.1247\nStructural (S) 4.3 4.4 3.7 4.1333 \u00b10.3091\nOriginality (O) 3.5 3.5 3.2 3.4000 \u00b10.1414\nMeaningfulness (M) 2.9 3.4 3.4 3.2333 \u00b10.2357\nFitness (F) 4.7 3.8 3.8 4.1000 \u00b10.4243Table 4-4\nResults of human evaluation (Rock2Pop).",
                "Model Metrics Rater Mean \u00b1Std\n1 2 3\nGPT-2Thematic (T) 4.4 4.5 3.9 4.2667 \u00b10.2625\nStructural (S) 1.6 1.9 1.9 1.8000 \u00b10.1414\nOriginality (O) 4.5 4.0 4.1 4.2000 \u00b10.2160\nMeaningfulness (M) 3.1 3.0 3.8 3.3000 \u00b10.3559\nFitness (F) 1.4 1.9 1.8 1.7000 \u00b10.2160\nGPT-2+DPThematic (T) 3.5 4.0 4.0 3.8333 \u00b10.2357\nStructural (S) 3.9 4.1 3.8 3.9333 \u00b10.1247\nOriginality (O) 3.0 3.5 3.6 3.3667 \u00b10.2625\nMeaningfulness (M) 3.2 3.7 3.2 3.3667 \u00b10.2357\nFitness (F) 3.1 3.0 3.5 3.2000 \u00b10.2160\nGPT-2+DP+RMThematic (T) 3.8 3.6 3.9 3.7667 \u00b10.1247\nStructural (S) 4.3 4.4 3.7 4.1333 \u00b10.3091\nOriginality (O) 3.5 3.5 3.2 3.4000 \u00b10.1414\nMeaningfulness (M) 2.9 3.4 3.4 3.2333 \u00b10.2357\nFitness (F) 4.7 3.8 3.8 4.1000 \u00b10.4243Table 4-4\nResults of human evaluation (Rock2Pop).",
                "The result of the GPT-2+DP+RM processing of the lyrics from Rock to Pop.\nFig. 4-9. Results of human evaluation (Pop2Rock).",
                "The result of the GPT-2+DP+RM processing of the lyrics from Rock to Pop.\nFig. 4-9. Results of human evaluation (Pop2Rock).",
                "Results of human evaluation (Rock2Pop).",
                "Results of human evaluation (Rock2Pop)."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [
                "This study collected lyrics from five genres: pop, country,\nrock, rap, and reggae; with the songs collected spanning different years.",
                "This study captured pop, rap, country, rock, and reggae lyric styles\nfrom the Genius website [22], which is non-parallel data, to overcome\nsparse data problems when generating different lyric styles."
            ],
            "country": [
                "This study collected lyrics from five genres: pop, country,\nrock, rap, and reggae; with the songs collected spanning different years.",
                "This study captured pop, rap, country, rock, and reggae lyric styles\nfrom the Genius website [22], which is non-parallel data, to overcome\nsparse data problems when generating different lyric styles."
            ],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "Computer Communications 168 (2021) 33\u201353\nContents lists available at ScienceDirect\nComputer Communications\njournal homepage: www.elsevier.com/locate/comcom\nSingability-enhanced lyric generator with music style transfer\nJia-Wei Changa, Jason C. Hunga,\u2217, Kuan-Cheng Linb\naNational Taichung University of Science and Technology, Taichung City, Taiwan\nbNational Chung Hsing University, Taichung City, Taiwan\nA R T I C L E I N F O\nKeywords:\nMusic style transfer\nLyric generator\nGPT-2\nNatural language processingA B S T R A C T",
                "52J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\n[16] H. Gong, S. Bhat, L. Wu, J. Xiong, W.M. Hwu, Reinforcement learning based text\nstyle transfer without parallel training corpus, in: Proceedings of the 17th Annual\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HLT, 2019, pp.",
                "[27] K. Watanabe, Y. Matsubayashi, S. Fukayama, M. Goto, K. Inui, T. Nakano,\nA melody-conditioned lyrics language model, in: Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol. 1, 2018, pp."
            ],
            "dubstep": [],
            "opera": [
                "Transformer can\nhandle time series and parallel operations, which greatly improvedTable 2-1\nThe four GPT-2 model sizes.",
                "Each location has a position encoding,\nwith the position encoding and token embeddings treated as a new\nembedding to perform the original self-attention operation, as shown\nin Fig.",
                "Each row of the embedding\nmatrix corresponds to the embedding vector for a word in the model\u2019s\nvocabulary, hence the result obtained by multiplication operation is the\ncorresponding score for each word in the vocabulary."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Human, I wrote a song for you: An experiment testing the influence of machines\u2019 attributes on the AI-composed music evaluation",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "Apart from GANs and Transformer, a hierarchical recurrent \nneural network (HRNN), which Deep Bach uses, is based on \npseudo-Gibbs sampling in order to produce notes in the style of Bach \nchorales, providing more techniques to create music (Hadjeres et al., \n2017 ; Wu et al., 2020 ).",
                "Deepbach: A steerable model for \nbach chorales generation."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [
                "Procedures \nFour AI-composed music pieces with different genres (rock, EDM, \nclassical, and country) were used for this study."
            ],
            "pop": [
                "Generative Adversarial Networks \n(GANs) have been a popular structure type for developing creative machines.",
                "Generative Adversarial Networks \n(GANs) have been a popular structure type for developing creative machines."
            ],
            "classical": [
                "Procedures \nFour AI-composed music pieces with different genres (rock, EDM, \nclassical, and country) were used for this study."
            ],
            "electronic": [
                "In this case, people will see this machine less as a musician but rather as \na musical instrument, such as software that electronic dance music \n(EDM) musicians use.",
                "Journal of Broadcasting & Electronic Media, 64(4), 566\u2013591. https://doi. \norg/10.1080/08838151.2020.1835136 \nYang, L., Chou, S., & Yang, Y. (2017)."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [
                "Procedures \nFour AI-composed music pieces with different genres (rock, EDM, \nclassical, and country) were used for this study."
            ],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "As a result, the concept of \nAI-generated art has emerged as an alternative art form (Smith & Leymarie, 2017 )."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "It may suggest that \nthere was a cognitive point where the evaluation of music was discon -\nnected from the perception of its creator.",
                "Do a robot \u2019s social skills and its objection discourage interactants from \nswitching the robot off?"
            ],
            "techno": [
                "On the other hand, the rapid development of Artificial Intelligence \n(AI) technologies is expected to transform the traditional role and \nperspective of machines within the art community.",
                "As noted above, autonomy is the most \ncrucial factor differentiating current technology from the traditional \nusage of machines while composing music (Fern\u02d8andez & Vico, 2013 ).",
                "Since then, there have been \nmore attempts to create music-generating machines as AI technology \nhas developed (Sturm et al., 2019 ).",
                "Based on current AI technology, machines cannot \ncreate music if there is no human involvement at all.",
                "Philosophy & Technology, 30(3), \n285\u2013303.",
                "Technological Forecasting and Social Change, 114, 254\u2013280.",
                "INSAM Journal of Contemporary \nMusic, Art and Technology, 1(2), 100\u2013114."
            ],
            "dubstep": [],
            "opera": [
                "I would feel nervous operating an autonomous machine in front of other people.",
                "A conceptual and \noperational definition of\u2019social role\u2019in online community."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Rethinking musicality in dementia as embodied and relational",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "Research on the impact of music programs is dominated by studies\nthat evaluate music as a therapeutic tool to achieve instrumental out-\ncomes ( DeNora & Ansdell, 2014).",
                "Lack of engagement with this sub-\ufb01eld has not only impoverished understandings of musicality, but has\nalso restricted music in dementia care to its application as a therapeutictool to achieve instrumental outcomes such as improving cognitive\nfunctioning.",
                "Further, given that our analysis demonstrates\nthat musicality is embodied and persists despite even severe cognitive\nimpairment, it is egregious that music is restricted in dementia care to\nits instrumental application as a therapeutic tool to improve \u2018beha-\nviours \u2019and cognitive functioning."
            ],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "It is this fuzzy and vague but no less masterful capacity torelate to the world that makes it possible for them to express themselves\nin such creative and complex music traditions as East European folk\nmusic and 1950s musical theatre respectively."
            ],
            "blues": [],
            "jazz": [],
            "rock": [
                "Social Science\nand Medicine, 58 (2), 369 \u2013378.\nDavis, J. I., Benforado, A., Esrock, E., Turner, A., Dalton, R. C., van Noorden, L., & Leman,\nM. (2012).",
                "In L. C. Hyd\u00e9n, J.\nBrockmeier, & H. Lindemann (Eds.)."
            ],
            "pop": [
                "Further, musicality in the context of dementia abounds in popular\nculture and empirical discourse ( Cuddy & Du \ufb03n, 2005; Oppenheimer,\n2005 ;Pickles & Jones, 2006).",
                "Further, musicality in the context of dementia abounds in popular\nculture and empirical discourse ( Cuddy & Du \ufb03n, 2005; Oppenheimer,\n2005 ;Pickles & Jones, 2006).",
                "These were\nparticularly popular with the residents.",
                "These were\nparticularly popular with the residents.",
                "Abe would frequently sing a popular Yiddish song repeating the one\nverse he knows over and over.",
                "Abe would frequently sing a popular Yiddish song repeating the one\nverse he knows over and over."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [
                "Such relationships are rarely\nexplicitly discussed in dementia care, despite the fact that the \u201crules,\nlaws and policies of the country or jurisdiction in which a person lives\u201d\n(Bartlett & O'Connor, 2010, p. 30) will inevitably in \ufb02uence the ex-\nperiences and opportunities associated with the health and social care aperson with dementia receives ( Kontos, Grigorovich, et al., 2016;\nKontos, Miller, & Kontos, 2017 ;Miller & Kontos, 2016; Reid, Ryan, &\nEnderby, 2001 )."
            ],
            "r&b": [],
            "heavy metal": [],
            "punk": [
                "and Bennett (2015) in drawing on empirical data gen-\nerated through research on the punk scene in Southeast Queensland,Australia, retheorize the concept of \u2018music scene \u2019to highlight the cri-\ntically important role of embodiment for how music scenes are con-structed, enacted, and maintained.",
                "Crossley\n(2015) takes up this theme in his exploration of the links between body\ntechniques and music scenes, or what he refers to as \u2018music worlds\u2019 ,\nfocusing on early UK punk in London."
            ],
            "alternative": [
                "In response to critique regarding the overreliance on pharma-\ncotherapies, non-pharmacological approaches are now recommendedas an alternative to psychotropic medication ( Fossey et al., 2006;\nMoniz-Cook, Woods, & Richards, 2001).",
                "This model has already\nbeen applied to explicate an ethic of sexuality that o \ufb00ers an important\nalternative to the positivist legacy of bioethical principles in the \ufb01eld of\ndementia ( Grigorovich & Kontos, 2016 ;Kontos, Grigorovich, et al.,\n2016 )."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "We argue that understanding and fully supporting the musicality of persons living withdementia requires engagement with citizenship discourse.",
                "Relational citizenship brings a new and critical di-\nmension to the discourse on music, ageing, and the body in contemporary society.",
                "To better understand and to more fully support musicality (i.e.\nmusical perception and engagement) of persons living with dementia\nrequires engagement with citizenship discourse.",
                "We argue that relational citizenship brings a new and\ncritical dimension to the discourse on music, ageing, and the body incontemporary society.",
                "This advances a notion of selfhood that\nconsiders both the pre-re \ufb02ective intentionality of the body and its\nnatural (pre-social) engagement with the world (the body's power of\nnatural expression), and the ongoing socio-cultural relationship be-\ntween the pre-re \ufb02ective body and the world (history, culture, power,\nand discourse).",
                "The application of the model to understanding and supporting\nmusicality in the context of dementia is novel; it stands to advance thediscourse on musicality by bringing a new and critical dimension to\nunderstanding self-expression, interdependence, and reciprocal en-\ngagement, all of which are fundamental to musical perception and\nengagement.",
                "Further, musicality in the context of dementia abounds in popular\nculture and empirical discourse ( Cuddy & Du \ufb03n, 2005; Oppenheimer,\n2005 ;Pickles & Jones, 2006)."
            ],
            "techno": [
                "Early diagnostics and Alzheimer's disease: Beyond\n\u2018cure \u2019and \u2018care \u2019.Technological"
            ],
            "dubstep": [],
            "opera": [
                "There are\nsome innovative musical programs o \ufb00ered to persons living with de-\nmentia in community settings such as the BUDI Symphony project(Bournemouth University Dementia Institute, 2015 ), the Scottish Opera\nproject ( Reynolds, Innes, Poyner, & Hambidge, 2016 ), and the Bitove\nWellness Academy ( Bitove Wellness Academy, 2016 ).",
                "Funding statement\nThis work was supported by a Canadian Institutes of Health\nResearch Operating Grant (MOP \u2013114953) and the Alzheimer Society of\nCanada and the Institute of Aging (Canadian Institutes of Health\nResearch) (Award #03-07)."
            ],
            "musical theatre": [
                "Speci \ufb01cally, they always sought to\nsupport her musicality by o \ufb00ering songs and melodies from the style of\nmusical theatre from the 1950s.",
                "Similarly, Betty's performance was consistent with\nher experience with improvisation and composition of musical theatrefrom the 1950s.",
                "It is this fuzzy and vague but no less masterful capacity torelate to the world that makes it possible for them to express themselves\nin such creative and complex music traditions as East European folk\nmusic and 1950s musical theatre respectively."
            ],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "deepsing: Generating sentiment-aware visual stories using cross-modal music translation",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2."
            ],
            "jazz": [
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2.",
                "Another\nquite interesting observation is that the effectiveness of the proposed\nmethod varies for different music genres, e.g., classical music leads to\nthe best results, followed by jazz.",
                "ImproViz: visual explorations of jazz\nimprovisations."
            ],
            "rock": [
                "For example, Generative Adversarial Networks (GANs)\n(Brock, Donahue, & Simonyan , 2018 ; Goodfellow, Pouget-Abadie, Mir-\nza, Xu, Warde-Farley, Ozair, et al. , 2014 ; Karras, Aila, Laine, & Lehti-\nnen, 2017 ) are capable of synthesizing highly realistic visual content\nthat has not been encountered during the training process, neural style\ntransfer methods can re-paint images to match the style of reference\nimages ( Luan, Paris, Shechtman, & Bala , 2017 ), or even to follow\nthe style of well-known artists ( Gatys, Ecker, & Bethge , 2015 ; Wang,\nOxholm, Zhang, & Wang , 2017 ), while deep dreaming methods have\ndemonstrated that neural networks can exhibit a behavior known as\npareidolia in humans, i.e., recognize and synthesize patterns on seem-\ningly random data ( Mordvintsev, Olah, & Tyka , 2015 ).",
                "(4)\nIt is worth noting that for the case of GANs, sampling the generator\nspace is easy, since GANs are typical trained to generate images from a\nGaussian distribution (Brock et al., 2018).",
                "However, in practice we observed that it was very difficult to fit\nsuch translation models, especially if GANs with very complex genera-\ntor spaces are used, e.g., GANs capable generating 1000 classes ( Brock\net al. , 2018 ).",
                "A pretrained BigGAN ( Brock et al. , 2018 ) model was used for\ngenerating images of 512 \u00d7512 pixels.",
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2.",
                "On the other hand metal, disco and\nrock consistently led to the lowest precision compared to the rest of the\nevaluated methods.",
                "Brock, Andrew, Donahue, Jeff, & Simonyan, Karen (2018)."
            ],
            "pop": [
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2.",
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2.",
                "Uehara, Misa, & Itoh, Takayuki Pop music visualization based on acoustic features and\nchord progression patterns applying dual scatterplots.",
                "Uehara, Misa, & Itoh, Takayuki Pop music visualization based on acoustic features and\nchord progression patterns applying dual scatterplots."
            ],
            "classical": [
                "More recent works also\nprovided tools for better understanding the harmonic structures in mu-\nsic (Malandrino et al. , 2015 ), the semantic structure in classical music\nworks ( Chan, Qu, & Mak , 2009 ) or even improving the understanding\nof music compositions ( De Prisco, Malandrino, Pirozzi, Zaccagnino, &\nZaccagnino , 2017 ), allowing for assisting the learning process ( Malan-\ndrino, Pirozzi, & Zaccagnino , 2019 ).",
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2.",
                "Another\nquite interesting observation is that the effectiveness of the proposed\nmethod varies for different music genres, e.g., classical music leads to\nthe best results, followed by jazz.",
                "Visualizing the semantic structure\nin classical music works.",
                "Music in India: The classical traditions ."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2."
            ],
            "country": [
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2.",
                "The generated images match this sentiment,\nsince they are quite neutral and with low arousal (a car mirror and\ncastle in the country side)."
            ],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2.",
                "On the other hand metal, disco and\nrock consistently led to the lowest precision compared to the rest of the\nevaluated methods.",
                "Examining the ability of the translator model to discover sentiment-rich regions of the generator space, along with the effect of the proposed hyper-stylization approach.\nwas set to 0.5: an image with an average sentiment (valence and\narousal) lower than \u22120.5 was considered as negative, while an average\nsentiment higher than 0.5 was considered positive."
            ],
            "techno": [
                "The advent\nof digital technology provided further opportunities toward integrating\n\u2217Corresponding author.\nE-mail addresses: passalis@csd.auth.gr (N. Passalis), sdoropoulos@gmail.com",
                "In Proceedings of the ACM symposium on virtual\nreality software and technology."
            ],
            "dubstep": [],
            "opera": [
                "We also introduce an attribute extractor for the visual\ndomain\ud835\udc54(\ud835\udc32) \u2208R\ud835\udc41\ud835\udc4e, where \ud835\udc32\u2208R\ud835\udc4a\u00d7\ud835\udc3b\u00d7\ud835\udc36is a\ud835\udc36-channel image of dimen-\nsions\ud835\udc4a\u00d7\ud835\udc3b. The visual attribute estimator extracts the same attributes as\nthe audio attribute estimator, but operates on images instead of audio.",
                "These two attribute estimators should aligned , i.e., extract the\nsame attributes and operate on the same output space."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "ComposeInStyle: Music composition with and without Style Transfer",
            "homophonic": [],
            "polyphonic": [
                "In the paper (Johnson, 2017), a set of\nparallel, tied weight Recurrent Neural Network (RNN) is used to predict\nand generate polyphonic music compositions which is transposition\ninvariant.",
                "Description of GAN architectures \ud835\udc3a\ud835\udc34,\ud835\udc3a\ud835\udc35,\ud835\udc37\ud835\udc34and\ud835\udc37\ud835\udc35\nIn step 2, paired vanilla GAN has been trained to generate multi-\ntrack polyphonic music.",
                "Since a single time step can have\nmultiple notes being played simultaneously, we can thus represent the\npolyphonic music as a matrix representation without information loss.",
                "Convolutional generative adversarial networks with\nbinary neurons for polyphonic music generation.",
                "Generating polyphonic music using tied parallel networks."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "In the field of genre rearrangement, (Hung, Chiang, Chen,\nYang, et al., 2019) has made an effort for arbitrary genre rearrangement\nusing instrumental real-world music.",
                "MIDI data can only\ncapture the instrumental information of musical data."
            ],
            "vocal": [
                "Some of the future directions in this aspect can be to\nintegrate vocals with melody in compositional style transfer, replacing\nthe static components in music generation with a layer of deep neural\nnetwork and training the classifiers with more data and observing\nthe performance."
            ],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "The Luo et al. (2020) explored the field of Chinese folk song\ngeneration by capturing specific music styles using VAE.",
                "The network has been trained on British and\nAmerican folk songs.",
                "Mg-VAE: Deep Chinese folk songs generation\nwith specific regional styles."
            ],
            "blues": [
                "The same composer\ncan compose music in all styles like jazz, pop, blues, etc. dependingon the requirement."
            ],
            "jazz": [
                "The machine learning models which focus on clas-\nsifying music based on genre like classical, jazz, pop, rock and others,\ndo not focus on the composer specific styles.",
                "The same composer\ncan compose music in all styles like jazz, pop, blues, etc. dependingon the requirement.",
                "The authors\nconvert MIDI music from one genre to another (say jazz to pop) which\nis evaluated using neural style classifiers."
            ],
            "rock": [
                "The machine learning models which focus on clas-\nsifying music based on genre like classical, jazz, pop, rock and others,\ndo not focus on the composer specific styles.",
                "A multimodal approach to song-level style identification in\npop/rock using similarity metrics.",
                "Classification of rockburst in underground projects:\nComparison of ten supervised learning methods.",
                "Predicting TBM penetration rate in hard rock condition: A comparative study\namong six XGB-based metaheuristic techniques."
            ],
            "pop": [
                "The machine learning models which focus on clas-\nsifying music based on genre like classical, jazz, pop, rock and others,\ndo not focus on the composer specific styles.",
                "The machine learning models which focus on clas-\nsifying music based on genre like classical, jazz, pop, rock and others,\ndo not focus on the composer specific styles.",
                "The same composer\ncan compose music in all styles like jazz, pop, blues, etc. dependingon the requirement.",
                "The same composer\ncan compose music in all styles like jazz, pop, blues, etc. dependingon the requirement.",
                "It shows that it works better\nthan Hard Thresholding (HT) and Bernoulli Sampling (BS) methods\npopularly used.",
                "It shows that it works better\nthan Hard Thresholding (HT) and Bernoulli Sampling (BS) methods\npopularly used.",
                "Nakamura, Shibata, Nishikimi, and Yoshii (2019)\nperforms genre based style conversion (say classical to pop) using\nunsupervised models.",
                "Nakamura, Shibata, Nishikimi, and Yoshii (2019)\nperforms genre based style conversion (say classical to pop) using\nunsupervised models.",
                "The authors\nconvert MIDI music from one genre to another (say jazz to pop) which\nis evaluated using neural style classifiers.",
                "The authors\nconvert MIDI music from one genre to another (say jazz to pop) which\nis evaluated using neural style classifiers.",
                "Wang and Tzanetakis (2018)\nperforms a singing style investigation on pop music using variants of\nneural network.",
                "Wang and Tzanetakis (2018)\nperforms a singing style investigation on pop music using variants of\nneural network.",
                "Popular 4/4 time signature signifies 1 bar, contains 4 beats and\neach beat can contain 1/4 note, 2/8 note or 4/16 note (Brunner et al.,\n2018b).",
                "Popular 4/4 time signature signifies 1 bar, contains 4 beats and\neach beat can contain 1/4 note, 2/8 note or 4/16 note (Brunner et al.,\n2018b).",
                "Audio thumbnailing of popular music using\nchroma-based representations.",
                "Audio thumbnailing of popular music using\nchroma-based representations.",
                "A multimodal approach to song-level style identification in\npop/rock using similarity metrics.",
                "A multimodal approach to song-level style identification in\npop/rock using similarity metrics."
            ],
            "classical": [
                "The machine learning models which focus on clas-\nsifying music based on genre like classical, jazz, pop, rock and others,\ndo not focus on the composer specific styles.",
                "(Mogren,\n2016) proposes a Convolutional-RNN-GAN (C-RNN-GAN) to generate\nclassical music trained n sequential data.",
                "Nakamura, Shibata, Nishikimi, and Yoshii (2019)\nperforms genre based style conversion (say classical to pop) using\nunsupervised models."
            ],
            "electronic": [
                "Electronics ,9(3),\n424."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [
                "Our human brain is wired in such a way that we tend\nto listen more to the musical compositions of the specific composers\nwho touches our soul."
            ],
            "funk": [],
            "disco": [
                "The\nfirst model uses Gated Recurrent Unit (GRU) and Long Short Term\nMemory (LSTM) to discover the mappings from the audio features and\nthe corresponding text features (captions)."
            ],
            "techno": [
                "Contents lists available at ScienceDirect\nExpert Systems With Applications\njournal homepage: www.elsevier.com/locate/eswa\nComposeInStyle: Music composition with and without Style Transfer\nSreetama Mukherjeea, Manjunath Mulimanib,\u2217\naMicrosoft R&D Pvt. Ltd., Hyderabad, 500 033, India\nbDepartment of Computer Science and Engineering, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576 104, India\nA R T I C L E I N F O\nKeywords:\nMusic composer classification\nStyle transfer\nGenerative Adversarial Networks (GAN)\nHybrid model\nMusical Instrument Digital Interface (MIDI)",
                "Introduction\nIn today\u2019s world of technological advancement, machines are the\nmost trusted assistants to humans.",
                "Starting with analytics to generative networks ( Good-\nfellow , 2016 ), machine learning has touched almost every domain be\nit information technology sector, medical sector or even artistic sectors\nlike painting, music.",
                "Discussion\nTechnological advancement has always been beneficial in determin-\ning the advancement in everything that we do now in a more precise\nand useful way.",
                "Conclusions\nSince time immemorial, technological boons have touched the lives\nof human beings in unprecedented ways.",
                "In International conference on sound and music technology\n(CSMT) (pp. 93\u2013106).",
                "Journal of Computer Science and Technology ,16(6), 582\u2013589."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "The algorithmic composition for music copyright protection under deep learning and blockchain",
            "homophonic": [],
            "polyphonic": [
                "[19] Q.Wang,R.Zhou,Y.Yan,Polyphonicpianotranscriptionwithanote-based\nmusiclanguagemodel,Appl.Sci.8(3)(2018)470\u2013484."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [
                "Inanar-\nrowsense,algorithms,artificialintelligence,neuralnetworks,and\nothertechnicalsolutionsareadopted,sothatthecomputershave\ntheskillstoanalyzeandcreatemusic[2].Duetothediversityand\ncomplexityofmusic,itisdifficulttoseparatetheaccompaniment\nandthevocalswithacomputerformusicalstructuressuchas\nsongsthatcombinemultipleinstrumentsandvocalsintheearly\nstage of research.",
                "Due to the breadth of vocal frequencies, it\nis difficult to match based on the frequency corresponding to\nthenoteslikeamusicalscore[3].Incomputermusic,mostof\nthe music creation is using artificial intelligence and machine\nlearningtomakethecomputermatchthecorrespondingpattern\nto compose with original single-note score as a model."
            ],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [
                "E-mail addresses: wangnana515520@gmail.com(N.Wang),\n841206bluesky@163.com(H.Xu),928618736@qq.com(F.Xu),\n27122979@qq.com(L.Cheng).Thepurposeofcomputermusicresearchreferstocollecting,\nrecording,andanalyzingabstractsymbolsinmusicthroughcom-\nputermeans,abstracthumanemotionstowardmusicthrough\nalgorithms,andgeneratenewmusic[1].Atpresent,theemerg-\ningofcomputermusichasbroadsenseandnarrowsense.",
                "Tosolvethis\nproblem,Ooreetal.(2020)usedtwoLSTMmodelstotrainand\ncreatebluesmusic,whichwereusedtolearnchordsandmelody,\nrespectively.",
                "Theoutputofthechordnetworkwasconnectedto\nthemelodynetworkastheinputofthemelodynetwork[20].The\nfinalexperimentalresultsshowedthatthesystemcanlearnthe\nstandard12-barblueschordbarandgeneratemusicthatfollowed\nthechordlaw.2.2."
            ],
            "jazz": [
                "More-\nover,theorganizationofrulesbetweendifferentelementscan\ncreatedifferentstylesofmusic,suchasclassical,rock,jazz,rap,\nandhip-hop."
            ],
            "rock": [
                "More-\nover,theorganizationofrulesbetweendifferentelementscan\ncreatedifferentstylesofmusic,suchasclassical,rock,jazz,rap,\nandhip-hop."
            ],
            "pop": [],
            "classical": [
                "More-\nover,theorganizationofrulesbetweendifferentelementscan\ncreatedifferentstylesofmusic,suchasclassical,rock,jazz,rap,\nandhip-hop.",
                "Thespecific\nstructureisshowninFig.8.TheLSTMgenerationnetworkispre-\ntrainedbyinputtingtheclassicalmusicdatabaseinMIDIformat,\nandtheinitialnotesaresettorandomlygeneratemusic.",
                "Data processing\nIntheexperiment,theClassicalPianoMidiPage(CPMG)data\nsetistakenasthetrainingsample."
            ],
            "electronic": [
                "Introduction\nWiththeprogressanddevelopmentofscienceandtechnology\nandthewidespreadapplicationofelectronicequipment,digital\nimagehasbecomeanindispensableinformationmediuminsocial\nproductionandpeople\u2019slife."
            ],
            "hip-hop": [
                "More-\nover,theorganizationofrulesbetweendifferentelementscan\ncreatedifferentstylesofmusic,suchasclassical,rock,jazz,rap,\nandhip-hop."
            ],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Ontheotherhand,itisnecessaryto\nenhancethediscoursepowerofmusiccreatorsinthecopyright\ntransactioninordertoobtainthebenefitsconsistentwiththeir\npsychologicalexpectations.",
                "\u03b8\u2212isthetargetvaluenetworkparameter, \u03b3is\nthediscountfactor,and Visthestatevaluefunction.",
                "(10)\n\u03b8isthetargetvaluenetworkparameter,and \u03bbisthediscount\nfactoroftheobjectivefunction."
            ],
            "techno": [
                "Introduction\nWiththeprogressanddevelopmentofscienceandtechnology\nandthewidespreadapplicationofelectronicequipment,digital\nimagehasbecomeanindispensableinformationmediuminsocial\nproductionandpeople\u2019slife.",
                "Theaudiofilesse-\nlectedbytheuserareconvertedintoablockchainstructureusing\ndifferenttechnologiesandalgorithmsandaresecurelystoredin\ntheuser\u2019smusicwallet[13].Althoughmanyscholarsproposed\ndifferent modes of music creation, they were basically in the\ntheoretical stage.",
                "There\nare many strategies for music copyright protection, but there\nisnosystemframeworkthatisimplemented,whichmakesit\ndifficult for these technologies to be applied in practice.",
                "The\ndevelopmentoflogisticsinformationsearchtechnologysuchas\nshipautomaticidentificationsystem,blockchain,andmodern\ninformationprocessingtechnologysuchasbigdataandcloud\ncomputinghasprovidedinformationtechnologysupportforthe\nsharingandallocationoflogisticsresourcesinportgroups.",
                "Thiskindof\ndistributeddatastorageprevails,sodotheadoptionofpoint-to-\npointtransmission,consensusmechanism,encryptionalgorithm,\nandothernewapplicationmodeofcomputertechnology.",
                "Ontheonehand,thetransmissionspeedandscopeof\ndigitalmusicworksarelesslimitedbytimeandspacecompared\nwith traditional music works, and the infringement is easier\nthrough peer-to-peer network technology.",
                "Blockchain,theunderlyingtechnologyofBitcoin,is\napublicchronologicalrecordofBitcointransactions.",
                "Blockchain technology\n3.2.1.",
                "Block principle and application of block chain\nBlockchainistheunderlyingtechnologyofBitcoin.",
                "Table2belowshowsthecharacteristicsofblockchaintechnol-\nogyinthetransactionscenario,andFig.5showstheapplication\nscenarioofblockchain.3.2.2.",
                "The net-\nworklayerisacollectionofmechanismsfortheoperationof\ntheblockchainsystem,includingdistributednetworkingmecha-\nnisms, data transmission mechanisms, and data verification\nmechanisms[31].ItadoptsP2Ptechnology,whichhasautomatic\n6N. Wang, H. Xu, F. Xu et al.",
                "Applied Soft Computing 112 (2021) 107763\nTable 2\nCharacteristicsofblockchaintechnologyintransactionscenarios.",
                "Throughthetimestamp,encryptionalgorithm,\nandothertechnologiesinblockchain,therightsofdigitalmusic\nworksareconfirmedintheearlystageoftheirproduction.",
                "Therightusingstageisthecorecompetitivenessofthisappli-\ncationsystem,asillustratedinFig.12.Blockchaintechnologyis\nemployedtointegratedigitalmusicfromproductiontomarketing\nandotherlinks,focusingonthecorestageoftherightusing.",
                "Technology Safety Protectionrate\nDeeplearning 65.36% 65.23%\nBigdata 66.14% 69.43%\nBlockchain 65.12% 70.05%\nDeeplearning+blockchain 72.14% 82.33%\ncompliance rate, which is far better than GAN and VAE\u2013GAN,\nandslightlybetterthanRL-RNN.Themusicgenerationalgorithm\nbasedonMCNNnetworkcanbasicallyguaranteetheintegrityof\nthegeneratedmusic.",
                "[7] P.Rippa,G.Secundo,Digitalacademicentrepreneurship:Thepotentialof\ndigitaltechnologiesonacademicentrepreneurship,Technol.",
                "Technol. 26",
                "Sci.Technol.21(1)(2021)2\u20133.",
                "[22] W.Jian,G.Li,Z.Jingning,Digitalcopyrightprotectionbasedonblockchain\ntechnology,RadioTel.Inform.7(2016)60\u201362.",
                "Wang,Theapplicationofblockchaintechnology\nindigitalcopyright,DEStechTrans.",
                "[25] Z. Feng, Z. Wei, Analysis of digital copyright protection based on block\nchaintechnology,Sci.Technol.Law1(2017)10\u201319.",
                "[31] S.T\u00f6nnissen,F.Teuteberg,Analysingtheimpactofblockchain-technology\nfor operations and supply chain management: An explanatory model\ndrawn from multiple case studies, Int.",
                "[32] C.Bai,J.Sarkis,Asupplychaintransparencyandsustainabilitytechnology\nappraisalmodelforblockchaintechnology,Int.J.Prod."
            ],
            "dubstep": [],
            "opera": [
                "In this work, the music audio\nisdeemedastheresearchobject,andanewautomaticmusic\nsynthesisalgorithmisproposedbasedonLSTMRNN.Theriseof\ndigitalcontentoperationshasdemonstratedstrongdevelopment\npotentialandacceleratedthediversifieddevelopmentofthecom-\nmerciallayoutofthecontentindustry.",
                "Inaddition,theyalso\nsummeduptherisksofapplyingblockchain-technicalproblems,\nmarket-orientedoperationproblems,andcompatibilityproblems\nofcopyrighttheory[25].",
                "Arti-\nficialneuralnetworkalgorithmadoptstheoperatingmechanism\nofthehumanbrain,andthebrainneuralnetworkisabstractly\nmodeledfromtheperspectiveofsignalprocessingbyimitating\nthetransmissiondecisionprincipleofneuronsinthebrain.",
                "The net-\nworklayerisacollectionofmechanismsfortheoperationof\ntheblockchainsystem,includingdistributednetworkingmecha-\nnisms, data transmission mechanisms, and data verification\nmechanisms[31].ItadoptsP2Ptechnology,whichhasautomatic\n6N. Wang, H. Xu, F. Xu et al.",
                "2 Informationmechanism Theblockchainsystembasedonclever\nmathematicalprinciplesandprogram\nalgorithmshasopenandtransparentoperating\nrules.",
                "Theblockchain-based\ndigital music copyright protection system proposed can accu-\nratelycompleterequestsunderconcurrentoperationsofmultiple\nuserswhileensuringahighprocessingspeed.\n5.5.",
                "Basedontheactual\noperatingstatusofdigitalmusiccopyrightmanagementinChina,\naprimaryapplicationsystemthatappliesblockchaintodigital\nmusiccopyrightmanagementinChinaisconceived.",
                "[31] S.T\u00f6nnissen,F.Teuteberg,Analysingtheimpactofblockchain-technology\nfor operations and supply chain management: An explanatory model\ndrawn from multiple case studies, Int."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Towards a Deep Improviser: a prototype deep learning post-tonal free music generator",
            "homophonic": [],
            "polyphonic": [
                "An approach for identifying salient\nrepetition in multidimensional representations of polyphonic\nmusic."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "Unlike the present paper, the previous deep learning\nmusic generation systems have mainly focused on gener-ation of common practice instrumental music (using sym-\nbolic representations); see reviews [ 17,21,23,24]."
            ],
            "vocal": [],
            "choral": [],
            "orchestral": [
                "Several of these algorithmic pieces were multi-strand in nature, that is, they have multiple simultaneous\nmelodic strands as in chamber and orchestral music, as\nFig. 1 Musical representation in the form of a single input matrix."
            ],
            "chamber": [
                "Several of these algorithmic pieces were multi-strand in nature, that is, they have multiple simultaneous\nmelodic strands as in chamber and orchestral music, as\nFig. 1 Musical representation in the form of a single input matrix."
            ],
            "symphonic": [],
            "folk": [
                "Considering the case\nof music with symbolic representation (and thus potentially\nconventional musical notation), the highly successfulFolkRNN",
                "[ 27] produces music closely akin to Irish Folk\nmusic, with clear tonal and metrical features very much in\ncommon with it."
            ],
            "blues": [
                "By the same\ntoken, a free improviser may choose, for example, to adopt\na tonal or metrical posture, or engage in blues-oriented\nphrases, at any time."
            ],
            "jazz": [
                "Jost E (1974) Free jazz, English edn.",
                "Pressing J (2002) Free Jazz and the avant-garde.",
                "The Cambridge companion to jazz."
            ],
            "rock": [
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material)."
            ],
            "pop": [
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material).",
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material)."
            ],
            "classical": [],
            "electronic": [
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material).",
                "Essentially, it is the nonlinear acti-\nvation components within neural nets, together with a\npossibly large number of input features, and the potentialElectronic supplementary material The online version of this\narticle ( https://doi.org/10.1007/s00521-018-3765-x ) contains\nsupplementary material, which is available to authorized\nusers.\n&Roger T. Dean\nroger.dean@westernsydney.edu.au\n1MARCS Institute for Brain, Behaviour and Development,\nWestern Sydney University, Sydney, Australia\n2austraLYSIS, Sydney, Australia\n3Department of Computing, Goldsmiths, University of\nLondon, London, UK\n123Neural Computing and Applications (2020)",
                "An audio excerpt of\none piece from the Algorithmic Corpus and another fromthe Seed Piece (to provide an example of improvised\nkeyboard playing) are provided within Electronic Supple-\nmentary Material, realised using the Pianoteq physicalsynthesis piano.",
                "Manning Electronic\nAdvanced Publication, Shelter Island\n21."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "These features imply a capacity for\nvery long-term hierarchical relationships to be grasped(which is more dif\ufb01cult to conceive with the mentioned\nalternative approaches).",
                "Thus, an\nalternative approach to assessing whether outputs are sta-\ntistically distinctive or simply recreative has been adopted,\nin which we undertake univariate and multivariate testingof the question: what is the probability that the distribution\nof pitch (or velocity, duration, ioi) values observed in one\ncase (e.g. the Algorithmic Corpus) and that observed inanother (e.g. the generated output from the Algorithmic\nCorpus when seeded with the external improvised\nsequences) both arise from a parent distribution (which\nremains unspeci\ufb01ed in nature)."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [],
            "dubstep": [],
            "opera": [
                "In the future development of this project, we want to\ncreate and use a system operative in real time, and given\npre-learned models, this is already feasible.",
                "In\nour previous work, we have demonstrated the utility ofanalytical autoregressive multivariate time series models as\ngenerators themselves and constructed a system operative\nin real time [ 19]."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "From artificial neural networks to deep learning for music generation: history, concepts and trends",
            "homophonic": [],
            "polyphonic": [
                "Examples of objectives are: a monophonic melody to beplayed by a human \ufb02utist and a polyphonic accompani-\nment played by a synthesizer.",
                "Piano roll Representation of a melody (monophonic or\npolyphonic) inspired from automated pianos.",
                "Modeling temporal dependencies in high-dimensional sequences:application to polyphonic music generation and transcription.",
                "Imposing higher-level\nstructure in polyphonic music generation using convolutional\nrestricted Boltzmann machines and constraints."
            ],
            "monophonic": [
                "Todd\u2019s objective was to generate a monophonic melody\nin some iterative way.",
                "The ABC notation is very compactbut can only represent monophonic melodies.",
                "The last solution, considering the hold symbol as a note, is\nsimple and uniform, but it only applies to the case of a\nmonophonic melody.",
                "Examples of objectives are: a monophonic melody to beplayed by a human \ufb02utist and a polyphonic accompani-\nment played by a synthesizer.",
                "Piano roll Representation of a melody (monophonic or\npolyphonic) inspired from automated pianos."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "An example is the way he wascomposing chorales by designing and applying (with talent) counter-point rules to existing melodies.40 Neural Computing and Applications (2021) 33:39\u201365\n123learn very well musical style from a given corpus and to\ngenerate new music that \ufb01ts into this style.",
                "The\nunderlying architecture, named Coconet [ 27], has been\ntrained on a dataset of 306 Bach chorales.",
                "Therefore, the dataset is\nconstructed by extracting all possible 4 measures long\nexcerpts from the original 352 chorales, also transposed inall possible keys.",
                "After training on several examples, generation can take\nplace, with an example of chorale counterpoint generated\nfrom a soprano melody shown in Fig.",
                "1 Example of chorale generation by Bach Doodle.",
                "3 Example of a chorale counterpoint generated by MiniBach\nfrom a soprano melody\nFig.",
                "This incremental instantiation strategy has been used in\nthe DeepBach architecture [ 21] for generation of Bach\nchorales.",
                "Reproduced from [ 67] with permission of the authors\n56Actually this architecture is replicated 4 times, one for each voice\n(4 in a chorale).57The two bottom lines correspond to metadata (fermata and beat\ninformation), not detailed here.56 Neural Computing and Applications (2021) 33:39\u201365\n123An example of counterpoint accompaniment generation is\nshown in Fig.",
                "An example is a chorale with 3 voices(alto, tenor and bass) matching a soprano melody.",
                "Hadjeres G, Pachet F, Nielsen F (2017) DeepBach: a steerable\nmodel for Bach chorales generation."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "11.\n\u2013Text\u2014A signi\ufb01cant example is the ABC notation [ 65], a\nde facto standard for folk and traditional music.",
                "It is trained\non examples selected from the folk music repository namedThe Session [ 30] and uses text (the ABC notation [ 65], see\nSect.",
                "An example generated after training\nan autoencoder on a set of Celtic melodies (selected from\nthe folk music repository The Session [ 30] introduced in\nSect. 7.2.1 ) is shown in Fig. 19(see [ 2] for more details)."
            ],
            "blues": [
                "In addition, the method becomes, in principle,independent of a speci\ufb01c musical style\n4(e.g., classical,\njazz, blues, serial).",
                "7.2.1 Recursive strategy\nThe \ufb01rst music generation experiment using current state of\nthe art of recurrent architectures, the LSTM (Long Short-\nTerm Memory [ 25]) architecture, is the generation of blues\nchord (and melody) sequences by Eck and Schmidhuber in[8]."
            ],
            "jazz": [
                "In addition, the method becomes, in principle,independent of a speci\ufb01c musical style\n4(e.g., classical,\njazz, blues, serial)."
            ],
            "rock": [],
            "pop": [
                "An example of the use of GAN for generating music is\nthe MidiNet system [ 67], aimed at the generation of single\nor multitrack pop music melodies.",
                "An example of the use of GAN for generating music is\nthe MidiNet system [ 67], aimed at the generation of single\nor multitrack pop music melodies."
            ],
            "classical": [
                "In addition, the method becomes, in principle,independent of a speci\ufb01c musical style\n4(e.g., classical,\njazz, blues, serial)."
            ],
            "electronic": [
                "6.2.2 SymbolicThe main symbolic formats used are:\n\u2013MIDI\n28\u2014It it is a technical standard that describes a\nprotocol based on events, a digital interface and connec-\ntors for interoperability betwee n various electronic musi-\ncal instruments, softwares and devices [ 43].",
                "A tech-\nnical standard that describes a protocol, a digital\ninterface and connectors for interoperability between\nvarious electronic musical instruments, softwares anddevices.",
                "Hiller LA, Isaacson LM (1959) Experimental music: composition\nwith an electronic computer."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "2.48 Neural Computing and Applications (2021) 33:39\u201365\n1237 Main basic architectures and strategies\nFor reasons of space limitation, we will now jointly introduce\narchitectures and strategies.34For an alternative analysis\nguided by requirements (challenges), please see [ 4].\n7.1 Feedforward architecture\nThe feedforward architecture35is the most basic and\ncommon type of arti\ufb01cial neural network architecture.",
                "47Convolutional architectures are actually an important component\nof the current success of deep learning and they recently emerged as\nan alternative, more ef\ufb01cient to train, to recurrent architectures [ 3,\nSection 8.2]."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "This forces the autoencoder to discover signi\ufb01cant\n(discriminating) features to encode useful information into\nthe hidden layer nodes (also named the latent variables ).",
                "9.2 Variational autoencoder architecture\nAlthough producing interesting results, an autoencoder\nsuffers from some discontinuity in the generation when\nexploring the latent space.49Avariational autoencoder\n(VAE)",
                "Reproduced from [ 34] with permission of the authors\n62As in the case of a good cook, whose aim is not to simply mix all\npossible ingredients but to discover original successful combinations.58 Neural Computing and Applications (2021) 33:39\u201365\n123the input (in the case of creation by re\ufb01nement, as intro-\nduced in Sect. 4.2, or by using an extra conditioning input,\nas in Anticipation-RNN [ 19]), the output (in the case of\nconstrained sampling, as used by C-RBM), or an encap-sulation (in the case of a reformulation through reinforce-\nment learning as in RL Tuner [ 29])."
            ],
            "techno": [
                "Emerging Technology from the arXiv: deep learning machine\nsolves the cocktail party problem.",
                "MIT Technology Review\n(2015).",
                "https://www.technologyreview.com/s/537101/deep-learn\ning-machine-solves-the-cocktail-party-problem/ ."
            ],
            "dubstep": [],
            "opera": [
                "\u2019\u2019\n\u2013Multiple time/clocks \u2014\u2018\u2018Of course, one way to present\nthis subsequence-generating network with the appro-\npriate sequence of plans is to generate those by another\nsequential network, operating at a slower time scale.",
                "6.2.2 SymbolicThe main symbolic formats used are:\n\u2013MIDI\n28\u2014It it is a technical standard that describes a\nprotocol based on events, a digital interface and connec-\ntors for interoperability betwee n various electronic musi-\ncal instruments, softwares and devices [ 43].",
                "26Indeed, at the level of processing by a deep network architecture,\nthe initial distinction between audio and symbolic representation boils\ndown, as only numerical values and operations are considered.",
                "For instance, the pitch of a note could be represented as a\nreal number (its frequency in Hertz), an integer number (its\nMIDI note number), or a one-hot vector (actually the most\ncommon strategy), as shown in the right part of Fig. 11.33\nThe advantage of value encoding is its compact represen-\ntation, at the cost of sensibility because of numericaloperations (approximations).",
                "The advantage of one-hot\nencoding is its robustness against numerical operations\napproximations (discrete versus analog), at the cost of ahigh cardinality and therefore a potentially large number of\nnodes for the architecture.",
                "9.2.1 Variational generationExamples of possible dimensions captured by latent vari-\nables learnt by the VAE are the note duration range (the\ndistance between shortest and longest note) and the note\npitch range (the distance between lowest and highest pitch).This latent representation (vector of latent variables) can beused to explore the latent space with various operations to\ncontrol/vary the generation of content.",
                "Some examples of\noperations on the latent space (as summarized in [ 53]) are:\n\u2013Translation ;\n\u2013Interpolation\n52;\n\u2013Averaging ;\n\u2013Attribute vector arithmetics , by addition or subtraction\nof an attribute vector capturing a given characteristic.53\nFig.",
                "Convolution In mathematics, a mathematical operation\non two functions sharing the same domain that produces\na third function which is the integral (or the sum in the\ndiscrete case\u2014the case of images made of pixels) of thepointwise multiplication of the two functions varying\nwithin the domain in an opposing way.",
                "A tech-\nnical standard that describes a protocol, a digital\ninterface and connectors for interoperability between\nvarious electronic musical instruments, softwares anddevices.",
                "Pooling For a convolutional architecture, a data dimen-\nsionality reduction operation (by max, average or sum)for each feature map produced by a convolutional stage,\nwhile retaining signi\ufb01cant information."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Conditional hybrid GAN for melody generation from lyrics",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "Three stylistic categoriessuch as nursery rhymes, folk songs, and rock songs are\ncomposed when given lyrics."
            ],
            "blues": [],
            "jazz": [],
            "rock": [
                "Three stylistic categoriessuch as nursery rhymes, folk songs, and rock songs are\ncomposed when given lyrics."
            ],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "In particular, this paper\ncontains several contributions: (i) A hybrid structure isproposed, which contains three independent branches (each\n&Yi Yu\nyiyu@nii.ac.jp\n1Digital Content and Media Sciences Research Division,\nNational Institute of Informatics and SOKENDAI,\nChiyoda-ku, Tokyo 101-8430, Japan\n2Indian Institute of Technology Delhi, Delhi 110016, India\n3Zhejiang University, Hangzhou 310027, Zhejiang, China\n123Neural Computing and Applications (2023) 35:3191\u20133202\nhttps://doi.org/10.1007/s00521-022-07863-5 (0123456789().,-volV) (0123456789()."
            ],
            "dubstep": [],
            "opera": [
                "For each head h,Mtis used to construct queries\nQ\u00f0h\u00de\nt\u00bcMtW\u00f0h\u00de\nq, and its combination with xtis used to\nconstruct keys K\u00f0h\u00de\nt\u00bc\u00bdMt;xt/C138W\u00f0h\u00de\nk and values\nV\u00f0h\u00de\nt\u00bc\u00bdMt;xt/C138W\u00f0h\u00de\nv, where [; ] represents the row-wise\nconcatenation operation, and W\u00f0h\u00de\nk ;W\u00f0h\u00de\nv ;W\u00f0h\u00de\nqare weights.",
                "Then, the memory\nMt\u00fe1is updated, and the output otis computed from ~Mt\u00fe1\nandMtby\nMt\u00fe1\u00bcfh1\u00f0~Mt\u00fe1;Mt\u00de;ot\u00bcfh2\u00f0~Mt\u00fe1;Mt\u00de; \u00f02\u00de\nwhere fh1andfh2are parameterized functions consisting of\nskip connections, multi-layer perceptron, and gated\noperation.",
                "The Gumbel-Softmax operation is performed on otto\nobtain the one-hot approximation of the pitch attribute\n^yp\nt2R100.^yp\n0/C24Uniform \u00f00;1\u00deis used for the initial time\nstep.",
                "Because thesampling operation in Eqn."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Scene2Wav: a deep convolutional sequence-to-conditional SampleRNN for emotional scene musicalization",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "The affective properties of keys in instrumental music from the late nineteenth and\nearly twentieth centuries."
            ],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [
                "Lee\nmholee@gmail.com\nGwenaelle Cunha Sergio\ngwena.cs@gmail.com\n1School of Electronics Engineering, Kyungpook National University, 80 Daehakro,\nBukgu, Daegu 41566, South KoreaMultimediaToolsandApplications(2021)80:1793\u20131812\nPublishedonline: 2020 September10and in all modern societies, visual arts and music are intimately intertwined [ 17].",
                "(2016-0-00564,1810 MultimediaToolsandApplications(2021)80: 1793\u20131812Development of Intelligent Interaction Technology Based on Context Awareness and\nHuman Intention Understanding) (50%) and Electronics and Telecommunications Research\nInstitute (ETRI) grant funded by the Korean government"
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Shan MK, Kuo FF, Chiang MF, Lee SY (2009) Emotion-based music recommendation by affinity\ndiscovery from film music.",
                "Shan MK, Kuo FF, Chiang MF, Lee SY (2009) Emotion-based music recommendation by affinity\ndiscovery from film music."
            ],
            "techno": [
                "The gap between raw audio and notation data research has been arguably getting smaller\ndue to the democratization of GPUs through cloud technology, but it\u2019s still very expensive\nto run a model there for extended periods of time.",
                "(2016-0-00564,1810 MultimediaToolsandApplications(2021)80: 1793\u20131812Development of Intelligent Interaction Technology Based on Context Awareness and\nHuman Intention Understanding) (50%) and Electronics and Telecommunications Research\nInstitute (ETRI) grant funded by the Korean government",
                "[18ZS1100, Core Technology\nResearch for Self-Improving Artificial Intelligence System] (50%).",
                "IEEE Trans Circ Syst Video Technol\n16(6):689\u2013704\n37."
            ],
            "dubstep": [],
            "opera": [
                "5, Tiers 3 and 2 are both frame-level modules, but each operates at different temporal\nresolutions: Tier 3 processes longer windows of audio, allowing for extraction of global1801 MultimediaToolsandApplications(2021)80: 1793\u20131812Fig.5"
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Attentional networks for music generation",
            "homophonic": [],
            "polyphonic": [
                "[ 3] attempted to man-\nage the test of learning complex polyphonic structure in music.",
                "They utilized a RecurrentTemporal Restricted Boltzmann machine (RTRBM) so as to demonstrate unconstrainedpolyphonic music.",
                "[24] 0.2317 0.9904 0.9808\nLSTM 0.5097 1.0919 1.1924\nLSTM + Attention 0.2286 0.8924 0.7864\nBi-LSTM + Attention + LSTM 0.1069 0.6694 0.4481\nFig.3 Graph: Categorical Cross Entropy Loss\nFig. 4 Performance Evaluation Graph: (a)-(c) shows Mean Square Error for LSTM, LSTM+attention and\nBi-LSTM+attention respectively5187 Multimedia Tools and Applications (2022) 81:5179\u20135189Fig.5 a) Input music sheet b) Output music sheet generated by the proposed framework for songs Chameleon\n(Top Row) and Last farewell (Bottom row)\n4.3 Comparisonwithrelatedworks\nIn [24], the author discusses about polyphonic midi sequences using LSTM networks.",
                "Modeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic music generation and transcription.",
                "Johnson DD (2017) Generating polyphonic music using tied parallel networks.",
                "A study on lstm networks for polyphonic music sequence modelling."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [
                "Blues improvisation with lstm recur-\nrent networks."
            ],
            "jazz": [
                "In this work, we propose a deep learning based music generation\nmethod in order to produce old style music particularly JAZZ with rehashed melodic struc-\ntures utilizing a Bi-directional Long Short Term Memory (Bi-LSTM) Neural Network with\nattention.",
                "In this work, we utilize an end to end pipeline based on Bi-LSTM network with an atten-\ntion module to produce old style Jazz music with rehashed melodic structures automaticallywithout any human intervention.",
                "1 a) Sheet Music of the song: \u201cThe Last Farewell\u201d by Roger Whittaker b) Musical Notes (Extracted\nfrom MIDI file) of the song: \u201cThe Last Farewell\u201d5181 Multimedia Tools and Applications (2022) 81:5179\u20135189Table1 Batch construction for the JAZZ ML ready MIDI dataset: Batch size 64 characters\nBatch-1",
                "4 Experimentationandresults\n4.1 Datasetdiscription\nWe used Jazz ML ready MIDI dataset2to train our model.",
                "The dataset comprises of 818\ndiverse Jazz music melodies.",
                "2https://www.kaggle.com/saikayala/jazz-ml-ready-midi5186 Multimedia Tools and Applications (2022)"
            ],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "This capacity of memory storagemakes it extremely helpful in applications such as discourse handling and music composi-tion."
            ],
            "techno": [
                "Mukherjee\nprerana@jnu.ac.in\nGullapalli Keerti\nkeerti.g17@iiits.in\nA N Vaishnavi\nvaishnavi.a17@iiits.in\nA Sree Vidya\nsreevidya.a17@iiits.in\nGattineni Sai Sreenithya\nsaisreenithya.g17@iiits.in\nDeeksha Nayab\ndeeksha.n17@iiits.in\n1Indian Institute of Information Technology, Sri City, Andhra Pradesh, India\n2Jawaharlal Nehru University, Delhi, IndiaPublished online: 21 January 2022Multimedia Tools and Applications (2022)"
            ],
            "dubstep": [],
            "opera": [
                "This transposition is quite easy in MIDI format as it\nis simply by doing arithmetic operations such as addition and subtraction to a fixed value."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Monophonic music composition using genetic algorithm and Bresenham\u2019s line algorithm",
            "homophonic": [
                "Music can have different\ntextures, namely monophonic, homophonic and polyphonic.",
                "While homophonic and poly-\nphonic textures also include harmony along with melody and rhythm."
            ],
            "polyphonic": [
                "Music can have different\ntextures, namely monophonic, homophonic and polyphonic."
            ],
            "monophonic": [
                "https://doi.or g/10.1007/s11042-022-12185-8\nMonophonicmusiccompositionusinggenetic\nalgorithmandBresenham\u2019slinealgorithm\nShipraShukla1\u00b7HaiderBanka1\nReceived:5August2020/Revised:30April2021/Accepted:10January2022\n\u00a9TheAuthor(s),underexclusivelicencetoSpringerScience+BusinessMedia,LLC,partofSpringerNature2022\nAbstract\nMusic composition is one of the oldest artistic pursuits.",
                "The type of music composed in this work is monophonic, which\nincludes melody and rhythm.",
                "Music can have different\ntextures, namely monophonic, homophonic and polyphonic.",
                "When organized with single\nmelody and rhythm, the music has a monophonic texture.",
                "We have considered\nmelodic and rhythmic aspect of music in this work, thereby creating monophonic music.",
                "[8 ] Deep learning for monophonic music Corpus of Irish folk songs is used to generate music with same style\nGuedes et al.",
                "[ 2] Generative RNN model for sheet music Uses dataset in ABC music notation26488 Multimedia Tools and Applications (2022) 81:26483\u201326503recurrent unit (GRU) to generate convincing monophonic melodies.",
                "While proposed methodgenerates new compositions with creative exploration of the search space.\n\u2013 Proposed method vs existing evolutionary approaches: The existing methods on\ngenerating monophonic music mostly work in two directions: creating compositionsidentical to the given reference [ 24,28,37] or generating new melodic ideas",
                "Finally,both the rhythm and pitch sequences are combined to achieve monophonic music."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "81:26483\u201326503F\nig.1 3/4 time signature\n2 Preliminaries\nMusic composition is the art of combining musical elements to create a piece of music,\neither vocal or instrumental."
            ],
            "vocal": [
                "81:26483\u201326503F\nig.1 3/4 time signature\n2 Preliminaries\nMusic composition is the art of combining musical elements to create a piece of music,\neither vocal or instrumental.",
                "Since the individuals can have different vocal ranges, thiswork provides an option to set the range within the comfort zone."
            ],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "[8 ] Deep learning for monophonic music Corpus of Irish folk songs is used to generate music with same style\nGuedes et al."
            ],
            "blues": [],
            "jazz": [
                "[ 3] designed GenJam to generated jazz solos using interactive GA.",
                "[32] used algorithmic fitness function to generate jazz melodies over thegiven chord progression.",
                "[3 ] Interactive GA for melody (Jazz solos).",
                "[32] GA for Jazz melody and rhythm Uses input chord progression for reference\nMatic",
                "Biles JA et al (1994) Genjam: a genetic algorithm for generating jazz solos.",
                "A genetic algorithm for the generation of jazz melodies."
            ],
            "rock": [],
            "pop": [
                "The process starts with a randomly generated population of individuals.",
                "The process starts with a randomly generated population of individuals.",
                "This makes it a good choice for music composition wherethe individuals of population can be represented by note sequences.",
                "This makes it a good choice for music composition wherethe individuals of population can be represented by note sequences.",
                "Literaturereview\nIn recent years, algorithmic music composition has become very popular among computer\nresearchers [ 16].",
                "Literaturereview\nIn recent years, algorithmic music composition has become very popular among computer\nresearchers [ 16].",
                "However, the user has to listen to each sample thoroughly to givefeedback which limits the population size and number of generations.",
                "However, the user has to listen to each sample thoroughly to givefeedback which limits the population size and number of generations.",
                "81:26483\u201326503Table1 Some popular approaches for music composition\nReference Composition Approach Remark\nBiles et al.",
                "81:26483\u201326503Table1 Some popular approaches for music composition\nReference Composition Approach Remark\nBiles et al.",
                "Rhythm and Uses hierarchical population representation\nother elements taken from progression file\nJohanson et al.",
                "Rhythm and Uses hierarchical population representation\nother elements taken from progression file\nJohanson et al.",
                "Although there\nare other durations possible but they are less popular and seldom used.",
                "Although there\nare other durations possible but they are less popular and seldom used.",
                "The motif pro-\nvided by user is included in the chromosomes while generating initial population.",
                "The motif pro-\nvided by user is included in the chromosomes while generating initial population.",
                "Thepopulation is evaluated based on fitness functions.",
                "Thepopulation is evaluated based on fitness functions.",
                "If the criterion is satisfied, then out-put melody is generated; else, the population is passed for the next step.",
                "If the criterion is satisfied, then out-put melody is generated; else, the population is passed for the next step.",
                "Further, selection,crossover and mutation operations are performed to generate new population.",
                "Further, selection,crossover and mutation operations are performed to generate new population.",
                "Example of chromosome with user\u2019s motif\n4.2.2 Initialpopulation\nThe population is initialized with 50 chromosomes.",
                "Example of chromosome with user\u2019s motif\n4.2.2 Initialpopulation\nThe population is initialized with 50 chromosomes.",
                "4.2.3 Evaluation\nThe fitness function for evaluating the population is defined in Eq.",
                "4.2.3 Evaluation\nThe fitness function for evaluating the population is defined in Eq.",
                "If the fitness of i\nthgenome is Fi,\nthe probability of its selection is defined as:\npi=Fi/summationtextN\ni=0Fi(5)\nwhere N denotes number of genomes in the population.\n\u2013 Survivor Selection: The (\u03bc+\u03bb)-Evolutionary Strategy is used for survivor selection.",
                "If the fitness of i\nthgenome is Fi,\nthe probability of its selection is defined as:\npi=Fi/summationtextN\ni=0Fi(5)\nwhere N denotes number of genomes in the population.\n\u2013 Survivor Selection: The (\u03bc+\u03bb)-Evolutionary Strategy is used for survivor selection.",
                "This crossed population is then passed for three different26494 Multimedia Tools and Applications (2022) 81:26483\u201326503Fig.9 (\u03bc+\u03bb)-Strategy for Survivor Selection\nmutation operators as shown in Fig. 9.",
                "This crossed population is then passed for three different26494 Multimedia Tools and Applications (2022) 81:26483\u201326503Fig.9 (\u03bc+\u03bb)-Strategy for Survivor Selection\nmutation operators as shown in Fig. 9.",
                "4.2.6 Mutation\nMutation operators are used to maintain diversity in the population.",
                "4.2.6 Mutation\nMutation operators are used to maintain diversity in the population.",
                "Size of the population is 50, and rest of the parameters are samein both algorithms.",
                "Size of the population is 50, and rest of the parameters are samein both algorithms.",
                "After 20 generations, the average fitness of the population is calculated.",
                "After 20 generations, the average fitness of the population is calculated.",
                "The size of population is 50, and rest of the\noperators and parameters are same in both algorithms.",
                "The size of population is 50, and rest of the\noperators and parameters are same in both algorithms.",
                "The average fitness of population iscalculated after 20 generations.",
                "The average fitness of population iscalculated after 20 generations."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Shan MK, Chiu SC (2010) Algorithmic compositions based on discovered musical patterns."
            ],
            "techno": [
                "Course Technology."
            ],
            "dubstep": [],
            "opera": [
                "To create a proper sequence of notes, we used genetic algo-\nrithm with suitably formulated crossover and mutation operators.",
                "Genetic algorithm works very effectively if appropriate fit-ness function and operators are used [ 11].",
                "The proposed work uses genetic algorithm with\ndomain-specific operators to produce a proper sequence of notes.",
                "\u2013 Domain-specific operators: To accomplish the objective, problem specific crossover\nand mutation operators are defined.",
                "Music theory rules are utilized for defining fitnessfunction and genetic operators.",
                "This approach is also\nutilized in this work by adding user\u2019s motif in the melody and expending it with the help ofmotif-based mutation operator (discussed in Section 4.2.6).",
                "Further, selection,crossover and mutation operations are performed to generate new population.",
                "This crossed population is then passed for three different26494 Multimedia Tools and Applications (2022) 81:26483\u201326503Fig.9 (\u03bc+\u03bb)-Strategy for Survivor Selection\nmutation operators as shown in Fig. 9.",
                "The output of all the three operators is com-\nbined to a pool of \u03bboff-springs.",
                "4.2.5 Crossover\nWe have opted for the single point crossover operator.",
                "4.2.6 Mutation\nMutation operators are used to maintain diversity in the population.",
                "Three mutation opera-tors are used in this work.",
                "All the operators are defined in such a way that they preserve theuser motif until the end of the process.",
                "Due to domain-specific genetic operators, the GA converged very rapidly to high fitness.",
                "Melody generated using user given parameters\nTo examine the performance of proposed method, we compared it with existing genetic\nalgorithm operators.",
                "The size of population is 50, and rest of the\noperators and parameters are same in both algorithms.",
                "Tests have shown that the proposed operators are more effective than existing\noperators for the given fitness function.",
                "\u2013 The algorithm is compared with the conventional genetic operators to examine the\nperformance.",
                "To achieve this, new crossover and mutation operators are defined.",
                "In second phase, we havegenerated pitch sequences using genetic algorithm with domain-specific operators.",
                "2The use of\na domain-specific crossover operator is one of the reasons for obtaining better results.",
                "Sur-vivor selection process and mutation operators also contribute to the enhanced performanceof the proposed method."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Polyphonic music generation generative adversarial network with Markov decision process",
            "homophonic": [],
            "polyphonic": [
                "Polyphonic music generation generative adversarial\nnetwork with Markov decision process\nWenkai Huang1&Yihao",
                "Abstract\nIn the process of polyphonic music creation, it is important to combine two or more\nindependent melodies through technical treatment.",
                "However, due to the diversity of\npolyphonic music sequences and the limitations of neural networks, it is difficult to\ncreate chords or melodies beyond the training data.",
                "Therefore, this paper proposes a novel polyphonic music creation\nmodel, combining the ideas of the Markov decision process (MDP) and Monte Carlo tree\nsearch (MCTS) and improving the Wasserstein Generative Adversarial Network\n(WGAN) theory.",
                "Experi-\nmental results show that the algorithm proposed here has a better effect on polyphonic\nmusic generation than the latest methods.",
                "Keywords Polyphonic music generation .Markov decision process (MDP) .Monte",
                "Generative Adversarial Network (WGAN)\n1 Introduction\nProducing various independent melodies and combining them harmoniously through technical\nprocessing are key to creating polyphonic music.",
                "For composers, this is a very heavy task, so\nthere is hope that polyphonic music may be generated through neural networks.",
                "i n a#The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022classic Generative Adversarial Network (GAN) [ 7,13] is limited for polyphonic music\ncreation.",
                "As a polyphonic music sequence grows, the probability of the generated sequence\nsamples continuously playing the same note increases, which would destroy music coherence.",
                "Therefore, the present research team designed a GAN model based on the Markov decisionprocess (MDP) [ 21] and Monte Carlo tree search (MCTS) [ 4] to generate polyphonic music.",
                "[ 15] have also\nproposed a Morpheus music generation system, which can generate polyphonic music atgiven tension profiles, with long-term and short-term repetition pattern structures.",
                "However, in polyphonic musicgeneration, as the length of the music sequence generated by WGAN increases, sequencecoherence is broken, and the discriminator in the WGAN model finds it difficult to evaluatethe incomplete sequence.",
                "Therefore, aiming atthe problems of WGAN polyphonic music generation, this paper improves the structure of the\ngenerator and discriminator in the WGAN model and adds a policy gradient algorithm [ 29]t o\nupdate the generator parameters.",
                "Forpolyphonic music generation, the generator in the GAN model supervises learning needs to beinformed of the appropriate notes required for each small segment sequence.",
                "Therefore, the present research team has the introduced MDP mechanism into the GAN modelto generate polyphonic music.",
                "In this\npaper, MCTS is introduced to take advantage of its efficient search strategy for exploringpolyphonic music sequences and completing sequences from the generator, thus solving theproblem of the discriminator being unable to evaluate incomplete sequences.",
                "In this way, themodel can be applied to any incomplete polyphonic music sequence at any time.",
                "By learning the characteristics of and mapping between various independentmelody sequences in a polyphonic music dataset, this model can generate polyphonic musicthat is closer to real-world music, effectively resolving the issues concerning the continuity ofpolyphonic music sequences and ensuring the diversity of generated samples \u2014making the\nmodel more powerful for unsupervised music sequence processing.",
                "This makes the networkmore quickly converge in the direction of ideal polyphonic music.",
                "Thus, the generatedpolyphonic music will not be destroyed with the growth of the sequence; better quality musicwill be generated; the shackles of the dataset will be broken; and melodies outside the datasetcan be created.",
                "At the same time, the model, after learning, will be able to generate polyphonic\nGeneratorMCTSIncomplete music sequence\nComplete \nsequence.\n.",
                "1 Flowchart of polyphonic music generation in this study29868 Multimedia Tools and Applications (2022)",
                "The model has no require-ments for the length of the input polyphonic music sequence and can automatically generateexcellent music corresponding to the input sequence.",
                "(4) This paper is the first to integrateMCTS into WGAN for polyphonic music generation, removing the issues in which it isdifficult for the discriminator, in the existing GAN model, to evaluate incomplete sequences.",
                "This proposed model, thus, improves the ability of neural networks to generate diversepolyphonic music.",
                "2 Technical details\n2.1 Network construction\nIn this paper, the researchers have built a neural network based on WGAN and MDP to learnand generate polyphonic music.",
                "When learning a polyphonic music sequence, the generator\nrandomly generates the polyphonic music sequence, and then inputs the music generated bythe model into the discriminator with the original music for classification.",
                "The discriminator model includes a convolution layer and a fully-\nconnected layer, which are used to accurately identify the real polyphonic music and thepolyphonic music generated by the model in the iterative learning process.",
                "To improve thestability of the model for polyphonic music generation, the research team has added batchnormalization to all layers except the input convolution layer of the discriminator [ 16].",
                "Figure 3\ndepicts the polyphonic music generation model of this paper, including the generator anddiscriminator models.",
                "81:29865 \u201329885 29869During the training process, the generator model learns to generate polyphonic music\nsimilar to real polyphonic music, while the discriminator model learns to more accurately\ndistinguish between the real and the model generated polyphonic music.",
                "This study combines\nWGAN and MDP to generate polyphonic music.",
                "The discrim-inator must be asked to score the complete polyphonic music sequence.",
                "In the process of polyphonic music\ngeneration, the prediction of the next music sequence depends on the previous one.",
                "The discriminator network of this polyphonic music generation model is composed of a\nconvolution layer and a fully-connected layer.",
                "3 Schematic diagram of the polyphonic music generation model in this study29870 Multimedia Tools and Applications (2022)",
                "After training, thegenerator network can be used alone to generate polyphonic music.",
                "2.2 Network strategy gradient\nWhen designing the strategy gradient for the polyphonic music generation model, it wasnecessary for the authors to introduce the relevant GAN theory proposed by Goodfellow et al.[13].",
                "Therefore, the team introduced WGAN into the polyphonic music generation model.",
                "WP\nr;Pg/C0/C1\n\u00bcinf/C13/C24\u03a0\u00f0Pr;Pg\u00deEx;y\u00f0\u00de /C24 /C13x/C0yjjjj\u00bd/C138 \u00f0 2\u00de\nPris the sample distribution of a real polyphonic music sequence;",
                "Pgis the sample distribution\nof a polyphonic music sequence produced by the generator; and \u03a0\u00f0Pr;Pg\u00deis the set of all\nTable 1",
                "The calculated J\u03b8\u00f0\u00deis the function the polyphonic music generation model seeks\nto maximize.",
                "When evaluating polyphonic music\nsequences at any time, they have considered the possible long-term rewards [ 20], as well as the\noverall situation for each sequence.",
                "Therefore, it\nhas been used here for the decision-making process of polyphonic music sequence generation.",
                "3 Experimental results and analysis\n3.1 Dataset composition and preprocessing\nThe research team have constructed 6,947 original datasets composed of polyphonic music\n[19] to serve as the dataset for the polyphonic music generation model.",
                "At the\nsame time, the research team has compared the effects of the model in this study with theMuseGAN v1 model [ 10], the MuseGAN v2 model [ 11], and the LSTM music generation\nmodel [ 23] for polyphonic music generation.",
                "eM C T S\nalgorithm can balance the exploration and use of a polyphonic music sequence.",
                "When the overall number of iterations in the network is700, the errors of the generator and discriminator become stable, and the model generates\npolyphonic music well.",
                "3.3 Experimental result\nBy studying the existing music generation methods and analyzing the WGAN model, through\na large number of experiments, it has been proven that the model proposed in this paper is\nmore suitable for polyphonic music generation than both the WGAN model and the WGANand MDP model.",
                "Figure 10shows the various styles of polyphonic\nmusic output by the model using the dataset created for this paper.",
                "The results show that,\ncompared with the music generation model proposed by Dong et al., the polyphonic musicgeneration model in this paper has better polyphonic music sequence generation ability.",
                "macro/C0P\u00bc1\nnXn\ni\u00bc1Pi \u00f017\u00de\nmacro/C0R\u00bc1\nnXn\ni\u00bc1Ri \u00f018\u00de\nmacro/C0F1\u00bc2/C2macro/C0P/C2macro/C0R\nmacro/C0P\u00femacro/C0R\u00f019\u00de\nIn this paper, all polyphonic music sequences in the original dataset are \u201ctrue\u201dsamples, while\nnoise sequences are \u201cfalse\u201dsamples.",
                "At the same time, in the polyphonic music generated by\nthe corresponding model, the regular music sequence is defined as the \u201ctrue\u201dsample predicted\nby the classifier, and the generated noise sequence is the sample predicted as \u201cfalse.",
                "During the experiments, the research team input the original polyphonic music dataset into\nthe proposed music generation model to obtain the polyphonic music generated by thealgorithm in this paper.",
                "Simultaneously, the researchers input the same polyphonic music\nd a t a s e ti n t",
                "[ 11] to obtain the polyphonic music generated in that\nstudy [ 11].",
                "After completing the above steps, the team used the polyphonic music sequence in\nthe original dataset and the formulas for precision and recall to obtain the comparison testresults of the two models, as shown in Table 2.",
                "It should be noted that, with the change of test range, the value obtained formacro/C0F1 is always higher than that obtained for macro/C0F1\nD. Thus, it is proven that the\npolyphonic music generation model based on WGAN theory, which integrates the ideas of\nMDP and MCTS, is better than the MuseGAN model proposed by Dong et al.",
                "The model proposedin this study seeks to learn polyphonic note sequences on a single-layer LSTM network.",
                "When training polyphonic music sequences in this model, there is noproblem of gradient explosion or gradient disappearance.",
                "3.6 Limitations\nThe polyphonic music generation model proposed in this paper has more hidden layers and\nhigher complexity because it contains a generator and a discriminator, adds the MDPmechanism, and integrates MCTS ideas.",
                "i o n s\nBased on the WGAN model, a novel polyphonic music generation method using MDP",
                "In addition, the polyphonic music model\nuses a GPU for training, and the team plans to collect more types of polyphonic music ofvarious styles so that the model can generate better music in subsequent training.",
                "Convolutional generative adversarial networks with binary neurons for\npolyphonic music generation."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [
                "MuseGAN V1 uses convolution in the generator and discrim-\ninator, which can generate multi-channel pop/rock music from scratch or can accompany a\ntrack provided by the user."
            ],
            "pop": [
                "MuseGAN V1 uses convolution in the generator and discrim-\ninator, which can generate multi-channel pop/rock music from scratch or can accompany a\ntrack provided by the user.",
                "MuseGAN V1 uses convolution in the generator and discrim-\ninator, which can generate multi-channel pop/rock music from scratch or can accompany a\ntrack provided by the user."
            ],
            "classical": [],
            "electronic": [
                "Conklin D, Gasser M, Oertl S (2018) Creative chord sequence generation for electronic dance music."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "SA-MCTS is also an effective\nalternative for domains in which offline parameter adjustment is expensive or infeasible."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "However, theMultimedia Tools and Applications (2022) 81:29865 \u201329885\nhttps://doi.org/10.1007/s11042-022-12925-w\n*Wenkai Huang\n16796796@qq.com\n1Center for Research on Leading Technology of Special Equipment, School of Mechanical and\nElectrical Engineering, Guangzhou University, Guangzhou 510006, China\n2School of Mechanical and Electrical Engineering, Guangzhou University, 510006 Guangzhou, China\n3Laboratory Center, Guangzhou University, Guangzhou 510006, People \u2019sR e p u b l i co fC h",
                "This technology can effectively find low cross entropy (high probability)solutions and improve the quality of music generation.",
                "3.4 Comparison of existing technologies\nThis paper uses the test music sequence set in the algorithm verification stage, and uses therelevant principles of macro precision (macro-P), macro recall (macro-R) and correspondingMacro-F1, and makes a detailed calculation and comparative analysis between the modelproposed in this paper and the model proposed by Dong et al.",
                "IEEE Trans Veh Technol 69:7146 \u20137158."
            ],
            "dubstep": [],
            "opera": [
                "Based on the current\nroad conditions and the predicted driving operation, deep MCTS can select the best track."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "A combination of multi-objective genetic algorithm and deep learning for music harmony generation",
            "homophonic": [],
            "polyphonic": [
                "Therefore, in this paper, we propose a Multi-Objective\nGenetic Algorithm (MO-GA) to ge nerate polyphonic music pieces, considering grammar and\nlistener satisfaction.",
                "Keywords Automatic Music generation .Polyphonic Music pieces .Harmony",
                "&Considering both expert and ordinary listeners as separate objective functions.\n&Introducing a polyphonic music generation system, including the composition of melody,\nharmony, and rhythm in desired styles and lengths.\n&Modeling audience behavior in understanding the beauty of music by a Bi-LSTM model,and using it as an evaluation function.",
                "Fra nklin created a recurrent neural network with Long\nShort-Term Memory (LSTM) that generates melody or monophonic music pieces [ 13].\nHarmony or polyphonic music is a process of combining individual voices that are\nanalyzed by hearing them simultaneously.",
                "The purpose of usingharmony is to accompany the melodies by considering the relevant rules that lead to the\ncreation of polyphonic music.",
                "The objective function of this GA considers the rhythms and similaritybetween generated pieces and a standard database of polyphonic human-made pieces.",
                "At each iteration of\nGA1, the best chromosomes in the population are selected based on minimum violation of therules and the maximum similarity to a human-made polyphonic music database.",
                "82:2419\u201324355 Experimental results\nThe proposed system is implemented in MATLAB R2018b, and all the executions have been\ndone on a system with CPU Intel Core i7, 8 Gigabytes of RAM, and Windows10 O.S.\nTo provide the human-made pieces of music, we have used the Steirar database,1which\ncontains 235 polyphonic music pieces in ABC notation.",
                "In fact, the main criterion for evaluation is the correct generation of polyphonicmusic based on the rules and opinions of the listeners, and the outputs should be acceptable\nand pleasant to listeners.",
                "7 Conclusion\nIn this paper, we proposed an evolutionary method for the automatic generation of polyphonic\nmusic.",
                "In the proposed method, we first generated a collection of polyphonic music piecesusing a genetic algorithm and a database of human-made pieces as the objective function."
            ],
            "monophonic": [
                "Fra nklin created a recurrent neural network with Long\nShort-Term Memory (LSTM) that generates melody or monophonic music pieces [ 13].\nHarmony or polyphonic music is a process of combining individual voices that are\nanalyzed by hearing them simultaneously."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "Chorale\ngeneration is one of the most popular works of music generation in terms of harmony andproduces very structured music."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [
                "Improvising jazz chord sequences by means of formal grammars."
            ],
            "rock": [],
            "pop": [
                "In the meantime, music may be one of the popular options because of its ability to evoke emotions.https://doi.org/10.1007/s11042-022-13329-6\n*Maryam Majidi\nMry20mj@gmail.com\nRahil Mahdian Toroghi\nMahdian.t.r@gmail.com\n1Faculty of Media Technology and Engineering, Iran Broadcasting University, Tehran, IranPublished online: 25 June 2022Multimedia Tools and Applications (2023) 82:2419\u20132435Digital advances have also changed the shape of m usic composing.",
                "In the meantime, music may be one of the popular options because of its ability to evoke emotions.https://doi.org/10.1007/s11042-022-13329-6\n*Maryam Majidi\nMry20mj@gmail.com\nRahil Mahdian Toroghi\nMahdian.t.r@gmail.com\n1Faculty of Media Technology and Engineering, Iran Broadcasting University, Tehran, IranPublished online: 25 June 2022Multimedia Tools and Applications (2023) 82:2419\u20132435Digital advances have also changed the shape of m usic composing.",
                "AMG models can be classified into these groups: Markov model-based methods [ 3,6,9,\n24,28\u201330], approaches based on music rules and regulations [ 4,8,18,20], neural network-\nbased models [ 1,2,5,11,15,21,22,25,26,32,35], methods based on evolutionary\noptimization algorithms, and population-based [ 12,16,17,19,31,34], and algorithms based\non local search [ 7,14].",
                "AMG models can be classified into these groups: Markov model-based methods [ 3,6,9,\n24,28\u201330], approaches based on music rules and regulations [ 4,8,18,20], neural network-\nbased models [ 1,2,5,11,15,21,22,25,26,32,35], methods based on evolutionary\noptimization algorithms, and population-based [ 12,16,17,19,31,34], and algorithms based\non local search [ 7,14].",
                "Furthermore, the idea and\ninnovative scientific contribution of study [ 34], design a computer program called GenDash\nthat employs evolutionary computation in the composing music and the MetaCompose music\ngenerator uses a novel combinatorial evolutionary technique with Feasible-Infeasible Two-Population (FI-2POP) for effective melody generation [ 31].",
                "Furthermore, the idea and\ninnovative scientific contribution of study [ 34], design a computer program called GenDash\nthat employs evolutionary computation in the composing music and the MetaCompose music\ngenerator uses a novel combinatorial evolutionary technique with Feasible-Infeasible Two-Population (FI-2POP) for effective melody generation [ 31].",
                "More sophis-\nticated deep learning models such as recursive n eural networks have become popular.",
                "More sophis-\nticated deep learning models such as recursive n eural networks have become popular.",
                "Chorale\ngeneration is one of the most popular works of music generation in terms of harmony andproduces very structured music.",
                "Chorale\ngeneration is one of the most popular works of music generation in terms of harmony andproduces very structured music.",
                "Therefore, in the proposed method, because the population is enough, by carefully\nencoding the chromosomes, selecting an appropriate objective function and algorithm param-eters, the genetic algorithm will be a desirable option for the core of the proposed method.",
                "Therefore, in the proposed method, because the population is enough, by carefully\nencoding the chromosomes, selecting an appropriate objective function and algorithm param-eters, the genetic algorithm will be a desirable option for the core of the proposed method.",
                "Objective FunctionStart\nGenerating Initial \nPopulation randomly\nCalculating the Fitness \nof each music piece\nSelection\nCrossover\nMutation\nReplacementMusic Grammar\nBi-LSTM 1\nBi-LSTM 2\nEnd+\nConver ged?Music\nScore\nFig. 1",
                "Objective FunctionStart\nGenerating Initial \nPopulation randomly\nCalculating the Fitness \nof each music piece\nSelection\nCrossover\nMutation\nReplacementMusic Grammar\nBi-LSTM 1\nBi-LSTM 2\nEnd+\nConver ged?Music\nScore\nFig. 1",
                "At each iteration of\nGA1, the best chromosomes in the population are selected based on minimum violation of therules and the maximum similarity to a human-made polyphonic music database.",
                "At each iteration of\nGA1, the best chromosomes in the population are selected based on minimum violation of therules and the maximum similarity to a human-made polyphonic music database.",
                "GrammarGenetic 1Calculate the probability\nof occurrence of notes in the\nprimar ydata setGenerating initial\npopulation of chromosomes\nbased on probabilities\nCalculating the grammatical\nfitness of each chromosomesusing the objective functionSelection\nbased roulette\nwheel\nDatabase 1( e x p e r t\naudience) and\n2 (ordinary audience)ScoreMusic\nNeural Network\nMusic\nScoreComparison\nprocess\n+-\nTarget valuesMinimize\nerror and\nadjust weightsInput OutputAudience listening\nGenetic 2Calculate the probability\nof occurrence of notes in the\nprimar ydata setGenerating initial\npopulation of chromosomes\nbased on probabilitiesCalculating the fitness of\neach chromosomes using the\nobjective functionSelection\nbased roulette\nwheelCrossover\n&\nMutation+Objective FunctionCrossover\n&\nMutationReplacement\nOr\nEnd\n\u00d7wError\nReplacement\nOr\nEnd\nFig.",
                "GrammarGenetic 1Calculate the probability\nof occurrence of notes in the\nprimar ydata setGenerating initial\npopulation of chromosomes\nbased on probabilities\nCalculating the grammatical\nfitness of each chromosomesusing the objective functionSelection\nbased roulette\nwheel\nDatabase 1( e x p e r t\naudience) and\n2 (ordinary audience)ScoreMusic\nNeural Network\nMusic\nScoreComparison\nprocess\n+-\nTarget valuesMinimize\nerror and\nadjust weightsInput OutputAudience listening\nGenetic 2Calculate the probability\nof occurrence of notes in the\nprimar ydata setGenerating initial\npopulation of chromosomes\nbased on probabilitiesCalculating the fitness of\neach chromosomes using the\nobjective functionSelection\nbased roulette\nwheelCrossover\n&\nMutation+Objective FunctionCrossover\n&\nMutationReplacement\nOr\nEnd\n\u00d7wError\nReplacement\nOr\nEnd\nFig.",
                "In fact, the use of rules individuallyTable 1 Parameters Settings\nParameter GA 1 GA 2\nNumber of Iterations 3600 3600\nPopulation Size 15 15\nCrossover rate 0.5 0.5Mutation rate 0.1 0.1\nObjective Function Grammar Grammar & Human\n00.250.50.7511.25\n20 40 60 80 100GA 1 GA 2Music Generation TimeTime (Sec)\nNumber of  Notes\nFig.",
                "In fact, the use of rules individuallyTable 1 Parameters Settings\nParameter GA 1 GA 2\nNumber of Iterations 3600 3600\nPopulation Size 15 15\nCrossover rate 0.5 0.5Mutation rate 0.1 0.1\nObjective Function Grammar Grammar & Human\n00.250.50.7511.25\n20 40 60 80 100GA 1 GA 2Music Generation TimeTime (Sec)\nNumber of  Notes\nFig."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [],
            "techno": [
                "In the meantime, music may be one of the popular options because of its ability to evoke emotions.https://doi.org/10.1007/s11042-022-13329-6\n*Maryam Majidi\nMry20mj@gmail.com\nRahil Mahdian Toroghi\nMahdian.t.r@gmail.com\n1Faculty of Media Technology and Engineering, Iran Broadcasting University, Tehran, IranPublished online: 25 June 2022Multimedia Tools and Applications (2023) 82:2419\u20132435Digital advances have also changed the shape of m usic composing."
            ],
            "dubstep": [],
            "opera": [
                "In fact, the main question of the research is: Is it possible to create an A.I.-based system togenerate all the elements of music by using a mode led human scoring function in cooperation with\ngrammar?",
                "The pseudocode for the crossover is as follows,\nSTART\nPROGRAM Crossover Operator\nREAD best1 and best2Half a child is equal to half a best1\nHalf a child is equals to half a best2\nEND\nWhere best1 andbest2 are the selected chromosomes, and the child is a new chromosome\nafter crossover.",
                "The pseudocode for mutation is as follows,\nSTART\nPROGRAM Mutation Operator\nREAD child\nIF Mutation Rate is equal to 0.1\nFOR 1 through Number of channels\nFOR 1 through Number of child sizes\nIF the random number is less than the Mutation rate and child is\n.not equal to zero\nSET the child equal to zero\nEND IF\nIF the random number is less than the Mutation rate and child is equal to \n."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "A Style-Specific Music Composition Neural Network",
            "homophonic": [],
            "polyphonic": [
                "a polyphonic generation framework by combining RNN and general neural network.",
                "[ 20] presents MIDI-V AE, a multi-track polyphonic\nmusic generation model, using interpolation algorithm to automatically change the pitch,dynamics and instruments of musical works, to transform the style, and to verify the effectof style migration by selecting a proper classi\ufb01er.",
                "\u2013Musical texture The use of texture is \ufb02exible, and the main tone texture is combined\nwith the polyphonic texture.",
                "Imposing higher-level structure in polyphonic music generation\nusing convolutional restricted Boltzmann machines and constraints."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [],
            "choral": [
                "An expert system for harmonizing chorales in the style of J. S. Bach.",
                "Hadjeres G, Pachet F, Nielsen F (2017) Deepbach: a steerable model for bach chorales generation."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [
                "Styles include classical, pop, jazz, rock and R&B etc.",
                "Biles JA (1994) GenJam: a genetic algorithm for generating jazz solos."
            ],
            "rock": [
                "Styles include classical, pop, jazz, rock and R&B etc."
            ],
            "pop": [
                "Styles include classical, pop, jazz, rock and R&B etc.",
                "Styles include classical, pop, jazz, rock and R&B etc.",
                "The symbolic model trained and generated at the note level is currently apopular method.",
                "The symbolic model trained and generated at the note level is currently apopular method.",
                "[ 21] introduces MidiNet, which combines GAN network\nand CNN network to generate popular melody.",
                "[ 21] introduces MidiNet, which combines GAN network\nand CNN network to generate popular melody."
            ],
            "classical": [
                "Styles include classical, pop, jazz, rock and R&B etc.",
                "The so-called\u201cstyle-speci\ufb01c\u201d can be de\ufb01ned as a certain preset style such as classical music.",
                "The LSTM network is pre-trained by inputting the\nclassical piano database in MIDI format.",
                "In order to generate classical style\u2019s music, we choose several compositionprinciples from the common introduction to classical music composition.",
                "[ 37\u201339], we de\ufb01ne the reward function r\nmr(a,s)for\nthe following classical music\u2019s characteristics.",
                "We don\u2019t claim these characteristics are exhaustive, but adding the rules will guide our\nmodel towards traditional composition structure and make the generated music more struc-tural and more obvious in classical style.",
                "We employed the matched subset of the Lakh MIDI dataset (LMD) and Classical Piano MidiPage dataset as the training dataset, Lakh MIDI dataset [ 47] provides a rich collection of real-\nworld MIDI \ufb01les and some associated meta-data.",
                "Classical Piano Midi Page dataset collectedclassical music sets of composers from the eighteenth century to the nineteenth century,including works of Bach, Beethoven, Chopin and other 25 composers.",
                "In the experiment, weselect 2000 music samples of Classical Piano Midi Page dataset with speci\ufb01c composers\u2019styles in MIDI format [ 48] as test dataset, and each sample was a single orbital with an average\ntime of 2\u20134 min. To meet the requirements of the experiment, each sample was divided into20 s to obtain more than 20,000 classical music samples.",
                "In order to train the generation and the reward model, we utilize the preprocessed classicalpiano music as training data.",
                "In the experiment, we selected 500 training samples for training and 300 generated samples\nfor testing, and extracted 8 features of classical music as the basis for comparison, which are:range, repeated notes, vertical perfect fourths, rhythmic variability, parallel motion, verticaltritones, chord duration, number of pitches [ 39,47].",
                "When d<d0, it\nis determined that the generated sample is a classical music that meets the requirements, andlabeled as 1.",
                "Minimum distance classi\ufb01er (MDC) algorithm is used to judge whether the generated\nmusic is classical music style, and we use receiver operator characteristics (ROC) curveto judge the performance of different models.",
                "Through the establishment\nof an expert review panel, the performance of different models in generating classical musicwas evaluated from the following \ufb01ve aspects:\n1231908 C. Jin et al.\nFig. 8 ROC curve of style classi\ufb01er for 4 models\n\u2013Melody The melody of classical music sounds more balanced and symmetrical.",
                "\u2013Rhythm Classical style includes unexpected pauses, syncopation, and frequent conver-\nsion from long to short notes.",
                "\u2013Emotion The \ufb02uctuation and contrast of classical music\u2019s emotion are more obvious.",
                "The rewardfunction is utilized to optimize the probability distribution of music generating network inreal time, so as to generate smooth and beautiful classical music."
            ],
            "electronic": [
                "In: Proceedings of international conference on computer,communications and electronics, pp 501\u2013504\n40."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [
                "Styles include classical, pop, jazz, rock and R&B etc."
            ],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "\u03b3is the discount factor and Vis the state value function.",
                "The loss function of Qaction network and target network can be calculated as,\n\u03c3(\u03b8v)=E(s,a,r,s/prime)/bracketleftbig\n\u03bb/parenleftbig\nJ/parenleftbig\ns,a,\u03b8\u2212\nv/parenrightbig\n\u2212Q(s,a,\u03b8v)/bracketrightbig\n(8)\nwhere \u03b8\u2212\nvis target value network parameters, \u03bbis the discount factor of the value of the\nobjective function.",
                "(2)SeqGAN [23] The novel collision of adversarial network and reinforcement learning,\nwhich is devoted to generating discontinuous sequence, such as text sequence generation.",
                "Chiu S, Shan M (2006) Computer music composition based on discovered music patterns."
            ],
            "techno": [
                "Moreover, with the fast development of neural network, most of musicgeneration technologies are based on one model or multi-model network.",
                "In: Proceedings of international conference on computational intelligence and informationtechnology, pp 503\u2013507\n52.",
                "In: Proceedings of international conferenceon wireless technologies, embedded and intelligent systems, pp 1\u20134\n54."
            ],
            "dubstep": [],
            "opera": [
                "d e s\nthe music into \ufb01ve levels including passage, phrase, measure, beat and pixel when studyingmulti-track sequence music generation and automatic accompaniment, then generates it layerby layer when each track is relatively independent and cooperates with each other.",
                "Minimum distance classi\ufb01er (MDC) algorithm is used to judge whether the generated\nmusic is classical music style, and we use receiver operator characteristics (ROC) curveto judge the performance of different models."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Self-Supervised Music Motion Synchronization Learning for Music-Driven Conducting Motion Generation",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "[68]Lee K, Junokas M J, Amanzadeh M, Garnett G E. An ana-\nlysis of basic expressive qualities in instrumental conduct-\ning."
            ],
            "vocal": [],
            "choral": [],
            "orchestral": [
                "Although\nrecent studies have successfully generated motion for singers, dancers, and musicians, few have explored motion generation\nfor orchestral conductors.",
                "2 Related Work\n2.1 Music-Driven Conducting Motion\nGeneration\nMusic-driven conducting motion generation involves\nthe generation of skeleton sequences of an orchestral\nconductor according to a given piece of music.",
                "[25] Dansereau D G, Brock N, Cooperstock J R. Predicting\nan orchestral conductor's baton movements using machine\nlearning.",
                "[51] Huang Y, Chen T, Moran N, Coleman S, Su L. Identifying\nexpressive semantics in orchestral conducting kinematics."
            ],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [
                "[25] Dansereau D G, Brock N, Cooperstock J R. Predicting\nan orchestral conductor's baton movements using machine\nlearning."
            ],
            "pop": [
                "For its part, our proposed approach has two\nlearning stages: we \frst obtain an optimal M2S-Net,\nand then apply it to M2S-GAN.\n5.2 Sync Loss vs Perceptual Loss\nPerceptual loss[61]is a popular choice in many ill-\nposed image manipulation tasks.",
                "For its part, our proposed approach has two\nlearning stages: we \frst obtain an optimal M2S-Net,\nand then apply it to M2S-GAN.\n5.2 Sync Loss vs Perceptual Loss\nPerceptual loss[61]is a popular choice in many ill-\nposed image manipulation tasks.",
                "In a study of music-driven dance\ngeneration, Ren et al.[1]proposed pose perceptual loss,\nwhere a motion encoder pre-trained on dance genre\nclassi\fcation (distinguishing ballet, pop, and hip-hop\ndance) was used as the perceptual loss network.",
                "In a study of music-driven dance\ngeneration, Ren et al.[1]proposed pose perceptual loss,\nwhere a motion encoder pre-trained on dance genre\nclassi\fcation (distinguishing ballet, pop, and hip-hop\ndance) was used as the perceptual loss network."
            ],
            "classical": [
                "[48] Saras\u0013 ua \u0013A. Context-aware gesture recognition in classical\nmusic conducting."
            ],
            "electronic": [],
            "hip-hop": [
                "In a study of music-driven dance\ngeneration, Ren et al.[1]proposed pose perceptual loss,\nwhere a motion encoder pre-trained on dance genre\nclassi\fcation (distinguishing ballet, pop, and hip-hop\ndance) was used as the perceptual loss network."
            ],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [
                "Conductors, the soul of an orchestra, perform el-\negantly and charmingly in every concert."
            ],
            "funk": [],
            "disco": [
                "In-\nstead, by learning from massive amounts of training\ndata, it can discover the music-motion relationships on\nits own.",
                "Importantly, we discover that the\n\\easy negatives\" that work well in the video domain\nare not suitable for learning music motion synchroniza-\ntion."
            ],
            "techno": [
                "JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY 37(3): 539{558 May 2022.",
                "\u0003Corresponding Author\n\u00a9Institute of Computing Technology, Chinese Academy of Sciences 2022540 J. Comput.",
                "Sci. & Technol., May 2022, Vol.37, No.3\neach performance.",
                "Sci. & Technol., May 2022, Vol.37, No.3\nmusical gesture (dance and instrument playing) gene-\nration, which share numerous similarities with conduct-\ning motion generation.",
                "Sci. & Technol., May 2022, Vol.37, No.3\nbranch network and applying self-supervised learning,\nthe music encoder and the motion encoder can su-\npervise each other, and subsequently construct a joint\nfeature space that is simultaneously music-related and\nmotion-related.",
                "Sci. & Technol., May 2022, Vol.37, No.3\nlayer has 64 channels with a kernel size of 5.",
                "Sci. & Technol., May 2022, Vol.37, No.3\nvalidated in the \feld of computer vision, but it is rarely\nused in motion generation tasks.",
                "Sci. & Technol., May 2022, Vol.37, No.3\ntion is performed by stretching the motion data along\nthe time axis.",
                "Sci. & Technol., May 2022, Vol.37, No.3\nstrength contour, and SCE is used to compare strength\ncontours.",
                "Sci. & Technol., May 2022, Vol.37, No.3\ndemonstrate this point in \fgures.",
                "Sci. & Technol., May 2022, Vol.37, No.3\nrence on Multimedia , October 2020, pp.147-155.",
                "the 8th IEEE International\nConference on Advanced Learning Technologies , July 2008,\npp.897-899.",
                "the\n2nd International Conference on Intelligent Technologies\nfor Interactive Entertainment , January 2008, Article No.\n12.",
                "the 8th Interna-\ntional Conference on Collaboration Technologies and Social\nComputing , Sept. 2016, pp.45-57.",
                "Sci.\nTechnol. , 2017, 32(3): 480-493.",
                "Sci. & Technol., May 2022, Vol.37, No.3\n[60]Choi H, Park C, Lee K. From inference to generation: End-\nto-end fully self-supervised generation of human face from\nspeech.",
                "He received\nhis B.S. degree in networking and Ph.D.\ndegree in technology for computer\napplications from Nanjing University\nof Science and Technology (NUST),\nNanjing, in 2009 and 2015, respectively.\nFrom September 2008 to December 2008, he studied at\nAjou University, Suwon City.",
                "He is\ncurrently a research assistant in Hohai\nUniversity, Nanjing, and a research\nintern at MEGVII Technology, Beijing.",
                "She\nreceived her M.S. degree in control\ntheory and control engineering from\nSchool of Mechanical and Electrical\nEngineering, Jiangxi University of\nScience and Technology, Nanchang,\nin 2010, and her Ph.D. degree in\npattern recognition and intelligent system from School of\nComputer Science and Engineering, Nanjing University of\nScience and Technology, Nanjing, in 2015.",
                "He received his B.S. and M.S.\ndegrees in technology for computer\napplications from Hohai University,\nNanjing, in 1998 and 2001, respectively."
            ],
            "dubstep": [],
            "opera": [
                "Only one downsam-\nple operation with a factor of 3 is performed in the time\naxis so that the output music features have the same\nsampling rate as the motion features.",
                "The de\fnition of\nLM2S-Net is shown in (1); here, f[\u0001] represents the fusing\nlayers of M2S-Net,\bdenotes the feature concatenation\noperation, and cijis the label indicating whether Xi;Yj\nis a positive pair or a negative pair.",
                "the 20th\nIEEE International Conference on Computer Supported\nCooperative Work in Design , May 2016, pp.486-491.",
                "[17] Korbar B, Tran D, Torresani L. Cooperative learning of\naudio and video models from self-supervised synchroniza-\ntion."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Integration of a music generator and a song lyrics generator to create Spanish popular songs",
            "homophonic": [],
            "polyphonic": [],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [
                "Years of ethnomusicological education are often required \nto master the idiosincracies of the modal system and under -\nstand the vocal music in a popular context.",
                "Additionally, among the vocal songs generated, some \nwere selected for another human evaluation.",
                "On the other hand, we aim to validate the musical and lyrics results, meaning the vocal songs generated can follow the style of the Spanish popular\u00a0music."
            ],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "Therefore, folk songs in Spain are as varied as its regions.",
                "In order to obtain this information and preserve popular \nmusic, some ethnomusicologists have visited different geo-graphical zones, transcribing folk songs that people sing in rituals or traditional holidays.",
                "They have studied Ethnomusicology or Musicology, or are very familiar with the Spanish tradition due to their background (for example, teachers of traditional instru-ments, or musicians of Spanish folk music).",
                "Alter -\nnatively, we also contacted people that worked as a musi-cian of Spanish folk music or teachers that teaches Span-ish traditional instruments, and they are familiar with the Spanish tradition.",
                "However, our proposal includes important novelties, such as the use of Spanish language and the use of a new corpus of folk songs, unlike the rest of the works, which are centered in English classical and pop music.",
                "Lyrics and music generation X X X\nIncludes folk songs X \u2212 \u2212\nGenerating music before lyrics X \u2212 \u2212\nLearning Machines X X X\nUse of Markov models X \u2212 X\nInteraction with the users \u2212 \u2212 X\nUse of Spanish language X \u2212 \u22124431 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n5.",
                "El folklore musical en espa\u00f1a, hoy."
            ],
            "blues": [],
            "jazz": [
                "This kind of music has some particular features that makes them very different to other genres like classical or jazz music.",
                "However, they are usually \nthought to generate classical or jazz music automatically, and not as a guide to generate popular songs."
            ],
            "rock": [],
            "pop": [
                "Vol.:(0123456789)1 3Journal of Ambient Intelligence and Humanized Computing (2020) 11:4421\u20134437 \nhttps://doi.org/10.1007/s12652-020-01822-5\nORIGINAL RESEARCH\nIntegration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create \nSpanish popular songs\nMar\u00eda\u00a0Navarro\u2011C\u00e1ceres1 \u00a0\u00b7 Hugo\u00a0Gon\u00e7alo\u00a0Oliveira2\u00a0\u00b7 Pedro\u00a0Martins2\u00a0\u00b7 Am\u00edlcar\u00a0Cardoso2\nReceived: 7 December 2018 / Accepted: 19 February 2020 / Published online: 11 March 2020 \n\u00a9 Springer-Verlag GmbH Germany, part of Springer Nature 2020",
                "Vol.:(0123456789)1 3Journal of Ambient Intelligence and Humanized Computing (2020) 11:4421\u20134437 \nhttps://doi.org/10.1007/s12652-020-01822-5\nORIGINAL RESEARCH\nIntegration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create \nSpanish popular songs\nMar\u00eda\u00a0Navarro\u2011C\u00e1ceres1 \u00a0\u00b7 Hugo\u00a0Gon\u00e7alo\u00a0Oliveira2\u00a0\u00b7 Pedro\u00a0Martins2\u00a0\u00b7 Am\u00edlcar\u00a0Cardoso2\nReceived: 7 December 2018 / Accepted: 19 February 2020 / Published online: 11 March 2020 \n\u00a9 Springer-Verlag GmbH Germany, part of Springer Nature 2020",
                "This work develops ETHNO-MUSIC, an intelligent system that gen-erates melodies based on popular music.",
                "This work develops ETHNO-MUSIC, an intelligent system that gen-erates melodies based on popular music.",
                "ETHNO-MUSIC generates melodies with Markov models, which learns from a corpus of Spanish popular music.",
                "ETHNO-MUSIC generates melodies with Markov models, which learns from a corpus of Spanish popular music.",
                "Briefly, they reflect that, on the one hand, the melodies transmit a feeling of Spanish popular music, and on the other hand, the text of the lyrics is related to the topics analyzed, and the rhythm follows the melodic aspects of the music.",
                "Briefly, they reflect that, on the one hand, the melodies transmit a feeling of Spanish popular music, and on the other hand, the text of the lyrics is related to the topics analyzed, and the rhythm follows the melodic aspects of the music.",
                "Keywords Computational creativity\u00a0\u00b7 Music generation\u00a0\u00b7 Lyrics generation\n1 Introduction\nPopular music has had a great influence in society, since it \nhas been transmitted orally from generation to generation.",
                "Keywords Computational creativity\u00a0\u00b7 Music generation\u00a0\u00b7 Lyrics generation\n1 Introduction\nPopular music has had a great influence in society, since it \nhas been transmitted orally from generation to generation.",
                "Unlike classical music, Spanish popular music is always \nlinked to a functionality, meaning the purpose for which the melody was conceived.",
                "Unlike classical music, Spanish popular music is always \nlinked to a functionality, meaning the purpose for which the melody was conceived.",
                "Consequently, most of the existing repertoire include the use of lyrics, which in fact, is one of the most important factors in the popular culture.",
                "Consequently, most of the existing repertoire include the use of lyrics, which in fact, is one of the most important factors in the popular culture.",
                "Although the popular melody usually follows the lyrics structure and rhythm, it also hap-pens to the contrary, lyrics that are adapted to a melody cre-ated beforehand.",
                "Although the popular melody usually follows the lyrics structure and rhythm, it also hap-pens to the contrary, lyrics that are adapted to a melody cre-ated beforehand.",
                "This case is specially interesting to analyze what kind of words are chosen to be part of the lyrics, what topics are more common and how they adapt to the popular tone.",
                "This case is specially interesting to analyze what kind of words are chosen to be part of the lyrics, what topics are more common and how they adapt to the popular tone.",
                "Years of ethnomusicological education are often required \nto master the idiosincracies of the modal system and under -\nstand the vocal music in a popular context.",
                "Years of ethnomusicological education are often required \nto master the idiosincracies of the modal system and under -\nstand the vocal music in a popular context.",
                "Additionally, these proposals usually generate music following the lyrics structure, whereas in \n * Mar\u00eda Navarro-C\u00e1ceres \n maria90@usal.es\n1 Expert Systems and\u00a0Applications Laboratory (ESALab), \nDepartment of\u00a0Computer Sciences, Universidad de Salamanca, Salamanca, Spain\n2 CISUC, Department of\u00a0Informatics Engineering, University of\u00a0Coimbra, 3004-504\u00a0Coimbra, Portugal4422 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\nthis work we pretend to adapt lyrics once the melody is con-\nstructed and cannot be changed, as it commonly occurs in the popular tradition.",
                "Additionally, these proposals usually generate music following the lyrics structure, whereas in \n * Mar\u00eda Navarro-C\u00e1ceres \n maria90@usal.es\n1 Expert Systems and\u00a0Applications Laboratory (ESALab), \nDepartment of\u00a0Computer Sciences, Universidad de Salamanca, Salamanca, Spain\n2 CISUC, Department of\u00a0Informatics Engineering, University of\u00a0Coimbra, 3004-504\u00a0Coimbra, Portugal4422 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\nthis work we pretend to adapt lyrics once the melody is con-\nstructed and cannot be changed, as it commonly occurs in the popular tradition.",
                "Moreover, the initiatives have usually focused on the generation of music of a rather tonal/classi-cal character and, to date, there has been no relevant studies addressing the generation of Spanish popular music.",
                "Moreover, the initiatives have usually focused on the generation of music of a rather tonal/classi-cal character and, to date, there has been no relevant studies addressing the generation of Spanish popular music.",
                "For this purpose, we present a system capable to generate \nmusic and lyrics whithin a popular context.",
                "For this purpose, we present a system capable to generate \nmusic and lyrics whithin a popular context.",
                "The contribu-tion is precisely how the popular music is integrated in two systems and adapted to generate popular songs successfully.",
                "The contribu-tion is precisely how the popular music is integrated in two systems and adapted to generate popular songs successfully.",
                "For this purpose, new melodies are generated following the style of original Spanish popular songs.",
                "For this purpose, new melodies are generated following the style of original Spanish popular songs.",
                "MMs are trained in a \ncorpus and then used to generate a new melody that fits the style of popular songs.",
                "MMs are trained in a \ncorpus and then used to generate a new melody that fits the style of popular songs.",
                "Tra-la-lyrics has been adapted to the Spanish language \nthrough an augmented semantic network and new line tem-plates, collected automatically from the lyrics of Spanish popular songs.",
                "Tra-la-lyrics has been adapted to the Spanish language \nthrough an augmented semantic network and new line tem-plates, collected automatically from the lyrics of Spanish popular songs.",
                "To imitate this behavior and set the generation domain, seed words were carefully selected according to the common sets that appear in popular songs.",
                "To imitate this behavior and set the generation domain, seed words were carefully selected according to the common sets that appear in popular songs.",
                "Once the melodies are generated, a listening test was \ndeveloped to evaluate the musical quality according to the Spanish popular music standards and to collect the users\u2019 opinion about the usefulness of the system to interact with them and generate music.",
                "Once the melodies are generated, a listening test was \ndeveloped to evaluate the musical quality according to the Spanish popular music standards and to collect the users\u2019 opinion about the usefulness of the system to interact with them and generate music.",
                "The evaluators had to score on the one hand, the quality of the melody, sound and rhythm according to the Spanish popular music standards.",
                "The evaluators had to score on the one hand, the quality of the melody, sound and rhythm according to the Spanish popular music standards.",
                "Section\u00a0 3 describes the gen-\neration process to create popular melodies.",
                "Section\u00a0 3 describes the gen-\neration process to create popular melodies.",
                "However, they are usually \nthought to generate classical or jazz music automatically, and not as a guide to generate popular songs.",
                "However, they are usually \nthought to generate classical or jazz music automatically, and not as a guide to generate popular songs.",
                "Recently, com-panies as Google have been significantly developed projects 4423 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nsuch as Magenta (Project 2018), which uses convolutional \nneural networks to create melodies.",
                "Recently, com-panies as Google have been significantly developed projects 4423 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nsuch as Magenta (Project 2018), which uses convolutional \nneural networks to create melodies.",
                "Although other techniques could have been adopted for both music and lyrics generation, we decided to tackle this goal with two systems that were familiar to us \u2014 ETHNO-MUSIC and Tra-la-Lyrics\u00a02.0 \u2014 and focus on their integration for the generation of Spanish popular songs.",
                "Although other techniques could have been adopted for both music and lyrics generation, we decided to tackle this goal with two systems that were familiar to us \u2014 ETHNO-MUSIC and Tra-la-Lyrics\u00a02.0 \u2014 and focus on their integration for the generation of Spanish popular songs.",
                "Other reasons that lead to this decision include the nature of ETHNO-MUSIC, developed with Spanish popular music in mind, and the ease of integrating and adapting Tra-la-Lyrics\u00a02.0 to this domain, given that it is built on top of PoeTryMe, a flexible poetry generation platform, already adapted to many different sce-narios, including the generation in three languagues\u00a0(Gon-\u00e7alo\u00a0Oliveira et\u00a0al. 2017): Portuguese, Spanish and English.",
                "Other reasons that lead to this decision include the nature of ETHNO-MUSIC, developed with Spanish popular music in mind, and the ease of integrating and adapting Tra-la-Lyrics\u00a02.0 to this domain, given that it is built on top of PoeTryMe, a flexible poetry generation platform, already adapted to many different sce-narios, including the generation in three languagues\u00a0(Gon-\u00e7alo\u00a0Oliveira et\u00a0al. 2017): Portuguese, Spanish and English.",
                "3  System description\nFigure\u00a0 1 gives an overview of the overall system to create \nSpanish popular music.",
                "3  System description\nFigure\u00a0 1 gives an overview of the overall system to create \nSpanish popular music.",
                "The system that generates music, ETHNO-MUSIC, \nis provided with a memory to store beforehand different melodies in the popular music style.",
                "The system that generates music, ETHNO-MUSIC, \nis provided with a memory to store beforehand different melodies in the popular music style.",
                "These music files are retrieved from a wide variety of popular music melodies 4424 M.",
                "These music files are retrieved from a wide variety of popular music melodies 4424 M.",
                "The results of the \nanalysis are used as the training corpus by the Markov mod-els, which will generate music in the popular style, further described in Sect.\u00a0 3.3.\n3.1  Music retrieval\nThe resulting platform aims at the generation of music based on the features of the Spanish popular music, where, accord-ing to ethnomusicologists\u00a0(Manzano\u00a0Alonso 2001), three main factors should be considered:\n\u2013 Melodic behavior: Unlike the tonal music, which is based \nin minor and major modes, Spanish popular music makes use of seven modes, each built from the seven notes of the natural scale, starting from different notes.",
                "The results of the \nanalysis are used as the training corpus by the Markov mod-els, which will generate music in the popular style, further described in Sect.\u00a0 3.3.\n3.1  Music retrieval\nThe resulting platform aims at the generation of music based on the features of the Spanish popular music, where, accord-ing to ethnomusicologists\u00a0(Manzano\u00a0Alonso 2001), three main factors should be considered:\n\u2013 Melodic behavior: Unlike the tonal music, which is based \nin minor and major modes, Spanish popular music makes use of seven modes, each built from the seven notes of the natural scale, starting from different notes.",
                "\u2013 Rhythm: Popular music usually resorts to uniform beat due to the syllabic text used in the lyrics, or their func-tionality, created to dance or to follow rituals.",
                "\u2013 Rhythm: Popular music usually resorts to uniform beat due to the syllabic text used in the lyrics, or their func-tionality, created to dance or to follow rituals.",
                "Generally, it is very difficult to understand the specific fea-tures that some songs can share in popular music, since this type of music has been in constant evolution due to its char -\nacteristic grammar and its oral transmission from generation to generation.",
                "Generally, it is very difficult to understand the specific fea-tures that some songs can share in popular music, since this type of music has been in constant evolution due to its char -\nacteristic grammar and its oral transmission from generation to generation.",
                "In order to obtain this information and preserve popular \nmusic, some ethnomusicologists have visited different geo-graphical zones, transcribing folk songs that people sing in rituals or traditional holidays.",
                "In order to obtain this information and preserve popular \nmusic, some ethnomusicologists have visited different geo-graphical zones, transcribing folk songs that people sing in rituals or traditional holidays.",
                "By analyzing many of the melodies collected over the \nyears by different ethnomusicologists and in different songs, we can draw some conclusions in this regard that allow us to discern the popular music.",
                "By analyzing many of the melodies collected over the \nyears by different ethnomusicologists and in different songs, we can draw some conclusions in this regard that allow us to discern the popular music.",
                "According to some authors (Schindler 1991), in Spanish popular music, work, love \nsongs and lullabies share features related to the key sig-nature, rhythmic patterns and general sonority or tonality, which makes them very interesting to use as a corpus in the development of the learning model.",
                "According to some authors (Schindler 1991), in Spanish popular music, work, love \nsongs and lullabies share features related to the key sig-nature, rhythmic patterns and general sonority or tonality, which makes them very interesting to use as a corpus in the development of the learning model.",
                "But to identify the popular music, we should not only analyze the particular duration or pitch.",
                "But to identify the popular music, we should not only analyze the particular duration or pitch.",
                "Unlike classical music, in popular music the harmonic tension and the use of the chords degrees are not particularly relevant, as it does not follow harmonic rules; they are only used according to the melodic course.",
                "Unlike classical music, in popular music the harmonic tension and the use of the chords degrees are not particularly relevant, as it does not follow harmonic rules; they are only used according to the melodic course.",
                "Draw -\ning on these properties and also inspired by the concept of viewpoints exposed by Whorley et\u00a0al. (2013), the following features of the popular songs were selected:\n\u2013 Pitch: musical note.",
                "Draw -\ning on these properties and also inspired by the concept of viewpoints exposed by Whorley et\u00a0al. (2013), the following features of the popular songs were selected:\n\u2013 Pitch: musical note.",
                "MemoryRetrieve \nFeature s\nSongs\nMarkov \nModel \nTraining\nCorpus \nRetrieval\nGeneration \nProcess\nNew MelodyListening \nProcess\nInteraction \nProcess\nFig. 1  Overview of ETHNO-MUSIC4425 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n\u2013 Time signature: it is a rate that represents the time signa-\nture of the melody.",
                "MemoryRetrieve \nFeature s\nSongs\nMarkov \nModel \nTraining\nCorpus \nRetrieval\nGeneration \nProcess\nNew MelodyListening \nProcess\nInteraction \nProcess\nFig. 1  Overview of ETHNO-MUSIC4425 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n\u2013 Time signature: it is a rate that represents the time signa-\nture of the melody.",
                "In our case, the popular music does not have so many musical resources as classical music, for example, which makes the number of states of the MM to decrease.",
                "In our case, the popular music does not have so many musical resources as classical music, for example, which makes the number of states of the MM to decrease.",
                "Typically, musical and text phrases in Spanish popular \nmusic are quite short in order to make them easier for people to learn and start singing right away.",
                "Typically, musical and text phrases in Spanish popular \nmusic are quite short in order to make them easier for people to learn and start singing right away.",
                "The generation of popular Spanish songs results from the integration of two creative systems: ETHNO-MUSIC, in charge of generating melodies, and Tra-la-Lyrics\u00a0(Gon-\u00e7alo\u00a0Oliveira 2015), in charge of generating suitable lyrics.",
                "The generation of popular Spanish songs results from the integration of two creative systems: ETHNO-MUSIC, in charge of generating melodies, and Tra-la-Lyrics\u00a0(Gon-\u00e7alo\u00a0Oliveira 2015), in charge of generating suitable lyrics.",
                "As we described in the previous section, music is gener -\nated based on the features of Spanish popular music, which include the rhythm and the sonority.",
                "As we described in the previous section, music is gener -\nated based on the features of Spanish popular music, which include the rhythm and the sonority.",
                "Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n4.1.4  Line generation\nPoeTryMe has line generation modules for producing text \nfragments in Portuguese, English and Spanish, based on a semantic network and a grammar with templates, tightly connected to the relation types in the network.",
                "Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n4.1.4  Line generation\nPoeTryMe has line generation modules for producing text \nfragments in Portuguese, English and Spanish, based on a semantic network and a grammar with templates, tightly connected to the relation types in the network.",
                "Moreover, for the creation of the grammar, a set of about 280 Spanish popular song lyrics, transcribed for this purpose, was exploited, in addition to the 400 poems, thus increasing variation in the produced text.",
                "Moreover, for the creation of the grammar, a set of about 280 Spanish popular song lyrics, transcribed for this purpose, was exploited, in addition to the 400 poems, thus increasing variation in the produced text.",
                "To complete the adaptation, lyrics had to be generated with seed words related to concepts typically invoked in Spanish popular lyrics.",
                "To complete the adaptation, lyrics had to be generated with seed words related to concepts typically invoked in Spanish popular lyrics.",
                "On the other hand, we aim to validate the musical and lyrics results, meaning the vocal songs generated can follow the style of the Spanish popular\u00a0music.",
                "On the other hand, we aim to validate the musical and lyrics results, meaning the vocal songs generated can follow the style of the Spanish popular\u00a0music.",
                "Consequently, the musical results should also be acceptable for our potential users or listeners from a popu-lar musical point of view.",
                "Consequently, the musical results should also be acceptable for our potential users or listeners from a popu-lar musical point of view.",
                "L\u00f3pez-Ortega and L\u00f3pez-Popa (2012) discuss the quality of the system theoretically applying some creative concepts, such as deliberation and spontaneity.",
                "L\u00f3pez-Ortega and L\u00f3pez-Popa (2012) discuss the quality of the system theoretically applying some creative concepts, such as deliberation and spontaneity.",
                "For the generation of the music, 280 popular songs were \nselected.",
                "For the generation of the music, 280 popular songs were \nselected.",
                "In a second stage, we produced several songs to perform a listening test and retrieve a subjective evaluation of the popular music and the lyrics generated.",
                "In a second stage, we produced several songs to perform a listening test and retrieve a subjective evaluation of the popular music and the lyrics generated.",
                "Finally, in order to \nknow the popular features that they contain and how they were generated, some of the examples produced are shown and analyzed in Section\u00a0 5.3.\n5.1  Analyzing the\u00a0interaction with\u00a0the\u00a0user\nIn the original ETHNO-MUSIC, during the generation pro -\ncess, each iteration of the system consists of adding of a new note in the melody assisted by the mouse position and the MM, and it is iterated until the user decides to stop.",
                "Finally, in order to \nknow the popular features that they contain and how they were generated, some of the examples produced are shown and analyzed in Section\u00a0 5.3.\n5.1  Analyzing the\u00a0interaction with\u00a0the\u00a0user\nIn the original ETHNO-MUSIC, during the generation pro -\ncess, each iteration of the system consists of adding of a new note in the melody assisted by the mouse position and the MM, and it is iterated until the user decides to stop.",
                "In percentages, for this listening test we have a relative standard error of 21.18%, which can be considered admissible for such sub-jective tests.4429 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nThe listening tests always include some level of subjec-\ntivity, as usually depends on the culture or even the mood \nof the people involved.",
                "In percentages, for this listening test we have a relative standard error of 21.18%, which can be considered admissible for such sub-jective tests.4429 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nThe listening tests always include some level of subjec-\ntivity, as usually depends on the culture or even the mood \nof the people involved.",
                "To minimize the subjectivity of our evaluation, we looked for expert users in Spanish popular music.",
                "To minimize the subjectivity of our evaluation, we looked for expert users in Spanish popular music.",
                "These objec-tive features include similarity between the music generated and the Spanish popular music style, similarity of lyrics, or significance, always from the popular music perspectives.",
                "These objec-tive features include similarity between the music generated and the Spanish popular music style, similarity of lyrics, or significance, always from the popular music perspectives.",
                "These experts were asked if they think the melodies generated follows the standards of popular music according to the following items:\n\u2013 Melody: how pleasant is the melody?\n\u2013 Sound: how well does the melody, in some way, give a \nfeeling of the popular style of the songs?\n\u2013 Rhythm of the melody: how well does the rhythm suit the popular music style?",
                "These experts were asked if they think the melodies generated follows the standards of popular music according to the following items:\n\u2013 Melody: how pleasant is the melody?\n\u2013 Sound: how well does the melody, in some way, give a \nfeeling of the popular style of the songs?\n\u2013 Rhythm of the melody: how well does the rhythm suit the popular music style?",
                "The statistical analysis suggests that the sys-\ntem can generate melodies with a good musical quality and that captures the style of the Spanish popular melodies.",
                "The statistical analysis suggests that the sys-\ntem can generate melodies with a good musical quality and that captures the style of the Spanish popular melodies.",
                "The rhythm is quite diffi-cult to capture, even when the user tries to make changes throught the device, as the the melodies in popular music often use very regular figures and it is difficult to extract new ones.",
                "The rhythm is quite diffi-cult to capture, even when the user tries to make changes throught the device, as the the melodies in popular music often use very regular figures and it is difficult to extract new ones.",
                "5.2  Analyzing the\u00a0melody and\u00a0lyrics\nFor the generation of lyrics, melodies were split into phrases using \nminP=8 and maxP=16 , because Spanish popular \nsongs typically use lines of 8 or 16 syllables.",
                "5.2  Analyzing the\u00a0melody and\u00a0lyrics\nFor the generation of lyrics, melodies were split into phrases using \nminP=8 and maxP=16 , because Spanish popular \nsongs typically use lines of 8 or 16 syllables.",
                "Given their commonality among Spanish popular music, the domains of sleep \u00a0(common in lullabies) and love\u00a0(used in love songs) \nwere used for lyrics generation, and represented by the fol-lowing groups of seed words:\n\u2013 Sleep: dormir, cuna, beb\u00e9, pa\u00f1al, noche, mam\u00e1, coco, \nsue\u00f1o, so\u00f1ar, estrellas, luna (in English, to sleep, cradle, \nbaby, diaper, night, mummy, poo, dream, to dream, stars, moon);Table 1  Statistics resulting from \nthe listening test results/u1D7122 MeMo\nMelody 6.238e \u2212044 4\nSound 2.136e \u2212024 4\nRhythm 5.641e \u2212034 3Table 2  Final statistics after the users finished testing the system\nEasy to use Interface Control \nQualityOverall Ratings\nMode 4 3 4 4\nMedian 4 3 4 34430 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\n\u2013 Love: amor, novia, moza, mozo, bella, belleza, feliz, \nalcoba, morena, guapa, sonrisa, ojos, bonito, bonita  \n(in English, love, girlfriend, girl, lad, beautiful, beauty, \nhappy, bedroom, brunette, pretty, smile, eyes, pretty).",
                "Given their commonality among Spanish popular music, the domains of sleep \u00a0(common in lullabies) and love\u00a0(used in love songs) \nwere used for lyrics generation, and represented by the fol-lowing groups of seed words:\n\u2013 Sleep: dormir, cuna, beb\u00e9, pa\u00f1al, noche, mam\u00e1, coco, \nsue\u00f1o, so\u00f1ar, estrellas, luna (in English, to sleep, cradle, \nbaby, diaper, night, mummy, poo, dream, to dream, stars, moon);Table 1  Statistics resulting from \nthe listening test results/u1D7122 MeMo\nMelody 6.238e \u2212044 4\nSound 2.136e \u2212024 4\nRhythm 5.641e \u2212034 3Table 2  Final statistics after the users finished testing the system\nEasy to use Interface Control \nQualityOverall Ratings\nMode 4 3 4 4\nMedian 4 3 4 34430 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\n\u2013 Love: amor, novia, moza, mozo, bella, belleza, feliz, \nalcoba, morena, guapa, sonrisa, ojos, bonito, bonita  \n(in English, love, girlfriend, girl, lad, beautiful, beauty, \nhappy, bedroom, brunette, pretty, smile, eyes, pretty).",
                "However, our proposal includes important novelties, such as the use of Spanish language and the use of a new corpus of folk songs, unlike the rest of the works, which are centered in English classical and pop music.",
                "However, our proposal includes important novelties, such as the use of Spanish language and the use of a new corpus of folk songs, unlike the rest of the works, which are centered in English classical and pop music.",
                "Additionally, we have to take into account that the Mi mode is more frequent than La mode in Spanish popular music.",
                "Additionally, we have to take into account that the Mi mode is more frequent than La mode in Spanish popular music.",
                "Among the possible options, Spanish popular music commonly has times of 6/8, 4/4, 3/4 and 2/4.",
                "Among the possible options, Spanish popular music commonly has times of 6/8, 4/4, 3/4 and 2/4.",
                "A total of 17 users with musical knowledge about popular \nmusic (more than 4 years of experience in popular music, or students of Musicology) were asked to answer and online form with questions about the melodies generated and how they follow the standards of popular music according to the sonority and the rhythm.",
                "A total of 17 users with musical knowledge about popular \nmusic (more than 4 years of experience in popular music, or students of Musicology) were asked to answer and online form with questions about the melodies generated and how they follow the standards of popular music according to the sonority and the rhythm.",
                "Sound: how well does the melody, in some way, give a \nfeeling of the popular style of the songs?",
                "Sound: how well does the melody, in some way, give a \nfeeling of the popular style of the songs?",
                "Rhythm of the melody: how well does the rhythm suit the popular music style?\n4.",
                "Rhythm of the melody: how well does the rhythm suit the popular music style?\n4.",
                "Lyrics and music generation X X X\nIncludes folk songs X \u2212 \u2212\nGenerating music before lyrics X \u2212 \u2212\nLearning Machines X X X\nUse of Markov models X \u2212 X\nInteraction with the users \u2212 \u2212 X\nUse of Spanish language X \u2212 \u22124431 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n5.",
                "Lyrics and music generation X X X\nIncludes folk songs X \u2212 \u2212\nGenerating music before lyrics X \u2212 \u2212\nLearning Machines X X X\nUse of Markov models X \u2212 X\nInteraction with the users \u2212 \u2212 X\nUse of Spanish language X \u2212 \u22124431 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n5.",
                "We expect the system to reflect the perceptual quality of \nthe melodies according to the popular songs style.",
                "We expect the system to reflect the perceptual quality of \nthe melodies according to the popular songs style.",
                "The 60% of the Fig. 5  Screenshot of the valida-\ntion form to analyze melody \nand lyrics\n4432 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\nusers considered that the music adapts to the popular music \nstandards well or very well, as does the melodic rhythm, where the 54% of the users evaluated this item as good or very good.",
                "The 60% of the Fig. 5  Screenshot of the valida-\ntion form to analyze melody \nand lyrics\n4432 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\nusers considered that the music adapts to the popular music \nstandards well or very well, as does the melodic rhythm, where the 54% of the users evaluated this item as good or very good.",
                "We selected those ones that present different popular features, like different time signatures, scales and lyrics.",
                "We selected those ones that present different popular features, like different time signatures, scales and lyrics.",
                "All the melodies generated share representative features of popular music, such as the constant repetition of pitches and the limited tes-situra.",
                "All the melodies generated share representative features of popular music, such as the constant repetition of pitches and the limited tes-situra.",
                "The melody represents a good example of popular music, as the median and modes obtained in the listening test of 4, even if it is not very pleasant according to the evalua-tions, with a mode and a median of 3.\nLyrics for this song were generated with the sleep-related \nseeds, which is clear by the presence of words like cuna\u00a0(cra-dle), dormir\u00a0(to sleep), sue\u00f1o\u00a0(dream) or so\u00f1ar\u00a0(to dream), \nfrom the seed set, and also other related words, such as som-noliento\u00a0(sleepy), dormido\u00a0(asleep), durmiendo\u00a0(sleeping), \nor sentir \u00a0(to feel, an hypernym of to dream).",
                "The melody represents a good example of popular music, as the median and modes obtained in the listening test of 4, even if it is not very pleasant according to the evalua-tions, with a mode and a median of 3.\nLyrics for this song were generated with the sleep-related \nseeds, which is clear by the presence of words like cuna\u00a0(cra-dle), dormir\u00a0(to sleep), sue\u00f1o\u00a0(dream) or so\u00f1ar\u00a0(to dream), \nfrom the seed set, and also other related words, such as som-noliento\u00a0(sleepy), dormido\u00a0(asleep), durmiendo\u00a0(sleeping), \nor sentir \u00a0(to feel, an hypernym of to dream).",
                "Exceptions occur in the word som-no-lien-to and in one of the occurrences of so-\u00f1ar, for which the two last Table 4  Overall validation \nresults for the 10 assessed songsItem Rating Mo Md\n1 2 3 4 5\nMelody 0 13 42 47 18 4 4\nSound 3 4 41 59 13 4 4\nRhythm (Melody) 0 8 47 52 13 4 4\nRhythm (Lyrics) 2 8 45 41 24 3 4\nSubject 5 12 25 53 24 4 4\nText Meaning 20 6 32 50 11 4 4\nOverall Quality 1 14 52 41 12 3 3\nTable 5  Validation of the lyrics for the 10 assessed songs, according \nto the semantic domain\nItem Sleep Love\nMo Md Mo Md\nRhythm (lyrics) 3 3 4 4\nSubject 4 4 4 4\nText meaning 4 4 4 4\nOverall quality 3 3 3 34433 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nFig.",
                "Exceptions occur in the word som-no-lien-to and in one of the occurrences of so-\u00f1ar, for which the two last Table 4  Overall validation \nresults for the 10 assessed songsItem Rating Mo Md\n1 2 3 4 5\nMelody 0 13 42 47 18 4 4\nSound 3 4 41 59 13 4 4\nRhythm (Melody) 0 8 47 52 13 4 4\nRhythm (Lyrics) 2 8 45 41 24 3 4\nSubject 5 12 25 53 24 4 4\nText Meaning 20 6 32 50 11 4 4\nOverall Quality 1 14 52 41 12 3 3\nTable 5  Validation of the lyrics for the 10 assessed songs, according \nto the semantic domain\nItem Sleep Love\nMo Md Mo Md\nRhythm (lyrics) 3 3 4 4\nSubject 4 4 4 4\nText meaning 4 4 4 4\nOverall quality 3 3 3 34433 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nFig.",
                "However, the melody behaves like the popular music, with rests in the second and third degrees, a typical feature of modal (and popular) music, in this case, of the Mi Mode.",
                "However, the melody behaves like the popular music, with rests in the second and third degrees, a typical feature of modal (and popular) music, in this case, of the Mi Mode.",
                "This happens for a series of reasons, including that this pattern has eight Fig. 8  Song in La mode, with \nSpanish lyrics on the domain \n\u2018amor\u2019\u00a0(love), including rough English translation\nyall\u00b4\u0131triste cantaba, disc\u00b4\u0131puloys eguido r\nnos\u00b4equ\u00b4eama rte, amo r\nyall\u00b4\u0131triste cantaba, disc\u00b4\u0131puloys eguido r\natusarasyalta r\namant eyamo rand there wassadly singin g\ndisciple and follo wer\nand there wassadly singin g\ndisciple and follo wer\ntoyouraltarsand altar\nlover and love\n4435 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nsyllables, a common length for musical phrases in Spanish \npopular music and half the value of minP, which becomes closer to minP with its variable part\u00a0(in this case, moza y menor ); and it can be used for three different semantic \nrelations.",
                "This happens for a series of reasons, including that this pattern has eight Fig. 8  Song in La mode, with \nSpanish lyrics on the domain \n\u2018amor\u2019\u00a0(love), including rough English translation\nyall\u00b4\u0131triste cantaba, disc\u00b4\u0131puloys eguido r\nnos\u00b4equ\u00b4eama rte, amo r\nyall\u00b4\u0131triste cantaba, disc\u00b4\u0131puloys eguido r\natusarasyalta r\namant eyamo rand there wassadly singin g\ndisciple and follo wer\nand there wassadly singin g\ndisciple and follo wer\ntoyouraltarsand altar\nlover and love\n4435 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nsyllables, a common length for musical phrases in Spanish \npopular music and half the value of minP, which becomes closer to minP with its variable part\u00a0(in this case, moza y menor ); and it can be used for three different semantic \nrelations.",
                "Additionally, it presents an integration of ETHNO-MUSIC and Tra-La-Lyrics to add lyrics to the melodies also following the style of the Spanish popular music.",
                "Additionally, it presents an integration of ETHNO-MUSIC and Tra-La-Lyrics to add lyrics to the melodies also following the style of the Spanish popular music.",
                "However, despite the users\u2019 indications, we do not avoid to follow the standards of the popular music in the generation process.",
                "However, despite the users\u2019 indications, we do not avoid to follow the standards of the popular music in the generation process.",
                "Evaluation aimed to demonstrate that both the melody \nand the lyrics share common features with the original Span-ish popular music.",
                "Evaluation aimed to demonstrate that both the melody \nand the lyrics share common features with the original Span-ish popular music.",
                "Although there are some open issues in current state of \nthe art, the main goal of this work was the application of different techniques to create popular songs, which is a nov -\nelty by itself.",
                "Although there are some open issues in current state of \nthe art, the main goal of this work was the application of different techniques to create popular songs, which is a nov -\nelty by itself.",
                "Finally, the use of MMs as a complement to help the users through a musical composition has been scored as quite interesting, as the users feel included in the composition process, and they are an essential part to create new melodies, preserving the Spanish popular music style.",
                "Finally, the use of MMs as a complement to help the users through a musical composition has been scored as quite interesting, as the users feel included in the composition process, and they are an essential part to create new melodies, preserving the Spanish popular music style.",
                "Given the importance of lyrics in popular music for eth-\nnomusicologist studies, a deeper analysis of popular songs and their lyrics should be addressed in order to improve the vocabulary and semantics of the lyrics generator.",
                "Given the importance of lyrics in popular music for eth-\nnomusicologist studies, a deeper analysis of popular songs and their lyrics should be addressed in order to improve the vocabulary and semantics of the lyrics generator.",
                "The MIT Press, Cambridge\nL\u00f3pez-Ortega O, L\u00f3pez-Popa SI (2012) Fractals, fuzzy logic and \nexpert systems to assist in the construction of musical pieces.",
                "The MIT Press, Cambridge\nL\u00f3pez-Ortega O, L\u00f3pez-Popa SI (2012) Fractals, fuzzy logic and \nexpert systems to assist in the construction of musical pieces.",
                "Bolet\u00edn \nInformativo de la Fundaci\u00f3n Juan March 204:3\u201318\nManzano\u00a0Alonso M (2001) Cancionero popular de burgos.",
                "Bolet\u00edn \nInformativo de la Fundaci\u00f3n Juan March 204:3\u201318\nManzano\u00a0Alonso M (2001) Cancionero popular de burgos.",
                "M\u00fasica y poes\u00eda popular de Espa\u00f1a y Portugal.",
                "M\u00fasica y poes\u00eda popular de Espa\u00f1a y Portugal.",
                "Centro de Cultura Tradicional4437 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nScirea M, Togelius J, Eklund P, Risi S (2016)",
                "Centro de Cultura Tradicional4437 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nScirea M, Togelius J, Eklund P, Risi S (2016)",
                "Xiaoice band: A melody and arrangement genera-tion framework for pop music.",
                "Xiaoice band: A melody and arrangement genera-tion framework for pop music."
            ],
            "classical": [
                "However, most works are centered in classical music.",
                "This kind of music has some particular features that makes them very different to other genres like classical or jazz music.",
                "They are commonly composed of melodies without musi-cal accompaniment, with complex rhythms and uncommon scales (from a classical perspective), rich in ornaments.",
                "Unlike classical music, Spanish popular music is always \nlinked to a functionality, meaning the purpose for which the melody was conceived.",
                "As we mentioned, this genre of music differs from the classical music in many aspects, including the sonority, the sounds disposition or the rhythmic formulas used.",
                "However, they are usually \nthought to generate classical or jazz music automatically, and not as a guide to generate popular songs.",
                "Unlike classical music, in popular music the harmonic tension and the use of the chords degrees are not particularly relevant, as it does not follow harmonic rules; they are only used according to the melodic course.",
                "In our case, the popular music does not have so many musical resources as classical music, for example, which makes the number of states of the MM to decrease.",
                "However, our proposal includes important novelties, such as the use of Spanish language and the use of a new corpus of folk songs, unlike the rest of the works, which are centered in English classical and pop music."
            ],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "Therefore, alternative strategies could be tested for matching the rhythm."
            ],
            "k-pop": [],
            "ambient": [
                "Vol.:(0123456789)1 3Journal of Ambient Intelligence and Humanized Computing (2020) 11:4421\u20134437 \nhttps://doi.org/10.1007/s12652-020-01822-5\nORIGINAL RESEARCH\nIntegration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create \nSpanish popular songs\nMar\u00eda\u00a0Navarro\u2011C\u00e1ceres1 \u00a0\u00b7 Hugo\u00a0Gon\u00e7alo\u00a0Oliveira2\u00a0\u00b7 Pedro\u00a0Martins2\u00a0\u00b7 Am\u00edlcar\u00a0Cardoso2\nReceived: 7 December 2018 / Accepted: 19 February 2020 / Published online: 11 March 2020 \n\u00a9 Springer-Verlag GmbH Germany, part of Springer Nature 2020"
            ],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Particle swarm optimization for time series \nmotif discovery.",
                "In: Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, ACM, pp 2837\u20132846\nZnidarsic M, Cardoso A, Gerv\u00e1s P, Martins P, Herv\u00e1s R, Alves AO, \nGon\u00e7alo\u00a0Oliveira H, Xiao P, Linkola S, Toivonen H, Kranjc J, Lavrac N (2016) Computational creativity infrastructure for online software composition: a conceptual blending use case."
            ],
            "techno": [
                "Encyclopedia of Information Science and Technology.",
                "In: Proceedings of the 2019 4th \ninternational conference on intelligent information technology, ACM, New York, NY, USA, ICIIT \u201919, pp 96\u2013100, https ://doi.org/10.1145/33214 54.33214 70,\nSpeer R, Chin J, Havasi C (2017) Conceptnet 5.5: An open multilin-\ngual graph of general knowledge.",
                "In: Proceedings of the 2018 conference of the North American chapter of the association for computational Linguistics: human language technologies, Volume 1 (Long Papers), ACL Press, pp 163\u2013172, https ://doi.org/10.18653 /v1/N18-1015, http://aclwe  \nb.org/antho logy/N18-1015\nWhorley RP, Conklin D (2016) Music generation from statistical mod -\nels of harmony."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
            "homophonic": [],
            "polyphonic": [
                "ReghunathandRajan EURASIPJournalonAudio,Speech,andMusic\nProcessing         (2022) 2022:11 \nhttps://doi.org/10.1186/s13636-022-00245-8\nEMPIRICAL RESEARCH OpenAccess\nTransformer-basedensemblemethod\nformultiplepredominantinstruments\nrecognitioninpolyphonicmusic\nLekshmiChandrikaReghunath*and RajeevRajan\nAbstract\nMultiplepredominantinstrumentrecognitioninpolyphonicmusicisaddressedusingdecisionlevelfusionofthree\ntransformer-basedarchitecturesonanensembleofvisualrepresentations.",
                "Thearchitecturalchoiceoftransformerswithensemblevotingon\nMel-spectro-/modgd-/tempogramhasmeritinrecognizingthepredominantinstrumentsinpolyphonicmusic.",
                "Non-\nnegative matrix factorization (NMF) model [ 6], end-to-\nend model [ 7], fusion model with spectral, temporal,\nand modulation features [ 8] can be referred to as initial\nattempts for the proposed task in a polyphonic environ-\nment.",
                "More recent works deal with instrument recogni-\ntion in polyphonic music, which is a more demanding\nand challenging problem.",
                "Both approaches were trained\nand validated by the IRMAS dataset of polyphonic music\nexcerpts.",
                "Furthermore,\nit was shown that even for shorter windows, the phase\nspectrum could contribute as much as the magnitude\nspectrumtospeechintelligibility[ 20].Inourwork,weare\nintroducingphase-basedmodgdgramasacomplementary\nfeature to magnitude-based spectrogram in recognizing\npredominant instruments from a polyphonic environ-\nment.",
                "In our work, we utilize transformer architec-\ntures to learn instrument-specific characteristics using\nMel-spectro-/modgd-/tempogram to estimate predomi-\nnant instruments from polyphonic music.",
                "Theproposedensemble\nvotingtechniquemakesuseofthepotentialofthree\nvisualrepresentationsinmakingafinaldecisionon\nrecognizingpredominantinstrumentsina\npolyphonicenvironment.",
                "In the proposed model, transform-\ners are used to learn the distinctive characteristics of\nMel-spectro/modgd/tempo-gram to identify the leadinginstrumentinapolyphoniccontext.",
                "5.3.2 Testingconfiguration\n2874 polyphonic files of variable length with multiple\npredominant instruments are used for the testing phase.",
                "The results show the potential of the ensemble vot-\ning technique in predominant instrument recognition in\npolyphonicmusic.",
                "Y.Han,J.Kim,K.Lee,Deepconvolutionalneuralnetworksfor\npredominantinstrumentrecognitioninpolyphonicmusic.",
                "F.Fuhrmann,P.Herrera,in Proc.of13thInternationalConferenceonDigital\nAudioEffects(DAFx10)Graz,Austria,September6-10,2010 .Polyphonic\ninstrumentrecognitionforexploringsemanticsimilaritiesinmusic,\n(2010),pp.1\u20138\n3. J.-Y.Liu,Y.-H.Yang,in Proc.ofthe24thACMMultimediaConference\nAmsterdam,NetherlandsOctober15-19,2016 .Eventlocalizationinmusic\nauto-tagging(AssociationforComputingMachinery,NewYork,2016),\npp.1048\u20131057\n4.",
                ",NewPaltz,NY,2003 .Non-negativematrix\nfactorizationforpolyphonicmusictranscription,(2003),pp.177\u2013180\n7. P.Li,J.Qian,T.Wang,Automaticinstrumentrecognitioninpolyphonic\nmusicusingconvolutionalneuralnetworks.arXivpreprint\narXiv:1511.05520(2015)\n8.",
                "T.Kitahara,M.Goto,K.Komatani,T.Ogata,H.G.Okuno,Instrument\nidentificationinpolyphonicmusic:Featureweightingtominimize\ninfluenceofsoundoverlaps.EURASIPJ.Adv.",
                "Instrumentactivitydetectioninpolyphonicmusicusingdeepneural\nnetworks,(2018),pp.569\u2013576.",
                "F.Fuhrmann,etal., Automaticmusicalinstrumentrecognitionfrom\npolyphonicmusicaudiosignals .(PhDthesis,UniversitatPompeuFabra,\n2012)\n18.",
                "A.Kratimenos,K.Avramidis,C.Garoufis,A.Zlatintsi,P.Maragos,in Proc.of\n28thEuropeanSignalProcessingConference(EUSIPCO2020),Virtual,January\n18-22,2021 .Augmentationmethodsonmonophonicaudiofor\ninstrumentclassificationinpolyphonicmusic,(2021),pp.156\u2013160."
            ],
            "monophonic": [
                "A.Kratimenos,K.Avramidis,C.Garoufis,A.Zlatintsi,P.Maragos,in Proc.of\n28thEuropeanSignalProcessingConference(EUSIPCO2020),Virtual,January\n18-22,2021 .Augmentationmethodsonmonophonicaudiofor\ninstrumentclassificationinpolyphonicmusic,(2021),pp.156\u2013160."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [
                "In music, the body of the musical\ninstrument is the counterpart of the vocal tract (system)\nin speech."
            ],
            "choral": [],
            "orchestral": [
                "*Correspondence: clekshmir04@gmail.com\nDepartmentofElectronicsandCommunicationEngineering,Collegeof\nEngineeringTrivandrum,APJAbdulKalamTechnologicalUniversity,\nTrivandrum,IndiaThe task of identifying the leading instrument in poly-\nphonic music is challenging due to the presence of inter-\nferingpartialsintheorchestralbackground."
            ],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [
                "J.S.G\u2019omez,J.Abe\u00dfer,E.Cano,in Proc.ofthe19thInternationalSocietyfor\nMusicInformationRetrievalConference,ISMIR,Paris,FranceSeptember23-27,\n2018.Jazzsoloinstrumentclassificationwithconvolutionalneural\nnetworks,sourceseparation,andtransferlearning,(2018),pp.577\u2013584.\nhttps://doi.org/10.5281/zenodo.1492481\n14. X.Li,K.Wang,J.Soraghan,J.Ren,in ProcofInternationalConferenceon\nComputationalIntelligenceinMusicSoundArtandDesign(PartofEvoStar) ."
            ],
            "rock": [],
            "pop": [],
            "classical": [],
            "electronic": [
                "*Correspondence: clekshmir04@gmail.com\nDepartmentofElectronicsandCommunicationEngineering,Collegeof\nEngineeringTrivandrum,APJAbdulKalamTechnologicalUniversity,\nTrivandrum,IndiaThe task of identifying the leading instrument in poly-\nphonic music is challenging due to the presence of inter-\nferingpartialsintheorchestralbackground."
            ],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [],
            "funk": [],
            "disco": [
                "Learningtodiscovercross-domainrelationswithgenerativeadversarial\nnetworks,vol.70(PMLR,2017),pp.1857\u20131865."
            ],
            "techno": [
                "*Correspondence: clekshmir04@gmail.com\nDepartmentofElectronicsandCommunicationEngineering,Collegeof\nEngineeringTrivandrum,APJAbdulKalamTechnologicalUniversity,\nTrivandrum,IndiaThe task of identifying the leading instrument in poly-\nphonic music is challenging due to the presence of inter-\nferingpartialsintheorchestralbackground.",
                "5 Performanceevaluation\n5.1 Dataset\nThe performance of the proposed system is evaluated\nusing the IRMAS (Instrument Recognition in Musical\nAudio Signals) dataset, developed by the Music Technol-\nogy Group (MTG) of Universitat Pompeu Fabra (UPF).",
                "Acknowledgements\nTheauthorswouldliketoacknowledgeJuanJ.Bosch,FerdinandFuhrmann,\nandPerfectoHerrera(MusicTechnologyGroup-UniversitatPompeuFabra)\nfordevelopingtheIRMASdatasetandmakingitpubliclyavailable.",
                "H.A.Murthy,B.Yegnanarayana,Groupdelayfunctionsandits\napplicationsinspeechtechnology.",
                "Eng.Inf.Technol.",
                "R.Rajan, EstimationofPitchinSpeechandMusicUsingModifiedGroupdelay\nFunctions.(Ph.D.thesis,SubmittedtoIndianInstituteofTechnology,\nMadras,2017)."
            ],
            "dubstep": [],
            "opera": [
                "In WaveGAN architecture, the transposed convolution\noperation is modified to widen its receptive field.",
                "To ensure that the discrimi-\nnator does not learn these artifacts, we use phase shuffle\noperation(withhyperparametern=2)assuggestedin[ 43].",
                "ReLUisusedastheactivationfortransposedconvolution\nlayers and LReLU with \u03b1= 0.2 is chosen for convolution\noperation."
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "Genre Recognition from Symbolic Music with CNNs: Performance and Explainability",
            "homophonic": [],
            "polyphonic": [
                "The sequence of vectors X  thus represents a \npolyphonic piece of music in the form of a pianoroll or a \nMIDI file.",
                "Algorithms for dis-\ncovering repeated patterns in multidimensional representa-\ntions of polyphonic music."
            ],
            "monophonic": [
                "Rizo et\u00a0al. propose a non-\nlinear representation of a melody based on trees and they \nstudy the influence of different tree representations on clas-\nsification rates in three corpora with monophonic melodies, \nconcluding that tree coding gives better results [ 35]."
            ],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [
                "The way multi-track MIDI files are handled is by averag-\ning the pianorolls of all instrumental tracks and all percus-\nsive tracks separately into two cumulative pianorolls."
            ],
            "vocal": [
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "We chose (b) and (c) as \nexamples of certain predictions, while (d) shows an errone-\nous prediction by the CNN (Electronic), and a relatively high \nvalue of 0.06 for the vocal genre which is interesting since \nthe introduction of the song features an a capella chorus.",
                "Jazz 0.02 Vocal 0.06\nPop\u2013Rock 0.24 Country 0.037 Vocal 0.003 RnB 0.04\n2 https:// www. reddit.",
                "For instance, the choice of \nQueen\u2014We Will Rock You as a prototype for Rap  could be \ndue to the rhythmic qualities of the vocal track, the loop-\ning music, and the repeating patterns which are prevalent \nin a lot of different music, including Rap .",
                "By Me (Pop-Rock, Electronic, Rap) Bobby Brown - Don\u2019t Be Cruel (Pop-Rock, Jazz)\nVocal Nana Mouskouri - Habanera (International, Vocal)",
                "Salvatore Licitra - E Lucevan Le Stelle (Vocal)\nNew Age Cock Robin -",
                "In parentheses are the ground-truth labels\nGenre Black-box prototype Black-box criticism\nPop-Rock Abba - The Winner Takes It All (Pop-Rock, Vocal)",
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)"
            ],
            "choral": [],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [
                "On the one hand, the effect of the imbal-\nanced dataset is apparent in the network\u2019s performance for \nthe most common label (Pop-Rock) when compared to those \nwith fewer files in the dataset such as Blues, Reggae, and \nFolk.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "Folk Judy Collins - Send In The Clowns (Folk)",
                "Edison Lighthouse - Love Grows (Folk)",
                "Folk The Roches - Do You Hear What I Hear?",
                "(Folk)",
                "The Roches - It Came Upon A Midnight Clear (Folk)"
            ],
            "blues": [
                "On the one hand, the effect of the imbal-\nanced dataset is apparent in the network\u2019s performance for \nthe most common label (Pop-Rock) when compared to those \nwith fewer files in the dataset such as Blues, Reggae, and \nFolk.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "Blues Deborah Coleman - Long Time (Pop-Rock, Blues)",
                "Jim Reeves - I Won\u2019t Forget You (Country, Blues)\nTable 6  Prototypes and Criticisms for each genre, generated by MMD-critic on the positive examples as predicted by the black box, on the \ntopMAGD test set.",
                "For Your Precious Love (Pop-Rock, RnB, \nReggae)\nBlues Bill Quinn - He\u2019ll Have To Go (Country, Blues) Jim Reeves - He\u2019ll Have To Go (Country) SN Computer Science (2023) 4:106\n 106 Page 16 of 18\nSN Computer Science\nDiscussion\nIn this section, we showed how various explainability meth-\nods may be used in the context of symbolic music classifi-\ncation."
            ],
            "jazz": [
                "It is interesting that genres such as Jazz which have lit-\ntle representation in the dataset are better classified than gen-\nres such as Electronic which has almost double the support.",
                "This could be due to distinguishing musical characteristics \nof each genre, which are apparent in symbolic representa-\ntions of music\u2014for instance, jazz music tends to have com-\nplex harmony and utilize more notes, while electronic music \ntends to contain loops of very few notes.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "For Here comes the Sun the first quarter of the \npianoroll seems to contribute more towards a Jazz predic-\ntion, while the third quarter contributes towards a Pop-Rock  \nprediction.",
                "0.40 Jazz 0.06 Electronic 0.03 Pop\u2013Rock 0.20\nRap 0.25 RnB 0.04",
                "Jazz 0.02 Vocal 0.06\nPop\u2013Rock 0.24 Country 0.037 Vocal 0.003 RnB 0.04\n2 https:// www. reddit.",
                "A different repetition of the \nsame notes has been highlighted as contributing towards Jazz.",
                "Tina Turner - Steamy Windows (RnB)\nJazz Lee Ritenour - Papa Was A Rolling Stone (Jazz)",
                "Procol Harum - A Whiter Shade Of Pale (Pop-Rock, Jazz)",
                "By Me (Pop-Rock, Electronic, Rap) Bobby Brown - Don\u2019t Be Cruel (Pop-Rock, Jazz)\nVocal Nana Mouskouri - Habanera (International, Vocal)",
                "- Never Gonna Let You Go (Jazz, New Age)",
                "Whitney Houston - So Emotional (RnB)\nJazz Abba - Take A Chance On Me (Pop-Rock) Vince Guaraldi Trio - Christmas Time Is Here (Jazz)"
            ],
            "rock": [
                "On the one hand, the effect of the imbal-\nanced dataset is apparent in the network\u2019s performance for \nthe most common label (Pop-Rock) when compared to those \nwith fewer files in the dataset such as Blues, Reggae, and \nFolk.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "For Here comes the Sun the first quarter of the \npianoroll seems to contribute more towards a Jazz predic-\ntion, while the third quarter contributes towards a Pop-Rock  \nprediction.",
                "The \ntracks are: Beethoven\u2014Moonlight Sonata, The Beatles\u2014Here Comes the Sun, Eminem\u2014The Real Slim Shady, Queen\u2014Bohemian Rhapsody\nBeethoven Beatles Eminem Queen\nInternational 0.69 Pop\u2013Rock 0.83 Rap 0.89 Electronic 0.54\nNew Age",
                "0.40 Jazz 0.06 Electronic 0.03 Pop\u2013Rock 0.20\nRap 0.25 RnB 0.04",
                "Jazz 0.02 Vocal 0.06\nPop\u2013Rock 0.24 Country 0.037 Vocal 0.003 RnB 0.04\n2 https:// www. reddit.",
                "For \nHere Comes the Sun, the fifth and sixth bars, along with \ntheir repetition four bars later contribute more towards the \nPop-Rock genre.",
                "Finally, the first measure of the piano part of Bohe-\nmian Rhapsody along with its preceding measure contrib-\nute towards Electronic, while the third measure after the \npiano is introduced contributes towards Pop-Rock .",
                "For \nthe local sample set generated around Here Comes the Sun \nand The Real Slim Shady, the CNN classified 0.75 and 0.93 \nas Pop-Rock and Rap  respectively (percentage of positive \n/u1D702i ).",
                "For Here \nComes the Sun, we attribute the failure of GPX to the bias \nof the classifier towards the Pop-Rock genre which is a result \nof dataset imbalance.",
                "For instance, the Pop-Rock genre \nis represented by a very diverse set of samples, ranging from \nHard Rock to Disco.",
                "A result of this is that almost half of the Fig. 11  Final programs gener -\nated by GPX as explanations \nfor the top prediction for each \nsample\nSN Computer Science (2023) 4:106 \n Page 15 of 18 106\nSN Computer Science\nselected prototypes and criticisms are labeled as Pop-Rock  \namong other labels.",
                "For \nPop-Rock the prototype chosen by MMD-critic is a power \nballad by Abba which is closer to Pop than Rock, which is \ninteresting when compared to the prototype selected from the ground-truth labels: a Nine Inch Nails song which is a \nlot closer to Rock.",
                "For instance, the choice of \nQueen\u2014We Will Rock You as a prototype for Rap  could be \ndue to the rhythmic qualities of the vocal track, the loop-\ning music, and the repeating patterns which are prevalent \nin a lot of different music, including Rap .",
                "In parentheses are the ground-truth \nlabels\nGenre Test set prototype Test set criticism\nPop-Rock Nine Inch Nails\u2014Piggy (Pop-Rock)",
                "Spice Girls\u2014Wannabe (Pop-Rock)\nElectronic Toy-Box\u2014Tarzan & Jane (Pop-Rock, Electronic) George Michael - Fast Love (Electronic)\nCountry Session Americana - John Brown (Country) Olivia Newton-John - Everything Love Is (Pop-Rock, Country)",
                "If It\u2019s Over (Pop-Rock, RnB)",
                "Procol Harum - A Whiter Shade Of Pale (Pop-Rock, Jazz)",
                "By Me (Pop-Rock, Electronic, Rap) Bobby Brown - Don\u2019t Be Cruel (Pop-Rock, Jazz)\nVocal Nana Mouskouri - Habanera (International, Vocal)",
                "Reggae Johnnie Taylor - For Your Precious Love (Pop-Rock, RnB, Reg-\ngae)The Elgins - When A Man Loves A Woman (Pop-Rock, Coun-\ntry, Latin, Reggae)",
                "Blues Deborah Coleman - Long Time (Pop-Rock, Blues)",
                "In parentheses are the ground-truth labels\nGenre Black-box prototype Black-box criticism\nPop-Rock Abba - The Winner Takes It All (Pop-Rock, Vocal)",
                "Donny Osmond - This Guy\u2019s In Love With You (Pop-Rock)\nElectronic Siniestro Total - C\u2019est Chic (Pop-Rock) Crystal Waters - 100% Pure Love (Electronic)",
                "John Fogerty - Big Train (Pop-Rock)\nRnB Stevie Wonder -",
                "Whitney Houston - So Emotional (RnB)\nJazz Abba - Take A Chance On Me (Pop-Rock) Vince Guaraldi Trio - Christmas Time Is Here (Jazz)",
                "Rap Queen - We Will Rock You (Pop-Rock) Phish - Wading In The Velvet (Pop-Rock)",
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)",
                "Reggae The Elgins - When A Man Loves A Woman (Pop-Rock, RnB, \nReggae)Johnnie Taylor -",
                "For Your Precious Love (Pop-Rock, RnB, \nReggae)\nBlues Bill Quinn - He\u2019ll Have To Go (Country, Blues) Jim Reeves - He\u2019ll Have To Go (Country) SN Computer Science (2023) 4:106\n 106 Page 16 of 18\nSN Computer Science\nDiscussion\nIn this section, we showed how various explainability meth-\nods may be used in the context of symbolic music classifi-\ncation."
            ],
            "pop": [
                "The Million Song Dataset is the \nlargest currently available collection of audio features and \nmetadata for a million contemporary popular music tracks.",
                "The Million Song Dataset is the \nlargest currently available collection of audio features and \nmetadata for a million contemporary popular music tracks.",
                "We arbitrarily chose the \nsmallest kernel size k=9 which has the same number of \ntrainable parameters as a 3\u00d73 kernel which is popular for \ncomputer vision two-dimensional CNNs.",
                "We arbitrarily chose the \nsmallest kernel size k=9 which has the same number of \ntrainable parameters as a 3\u00d73 kernel which is popular for \ncomputer vision two-dimensional CNNs.",
                "On the one hand, the effect of the imbal-\nanced dataset is apparent in the network\u2019s performance for \nthe most common label (Pop-Rock) when compared to those \nwith fewer files in the dataset such as Blues, Reggae, and \nFolk.",
                "On the one hand, the effect of the imbal-\nanced dataset is apparent in the network\u2019s performance for \nthe most common label (Pop-Rock) when compared to those \nwith fewer files in the dataset such as Blues, Reggae, and \nFolk.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "For Here comes the Sun the first quarter of the \npianoroll seems to contribute more towards a Jazz predic-\ntion, while the third quarter contributes towards a Pop-Rock  \nprediction.",
                "For Here comes the Sun the first quarter of the \npianoroll seems to contribute more towards a Jazz predic-\ntion, while the third quarter contributes towards a Pop-Rock  \nprediction.",
                "The \ntracks are: Beethoven\u2014Moonlight Sonata, The Beatles\u2014Here Comes the Sun, Eminem\u2014The Real Slim Shady, Queen\u2014Bohemian Rhapsody\nBeethoven Beatles Eminem Queen\nInternational 0.69 Pop\u2013Rock 0.83 Rap 0.89 Electronic 0.54\nNew Age",
                "The \ntracks are: Beethoven\u2014Moonlight Sonata, The Beatles\u2014Here Comes the Sun, Eminem\u2014The Real Slim Shady, Queen\u2014Bohemian Rhapsody\nBeethoven Beatles Eminem Queen\nInternational 0.69 Pop\u2013Rock 0.83 Rap 0.89 Electronic 0.54\nNew Age",
                "0.40 Jazz 0.06 Electronic 0.03 Pop\u2013Rock 0.20\nRap 0.25 RnB 0.04",
                "0.40 Jazz 0.06 Electronic 0.03 Pop\u2013Rock 0.20\nRap 0.25 RnB 0.04",
                "Jazz 0.02 Vocal 0.06\nPop\u2013Rock 0.24 Country 0.037 Vocal 0.003 RnB 0.04\n2 https:// www. reddit.",
                "Jazz 0.02 Vocal 0.06\nPop\u2013Rock 0.24 Country 0.037 Vocal 0.003 RnB 0.04\n2 https:// www. reddit.",
                "For \nHere Comes the Sun, the fifth and sixth bars, along with \ntheir repetition four bars later contribute more towards the \nPop-Rock genre.",
                "For \nHere Comes the Sun, the fifth and sixth bars, along with \ntheir repetition four bars later contribute more towards the \nPop-Rock genre.",
                "Finally, the first measure of the piano part of Bohe-\nmian Rhapsody along with its preceding measure contrib-\nute towards Electronic, while the third measure after the \npiano is introduced contributes towards Pop-Rock .",
                "Finally, the first measure of the piano part of Bohe-\nmian Rhapsody along with its preceding measure contrib-\nute towards Electronic, while the third measure after the \npiano is introduced contributes towards Pop-Rock .",
                "Genetic Programming (GP), in general, generates a random \npopulation and evaluates the fitness of each individual, in \nterms of effectiveness in solving the problem, favoring the \nbetter individuals.",
                "Genetic Programming (GP), in general, generates a random \npopulation and evaluates the fitness of each individual, in \nterms of effectiveness in solving the problem, favoring the \nbetter individuals.",
                "For the genetic programming algorithm, we used a popu -\nlation size of 110 evolved over 110 generations, to attempt \nto mimic the classifier in a local neighborhood of 11,000 \nsamples /u1D702i .",
                "For the genetic programming algorithm, we used a popu -\nlation size of 110 evolved over 110 generations, to attempt \nto mimic the classifier in a local neighborhood of 11,000 \nsamples /u1D702i .",
                "The feature importance \nis calculated as the number of appearances of each feature in \nthe final population, which consists of 110 algebraic expres-\nsions.",
                "The feature importance \nis calculated as the number of appearances of each feature in \nthe final population, which consists of 110 algebraic expres-\nsions.",
                "For the other two sam-\nples, the best explainer was a constant function; however, the \nexistence of features in the final population can give us some \nmusical insight.",
                "For the other two sam-\nples, the best explainer was a constant function; however, the \nexistence of features in the final population can give us some \nmusical insight.",
                "For \nthe local sample set generated around Here Comes the Sun \nand The Real Slim Shady, the CNN classified 0.75 and 0.93 \nas Pop-Rock and Rap  respectively (percentage of positive \n/u1D702i ).",
                "For \nthe local sample set generated around Here Comes the Sun \nand The Real Slim Shady, the CNN classified 0.75 and 0.93 \nas Pop-Rock and Rap  respectively (percentage of positive \n/u1D702i ).",
                "Even though the best explainer for these two samples \nwas (essentially) a constant function, the prevalence of the \ntonic note as an important feature for Rap  classification in \nthe final population makes sense intuitively.",
                "Even though the best explainer for these two samples \nwas (essentially) a constant function, the prevalence of the \ntonic note as an important feature for Rap  classification in \nthe final population makes sense intuitively.",
                "For Here \nComes the Sun, we attribute the failure of GPX to the bias \nof the classifier towards the Pop-Rock genre which is a result \nof dataset imbalance.",
                "For Here \nComes the Sun, we attribute the failure of GPX to the bias \nof the classifier towards the Pop-Rock genre which is a result \nof dataset imbalance.",
                "Furthermore, the prototype \nfor the Latin genre does not have any characteristics of Latin  \nmusic besides the language and would be considered Pop.",
                "Furthermore, the prototype \nfor the Latin genre does not have any characteristics of Latin  \nmusic besides the language and would be considered Pop.",
                "Similarly, the prototype for Rap  could be considered RnB \n(which often contains Rap in modern music), while the \nprototype for New Age  could be considered New Wave/Pop \ninstead.",
                "Similarly, the prototype for Rap  could be considered RnB \n(which often contains Rap in modern music), while the \nprototype for New Age  could be considered New Wave/Pop \ninstead.",
                "For instance, the Pop-Rock genre \nis represented by a very diverse set of samples, ranging from \nHard Rock to Disco.",
                "For instance, the Pop-Rock genre \nis represented by a very diverse set of samples, ranging from \nHard Rock to Disco.",
                "A result of this is that almost half of the Fig. 11  Final programs gener -\nated by GPX as explanations \nfor the top prediction for each \nsample\nSN Computer Science (2023) 4:106 \n Page 15 of 18 106\nSN Computer Science\nselected prototypes and criticisms are labeled as Pop-Rock  \namong other labels.",
                "A result of this is that almost half of the Fig. 11  Final programs gener -\nated by GPX as explanations \nfor the top prediction for each \nsample\nSN Computer Science (2023) 4:106 \n Page 15 of 18 106\nSN Computer Science\nselected prototypes and criticisms are labeled as Pop-Rock  \namong other labels.",
                "For \nPop-Rock the prototype chosen by MMD-critic is a power \nballad by Abba which is closer to Pop than Rock, which is \ninteresting when compared to the prototype selected from the ground-truth labels: a Nine Inch Nails song which is a \nlot closer to Rock.",
                "For \nPop-Rock the prototype chosen by MMD-critic is a power \nballad by Abba which is closer to Pop than Rock, which is \ninteresting when compared to the prototype selected from the ground-truth labels: a Nine Inch Nails song which is a \nlot closer to Rock.",
                "In parentheses are the ground-truth \nlabels\nGenre Test set prototype Test set criticism\nPop-Rock Nine Inch Nails\u2014Piggy (Pop-Rock)",
                "In parentheses are the ground-truth \nlabels\nGenre Test set prototype Test set criticism\nPop-Rock Nine Inch Nails\u2014Piggy (Pop-Rock)",
                "Spice Girls\u2014Wannabe (Pop-Rock)\nElectronic Toy-Box\u2014Tarzan & Jane (Pop-Rock, Electronic) George Michael - Fast Love (Electronic)\nCountry Session Americana - John Brown (Country) Olivia Newton-John - Everything Love Is (Pop-Rock, Country)",
                "Spice Girls\u2014Wannabe (Pop-Rock)\nElectronic Toy-Box\u2014Tarzan & Jane (Pop-Rock, Electronic) George Michael - Fast Love (Electronic)\nCountry Session Americana - John Brown (Country) Olivia Newton-John - Everything Love Is (Pop-Rock, Country)",
                "If It\u2019s Over (Pop-Rock, RnB)",
                "If It\u2019s Over (Pop-Rock, RnB)",
                "Procol Harum - A Whiter Shade Of Pale (Pop-Rock, Jazz)",
                "Procol Harum - A Whiter Shade Of Pale (Pop-Rock, Jazz)",
                "By Me (Pop-Rock, Electronic, Rap) Bobby Brown - Don\u2019t Be Cruel (Pop-Rock, Jazz)\nVocal Nana Mouskouri - Habanera (International, Vocal)",
                "By Me (Pop-Rock, Electronic, Rap) Bobby Brown - Don\u2019t Be Cruel (Pop-Rock, Jazz)\nVocal Nana Mouskouri - Habanera (International, Vocal)",
                "Reggae Johnnie Taylor - For Your Precious Love (Pop-Rock, RnB, Reg-\ngae)The Elgins - When A Man Loves A Woman (Pop-Rock, Coun-\ntry, Latin, Reggae)",
                "Reggae Johnnie Taylor - For Your Precious Love (Pop-Rock, RnB, Reg-\ngae)The Elgins - When A Man Loves A Woman (Pop-Rock, Coun-\ntry, Latin, Reggae)",
                "Blues Deborah Coleman - Long Time (Pop-Rock, Blues)",
                "Blues Deborah Coleman - Long Time (Pop-Rock, Blues)",
                "In parentheses are the ground-truth labels\nGenre Black-box prototype Black-box criticism\nPop-Rock Abba - The Winner Takes It All (Pop-Rock, Vocal)",
                "In parentheses are the ground-truth labels\nGenre Black-box prototype Black-box criticism\nPop-Rock Abba - The Winner Takes It All (Pop-Rock, Vocal)",
                "Donny Osmond - This Guy\u2019s In Love With You (Pop-Rock)\nElectronic Siniestro Total - C\u2019est Chic (Pop-Rock) Crystal Waters - 100% Pure Love (Electronic)",
                "Donny Osmond - This Guy\u2019s In Love With You (Pop-Rock)\nElectronic Siniestro Total - C\u2019est Chic (Pop-Rock) Crystal Waters - 100% Pure Love (Electronic)",
                "John Fogerty - Big Train (Pop-Rock)\nRnB Stevie Wonder -",
                "John Fogerty - Big Train (Pop-Rock)\nRnB Stevie Wonder -",
                "Whitney Houston - So Emotional (RnB)\nJazz Abba - Take A Chance On Me (Pop-Rock) Vince Guaraldi Trio - Christmas Time Is Here (Jazz)",
                "Whitney Houston - So Emotional (RnB)\nJazz Abba - Take A Chance On Me (Pop-Rock) Vince Guaraldi Trio - Christmas Time Is Here (Jazz)",
                "Rap Queen - We Will Rock You (Pop-Rock) Phish - Wading In The Velvet (Pop-Rock)",
                "Rap Queen - We Will Rock You (Pop-Rock) Phish - Wading In The Velvet (Pop-Rock)",
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)",
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)",
                "Reggae The Elgins - When A Man Loves A Woman (Pop-Rock, RnB, \nReggae)Johnnie Taylor -",
                "Reggae The Elgins - When A Man Loves A Woman (Pop-Rock, RnB, \nReggae)Johnnie Taylor -",
                "For Your Precious Love (Pop-Rock, RnB, \nReggae)\nBlues Bill Quinn - He\u2019ll Have To Go (Country, Blues) Jim Reeves - He\u2019ll Have To Go (Country) SN Computer Science (2023) 4:106\n 106 Page 16 of 18\nSN Computer Science\nDiscussion\nIn this section, we showed how various explainability meth-\nods may be used in the context of symbolic music classifi-\ncation.",
                "For Your Precious Love (Pop-Rock, RnB, \nReggae)\nBlues Bill Quinn - He\u2019ll Have To Go (Country, Blues) Jim Reeves - He\u2019ll Have To Go (Country) SN Computer Science (2023) 4:106\n 106 Page 16 of 18\nSN Computer Science\nDiscussion\nIn this section, we showed how various explainability meth-\nods may be used in the context of symbolic music classifi-\ncation."
            ],
            "classical": [
                "In [12], Karydis \net\u00a0al. combine pattern recognition with statistical approaches \nto successfully achieve genre recognition for five sub-gen-\nres of classical music."
            ],
            "electronic": [
                "It is interesting that genres such as Jazz which have lit-\ntle representation in the dataset are better classified than gen-\nres such as Electronic which has almost double the support.",
                "This could be due to distinguishing musical characteristics \nof each genre, which are apparent in symbolic representa-\ntions of music\u2014for instance, jazz music tends to have com-\nplex harmony and utilize more notes, while electronic music \ntends to contain loops of very few notes.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "We chose (b) and (c) as \nexamples of certain predictions, while (d) shows an errone-\nous prediction by the CNN (Electronic), and a relatively high \nvalue of 0.06 for the vocal genre which is interesting since \nthe introduction of the song features an a capella chorus.",
                "Finally, for Bohemian Rhapsody, the second half of the \npianoroll contributes more towards the Electronic genre, \nafter the piano part is introduced.",
                "The \ntracks are: Beethoven\u2014Moonlight Sonata, The Beatles\u2014Here Comes the Sun, Eminem\u2014The Real Slim Shady, Queen\u2014Bohemian Rhapsody\nBeethoven Beatles Eminem Queen\nInternational 0.69 Pop\u2013Rock 0.83 Rap 0.89 Electronic 0.54\nNew Age",
                "0.40 Jazz 0.06 Electronic 0.03 Pop\u2013Rock 0.20\nRap 0.25 RnB 0.04",
                "Finally, the first measure of the piano part of Bohe-\nmian Rhapsody along with its preceding measure contrib-\nute towards Electronic, while the third measure after the \npiano is introduced contributes towards Pop-Rock .",
                "For Bohemian Rhapsody-Electronic and Moonlight \nSonata-International, which are both erroneous predictions, \nGPX has shown two intervals as important features.",
                "This rule says that if both the minor third \nand the fifth appear at least half as often as the tonic, then \nthe sample is classified as Electronic, which makes intui-\ntive sense since Electronic music tends not to have complex \nharmony, and the rule represents the prevalence of a minor \ntriad in the pitches which appear in the pianoroll.",
                "For genres with more than 100 samples \nin the test set, in which the CNN does not perform well \n(Electronic, Latin, International, Rap) the generated pro-\ntotypes are, as expected, far from representative of each \ngenre, however, they are still useful for gaining insight \non what the CNN has learned.",
                "Spice Girls\u2014Wannabe (Pop-Rock)\nElectronic Toy-Box\u2014Tarzan & Jane (Pop-Rock, Electronic) George Michael - Fast Love (Electronic)\nCountry Session Americana - John Brown (Country) Olivia Newton-John - Everything Love Is (Pop-Rock, Country)",
                "By Me (Pop-Rock, Electronic, Rap) Bobby Brown - Don\u2019t Be Cruel (Pop-Rock, Jazz)\nVocal Nana Mouskouri - Habanera (International, Vocal)",
                "Donny Osmond - This Guy\u2019s In Love With You (Pop-Rock)\nElectronic Siniestro Total - C\u2019est Chic (Pop-Rock) Crystal Waters - 100% Pure Love (Electronic)",
                "International Brasilian Tropical Orchestra - Yesterday (International) Uniting Nations - Uniting Nations (Electronic)"
            ],
            "hip-hop": [],
            "reggae": [
                "On the one hand, the effect of the imbal-\nanced dataset is apparent in the network\u2019s performance for \nthe most common label (Pop-Rock) when compared to those \nwith fewer files in the dataset such as Blues, Reggae, and \nFolk.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "For instance, none of the Reggae  \nsamples in the table are actually Reggae, but would probably \nbe considered Soul/RnB/Gospel.",
                "Reggae Johnnie Taylor - For Your Precious Love (Pop-Rock, RnB, Reg-\ngae)The Elgins - When A Man Loves A Woman (Pop-Rock, Coun-\ntry, Latin, Reggae)",
                "Reggae The Elgins - When A Man Loves A Woman (Pop-Rock, RnB, \nReggae)Johnnie Taylor -",
                "For Your Precious Love (Pop-Rock, RnB, \nReggae)\nBlues Bill Quinn - He\u2019ll Have To Go (Country, Blues) Jim Reeves - He\u2019ll Have To Go (Country) SN Computer Science (2023) 4:106\n 106 Page 16 of 18\nSN Computer Science\nDiscussion\nIn this section, we showed how various explainability meth-\nods may be used in the context of symbolic music classifi-\ncation."
            ],
            "country": [
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "Jazz 0.02 Vocal 0.06\nPop\u2013Rock 0.24 Country 0.037 Vocal 0.003 RnB 0.04\n2 https:// www. reddit.",
                "Spice Girls\u2014Wannabe (Pop-Rock)\nElectronic Toy-Box\u2014Tarzan & Jane (Pop-Rock, Electronic) George Michael - Fast Love (Electronic)\nCountry Session Americana - John Brown (Country) Olivia Newton-John - Everything Love Is (Pop-Rock, Country)",
                "Jim Reeves - I Won\u2019t Forget You (Country, Blues)\nTable 6  Prototypes and Criticisms for each genre, generated by MMD-critic on the positive examples as predicted by the black box, on the \ntopMAGD test set.",
                "Country Boots Randolph - Bridge Over Troubled Water (Country)",
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)",
                "For Your Precious Love (Pop-Rock, RnB, \nReggae)\nBlues Bill Quinn - He\u2019ll Have To Go (Country, Blues) Jim Reeves - He\u2019ll Have To Go (Country) SN Computer Science (2023) 4:106\n 106 Page 16 of 18\nSN Computer Science\nDiscussion\nIn this section, we showed how various explainability meth-\nods may be used in the context of symbolic music classifi-\ncation."
            ],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [],
            "k-pop": [],
            "ambient": [],
            "gospel": [
                "For instance, none of the Reggae  \nsamples in the table are actually Reggae, but would probably \nbe considered Soul/RnB/Gospel."
            ],
            "soul": [
                "For instance, none of the Reggae  \nsamples in the table are actually Reggae, but would probably \nbe considered Soul/RnB/Gospel.",
                "The Promise You Made (New Age) Slavic Soul Party!"
            ],
            "funk": [],
            "disco": [
                "For instance, the Pop-Rock genre \nis represented by a very diverse set of samples, ranging from \nHard Rock to Disco.",
                "In: Proceedings of \nthe 22nd ACM SIGKDD international conference on knowledge \ndiscovery and data mining, 2016;1135\u20131144.\n 35."
            ],
            "techno": [
                "In: Proceedings \nof the 6th international conference on PErvasive technologies \nrelated to assistive environments, 2013;1\u20137.\n 17."
            ],
            "dubstep": [],
            "opera": [
                "A one-dimensional convolution with kernels of size k  is \nan operation, which acts on a sequence of T  vectors X  of size \nN to produce a sequence of vectors Y , where at timestep i \nand channel j:\nThe matrix W  consists of the convolution\u2019s trainable param-\neters, while the function /u1D70E enforces non-linearity.",
                "consecutive ele-\nments of the input sequence X , leading to the effectiveness \nof the convolutional operation for capturing local structures \nand patterns in the data.",
                "A deep neural network may then \nbe constructed by stacking such operations in a depth-wise \nfashion.",
                "This results in the first convolutional operations \ncapturing low-level features in the data, while deeper opera-\ntions capture more complex high-level features.",
                "A common pool-\ning operation is max pooling with a stride equal to kernel \nsize K=S .",
                "In this case for the output sequence Y:\nAnother common pooling operation is average pooling with \nK=S , for which case:\nA pooling operation has a receptive field K  and does not \nhave any trainable parameters.",
                "Each of two consecutive \nsamples in the pooling operation\u2019s output depends on K \ninput samples; however, these samples are spaced apart by \non average S samples in the input sequence.",
                "This means \nthat feeding the pooling operation\u2019s output to another layer \neffectively increases receptive field by a factor of S  without \nan increase in trainable parameters.",
                "Finally, pooling opera-\ntions introduce invariance to local translations of an input \nsequence which could be useful for the learning process but \nthey entail information loss which could be detrimental to \nlearning.",
                "The first module of a MuSeReNet is a set of average \npooling operations which act on the original sequence, each \nproducing a version of the original sequence at a different \nresolution (Fig.\u00a0 1).",
                "In the first case, in which information flows from the \nleaves to the root (Fig.\u00a0 2), each input is fed through a block \nwhich consists of convolutional layers followed by a max-\npooling operation with the same stride and kernel size as \nFig. 1  Constructing a binary tree where levels are equivalent to the \ninput sequence at lower resolutionsSN Computer Science (2023) 4:106 \n Page 5 of 18 106\nSN Computer Science\nthe average pooling operation which generated the specific \ninput from its higher resolution counterpart.",
                "This way, and \nusing \u2019same\u2019 padding for convolutional operations, the out-\nput of a specific block has the same sequence length as the \noriginal input at the previous resolution level and may be \nconcatenated along their second axis, producing a sequence \nof the same length and with more channels.",
                "In the second case, in which information flows from \nthe root to the leaves (Fig.\u00a0 3), max-pooling operations are \nreplaced with upsampling operations, and the order with \nwhich inputs are fed to the network is reversed (the root \nfirst instead of the leaves first).",
                "Intuitively, via \nthe upsampling and concatenation operations, this network could learn features that are local but are affected by the con-\ntext provided by the lower-resolution version at a previous \nlayer.",
                "SN Computer Science (2023) 4:106\n 106 Page 6 of 18\nSN Computer Science\nModels\nWe construct neural networks using blocks of 1D convolu-\ntions followed by max-pooling operations of kernel size and \nstride 2.",
                "Specifically, we use a shallow block, which consists of only one convolutional layer prior to the pooling opera-\ntion, and a deep block which consists of three convolutional \nlayers before each pooling operation (Fig.\u00a0 6).",
                "It works by computing the gradient of the \ntarget output neuron with respect to the activation-map of \nthe final convolutional layer of the CNN, averaging over \nthe channels, thus leading to a coarse heatmap of the same size as the convolutional feature-maps, which in our case \nis always a sequence of length 4, due to the application of \nmax-pooling operations within our networks.",
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)",
                "These give us some insight concerning the opera-\ntion of the CNN and may be used for guiding future design \ndecisions.",
                "The project \nis co-financed by Greece and the European Union (European Social \nFund- ESF) by the Operational Programme Human Resources Devel-\nopment, Education and Lifelong Learning 2014\u20132020.\u201d"
            ],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        },
        {
            "title": "CycleDRUMS: automatic drum arrangement for bass lines using CycleGAN",
            "homophonic": [],
            "polyphonic": [
                "In [13], CNNs are used for generating melody as a series of MIDI notes either from scratch, by following a chord sequence, \nor by conditioning on the melody of previous bars, whereas in [14\u2013 17] LSTMs are used to generate musical notes, melo -\ndies, polyphonic music pieces, and long drum sequences under constraints imposed by metrical rhythm information and Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7 \n Research\n1 3\na given bass sequence.",
                "In [21], symbolic sequences of polyphonic music are modeled in an entirely general piano-roll representation, while the \nauthors of [22] propose a novel architecture to generate melodies satisfying positional constraints in the style of the \nsoprano parts of the J.S. Bach chorale harmonizations encoded in MIDI.",
                "In [23], RNNs are used for the prediction and \ncomposition of polyphonic music; in [24], highly convincing chorales in the style of Bach were automatically generated \nusing note names [25]; added higher-level structure on generated polyphonic music, whereas in [26] an end-to-end \ngenerative model capable of composing music conditioned on a specific mixture of composer styles was designed.",
                "Generating polyphonic music using tied parallel networks.",
                "Lattner S, Grachten M, Widmer G. Imposing higher-level structure in polyphonic music generation using convolutional restricted Boltz-\nmann machines and constraints.",
                "Sigtia S, Benetos E, Dixon S. An end-to-end neural network for polyphonic piano music transcription.",
                "Sound-object oriented analysis and note-object oriented processing of polyphonic sound recordings."
            ],
            "monophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "instrumental": [],
            "vocal": [
                "Since the FMA songs lack source-separated channels (i.e., differentiated vocals, bass, drums), it was \npre-processed first.",
                "3.1  Source separation for\u00a0music\nA key challenge to our approach is the scarce availability of music data featuring source-separated channels (i.e., dif-\nferentiated vocals, bass, drums).",
                "However, a potential weakness of this method is that it sometimes produces noisy separations, with watered-down \nharmonics and traces of other instruments in the vocal segment.",
                "In practice, given an input song, we use Demucs to separate it into vocals, bass, drums, and others, \nkeeping the original mixture.\n3.2  Music representation\u2014from raw audio to\u00a0mel\u2011spectrograms\nOur method\u2019s distinguishing feature is mel-spectrograms instead of waveforms.",
                "Since the FMA songs lack source-separated channels (i.e., dif-\nferentiated vocals, bass, drums, etc.), the bass and drum channels were extracted using Demucs [10].",
                "Each song comes as five audio files\u2014vocals, bass, drums, others, and \nfull song\u2014perfectly separated at the master level.",
                "[66, 67] delivers producers a powerful user \ninterface to directly modify and adjust a spectrogram-based representation of audio signals to correct, perfect, reshape \nand restructure vocals, samples, and recordings of all kinds."
            ],
            "choral": [
                "In [21], symbolic sequences of polyphonic music are modeled in an entirely general piano-roll representation, while the \nauthors of [22] propose a novel architecture to generate melodies satisfying positional constraints in the style of the \nsoprano parts of the J.S. Bach chorale harmonizations encoded in MIDI.",
                "In [23], RNNs are used for the prediction and \ncomposition of polyphonic music; in [24], highly convincing chorales in the style of Bach were automatically generated \nusing note names [25]; added higher-level structure on generated polyphonic music, whereas in [26] an end-to-end \ngenerative model capable of composing music conditioned on a specific mixture of composer styles was designed.",
                "Hadjeres G, Pachet F, Nielsen F. Deepbach: a steerable model for bach chorales generation."
            ],
            "orchestral": [],
            "chamber": [],
            "symphonic": [],
            "folk": [],
            "blues": [],
            "jazz": [],
            "rock": [
                "Given the size of FMA, we chose to select only untrimmed songs tagged as either pop, soul-RnB, or indie-\nrock, for approximately 10,000 songs ( \u2248700 h of audio).",
                "This rather small \ndataset comprises 100 tracks taken from the DSD100 dataset, 46 tracks from the MedleyDB, two tracks kindly provided by \nNative Instruments, and two tracks from the Canadian rock band The Easton Ellises.",
                "We \nthen asked a professional guitarist who has been playing in a pop-rock band for more than ten years, a professional \ndrummer from the same band, and two pop and indie-rock music producers with more than four years of experience \nto manually annotate these samples, capturing the following musical dimensions: sound quality, contamination, \ncredibility, and whether the generated drums followed the beat."
            ],
            "pop": [
                "Keywords Automatic music arrangement\u00a0\u00b7 Cycle-GAN\u00a0\u00b7 Deep learning\u00a0\u00b7 Source separation\u00a0\u00b7 Audio and speech \nprocessing\n1 Introduction\nThe development of home music production has brought significant innovations into the process of pop music composi-\ntion.",
                "Keywords Automatic music arrangement\u00a0\u00b7 Cycle-GAN\u00a0\u00b7 Deep learning\u00a0\u00b7 Source separation\u00a0\u00b7 Audio and speech \nprocessing\n1 Introduction\nThe development of home music production has brought significant innovations into the process of pop music composi-\ntion.",
                "In addition, more solutions based on deep learning \ntechniques, such as RL-Duet [4 ]\u2014a deep reinforcement learning algorithm for online accompaniment generation\u2014or \nPopMAG, a transformer-based architecture which relies on a multi-track MIDI representation of music [5], continue to be \nstudied.",
                "In addition, more solutions based on deep learning \ntechniques, such as RL-Duet [4 ]\u2014a deep reinforcement learning algorithm for online accompaniment generation\u2014or \nPopMAG, a transformer-based architecture which relies on a multi-track MIDI representation of music [5], continue to be \nstudied.",
                "The results were then compared to Pix2Pix \n[11], another popular paired image-to-image translation network.",
                "The results were then compared to Pix2Pix \n[11], another popular paired image-to-image translation network.",
                "To sum up, our main contributions are the following:\n\u2022 we trained a CycleGAN architecture on bass and drum mel-spectrograms in order to automatically generate drums \nthat follow the beat and sound credible for any given bass line;\n\u2022 our approach can generate drum arrangements with low computational resources and limited inference time, if \ncompared to other popular solutions for automatic music generation [12];\n\u2022 we developed a metric\u2014partially based on or correlated to human (and expert) judgment\u2014to automatically evaluate \nthe obtained results and the creativity of the proposed system, given the challenges of a quantitative assessment of \nmusic;\n\u2022 we compared our method to Pix2Pix, another popular image transfer network, showing that the music arrangement \nproblem can be better tackled with an unpaired approach and adding a cycle-consistency loss.",
                "To sum up, our main contributions are the following:\n\u2022 we trained a CycleGAN architecture on bass and drum mel-spectrograms in order to automatically generate drums \nthat follow the beat and sound credible for any given bass line;\n\u2022 our approach can generate drum arrangements with low computational resources and limited inference time, if \ncompared to other popular solutions for automatic music generation [12];\n\u2022 we developed a metric\u2014partially based on or correlated to human (and expert) judgment\u2014to automatically evaluate \nthe obtained results and the creativity of the proposed system, given the challenges of a quantitative assessment of \nmusic;\n\u2022 we compared our method to Pix2Pix, another popular image transfer network, showing that the music arrangement \nproblem can be better tackled with an unpaired approach and adding a cycle-consistency loss.",
                "Given the size of FMA, we chose to select only untrimmed songs tagged as either pop, soul-RnB, or indie-\nrock, for approximately 10,000 songs ( \u2248700 h of audio).",
                "Given the size of FMA, we chose to select only untrimmed songs tagged as either pop, soul-RnB, or indie-\nrock, for approximately 10,000 songs ( \u2248700 h of audio).",
                "The /u1D706 weights for cycle losses were both equal to 10.\n4.3  Experimental setting\nEven though researchers proposed some effective metrics to predict how popular a song will become [60], there \nis an intrinsic difficulty in objectively evaluating artistic artifacts such as music.",
                "The /u1D706 weights for cycle losses were both equal to 10.\n4.3  Experimental setting\nEven though researchers proposed some effective metrics to predict how popular a song will become [60], there \nis an intrinsic difficulty in objectively evaluating artistic artifacts such as music.",
                "We \nthen asked a professional guitarist who has been playing in a pop-rock band for more than ten years, a professional \ndrummer from the same band, and two pop and indie-rock music producers with more than four years of experience \nto manually annotate these samples, capturing the following musical dimensions: sound quality, contamination, \ncredibility, and whether the generated drums followed the beat.",
                "We \nthen asked a professional guitarist who has been playing in a pop-rock band for more than ten years, a professional \ndrummer from the same band, and two pop and indie-rock music producers with more than four years of experience \nto manually annotate these samples, capturing the following musical dimensions: sound quality, contamination, \ncredibility, and whether the generated drums followed the beat.",
                "Ren Y, He J, Tan X, Qin T, Zhao Z, Liu T-Y. Popmag: pop music accompaniment generation.",
                "Ren Y, He J, Tan X, Qin T, Zhao Z, Liu T-Y. Popmag: pop music accompaniment generation.",
                "Zhu H, Liu Q, Yuan NJ, Qin C, Li J, Zhang K, Zhou G, Wei F, Xu Y, Chen E. Xiaoice band: a melody and arrangement generation framework for \npop music.",
                "Zhu H, Liu Q, Yuan NJ, Qin C, Li J, Zhang K, Zhou G, Wei F, Xu Y, Chen E. Xiaoice band: a melody and arrangement generation framework for \npop music.",
                "Lee J, Lee J. Music popularity: metrics, characteristics, and audio-based prediction.",
                "Lee J, Lee J. Music popularity: metrics, characteristics, and audio-based prediction."
            ],
            "classical": [],
            "electronic": [],
            "hip-hop": [],
            "reggae": [],
            "country": [],
            "r&b": [],
            "heavy metal": [],
            "punk": [],
            "alternative": [
                "Although arrangement generation has been \nextensively studied in symbolic audio, switching to mel-spectrograms allowed us to preserve the sound heritage of other \nmusical pieces and represent a valid alternative for real-case scenarios."
            ],
            "k-pop": [],
            "ambient": [],
            "gospel": [],
            "soul": [
                "Given the size of FMA, we chose to select only untrimmed songs tagged as either pop, soul-RnB, or indie-\nrock, for approximately 10,000 songs ( \u2248700 h of audio)."
            ],
            "funk": [],
            "disco": [
                "Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7\n1 3Discover Artificial IntelligenceResearch\nCycleDRUMS: automatic drum arrangement for\u00a0bass lines using \nCycleGAN\nGiorgio\u00a0Barnab\u00f21\u00a0\u00b7 Giovanni\u00a0Trappolini1\u00a0\u00b7 Lorenzo\u00a0Lastilla1\u00a0\u00b7 Cesare\u00a0Campagnano2\u00a0\u00b7 Angela\u00a0Fan3\u00a0\u00b7 Fabio\u00a0Petroni3\u00a0\u00b7 \nFabrizio\u00a0Silvestri1\nReceived: 12 September 2022 / Accepted: 4 January 2023",
                "3FAIR, META, London, UK.Vol:.(1234567890)Research Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7\n1 3\nto assist human musicians has become central in the field of automatic music generation [1 ].",
                "In [13], CNNs are used for generating melody as a series of MIDI notes either from scratch, by following a chord sequence, \nor by conditioning on the melody of previous bars, whereas in [14\u2013 17] LSTMs are used to generate musical notes, melo -\ndies, polyphonic music pieces, and long drum sequences under constraints imposed by metrical rhythm information and Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7 \n Research\n1 3\na given bass sequence.",
                "[49].Vol:.(1234567890)Research Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7\n1 3\n3  Method\nWe present CycleDRUMS, a novel approach for automatically adding credible drums to bass lines based on an adversari-\nally trained deep learning model.",
                "html .Vol.:(0123456789)Discover",
                "Since FMA is much larger than musdb18 but also lower quality due to the artificial separation \nof sources, we used FMA to train the model, and then we fine-tuned it with musdb18, which comes in a source-separated fashionVol:.(1234567890)Research Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7\n1 3\nand by adding a cycle-consistency loss that encourages F(G(x ))",
                "[7]Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7 \n Research\n1 3\nmusic arrangement.",
                "Vol:.(1234567890)Research Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7\n1 3\nobjective, universal criteria for appreciating music.",
                "NAVol.:(0123456789)Discover",
                "Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7\n1 3\nFinally, concerning the computational resources and time required to generate new arrangements, our approach \nshows several advantages compared to auto-regressive models [12].",
                "The \nhistogram above shows the distribution of the predicted class for these samplesVol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7 \n Research\n1 3\nAuthor contributions GB has contributed to the conceptualization of the manuscript and has lead every part of the research effort.",
                "05337.Vol:.(1234567890)Research Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7\n1 3\n 9.",
                "In: Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining; 2018.",
                "2019;21(12):3150\u201363.Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7 \n Research\n1 3\n 45."
            ],
            "techno": [
                "Software like Pro Tools, Cubase, and Logic\u2014as well as MIDI-based technologies and digital instruments\u2014provide a \nbroad set of tools to manipulate recordings and simplify the composition process for artists and producers."
            ],
            "dubstep": [],
            "opera": [],
            "musical theatre": [],
            "bluegrass": [],
            "flamenco": []
        }
    ]
}