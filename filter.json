{
    "data_collection": [
        {
            "title": "MP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN",
            "architecture": [
                "The model architecture\nis a deep 2D convolutional network, where each\nsubsequent generator model block increases the\nresolution along the time axis and adds a higher\noctave along the frequency axis.",
                "An additional\nbene\ufb01t of the CNN-based model architecture is\nthat generation of new songs is almost instanta-\nneous.",
                "The architecture of our network is in-\nspired by the ProGAN model (Karras et al., 2018) with the\nnoticeable difference that we don\u2019t increase/decrease the\npixel density along the frequency axis with each successive\nmodel-block in the generator/discriminator.",
                "More-\nover, the model CNN-based architecture allows for almost\ninstantaneous generation of new samples.2.",
                "Network architecture\n3.1.",
                "2D convolutions to up- and downscale the MDCT\namplitude representation\nOur network architecture is based on the architecture of\nthe ProGAN network (Karras et al., 2018) with successive\nmodel blocks which scale up/down the image using strided\n2D convolutions in the generator and discriminator respec-\ntively.",
                "Overall model architecture\nThe model architecture of MP3net is shown in \ufb01gure 4.",
                "Model architecture of MP3net with 4 model blocks.",
                "The GANsynth architecture is based on Pro-\nGAN.",
                "Future work\nSince the model architecture of MP3net is very similar to\nthe convolutional networks well studies in the \ufb01eld of image\ngeneration, we can borrow many of the techniques of image\ngeneration.",
                "While the MP3net architecture is similar in many\nrespects to ProGAN, the characteristics of the data represen-\ntation is rather different.",
                "This modi\ufb01ed architecture\nwould allow us to control the generated audio at different\nscales.",
                "7. Conclusion\nIn this paper, we introduce MP3net, a 2D convolutional\nGAN with an architecture similar to some of the most suc-\ncessful image generation GANs (Karras et al., 2018; Zhang\net al., 2019; Karras et al., 2019).",
                "Compared to other gen-\nerational models generating multi-minute samples, training\ntimes of MP3net are much shorter and inference is quasi-\ninstantaneous given the inherent CNN-model architecture.",
                "Karras, T., Laine, S., and Aila, T. A style-based generator\narchitecture for generative adversarial networks, 2019."
            ],
            "training parameters": [],
            "learning rate": [
                "We used the Adam optmizer\nwith learning rate 0:0001 ,\f1= 0:5and\f2= 0:9."
            ],
            "batch size": [
                "We trained the model for 250h on a single Cloud TPUv2,\ncorresponding with 750,000 iterations of the discrimina-\ntor (batch size 8), with two discriminator updates for each\ngenerator update.",
                "The 5s model was trained for 120h on a single Cloud TPUv2,\ncorresponding with 160,000 iterations of the discriminator\n(batch size 64).",
                "layers is much smaller allowing for larger batch sizes and\nsince less weights have to be optimized."
            ],
            "ethical": [],
            "impact": [
                "The psychoa-\ncoustic properties, on the other hand, allow us to remove\na signi\ufb01cant amount of data from the audio signal without\nan audible impact, hence reducing the computing power\nrequired to generate raw audio samples with a generative\nmodel.",
                "Siegert, I., Lotz, A. F., Duong, L. L., and Wendemuth, A.\nMeasuring the impact of audio compression on the spec-\ntral quality of speech data."
            ],
            "society": [
                "In Audio Engineer-\ning Society Conference: 17th International Conference:\nHigh-Quality Audio Coding , Sep 1999."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "The SAGAN model\n(Zhang et al., 2019) uses a self-attention layer to increase the\nlong-range coherence of convolutional neural nets and boost\nmodel performance for images which contain geometric\nstructures.",
                "Lim, J. H. and Ye, J. C. Geometric gan, 2017."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "Hadjeres, G., Pachet, F., and Nielsen, F. Deepbach: a\nsteerable model for bach chorales generation, 2017."
            ],
            "orchestral": [],
            "electronic": [
                "On the other hand, audio is an integral part of consumer\nelectronic devices, which have much less computing power."
            ],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "Experiments with the MAESTRO dataset\n4.1.",
                "Dataset description\nOur experiments are based on the MAESTRO-V2.0.0\ndataset (Hawthorne et al., 2019).",
                "This dataset contains over\n200h of classical piano music, recorded over nine years of\nthe International Piano-e-Competition.",
                "This latter isMP3net: coherent, minute-long music generation from raw audio with a simple convolutional GAN\nto be expected since \u00184%of the training dataset contains\natonal pieces (mostly from Alexander Scriabin).",
                "S AMPLE DIVERSITY\nThe MAESTRO dataset consists of music from the Baroque\nera over the Classical, Romantic and Impressionist styles,\nto the Expressionist period.",
                "In particular, training on the\nBlizzard (King & Karaiskos, 2011) and V oxCeleb2 datasets\n(Chung et al., 2018) would test the model\u2019s ability to synthe-\nsis speech.",
                "Hawthorne, C., Stasyuk, A., Roberts, A., Simon, I., Huang,\nC.-Z. A., Dieleman, S., Elsen, E., Engel, J., and Eck, D.\nEnabling factorized piano music modeling and generation\nwith the maestro dataset, 2019.Jenni, S. and Favaro, P."
            ]
        },
        {
            "title": "Hierarchical Recurrent Neural Networks for Conditional Melody Generation with Long-term Structure",
            "architecture": [
                "With this new data representation,\nthe proposed architecture is able to simultaneously model the\nrhythmic, as well as the pitch structures effectively.",
                "In Section III and\nIV, we describe the proposed event representation and CM-\nHRNN architecture in detail.",
                "The core design of the architecture is\nto \ufb01rst generate the rhythmic patterns and then condition the\npitch generation with both coarse and \ufb01ne rhythmic patterns.",
                "In this paper, we propose an architecture similar to HRNN\nbut with different design motivations.",
                "B. Hierarchical architectures for long term dependencies\nDrawing inspiration from other \ufb01elds in which long-term\ndependencies of sequential data have been modelled by RNNs,\nwe \ufb01nd that multiple hierarchical architectures have been\nproposed for this challenge [20], [21], [26]\u2013[28].",
                "These architectures can be applied to the domain of audio\ngeneration.",
                "P ROPOSED MODEL : CM-HRNN\nA. Model Architecture\nOur proposed CM-HRNN architecture consists of multiple\nhierarchical tiers (see Fig. 3).",
                "In this paper, we\nfocus on two variants of the proposed architecture: a 2-tier\nCM-HRNN and a 3-tier CM-HRNN.",
                "The architecture of the\nproposed 3-tier CM-HRNN is shown in Fig.",
                "By removing\nthe top tier of the 3-tier CM-HRNN, we obtain the architecture\nof the 2-tier CM-HRNN.",
                "3: Our proposed CM-HRNN architecture.",
                "V. E XPERIMENTS\nA. Experimental setup\nWe set up several experiments to determine the optimal\nnetwork architecture; evaluate the model\u2019s ability to generate\nhigh-quality music with structure, and to compare it with a\nstate-of-the-art system, AttentionRNN.",
                "Even though other research may share some similarities in\nterms of model architecture [22], [23], they either work on a\ndifferent problem domain or with different data representation,\nthus making comparison hard."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [
                "The residual connection is able to\noffset the impact of vanishing gradients and allows the model\nto generate more repetitive patterns."
            ],
            "society": [],
            "copyright": [],
            "evaluation metrics": [
                "Below, we \ufb01rst describe our dataset and pre-processing\nmethod, followed by a description of the evaluation metrics\nand the listening test setup."
            ],
            "metric": [
                "Below, we \ufb01rst describe our dataset and pre-processing\nmethod, followed by a description of the evaluation metrics\nand the listening test setup.",
                "COSIATEC utilizes a geometric approach much like zip-\ufb01le\ncompression, to detect repeated patterns in symbolic music\ndata.",
                "[16] W. B. De Haas and A. V olk, \u201cMeter detection in symbolic music using\ninner metric analysis,\u201d in Proc. 17th Int.",
                "[17] D. Herremans, S. Weisser, K. S \u00a8orensen, and D. Conklin, \u201cGenerating\nstructured music for bagana using quality metrics based on markov\nmodels,\u201d Expert Syst. with Appl. , vol."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "For example,a melodic motif may be repeated several times in a jazz\npiece with slight rhythmic variations."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [
                "There\nare two different types of music generation systems: those that\ngenerate symbolic music and those that generate raw audio."
            ],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Popular data encoding schemes for symbolic music include\npiano-roll representation",
                "Composing melodies from a given set of chords is a task\nfaced by many musicians in the real world, e.g. in pop music\ncomposition or jazz improvisation."
            ],
            "Demo availability": [],
            "dataset": [
                "Below, we \ufb01rst describe our dataset and pre-processing\nmethod, followed by a description of the evaluation metrics\nand the listening test setup.",
                "B. Dataset and pre-processing\nAll training data (XML format) was parsed from Theory-\ntab",
                "[32] \u201cTheorytab dataset,\u201d https://www.hooktheory.com/theorytab."
            ]
        },
        {
            "title": "LEARNING TO GENERATE MUSIC WITH SENTIMENT",
            "architecture": [],
            "training parameters": [
                "Training parameters (learn-\ning rate and decay, epochs, batch size, etc) were the same\nones of the the generative mLSTM."
            ],
            "learning rate": [
                "The learning rate was set to 5\u000310\u00006at the\nbeginning and decayed linearly (after each epoch) to zero\nover the course of training."
            ],
            "batch size": [
                "Training parameters (learn-\ning rate and decay, epochs, batch size, etc) were the same\nones of the the generative mLSTM."
            ],
            "ethical": [],
            "impact": [],
            "society": [
                "\u201cLearning to Generate\nMusic With Sentiment\u201d, 20th International Society for Music Informa-\ntion Retrieval Conference, Delft, The Netherlands, 2019.learn an excellent representation of sentiment (positive-\nnegative) on text, despite being trained only to predict the\nnext character in the Amazon reviews dataset [6].",
                "In Audio Engi-\nneering Society Conference: 56th International Con-\nference: Audio for Games .",
                "Audio Engineering Society,\n2015."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "Data analysis process used to de\ufb01ne the \ufb01nal\nlabel of the phrases of a piece.\nand clustering all 30 time-series into 3 clusters (positive,\nnegative and noise) according to the dynamic time-warping\ndistance metric."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "[5] use a\ndependency network and a Gibbs-like sampling procedure\nto generate high-quality four-part chorales in the style of\nBach.",
                "Deepbach: a steerable model for bach chorales gen-\neration."
            ],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Each\nindividual in the population of this GA has 161real-valued\ngenes representing a small noise to be added to the weights\nof the 161 L1 neurons.",
                "The GA starts with a random population of size 100\nwhere each gene of each individual is an uniformly sam-\npled random number \u00002\u0014r\u00142.",
                "For each generation,\nthe GA (i) evaluates the current population, (ii) selects 100\nparents via a roulette wheel with elitism, (iii) recombines\nthe parents (crossover) taking the average of their genes\nand (iv) mutates each new recombined individual (new\noffspring) by randomly setting each gene to an uniformly\nsampled random number \u00002\u0014r\u00142.",
                "This means that if we add the genes of the\nbest individual of the \ufb01nal population to the weights of the\ngenerative mLSTM, we generate positive pieces with 84%\naccuracy and negative pieces with 67% accuracy."
            ],
            "Demo availability": [],
            "dataset": [
                "We evaluate the accuracy of the model\nin classifying sentiment of symbolic music using a new\ndataset of video game soundtracks.",
                "1. INTRODUCTION\nMusic Generation is an important application domain of\nDeep Learning in which models learn musical features\nfrom a dataset in order to generate new, interesting music.",
                "\u201cLearning to Generate\nMusic With Sentiment\u201d, 20th International Society for Music Informa-\ntion Retrieval Conference, Delft, The Netherlands, 2019.learn an excellent representation of sentiment (positive-\nnegative) on text, despite being trained only to predict the\nnext character in the Amazon reviews dataset [6].",
                "When\ncombined to a Logistic Regression, this LSTM achieves\nstate-of-the-art sentiment analysis accuracy on the Stan-\nford Sentiment Treebank dataset and can match the per-\nformance of previous supervised systems using 30-100x\nfewer labeled examples.",
                "In order to evaluate this approach, we need a dataset of\nmusic in symbolic format that is annotated by sentiment.",
                "To the best of our knowledge, there\nare no datasets of symbolic music annotated according to\nsentiment.",
                "Therefore, we created a new dataset composed\nof 95 MIDI labelled piano pieces (966 phrases of 4 bars)\nfrom video game soundtracks.",
                "The same dataset\nalso contains another 728 non-labelled pieces, which were\nused for training the generative LSTM.",
                "Another contribution of this paper is a la-\nbelled dataset of symbolic music annotated according to\nsentiment.",
                "A second-order Markov\nmodel is used to learn melodies from a dataset and are\nthen transformed by a rule-based system to \ufb01t the anno-\ntated emotions in the graph.",
                "With this\nnew representation and dataset, Oore et al.",
                "This mLSTM was trained on the Amazon product re-\nview dataset, which contains over 82 million product re-\nviews from May 1996 to July 2014 amounting to over 38\nbillion training bytes",
                "[13] used the\ntrained mLSTM to encode sentences from four different\nSentiment Analysis datasets.",
                "With the\nencoded datasets, Radford et al.",
                "By inspecting the relative contributions of features on\nvarious datasets, Radford et al.",
                "4. SENTIMENT DATASET",
                "[13] method to com-\npose music with sentiment, we also need a dataset of MIDI\n\ufb01les to train the LSTM and another one to train the lo-\ngistic regression.",
                "There are many good datasets of music\nin MIDI format in the literature.",
                "Thus, we created a new dataset called VGMIDI which iscomposed of 823 pieces extracted from video game sound-\ntracks in MIDI format.",
                "The data collection process provides a time series of\nvalence-arousal values for each piece, however to create a\nmusic sentiment dataset we only need the valence dimen-\nsion, which encodes negative and positive sentiment."
            ]
        },
        {
            "title": "Personalized Popular Music Generation Using Imitation and Structure",
            "architecture": [
                "The Transformer architecture does\nnot o\u000ber explicit representations of abstract music qualities such as chords, bass lines, or\nrhythms, so there is no straightforward way to control or bias the model to produce certain\nscales, hierarchical structure, repetition or even to limit repetition."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "In the 12th\ninternational conference on music perception and cognition and the 8th triennial conference\nof the european society for the cognitive sciences of music (pp. 276{285).",
                "In Proceedings of the 21st international society for music\ninformation retrieval conference, ismir 2020.",
                "In\nProceedings of the 21st international society for music information retrieval conference,\nismir 2020.",
                "Movement\ndisorders: o\u000ecial journal of the Movement Disorder Society ,11(2), 193{200.\nThornton, C. (2009)."
            ],
            "copyright": [
                "Our\nCopyright \u00a92021 Shuqi Dai, Xichu Ma, Ye Wang, Roger B. DannenbergarXiv:2105.04709v1",
                "In addition, automatic generation of\nmusic for music therapy can avoid the copyright issues and access to large corpora of popular\nmusic, especially in communities with limited resources and budgets."
            ],
            "evaluation metrics": [],
            "metric": [
                "Users can specify input settings\nlike phrase length, chord transition matrix, and metrical salience histogram.",
                "The\nrhythm similarity metric is based on matching note onset times."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "How-\never, every piece of music has its own distinctive abstract qualities, for example, structure,\nmelodic contour, rhythmic pattern, chord progression, bass line pattern, etc.",
                "While the results from this ap-\nproach are impressive, they often lack common popular hierarchical music structure such as\n4-or-8-bar phrases or repetition of melodic and rhythmic patterns.",
                "We compute the\nedit distance using a substitution cost based on similar absolute pitch di\u000berence and similar\nmelodic direction.",
                "Given two melodic sequences we de\fne similarity rhythm to be the proportion of\n16thnote o\u000bsets where both sequences or neither sequence contains an onset (i.e. accuracy ).",
                "To remove any coordination with chords, bass and structure, we derive these non-\nmelodic elements by imitating a third song (C).",
                "In addition, the combination of melodic\ncontour and chords might be enough to substantially reproduce the seed song melody.",
                "From experience, we believe that the melodic contour and rhythmic onset similarity play\nthe most important roles in generating similar songs, at least within the probabilistic frame-\nwork that we introduced.",
                "Melodic contour typology."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "Deepbach: a steerable model for bach chorales\ngeneration.",
                "Automatic composition in the style of bach chorales."
            ],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Personalized Popular Music Generation Using Imitation\nand Structure\nShuqi Dai\nCarnegie Mellon University\nshuqid@cs.cmu.eduXichu Ma\nNational University of Singapore\nmaxichu@u.nus.edu\nYe Wang\nNational University of Singapore\nwangye@comp.nus.edu.sgRoger B. Dannenberg\nCarnegie Mellon University\nrbd@cs.cmu.edu\nMay 8, 2021\nAbstract\nMany practices have been presented in music generation recently.",
                "An evaluation using 10 pop songs shows\nthat our new representations and methods are able to create high-quality stylistic mu-\nsic that is similar to a given input song.",
                "[cs.SD]  10 May 2021Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\ngoal is to create personalized music automatically by imitating one or more songs that are\nselected by a user.",
                "In addition, automatic generation of\nmusic for music therapy can avoid the copyright issues and access to large corpora of popular\nmusic, especially in communities with limited resources and budgets.",
                "Another signi\fcant issue in music generation is music structure, particularly at the level\n2Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nFigure 1: Inverted-U relationship: the Wundt Curve originally suggested by Wundt and\nlater adapted by Berlyne (1971), and the linking of favorability to familiarity/time curve by\nSluckin et al.",
                "For example a common pattern in popular music songs is ABABB , where letters\nstand for sections and repeated letters represent an approximate repetition of a section.",
                "We introduce a stylistic music generation model that is able to capture melody, chord\nand bass style from a single pop song and imitate them with structure information in a new\ncomplete piece.",
                "We note that imitation of performance, orchestration,\nand production (especially in pop music) are also important aspects of style and perceived\nsimilarity, but we leave these to future work.",
                "Our work focuses on popular music for multiple reasons.",
                "The second reason\nfor studying popular music is the practical consideration that most people have familiarity\n3Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nand knowledge of popular music styles.",
                "Therefore, it is easier to compare, discuss, and\nevaluate popular music than other music.",
                "It is also the case that popular music has many\nconventions that seem to simplify the music generation problem.",
                "This is not to say that\ntruly great popular music is easy to make, but at least we can formalize approaches that\ngenerate serviceable popular music.",
                "Thus, we are able to learn enough from\na single input song to form an imitation, even when seeds vary from Chinese Pop to Western\nPop songs.",
                "Recently,\ndeep generative models have become popular in this \feld (Briot, Hadjeres, & Pachet, 2019).",
                "Also, the quality\n4Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nof the model largely depends on the style/rule design.",
                "While the results from this ap-\nproach are impressive, they often lack common popular hierarchical music structure such as\n4-or-8-bar phrases or repetition of melodic and rhythmic patterns.",
                "Elowsson and Friberg (2012) created an algorithmic composition system for popular\nmusic using probabilistic methods guided by music theory.",
                "This system generates a generic popular music style, but\ncannot represent style variations, personalize music, or model bass style.",
                "5Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n3 Data Representation and Pre-processing\nThe input/output data is represented in quantized MIDI format.",
                "6Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n4.1",
                "We developed three ways to generate new structures: (1) copy the seed song structure; (2)\ngenerate according to a speci\fcation string such as `AABABC' from the user and treat each\n1Seed songs for di\u000berent modules can be di\u000berent, e.g. taking the melody from seed A and bass from\nseed B.\n7Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nsection as 8 bars; (3) generate random structures, which includes selecting from a collection\nof typical structures.",
                "4.3 Stylistic Chord Generation\nTo generate convincing chord progressions while imitating the harmonic style of the seed\nsong, we combine statistical features from a general popular music data set2, seed song\nstatistics, and distinctive sequences from the seed song.",
                "2https://github.com/tmc323/Chord-Annotations\n8Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nBlending Transition Matrix Ignoring distinctive sequences for the moment, we create\n\frst-order Markov chain chord transition matrices from both general statistics and the seed\nsong, then blend them together using Ptrans = (1\u0000\u000b)",
                "We identify distinc-\ntive chord sequences in the seed song by comparing statistical features between the seed song\nand a general dataset consisting of 300 annotated pop songs.",
                "Stylistic Cadence Cadences are commonly used in popular music to end a phrase or sec-\ntion.",
                "Again,\n9Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nthe cadence transition matrix combines both general statistics and seed song statistics, i.e.\nspeci\fc cadences in the seed song.",
                "The stylistic rating functions used to extract and represent melody style feature are\ninspired by Elowsson and Friberg (2012), who use similar methods to generate popular\nmusic.",
                "10Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nPitch frequency Frequency of pitch poccurring in both the seed song statistics and\ngeneral popular music statistics:\nPfreq(p)",
                "Pitch harmony with chord progression Given chord c, probability of pitch poccurring\nin both the seed song statistics and general popular music statistics:\nPhar(pjc) =\u000bPhar;seed (pjc) + (1\u0000\u000b)Phar;general (pjc);\nwherePhar;seed (pjc) =jnotes with pitch pand chordcin seed songj\njnotes with chord cin seed songj(4)\nPitch interval frequency Frequency of consecutive pitch interval p\u0000pprevoccurring in\nboth the seed song statistics and general popular music statistics.",
                "Pinterval (p\u0000pprev) =Pinterval (\u0001p) =\u000bPinterval;seed (\u0001p) + (1\u0000\u000b)Pinterval;general (\u0001p);\nwherePinterval;seed (\u0001p) =jinterval \u0001 pin seed songj\njintervals in seed song j(5)\nPitch interval with harmony Given chords c1andc2, probability of the pitch interval\np\u0000pprevoccurring in both the seed song statistics and general popular music statistics.",
                "11Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nNote duration frequency Frequency of note duration (length) dappearing in both the\nseed song statistics and general popular music statistics,\nPdurfreq (d) =\u000bPdurfreq;seed (d) + (1\u0000\u000b)Pdurfreq;general (djtempo );\nwherePdurfreq;seed (d) =jnotes with duration din seed songj\njnotes in seed song j(7)",
                "Note duration transition Probability of a note duration given previous consecutive note\ndurationdin both the seed song statistics and general popular music statistics,\nPdurtrans (d1jd2) =\u000bPdurtrans;seed (d1jd2) + (1\u0000\u000b)Pdurtrans;general (d1jd2);\nwherePdurtrans;seed (d1jd2) =jnote transitions with duration d2tod1in seed songj\njnote transitions in seed song j(8)\nRest note duration Rest notes are more common to see in the last bar of a phrase/section.",
                "If we use Phar,\n12Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nbased on song statistics, to capture the notion of chord tones, the following rather obscure\nequation captures this heuristic:\nP(p;djc) = ((Phar(pjc)\u00000:5)\u0003log2(d=4) +",
                "Last bar and the tonic Sections ending with a strong perfect authentic cadence almost\nalways end on the tonic (i.e. the pitch C) in popular music.",
                "+w2\u0001dist dir(prev 1\u0000p1;prev 2\u0000p2);\nwherew1= 1:0 andw2= 2:0 are weights\nandprev iis the pitch before pi: (13)\n13Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\ndist pit(p1;p2)",
                "14Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "15Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n5 Evaluation\nWe conducted both objective and subjective evaluations of our stylistic music generation\nsystem.",
                "The other ten songs, \fve Western pop songs and\n\fve Chinese pop songs, are used as seed songs for evaluation.",
                "These test songs are quite\npopular and recognizable.",
                "One application of our system is to imitate Parkinson patients'\nfavorite pop songs, so we wanted to imitate songs that are already popular and familiar.",
                "Then, the listeners answered four parts of the evaluation (melody, chord,\n16Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nbass and combination) in a random order.",
                "We also adjust song settings such as pitch\n17Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\n(a) Gender\n (b) Age.",
                "A problem with measuring preference is that familiar music is generally preferred over\n18Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "19Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nFigure 12: Correlation between preference-of-A \u0000preference-of-B and preference-of-A' \u0000\npreference-of-B' (all four values from the same listener), where A and B are seed songs, A'\nand B' are imitations.",
                "Perhaps when listeners are more familiar with the seed song, they form strong\nexpectations and are therefore more attentive to di\u000berences between the imitation and seed\nsong.\n20Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "6 Conclusions\nWe have described techniques to automatically generate stylistic melody, harmony and bass\nlines for pop songs with a logical and hierarchical music structure.",
                "Our original motivation\n21Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nwas to enable the creation of therapeutic music where repeated listening might call for\nsome variety, but where therapists might not have the time, resources or permission to use\nexisting popular music.",
                "AAAI Press.\n22Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nBriot, J.-P., Hadjeres, G., & Pachet, F. (2019).",
                "Automatic analysis and in\ruence of\nhierarchical structure on melody, rhythm and harmony in popular music.",
                "Algorithmic composition of popular music.",
                "University\nof Cambridge ,8, 19{48.\n23Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)\nLo, M., & Lucas, S. M. (2006).",
                "24Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)",
                "(multiple selection)\n{Pop, Rock, Country, R&B/Hip-Hop, Jazz/Blues, Western Classical, Religious,\nFolk/Regional, Children\n\u2022What is your age?",
                "25Dai, Ma, Wang and Dannenberg Popular Music Generation (arXiv preprint)"
            ],
            "Demo availability": [],
            "dataset": [
                "We identify distinc-\ntive chord sequences in the seed song by comparing statistical features between the seed song\nand a general dataset consisting of 300 annotated pop songs."
            ]
        },
        {
            "title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network",
            "architecture": [
                "A performance comparison and description of the various GAN architecture are also presented.",
                "Visual art mainly contains works involving painting, sculpture, and architecture [1].",
                "Generative Adversarial Networks (GANs) use deep learning architectures to facilitate generative modeling.",
                "Following are the key contributions of this paper: \u2022 It provides an overview of the primary GAN architectures for generative arts.",
                "In this Section, the necessary background information including relevant GAN architectures will be discussed.",
                "Figure 2 depicts a basic GAN architecture consisting of a single generator and discriminator.",
                "Meanwhile, the discriminator has access to the ground truths, whether the data came from the generator or real dataset, which it can use to minimize its error.  \n Figure 2 A Basic GAN Architecture More sophisticated GAN architectures can make use of labels to generate data points for a specific category.",
                "Next, various GAN architectures relevant to art generation is discussed.",
                "1) Conditional GAN: Extended from the regular GAN, it is a conditional architecture if the generator and discriminator are conditioned on auxiliary information such as class labels [15].",
                "This type of architecture is suitable for multimodal generation of data points.",
                "[16], the generator and discriminator in this architecture is made up of convolutional networks.",
                "Given the success of convolutional neural network in image and video classification in recent years, DCGAN remains a suitable architecture for image generation applications.   3) Recurrent Adversarial Netwokrs: In this architecture, a recurrent computation is obtained by unrolling the gradient descent optimization [18].",
                "The two main components of this architecture are the convolutional encoder which extracts images of the current \u201ccanvas\u201d and the decoder which decides whether or not to update the \u201ccanvas\u201d by looking at the code for the reference image.",
                "Recurrent architectures are suitable for sequential and time-dependent data and is generally used for text and audio related applications.",
                "Other common GAN architectures include InfoGAN",
                "For a comprehensive comparison of GAN architectures, the readers are encouraged to refer to [17] and [22].",
                "The proposed architecture is a supervised learning approach such that given a black-and-white sketch, the model can create a painted colorful image.",
                "Based on this, the proposed architecture outperformed the three existing ones.",
                "A similar conditional GAN architecture using a U-Net generator for generating shoe image from a given shoe sketch is presented in [25].",
                "However, unlike previous works, their architecture allows the user to specify a style which could be based on the artist\u2019s name or a style category.",
                "The generator utilizes a U-Net architecture whereas the discriminator produces predictions of a style vector, sketch, and \u2018real\u2019 or \u2018fake\u2019 indicator by using the input image sketch and the image produced by the generator.",
                "In terms of both these metrics, the proposed architecture outperformed the existing works.",
                "The brushstrokes generated by the GAN architecture were rougher and more realistic.",
                "Figure 5 Samples from Proposed Architecture in [28]",
                "The authors then explored with an existing GAN architecture known as styleGAN",
                "[32] introduced StrokeNET, a GAN-based architecture to generate digits and character strokes.",
                "Furthermore, the table also summarizes the GAN type used, the loss function as well as the generator and discriminator architectures.",
                "Recent Advances in Visual Arts Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Result",
                "The authors in [37] presented an adversarial and convolutional based architecture known as MidiNet for generating pop music monophonic melodies using 1022 pop music from an online MIDI database called TheoryTab [38].",
                "The architecture of both the generator and discriminator contains 2 LSTM layers of 350 hidden units.",
                "The proposed \u2018MuseGAN\u2019 architecture utilized three different GANs namely the jamming model, the composer model, and the hybrid model.",
                "Moreover, the table also summarizes the GAN type used, the loss function as well as the generator and discriminator architectures.",
                "Recent Advances in Music and Melody Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Result",
                "Most GAN architectures are restricted by several factors when it comes to text and sequential generation.",
                "The SeqGAN architecture is illustrated in Figure 8.",
                "Architecture",
                "Among several experiments using this architecture, the authors also employed the model for Chinese poem generation.",
                "The generative model is a CNN-RNN architecture, acting as an agent.",
                "The proposed image to poetry GAN (I2P-GAN) was evaluated on several metrics such as relevance, novelty, and BLEU scores against different architectures including SeqGAN.",
                "The architecture used is displayed in Figure 10.",
                "Figure 10 Proposed Prose Generation Architecture in [46]",
                "This architecture of the CNN-RNN model as an agent and two discriminators is similar to that of [45].",
                "The overall goal of the poem generation architecture is to generate a sequence of words as a poem for an image by maximizing the expected return.",
                "This architecture introduced a low-variance objective function using the discriminator\u2019s result following the corresponding log-likelihood.",
                "Both LSTM-based, as well as CNN-based GAN architectures, were experimented and the proposed model with LSTM outperformed the existing works on the aforementioned Poem-5 and Poem-7 datasets.",
                "The overall architecture of RankGAN is presented in Figure 12.",
                "Figure 12 Proposed RankGAN Architecture in [50] The RankGAN differs from traditional GAN by including a sequence of generators and a ranker.",
                "The proposed architecture was evaluated for poetry generation on a Chinese poem dataset containing over 13,000 five-word quatrain poems.",
                "[51] proposed a GAN architecture for creative text generation.",
                "The discriminator contains an encoder-decoder pair with the encoder having the same architecture as the generator, with an added pooled decoder layer.",
                "Recent Advances in Literary Text Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Dataset Result",
                "Therefore, training large scale GAN architectures will not be suitable.",
                "Therefore, future research should focus on implementing GAN architectures for generating music in raw audio format.",
                "In summary, for future work on art generation using GANs, we recommend the following: \u2022 Experiment with smaller dataset and GAN architectures for visual arts generation.",
                "[30] T. Karras, S. Laine, and T. Aila, \u201cA style-based generator architecture for generative adversarial networks,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [
                "It provides a comparison of the recent works in generative arts categorized by visual arts, music, and literary text generation along with their impacts."
            ],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "In terms of both these metrics, the proposed architecture outperformed the existing works.",
                "The proposed model on average obtained a score 3.27 on the three metrics, outperforming the baseline models.",
                "For intra-track metrics, the jamming model provided the best performance whereas for inter-track, the hybrid and composer performed better.",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds",
                "For intra-track metrics, jamming model performed best [44] Generate folk music RL GAN Cross entropy and policy gradient RNN generators, CNN discriminators BLEU score of 0.94 and MSE of 20.6 outperformed baseline maximum likelihood estimation D. Poetry and Literary Text Generation using GANs Researchers have also focused on literary text generation using GANs.",
                "The proposed image to poetry GAN (I2P-GAN) was evaluated on several metrics such as relevance, novelty, and BLEU scores against different architectures including SeqGAN.",
                "The proposed model outperformed \nseveral baselines such as SeqGAN in terms of several metrics including BLUE score.",
                "Furthermore, there does not exist a well-defined metric in terms of quantitative evaluation that can be used to assess the quality of the generated music.",
                "Therefore, it is recommended to carry out a study that can define new metrics which measures the quality of music generation adequately.",
                "[23] \u201cWasserstein metric,\u201d Wikipedia.",
                "Available: https://en.wikipedia.org/w/index.php?title=Wasserstein_metric&oldid=1019498722 [24] Y. Liu, Z. Qin, Z. Luo, and H. Wang, \u201cAuto-painter: Cartoon image generation from sketch by using conditional generative adversarial networks,\u201d arXiv preprint arXiv:1705.01908, 2017."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "[25] B. Kuriakose, T. Thomas, N. E. Thomas, S. J. Varghese, and V. A. Kumar, \u201cSynthesizing Images from Hand-Drawn Sketches using Conditional Generative Adversarial Networks,\u201d in 2020 International Conference on Electronics and Sustainable Communication Systems (ICESC), 2020, pp."
            ],
            "pop": [
                "The other popular loss is the mean squared error (MSE) loss.",
                "The authors in [37] presented an adversarial and convolutional based architecture known as MidiNet for generating pop music monophonic melodies using 1022 pop music from an online MIDI database called TheoryTab [38].",
                "[36] Melody Generation LSTM-based GAN Bayesian Bi-LSTM generator and LSTM discriminator Average score of 3.27 on the three qualitative metrics, 48% likely to be detected as synthetic [37] Generate pop music monophonic melodies Modified DCGAN Cross entropy Two dense layers followed by four transposed CONV for generator; 2 CONV layers followed by a dense layer discriminator Mean score around 3 for being pleasant & realistic, 4 for interesting people with musical backgrounds, 3.4 for people without musical backgrounds"
            ],
            "Demo availability": [],
            "dataset": [
                "This is achieved upon successful training where the adversarial network can identify patterns in the data and learn the distribution of the dataset.",
                "Also, due to the advancement of technology and digitization, the two main requirements of GANs, datasets and computing power, have become widely available.",
                "The generator tries to generate fake samples, having similar distribution to the examples in the real dataset and continues to improve its network to trick the discriminator.",
                "The generator has no access to data points from the real dataset.",
                "Meanwhile, the discriminator has access to the ground truths, whether the data came from the generator or real dataset, which it can use to minimize its error.  \n Figure 2 A Basic GAN Architecture More sophisticated GAN architectures can make use of labels to generate data points for a specific category.",
                "The training dataset contains the pair of sketches as well as ground truth colored images.",
                "The authors utilized two datasets, namely the Minions dataset containing 1100 colored minions and the Japanimation dataset containing 6000 pictures of Japanese anime with 90% of the data being used for training and the remaining 10% for evaluation.",
                "The dataset used contains 10k images of 55 different styles such as impressionism, realism, and symbolism.",
                "The authors utilized the WikiArt dataset which consists of more than 80k paintings from 1119 artists ranging from the 15th to 20th century.",
                "[29] introduced a pre-modern Japanese art facial expression dataset.",
                "This dataset contains more than 5500 RGB images alongside labeled gender and social status.",
                "The authors did not evaluate the performance of the generations but rather introduced the \ndataset and encouraged researchers to work on improving the quality of the generations.",
                "The training data used contained labeled face and sketch pairs from two different datasets which were combined.",
                "Several datasets including MNIST was used to evaluate the network.",
                "The total number of samples after adding all the genres was 1998 (10 seconds MIDI files) from the Nottingham dataset [35].",
                "The dataset provides two channels for each tab, one for melody and the other one for the underlying chord progression.",
                "To generate additional training examples, augmentation was performed by circularly shifting all melodies and chords to any of the 12 keys, resulting in a final dataset of 50,496 melody and chord pairs.",
                "[40] and Reddit MIDI datasets were combined, which resulted in a total of 12,197 MIDI files.",
                "A similar study was conducted in [41] using the exact dataset and conditional-LSTM GANs.",
                "The dataset used consists of 3697 classical music from 160 different composers.",
                "To implement the GANs, the Lakh MIDI dataset was transformed into a multi-track piano-rolls representation.",
                "The dataset used comprises of over 16, 000 poems with each poem containing four lines of twenty characters.",
                "The approach of extending GANs to generate sequences of discrete tokens is suitable for poetry and other text generations because of the sequential nature of text datasets.",
                "As part of this work, the dataset collected for pairing image and poem by human annotators is made available online.",
                "Due to the dataset of painting to Shakespearean prose not being available, the authors utilized intermediate English poem description of the paintings before applying language style transfer to obtain the relevant prose style.",
                "A total of three datasets were used to solve this application.",
                "Two of the datasets were used for generating an English poem for a given image.",
                "For text style transfer, Shakespeare plays, and their corresponding English translation dataset was used.",
                "Another limitation of this approach is that the generations may suffer in quality when the style transfer dataset does not have similar words in the training set of sequences and consequently, the dataset must be expanded.",
                "For poetry generation, two Chinese poem datasets, named Poem-5 and Poem-7, were used with each containing 5 or 7 characters in short sentences, respectively.",
                "Both LSTM-based, as well as CNN-based GAN architectures, were experimented and the proposed model with LSTM outperformed the existing works on the aforementioned Poem-5 and Poem-7 datasets.",
                "The objective function of the generator is to generate a sentence that can potentially obtain a greater ranking score than the ones taken from the real dataset.",
                "On the other hand, the ranker\u2019s task is to rank the generated sentence to be of lower importance than the sentences from the real dataset.",
                "The proposed architecture was evaluated for poetry generation on a Chinese poem dataset containing over 13,000 five-word quatrain poems.",
                "The dataset used for poetry generation consists of 740 classical and contemporary English poems.",
                "Recent Advances in Literary Text Generation using GANs Source Task GAN Type Loss Function Generator-Discriminator Architecture Dataset Result",
                "[44] Chinese Poetry generation RL GAN Cross entropy and policy gradient RNN Generators and CNN discriminators 16394 Chinese quatrains BLEU-2 score of 0.74, overall score of 0.54 by human evaluators [45] Generate poetry from an image Multiadversarial GAN with an embedding model Cross entropy and policy gradient RNN Generators, GRU-based discriminator, CNN image encoder and RNN poem decoder Novel dataset with paired image and poetry Overall BLEU score of 0.77, 7.18 out of 10 overall score by human evaluators [46] Generate Shakespearean prose from a painting Multiadversarial GAN with encoder and decoder Cross entropy and policy gradient RNN Generator, CNN-RNN agent for encoding and decoding painting, LSTM encoder and decoder for generating prose Two datasets for generating English poem from an image, and Shakespeare plays and their English translations for text style transfer Average scores of 3.7, 3.9, and 3.9 out of 5 by evaluators for content, creativity, and similarity to Shakespearean style respectively [47] Chinese Poetry generation RL GAN Maximum-likelihood A single layer LSTM generator, two-layer Bi-directional LSTMs discriminator Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.76 and 0.55 for the two datasets respectively",
                "[48] Chinese Poetry generation RNN GAN Wasserstein distance LSTM generator and discriminator with WGAN-GP training Poem-5 and Poem-7 Chinese Poem dataset BLEU-2 scores of 0.88 and 0.67 for the two datasets respectively",
                "Moreover, the size of the dataset required for GAN training remains a challenge.",
                "The lowest dataset size from the existing works reviewed in Table 1 utilized at least 5000 images [29].",
                "Therefore, for novel applications, a lot of time is required in gathering the dataset first before applying the GAN training.",
                "B. Recommendations and Future Work In the context of visual arts generation, an experiment with smaller datasets should be carried out.",
                "It could potentially lead to the development of a GAN framework that is well suited to dealing with such datasets.",
                "In summary, for future work on art generation using GANs, we recommend the following: \u2022 Experiment with smaller dataset and GAN architectures for visual arts generation.",
                "[29] Y. Tian, C. Suzuki, T. Clanuwat, M. Bober-Irizar, A. Lamb, and A. Kitamoto, \u201cKaoKore: A Pre-modern Japanese Art Facial Expression Dataset,\u201d arXiv preprint arXiv:2002.08595, 2020.",
                "[35] \u201cjukedeck/nottingham-dataset,\u201d GitHub.",
                "https://github.com/jukedeck/nottingham-dataset (accessed May 25, 2021).",
                "[40] \u201cThe Lakh MIDI Dataset v0.1.\u201d"
            ]
        },
        {
            "title": "CONTROLLABLE DEEP MELODY GENERATION VIA HIERARCHICAL MUSIC STRUCTURE REPRESENTATION",
            "architecture": [
                "2 Sep 2021that analyze a song to derive music frameworks that can\nbe used in music imitation and subsequent deep learning\nprocesses, (4) a set of neural networks that generate a song\nusing the MusicFrameworks approach, (5) useful musical\nfeatures and encodings to introduce musical inductive bi-\nases into deep learning, (6) comparison of different deep\nlearning architectures for relatively small amounts of train-\ning data and a sizable listening test evaluating the musical-\nity of our method against human-composed music.",
                "Architecture of MusicFrameworks .",
                "3.2.2 Network Architecture\nWe use an auto-regressive model based on Transformer\nand LSTM.",
                "The architecture (Figure 5) consists of an en-\ncoder and a decoder.",
                "Transformer-LSTM architecture for melody, ba-\nsic melody and rhythmic pattern generation.",
                "We also use a Transformer-LSTM architecture (Figure\n5), but with different model settings (size).",
                "We also experimented\nwith other deep neural network architectures described in\nSection 4.1 for comparison.",
                "More details about the\nnetwork are in Section 4.1.\n4. EXPERIMENT AND EVALUATION\n4.1 Model Evaluation and Comparison\nAs a model-selection study, we compared the ability of\ndifferent deep neural network architectures implementing\nMusicFrameworks to predict the next element in the se-\nquence."
            ],
            "training parameters": [],
            "learning rate": [
                "For learning rate, we used the Adam opti-\nmizer with\f1 = 0:9;\f2 = 0:99;\u000f= 10\u00006, and the same\nformula in [22] to vary the learning rate over the course of\ntraining, with 2000 warmup steps."
            ],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "Society for Music Information\nRetrieval Conf., Online, 2021.tion",
                "[7] S. H. Hakimi, N. Bhonker, and R. El-Yaniv, \u201cBebop-\nnet: Deep neural models for personalized jazz impro-\nvisations,\u201d in Proc. of the 21st International Society for\nMusic Information Retrieval Conference, ISMIR .",
                "[20] A. Elowsson and A. Friberg, \u201cAlgorithmic composition\nof popular music,\u201d in the 12th International Confer-\nence on Music Perception and Cognition and the 8th\nTriennial Conference of the European Society for the\nCognitive Sciences of Music , 2012, pp.",
                "[32] L. Kawai, P. Esling, and T. Harada, \u201cAttributes-aware\ndeep music transformation,\u201d in Proc. of the 21st Inter-\nnational Society for Music Information Retrieval Con-\nference, ISMIR .",
                "Wang, T. Berg-Kirkpatrick, and S. Dub-\nnov, \u201cMusic sketchnet: Controllable music generation\nvia factorized representations of pitch and rhythm,\u201d in\nProc. of the 21st International Society for Music Infor-\nmation Retrieval Conference ."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "Instead of generating note onset time one by one, we gen-\nerate 2-beat rhythm patterns, which more readily encode\nrhythmic patterns and metrical structure."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [
                "Rhythm Accuracy is the percent of correctly pre-\ndicted 2-beat rhythm patterns."
            ],
            "melodic": [
                "This paper introduces MusicFrame-\nworks , a hierarchical music structure representation and\na multi-step generative process to create a full-length\nmelody guided by long-term repetitive structure, chord,\nmelodic contour, and rhythm constraints.",
                "Ad-\nditionally, we introduce new features to encode musical\npositional information, rhythm patterns, and melodic con-\ntours based on musical domain knowledge.",
                "A music framework is an abstract hierarchical description\nof a song, including high-level music structure such as re-\npeated sections and phrases, and lower-level representa-\ntions such as rhythm structure and melodic contour.",
                "We considered several options for imita-\ntion at this top level: (1) copy the \ufb01rst several measures\nof melody from the previously generated phrase (teacher\nforcing mode) and then complete the current phrase; (2)\nuse the same or similar basic melody from the previ-\nous phrase to generate an altered melody with a similar\nmelodic contour; (3) use the same or similar basic rhythm\nform of the previous phrase to generate a similar rhythm."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "[16] F. Liang, \u201cBachbot: Automatic composition in the\nstyle of bach chorales,\u201d University of Cambridge ,\nvol. 8, pp.",
                "[17] G. Hadjeres, F. Pachet, and F. Nielsen, \u201cDeepbach:\na steerable model for bach chorales generation,\u201d in\nProc. of the 34th International Conference on Machine\nLearning-Volume 70 ."
            ],
            "orchestral": [],
            "electronic": [
                "[9] L. Hiller and L. Isaacson, Experimental Music: Com-\nposition with an Electronic Computer ."
            ],
            "pop": [
                "A listening test\nreveals that melodies generated by our method are rated\nas good as or better than human-composed music in the\nPOP909 dataset about half the time.",
                "However, there are three problems that are dif\ufb01cult\nto address: (1) Modeling larger scale music structure and\nmultiple levels of repetition as seen in popular songs, (2)\nControllability to match music to video or create desired\ntempo, styles, and mood, and (3) Scarcity of training data\ndue to limited curated and machine-readable compositions,\nespecially in a given style.",
                "There are a few\nmodels using rule-based and statistical methods to con-\nstruct long-term repetitive structure in classical music [19]\nand pop music",
                "Machine learning models with\nmemory and the ability to associate context have also been\npopular in this area and include LSTMs and Transformers\n[4, 6, 22, 23], which operate by generating music one or a\nfew notes at a time, based on information from previously\ngenerated notes.",
                "StructureNet [3], PopMNet",
                "The most popular models are Vari-\national Auto-Encoders (V AE) and their variants",
                "We describe a controllable melody generation system that\nuses hierarchical music representation to generate full-\nlength pop song melodies with multi-level repetition and\nstructure.",
                "Our work is with pop music because structures are rel-\natively simple and listeners are generally familiar with the\nstyle and thus able to evaluate compositions.",
                "We use a Chi-\nnese pop song dataset, POP909",
                "Sections contain multi-\nple phrases, e.g., the illustrated song has an intro, a main\ntheme section (phrase A as verse and phrase B as chorus),\na bridge section followed by a repeat of the theme, and an\noutro section, which is a typical pop song structure.",
                "With the analysis algorithm, we can process a music\ndataset such as POP909 for subsequent machine learning\nand music generation.",
                "We used 4188 phrases from 528 songs in major mode\nfrom the POP909 dataset, using 90% of them as training\ndata and the other 10% for validation.",
                "We con\ufb01rmed that our generation exhibits sim-\nilar structure-related distributions to that of the POP909\ndataset.",
                "The demographics information about the\nlisteners are as follows:\nGender male: 120, female: 75, other: 1;\nAge distribution 0-10: 0, 11-20: 17, 21-30: 149, 31-40:\n28, 41-50: 0, 51-60: 2, >60: 0;\nMusic pro\ufb01ciency levels lowest (listen to music <1\nhour/week): 16, low (listen to music 1\u201315 hours/week):\n62, medium (listen to music >15 hours/week): 21, high\n(studied music for 1\u20135 years): 52, expert ( >5 years of\nmusic practice): 44;\nNationality Chinese: 180, Others: 16 (note that the\nPOP909 dataset is primarily Chinese pop songs, and lis-\nteners who are more familiar with this style are likely to be\nmore reliable and discriminating raters.)",
                "The last two pairs show the ratings of our\nmethod compared to music in the POP909 dataset.",
                "The human-composed songs used in this study are from\nthe most popular ones in Chinese pop history.",
                "Yang, \u201cPop music transformer:\nGenerating music with rhythm and harmony,\u201d arXiv\npreprint arXiv:2002.00212 , 2020.",
                "[20] A. Elowsson and A. Friberg, \u201cAlgorithmic composition\nof popular music,\u201d in the 12th International Confer-\nence on Music Perception and Cognition and the 8th\nTriennial Conference of the European Society for the\nCognitive Sciences of Music , 2012, pp.",
                "Wang, and R. B. Dannenberg, \u201cPer-\nsonalized popular music generation using imitation and\nstructure,\u201d arXiv preprint arXiv:2105.04709 , 2021.",
                "[24] J. Wu, X. Liu, X. Hu, and J. Zhu, \u201cPopmnet: Gener-\nating structured pop music melodies using neural net-\nworks,\u201d Arti\ufb01cial Intelligence , vol.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset for\nmusic arrangement generation,\u201d in Proc. of 21st Inter-\nnational Conference on Music Information Retrieval,\nISMIR , 2020.",
                "[36] S. Dai, H. Zhang, and R. B. Dannenberg, \u201cAuto-\nmatic analysis and in\ufb02uence of hierarchical structure\non melody, rhythm and harmony in popular music,\u201d in\nProc. of the 2020 Joint Conference on AI Music Cre-\nativity (CSMC-MuMe) , 2020.",
                "[38] S. Dai, H. Zhang, and R. B. Dannenberg, \u201cAuto-\nmatic analysis and in\ufb02uence of hierarchical structure\non melody, rhythm and harmony in popular music,\u201d in\nin Proc. of the 2020 Joint Conference on AI Music Cre-\nativity (CSMC-MuMe 2020) , 2020."
            ],
            "Demo availability": [],
            "dataset": [
                "A listening test\nreveals that melodies generated by our method are rated\nas good as or better than human-composed music in the\nPOP909 dataset about half the time.",
                "We use a Chi-\nnese pop song dataset, POP909",
                "With the analysis algorithm, we can process a music\ndataset such as POP909 for subsequent machine learning\nand music generation.",
                "We used 4188 phrases from 528 songs in major mode\nfrom the POP909 dataset, using 90% of them as training\ndata and the other 10% for validation.",
                "The full Transformer model performed poorly on this\nrelatively small dataset due to over\ufb01tting.",
                "We con\ufb01rmed that our generation exhibits sim-\nilar structure-related distributions to that of the POP909\ndataset.",
                "The demographics information about the\nlisteners are as follows:\nGender male: 120, female: 75, other: 1;\nAge distribution 0-10: 0, 11-20: 17, 21-30: 149, 31-40:\n28, 41-50: 0, 51-60: 2, >60: 0;\nMusic pro\ufb01ciency levels lowest (listen to music <1\nhour/week): 16, low (listen to music 1\u201315 hours/week):\n62, medium (listen to music >15 hours/week): 21, high\n(studied music for 1\u20135 years): 52, expert ( >5 years of\nmusic practice): 44;\nNationality Chinese: 180, Others: 16 (note that the\nPOP909 dataset is primarily Chinese pop songs, and lis-\nteners who are more familiar with this style are likely to be\nmore reliable and discriminating raters.)",
                "The last two pairs show the ratings of our\nmethod compared to music in the POP909 dataset.",
                "This observation can be de-\nrived from similar distribution and near random prefer-\nence distribution in \u201c1 vs 2\u201d and \u201c1 vs 4,\u201d indicating that\npreference for the generated basic melody and rhythm\nform are close to those of music in our dataset.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset for\nmusic arrangement generation,\u201d in Proc. of 21st Inter-\nnational Conference on Music Information Retrieval,\nISMIR , 2020."
            ]
        },
        {
            "title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer",
            "architecture": [
                "Second,\nwe propose a novel gated parallel attention module to be used in\na sequence-to-sequence (seq2seq) encoder/decoder architecture\nto more effectively account for a given conditioning thematic\nmaterial in the generation process of the Transformer decoder.",
                "Furthermore, we show that the vanilla sequence-to-sequence\n(seq2seq) encoder/decoder architecture [13], [30], [31] is still\nnot suf\ufb01cient to enforce the in\ufb02uence of the condition when the\ngenerated music gets longer.",
                "The network uses a BERT-like\narchitecture",
                "From EncoderOnly for\ntop half layers \n(a) (b) (c)\nFig. 4: (a) Schematic overview of the proposed Transformer architecture for theme-conditioned music generation.",
                "Diagrams of\nthe decoder architecture for (b) the basic seq2seq model that utilizes segment embedding ( SE), and (c) the proposed Theme\nTransformer that utilizes parallel attention modules with XOR gating (the two \u201c \n\u201ds) and separate positional encodings ( PEs).\nof some toy sequences with perceptually tolerable alternations\nwe fed to the embedding model (when the model converges)."
            ],
            "training parameters": [],
            "learning rate": [
                "[66] and teacher forcing to minimize the training negative\nlog-likelihood,\u0000PT\nt=1logp(xtjx<t), whereT= 512 , using\n2\u000210\u00004learning rate and a batch size of 8.\nAll our models have 8 heads for multi-head attention,\n256 hidden dimensions, 1,024-dim feed-forward layers, and\nGeLU as the activation function."
            ],
            "batch size": [
                "(3) with the temperature\nhyperparameter \u000bset to 0.5, using Adam as the optimizer\nand a batch size of 128 fragments, all of which are picked\nrandomly from a different piece each time.",
                "[66] and teacher forcing to minimize the training negative\nlog-likelihood,\u0000PT\nt=1logp(xtjx<t), whereT= 512 , using\n2\u000210\u00004learning rate and a batch size of 8.\nAll our models have 8 heads for multi-head attention,\n256 hidden dimensions, 1,024-dim feed-forward layers, and\nGeLU as the activation function."
            ],
            "ethical": [],
            "impact": [],
            "society": [
                "Society for Music Information Retrieval , vol."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "In the objective evaluation, weuse some general metrics from [26] and some novel theme-\nspeci\ufb01c metrics proposed here.",
                "Furthermore, the retrieval model learns a representation\nz= Emb(S)for each fragment S, and accordingly a distance\nmetric for evaluating the similarity of two fragments through\na neural network, instead of using hand-crafted features.",
                "Because the embedding model is trained to discriminate\nsimilar and dissimilar fragments, we will also use it to set up\nobjective metrics to evaluate the performance of our model:\nD(Si;Sj)",
                "Second, following [17], we use\nthe following metric-related tokens to represent the advance\nin time.",
                "Accordingly, we can faithfully represent a\nmelody without metric-related tokens.",
                "However, this is not the\ncase for the theme-based models, because the models may not\n10We note that, the pieces generated by our Transformer models are\nsequences of the piano tokens using the vocabulary on the left hand side of\nTable I. To get the melody embedding of two-bar fragments of the generated\npieces for computing the objective metrics theme inconsistency and theme\nuncontrollability (cf.",
                "O BJECTIVE EVALUATION\nA. Evaluation Setup and Metrics",
                "We evaluate the result using two sets of metrics.",
                "The \ufb01rst set has three metrics, each concerning a different\naspect.",
                "The \ufb01rst metric uses the overlapping area",
                "The last\nmetric, grooving consistency , as proposed by [26], studies the\ncoherence in rhythm.",
                "The second set of metrics are all theme-related and are\nnewly proposed here.",
                "The result on the theme-related metrics shows that the\nTheme Transformer performs much better than the basic\nseq2seq Transformer model in harnessing the condition inJOURNAL OF L ATEX CLASS FILES, VOL.",
                "We do not use theme-related metrics for the prompt-based baseline, for it does not generate theme tokens.\nsequence #self-attnSEseparate Melody Theme incon-",
                "The third row of Table III shows that this degrades the perfor-\nmance in all the metrics, in particular theme uncontrollability.",
                "This\nsuggests that the proposed gated parallel attention contributes\nto the \ufb01rst two metrics, while the idea of separate PEs\ncontributes to the latter metric.\n3) On the number of trainable parameters: Because of the\nadditional encoder part of the Theme Transformer, the total\nnumber of trainable parameters of the Theme Transformer is\nroughly twice than that of the baseline decoder-only model in\nthe evaluations reported in the main body of the paper, which\nmay not be fair.",
                "For listeners,\nwe convert each original and generated token sequence into a\nMIDI \ufb01le, and then render it into audio by FluidSynth (https:\n//pypi.org/project/midi2audio/), without additional mixing.11\nB. Listening Test: Result\nTable IV shows that, from the result of both user groups,\nthe Theme Transformer outperforms the other two models\nacross all metrics, especially in the \ufb01rst three theme-related\nones.",
                "Besides, the Theme\nTransformer obtains similar MOS as the original data for all\nthe theme-related metrics, demonstrating its effectiveness.",
                "Moreover, there is a gap between the Theme Transformer\nand the original pieces in the last two \u201coverall\u201d metrics, calling\nfor future improvement again.",
                "Interestingly, comparing the\nresult of the overall metrics of the two user groups, it appears\nthat the subjects can more easily notice the difference between\nthe prompt-based baseline and the Theme Transformer with\nthe original pieces as a reference.",
                "[51], a geometric-based musical pattern min-\ning method that views the melody notes as dots in a\n2-dimensional space of fonset, pitchgand then extracts\nthe \u201ctranslational equivalence class\u201d (TEC) groups from\nthe melody.",
                "[48] O. Lartillot, \u201cIn-depth motivic analysis based on multiparametric closed\npattern and cyclic sequence mining,\u201d in Proc."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "Qiu, Z. Zhang, and G. Xia, \u201cMelodic\nphrase segmentation by deep neural networks,\u201d 2018, arXiv preprint\narXiv:1811.05688 .",
                "[59] A. Uitdenbogerd and J. Zobel, \u201cMelodic matching techniques for large\nmusic databases,\u201d in Proc.",
                "Those valleys in the curves, especially those below \u000f= 0:13, indicate fragments that are similar to S1(which\nis highly related to the conditioning theme) in their melodic content."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "To condition\nthe generation process of such a model with a user-speci\ufb01ed\nsequence, a popular approach is to take that conditioning se-\nquence as a priming sequence and ask a Transformer decoder to\ngenerate a continuation.",
                "We report on objective and subjective evaluations of variants of\nthe proposed Theme Transformer and the conventional prompt-\nbased baseline, showing that our best model can generate, to\nsome extent, polyphonic pop piano music with repetition and\nplausible variations of a given condition.",
                "Among such efforts, the adoption of the Transformer decoder-\nbased neural network [12] as the backbone generative model\nhas become popular [13]\u2013[24].",
                "The original piece here is an excerpt of the\nsong \u2018907.mid\u2019 from the test split of the POP909 dataset\n[25], starting from its thematic fragment.",
                "Note that the embedding distance as the\naverage of the whole POP909 dataset is 0.895.",
                "Similar to the popular \u201cprompt-based\u201d approach,\nthe proposed approach also uses a music fragment (e.g., a\nmusical theme) as input to condition the generative model.",
                "2: The piano roll of the \ufb01rst 24 bars of a composition by Theme Transformer, conditioned on the theme of an unseen\ntesting song \u2018899.mid\u2019 from POP909.",
                "Speci\ufb01cally, we train the\nmodels using the training split of the POP909 dataset [25],\nand conduct objective and subjective evaluations on its test\nsplit.",
                "The subjective evaluation en-\ntails an online listening test that involves listeners familiar\nand unfamiliar with the music pieces in the POP909 dataset.",
                "Our evaluation shows that the proposed Theme Transformer\nexhibits similar perceptual theme controllability and theme\nvariation as original pieces in the test split of POP909.",
                "Prompt-based conditioning is arguably to date the most\npopular approach to condition the generation process of the\nTransformer decoder via a conditioning sequence [42]\u2013[45].",
                "We identify the melody notes for each fragment using\nthe annotations of POP909 dataset.",
                "Speci\ufb01cally, we adopt the\nfollowing three rules, motivated by common techniques in pop\nsong writing:\n\u000fPitch shift on scale : Keep the same contour of a melody\nbut shift them according to their positions in the musical\nscale.",
                "5According to our own listening of the original pieces in POP909, a music\nphrase in POP909 usually lasts for two bars, possibly due to the comfortable\nbreath length for human singing.",
                "I MPLEMENTATION DETAILS\nA. Dataset\nWe train our models using the piano covers of Mandarin\npop music from the POP909 dataset [25], which is composed\nof the piano covers of 909 Mandarin pop songs originally\ncomposed by 462 artists, released from the earliest in 1950s\nto the latest around 2010",
                "Following [22], we pick only the songs with a 4/4 time\nsignature and quantize the note onset times and duration to be\nmultiples of 1/4 beat (ignoring triplets) for simplicity, using the\nbeat annotations provided by POP909.",
                "B. Token representation\nWe consider three types of tokens to represent each song\nfrom POP909 by a token sequence.",
                "In the objective evaluation, we let each model generate 64-\nbar polyphonic piano music using the thematic conditions\nretrieved from each of the 29 testing songs of POP909.",
                "The scores are in general closer to those of the original pieces (i.e., test split of\nPOP909) the better.",
                "The \ufb01rst user group\nis familiar with Mandarin pop music, while the second group does not.",
                "Therefore, we require\nour subjects to self-report whether they are familiar with\nMandarin pop music, and only ask those who are unfamiliar\nto listen to the original pieces.",
                "We ask the\nsubjects to indicate (yes or no) whether they are familiar\nwith Mandarin pop music by the question: \u201cDo you listen\nto Mandarin pop songs?\u201d",
                "11Please note that, the \u201coriginal pieces\u201d used in the objective and subjective\nevaluations are actually not the original MIDI \ufb01les provided by POP909, but\ninstead MIDI \ufb01les converted from our token sequences.",
                "We conjecture that this can be attributed to\nthe short sequence length N= 512 , which amounts to about\n9 bars of music only in POP909.",
                "On the Quality of Theme Retrieval\nTo have an idea of the performance and limit of the pro-\nposed \u201ccontrastive learning +clustering\u201d based theme retrieval\nmethod (denoted as CLhereafter), we invite three annotators\nwith some musical training to annotate the theme and its occur-\nrences of six random songs from the test set of POP909.12We\nhave three annotators to account for the possible subjectivity\nof the theme labeling.",
                "However, as\nmentioned in Section IX-B, a music piece (e.g., a sonata, or\na pop song in verse-chorus form) can often contain multiple\nthemes, which may be similar or in stark contrast to each\nother.",
                "[2] A. Elowsson and A. Friberg, \u201cAlgorithmic composition of popular\nmusic,\u201d in Proc.",
                "Yang, \u201cPop Music Transformer: Beat-based\nmodeling and generation of expressive pop piano compositions,\u201d in Proc.",
                "Liu, \u201cPopMAG:\nPop music accompaniment generation,\u201d in Proc.",
                "Zhang, M. Xu, S. Dai, X. Gu, and G. Xia,\n\u201cPOP909: A pop-song dataset for music arrangement generation,\u201d in\nProc.",
                "6: The piano roll of the \ufb01rst 24 bars of a composition by Theme Transformer, conditioned on the theme of unseen testing\nsongs (from top to bottom : \u2018875.mid\u2019 \u2018888.mid\u2019 \u2018890.mid\u2019 \u2018893.mid\u2019 \u2018894.mid\u2019 \u2018900.mid\u2019 \u2018901.mid\u2019 \u2018904.mid\u2019) from POP909.\n(Melody in magenta, accompaniment in grey, generated theme regions shaded in pink)."
            ],
            "Demo availability": [],
            "dataset": [
                "The original piece here is an excerpt of the\nsong \u2018907.mid\u2019 from the test split of the POP909 dataset\n[25], starting from its thematic fragment.",
                "Note that the embedding distance as the\naverage of the whole POP909 dataset is 0.895.",
                "Speci\ufb01cally, we train the\nmodels using the training split of the POP909 dataset [25],\nand conduct objective and subjective evaluations on its test\nsplit.",
                "The subjective evaluation en-\ntails an online listening test that involves listeners familiar\nand unfamiliar with the music pieces in the POP909 dataset.",
                "The only\npublic theme-related dataset, the Musical Theme Dataset",
                "We identify the melody notes for each fragment using\nthe annotations of POP909 dataset.",
                "[64]), in our music generation task the length\nof the target sequences may well be longer than 64 bars, which\ntranslate to roughly 4,000 tokens (elements) for our dataset.",
                "I MPLEMENTATION DETAILS\nA. Dataset\nWe train our models using the piano covers of Mandarin\npop music from the POP909 dataset [25], which is composed\nof the piano covers of 909 Mandarin pop songs originally\ncomposed by 462 artists, released from the earliest in 1950s\nto the latest around 2010",
                "In total, our vocabulary for the piano contains 730 unique\ntokens.9The songs in our dataset have \u001895 bars on average,\nwhich translate to 5,249 tokens per song on average using\nthe piano representation.",
                "In our dataset, the\naverage length of such a melody sequence, which is fed to\nthe embedding model, is 24.3 tokens.10\nTable I lists the token types and the number of unique tokens\nfor each type in our token representation of the piano and the\nmelody, respectively.\nC. Model settings\nWe use a 6-layer encoder and a 6-layer decoder (i.e.,\nL= 6) for both the basic seq2seq Transformer and the\nproposed Theme Transformer .",
                "12We are aware of two other relevant dataset, the MTD",
                "[3] and JKU\nPattern Dataset",
                "A multi-\nmodal dataset of musical themes for MIR research,\u201d Transactions of the\nInt.",
                "Zhang, M. Xu, S. Dai, X. Gu, and G. Xia,\n\u201cPOP909: A pop-song dataset for music arrangement generation,\u201d in\nProc."
            ]
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "architecture": [
                "The tiered architecture of\nthe SampleRNN allows for different computational focus to be\napplied to different levels of abstraction of the audio, which\nallows long term dependencies to be modelled ef\ufb01ciently.\nJukebox.",
                "We use a classi\ufb01er architecture [25] that came fourth in the\nMediaEval 2019 competition with respect to macro-averaged\nPR-AUC.",
                "This analysis depends on the behaviour of the classi\ufb01er, so\nfuture work should explore the effect of different classi\ufb01er\narchitectures."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [
                "One hypothesis for\nthis behaviour is that these are both priming based generation\nmethods: the \ufb01nal generated sample may deviate signi\ufb01cantly\nfrom the priming sample class, impacting the augmentation\nbehaviour in this numerous class setting."
            ],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "Recent work [12] proposes simple,\nmusically informed metrics to evaluate the musicality of gen-\nerated music.",
                "However these metrics do not capture abstract\nqualities of music such as its emotion or mood.",
                "We measure classi\ufb01er performance according to the three\nmetrics that were used in the MediaEval 2019 competition:\nF1-score, area under the precision-recall curve (PR-AUC),\nand the area under the Receiver Operator Characteristics\ncurve (ROC-AUC).",
                "We adopt two means of averaging each\nmetric: a) micro-averaged, which is the harmonic mean of the\noverall metric scores and b) macro-averaged, which performs\nan unweighted average of class-speci\ufb01c metric scores.",
                "We\nevaluate performance on the test partition of each dataset with\nrespect to micro and macro averaged metrics, calculated in the\nsame manner as the MediaEval 2019 competition submission.",
                "We observe from Table III(a) that DDSP\u2019s\nsamples yielded a consistent increase in performance with\nrespect to micro and macro averaged metrics; a quantitative\nindication that these samples contain meaningful information.",
                "Only Jukebox samples yield consistent\nperformance increase with respect to micro-averaged metrics,implying that performance was unequal across classes.",
                "[10] N. Chen, A. Klushyn, R. Kurle, X. Jiang, J. Bayer, and P. Smagt,\n\u201cMetrics for deep generative models,\u201d in International Conference on\nArti\ufb01cial Intelligence and Statistics ."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [
                "In order to\ngain insights respective to which types of music the generation\nmodels can generate, we utilise a second label set: this is\nan almost fully overlapping subset of the \ufb01rst, on which we\nperformed a more coarse relabelling based on a psychology-\nbased interpretation of the original labels."
            ],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "This is the \ufb01rst attempt at\naugmenting a music genre classi\ufb01cation dataset with conditionally\ngenerated music.",
                "We investigate the classi\ufb01cation performance\nimprovement using deep music generation and the ability of\nthe generators to make emotional music by using an additional,\nemotion annotation of the dataset.",
                "Finally, we\nperform the same experiments on an almost comprehensive\nsubset of our dataset with a relabelling, which we introduce,\npertaining to coarser arousal/valence emotion classes.",
                "MTG-J AMENDO DATASET",
                "For our experiments we use the MTG-Jamendo dataset [21],\nwhich is a large collection of labelled, high-quality commercial\nmusic.",
                "Distribution of total duration of music for each emotional class for\nthe dataset used in this study.",
                "We\nevaluate performance on the test partition of each dataset with\nrespect to micro and macro averaged metrics, calculated in the\nsame manner as the MediaEval 2019 competition submission.",
                "the same hyperparameters reported in its submission paper\n[25].\nA. Augmentation Policy\nFor each generative method, we generate an equal duration\nof music per class, with the total length of generated music\nequal to 5 % of the duration of the train split of each dataset\n(8 hours for mood/theme, 7.85 hours for emotional).",
                "We trained\nSampleRNN and DDSP on the training partition of each\ndataset and used performance on the validation partition to\ntune hyperparameters.",
                "C ONCLUSION & F UTURE WORK\nFigure 3 reveal that no model could generate music with\nmeaningful features for allclasses of each dataset.",
                "A gener-\native model for raw audio,\u201d arXiv preprint arXiv:1609.03499 , 2016.[6] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-Z. A. Huang,\nS. Dieleman, E. Elsen, J. Engel, and D. Eck, \u201cEnabling factorized\npiano music modeling and generation with the MAESTRO dataset,\u201d in\nInternational Conference on Learning Representations , 2019.",
                "[21] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra,\n\u201cThe mtg-jamendo dataset for automatic music tagging,\u201d in Machine\nLearning for Music Discovery Workshop, International Conference on\nMachine Learning (ICML 2019) , Long Beach, CA, United States,\n2019."
            ]
        },
        {
            "title": "FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control",
            "architecture": [
                "Especially the Transformer architecture (Vaswani\net al., 2017), popularized in the context of Natural Language\nprocessing (Brown et al., 2020) and then successfully ap-\n1Department of Computer Science, ETH Z \u00a8urich, Z \u00a8urich,\nSwitzerland.",
                "Karras, T., Laine, S., and Aila, T. A Style-Based\nGenerator Architecture for Generative Adversarial Net-\nworks."
            ],
            "training parameters": [],
            "learning rate": [
                "We use the inverse-square-root learning rate\nschedule with initial constant warmup at 10\u00004given by\n10\u00004=max(1;p\nn=N)whereN= 4000 is the number of\nwarmup steps."
            ],
            "batch size": [
                "We train each model for\n100k steps with a batch size of 512 sequences.",
                "Due to GPU\nmemory constraints we reduce the context size from 2048 to 1024 and use an accumulated batch size of 16, ensuring stable\ntraining."
            ],
            "ethical": [
                "Ethical Considerations\nAutomatic music generation may raise ethical concerns sim-\nilar to those of large language models."
            ],
            "impact": [],
            "society": [],
            "copyright": [
                "Copyright 2022 by the authors.plied in several other Machine Learning tasks (Dosovitskiy\net al., 2021; Lample & Charton, 2019; Biggio et al., 2021),\nhas proven to be a powerful tool for musical sequence mod-\nelling.",
                "The model might also reproduce copyrighted material that is\npresent in the training data and potentially generate samples\nthat infringe on copyright law."
            ],
            "evaluation metrics": [
                "Evaluation Metrics\n5.3.1.",
                "We see a drop in performance in all evaluation metrics for\nevery model compared to the conditional generation task,\nwhich is expected due to distributional shifts in the data.",
                "mv 1\njNjP\nn2NVELOCITY (n)\nmd 1\njNjP\nn2NDURATION (n)\nQuantize nd,mp,mvandmd\ndi (i;ts;nd;mp;mv;md)klist(I)klist(C)\nd dkdi\nend for\nreturn d\nC. Evaluation Metrics\nC.1."
            ],
            "metric": [
                "Evaluation Metrics\n5.3.1.",
                "We use perplexity (PPL) as a way to measure \ufb02uency and\nto compare the likelihood of different models in addition\nto task-speci\ufb01c metrics.",
                "Metrics are computed as an empirical estimate\nover the test distribution.",
                "We compute accuracy metrics for categorical\nvalues, namely for instruments, chords and time signature.",
                "Previous work has used the\noverlapping area (OA) metric to quantify similarity between\ntwo musical sequences for a given feature (Choi et al., 2020;\nWu & Yang, 2021).",
                "However, we \ufb01nd that the standard OA\nmetric fails to take the order of the sequences into account,\nas feature histograms are computed over the entire sequence.",
                "We use the MOA metric to compute\nsimilarity in pitch, velocity and duration between ground\ntruth and reconstruction.",
                "Perhaps unsurprisingly, FIGARO (expert) performs very\nwell on all metrics that are directly present in the expert\ndescription.",
                "(2020) on most metrics by a slight\nmargin.",
                "However, the model experiences\nposterior collapse in our experiments when trained on the\ndiverse LakhMIDI dataset, which is apparent by the low\nentropy of the model distribution (see Table 2) as well as the\nworse-than-unconditional performance on some description\nmetrics.",
                "0.561 0.996 0.319 0.759 0.658 0.514 0.712 0.637\nFIGARO (learned) 1.973 0.594 0.195 0.969 0.738 0.701 0.653 0.546 0.544 0.697\nFIGARO 1.705 0.960 0.593 0.997 0.238 0.827 0.735 0.748 0.790 0.853\n(a) Conditional generation perplexity and similarity metrics.",
                "ModelFluency Accuracy Fidelity\nPPL# I\" C\" TS\" ND# P\" V\" D\"sc\"sg\"\nChoi et al. (2020) 2.213 0.441 0.129 0.808 1.407 0.603 0.396 0.448 0.437 0.643\nFIGARO (expert) 1.824 0.944 0.524 0.992 0.384 0.741 0.559 0.497 0.705 0.575\nFIGARO (learned) 2.186 0.381 0.128 0.829 0.831 0.649 0.424 0.478 0.446 0.614\nFIGARO 1.782 0.917 0.514 0.988 0.335 0.807 0.702 0.694 0.748 0.744\n(b) Zero-shot medley generation perplexity and similarity metrics.",
                "ModelFluency Accuracy Fidelity\nPPL# I\" C\" TS\" ND# P\" V\" D\"sc\"sg\"\nFIGARO (expert) 1.894 0.955 0.553 0.996 0.360 0.700 0.646 0.434 0.710 0.639\n- w/o instruments 1.980 0.373 0.568 1.000 0.424 0.674 0.586 0.436 0.687 0.625\n- w/o chords 2.023 0.895 0.100 0.995 0.564 0.672 0.603 0.413 0.294 0.615\n- w/o meta information 1.966 0.908 0.536 0.795 0.878 0.574 0.205 0.334 0.636 0.584\n(c) Ablation study perplexity and similarity metrics.",
                "(2018) and MuseMorphose (Wu & Yang, 2021)\non perplexity (PPL) and similarity metrics.",
                "Similarity metrics include instrument F1-score (I), chord F1-score (C) and time signature\naccuracy (TS) as well as note density NRMSE (ND), pitch MOA (P), velocity MOA (V), duration MOA (D), chroma similarity scand\ngrooving similarity sg.",
                "We see a drop in performance in all evaluation metrics for\nevery model compared to the conditional generation task,\nwhich is expected due to distributional shifts in the data.",
                "As one would expect, removing each component reduces\nthe performance signi\ufb01cantly in the respective metrics, in-\ndicating that each component carries useful information\nnot entirely inferable through the remaining components.",
                "Interestingly, our experiments show that removing any com-\nponent slightly decreases the over-all performance even in\nmetrics that we would not necessarily expect to be affected.",
                "mv 1\njNjP\nn2NVELOCITY (n)\nmd 1\njNjP\nn2NDURATION (n)\nQuantize nd,mp,mvandmd\ndi (i;ts;nd;mp;mv;md)klist(I)klist(C)\nd dkdi\nend for\nreturn d\nC. Evaluation Metrics\nC.1.",
                "Macro Overlapping Area\nAs used by previous work, the overlapping area (OA) metric does not consider the sequential order of the investigated\nfeature, as feature histograms are computed over the entire sequence.",
                "To alleviate this limitation, we adapt the OA metric to also consider temporal order."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Especially the Transformer architecture (Vaswani\net al., 2017), popularized in the context of Natural Language\nprocessing (Brown et al., 2020) and then successfully ap-\n1Department of Computer Science, ETH Z \u00a8urich, Z \u00a8urich,\nSwitzerland.",
                "Brunner et al. (2018) propose MIDI-\nV AE as a method for conditional generation as well as genre\ntransfer between pop and jazz music.",
                "Pop Music Transformer: Beat-\nbased Modeling and Generation of Expressive Pop Piano\nCompositions."
            ],
            "Demo availability": [],
            "dataset": [
                "Dataset\nWe use the LakhMIDI dataset (Raffel, 2016) as training data\nin all of our experiments, which to the best of our knowl-\nedge is the largest publicly available symbolic music dataset.",
                "However, the model experiences\nposterior collapse in our experiments when trained on the\ndiverse LakhMIDI dataset, which is apparent by the low\nentropy of the model distribution (see Table 2) as well as the\nworse-than-unconditional performance on some description\nmetrics.",
                "In terms of sample quality, our method\nbeats other state-of-the-art symbolic music generative mod-\nels trained on the LakhMIDI dataset.",
                "Unlike the original work, we do not use data\naugmentation since the dataset is large enough and in order to allow for fair comparison between the models.",
                "We limit the training data to a subset of the entire dataset (20k samples) due to technical limitations.",
                "This is still considerably more training data than what was used in the original paper (1k samples) and should not affect\nperformance signi\ufb01cantly compared to using the full dataset."
            ]
        },
        {
            "title": "Music Generation Using an LSTM",
            "architecture": [
                "Enlisting a mapping \nfunction and designing a model architecture implementing fo ur-layer types, LSTM, \ndropout, dense, and the activation layer, we used the structure of the network presented \nhere as a springboard for our work.",
                "Although there are a multitude of methods that apply to artificially generating music, \narchitecture design r emains nontrivial, training parameters remain poorly understood, and \nissues of overfitting and ambiguity in the most effective method to model complex data \nfeatures abound.",
                "Ultimately, in their paper An Empirical Exploration of Recurrent Network Architectures \nthey found through a straightforward architecture search that  the existing  structure of \nLSTMs can\u2019t be beat for most applications.",
                "We then repeated the architecture \nsearch process on the best performing models.",
                "We \ntake inspiration from evolutionary architecture and \nhyperparameter search in this process, however as \nmusic quality is a subjective metric without a trivial \nautomatic validation metric there is no way that \nevolution can be done without human intervention.",
                "In the same sense, \nour network\u2019s tiered LSTM architecture  may allow it to pick apart the various  aspects of \nmusic such as if it is building tension , speeding up, or looking at any other type of pattern \nthat may be found in music.",
                "\u201cAn Empirical Exploration \nof Recurrent Network Architectures.\u201d"
            ],
            "training parameters": [
                "Although there are a multitude of methods that apply to artificially generating music, \narchitecture design r emains nontrivial, training parameters remain poorly understood, and \nissues of overfitting and ambiguity in the most effective method to model complex data \nfeatures abound."
            ],
            "learning rate": [
                "8  \n 3 Experiments  \nTo find which network structures and \nhyperparameters would yield the best results, we \ncreated a multitude of models with variations in \nhyperparameters such as batch size and learning rate, \nas well as the optimizations and activation functions, \nand the struct ure and size of our layers , then for each \nof those variations we generated a set of five songs \nwhich start predictions from the same input."
            ],
            "batch size": [
                "8  \n 3 Experiments  \nTo find which network structures and \nhyperparameters would yield the best results, we \ncreated a multitude of models with variations in \nhyperparameters such as batch size and learning rate, \nas well as the optimizations and activation functions, \nand the struct ure and size of our layers , then for each \nof those variations we generated a set of five songs \nwhich start predictions from the same input."
            ],
            "ethical": [],
            "impact": [
                "This paper presents the application of a n LSTM network for music  generation , includes a \nbrief, comprehensible overview into the intuition and theory behind LSTMs and their \napplication in music sequence modeling, provides an analysis on the benefits and \ndisadvantages of our network and training data, shows the qualitati ve impact on our \nmodel output, and discusses potential improvements for our network."
            ],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "We \ntake inspiration from evolutionary architecture and \nhyperparameter search in this process, however as \nmusic quality is a subjective metric without a trivial \nautomatic validation metric there is no way that \nevolution can be done without human intervention."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "1  \n  \n \nMusic Generation Using an LSTM  \n \nMichael Conner, Lucas Gral,  Kevin Adams  \nDavid Hunger, Reagan Strelow, and Alexander Neuwirth  \nDepartment of Electrical Engineering and Computer Science  \nMilwaukee School of Engineering  \n{connerm, grall, adamsk, hungerd, strelowr, neuwirtha}@msoe.edu  \n \n \nAbstract  \nOver the past several years, deep learning for sequence modeling has grown in popularity."
            ],
            "Demo availability": [],
            "dataset": [
                "To convert the MIDI files in our dataset into an input format that our model can train on, \nwe first utilized the Music21 library\u2019s  converter to parse MIDI data into a numerical \nformat."
            ]
        },
        {
            "title": "Prote\u00e7\u00e3o intelectual de obras produzidas por sistemas baseados em intelig\u00eancia artificial: uma vis\u00e3o tecnicista sobre o tema",
            "architecture": [],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [
                "como um hitem potencial, mas o seu lan\u00e7amento n\u00e3o teve o impacto esperado\nlogo de imediato.",
                "e o potencial impacto catastr\u00f3\ufb01co de tais sistemas",
                "A de\ufb01ni\u00e7\u00e3o\ndas medidas de desempenho tem impacto crucial na utilidade do modelo-padr\u00e3o, tal como\nevidenciado no supramencionado document\u00e1rio Coded Bias .",
                "Positioning and presenting\ndesign science research for maximum impact."
            ],
            "society": [
                "Resumo\nThe pervasiveness of Arti\ufb01cial Intelligence (AI) is unquestionable in our society."
            ],
            "copyright": [
                "On this occasion, who owns the\ncopyrightofaworkproducedfromasystembasedonArti\ufb01cialIntelligence?",
                "Thisessayaimstocontributewithatechnicistviewonthediscussionof\ncopyright applicability from works produced by AI."
            ],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": []
        },
        {
            "title": "An adaptive music generation architecture for games based on the deep learning Transformer model",
            "architecture": [
                "An adaptive music generation architecture for games\nbased on the deep learning Transformer model\nGustavo Amaral Costa dos Santos1Augusto Ba\u000ba1Jean-Pierre Briot2;1",
                "furtado@inf.puc-rio.br\nAbstract: This paper presents an architecture for generating music for video games based on the Trans-\nformer deep learning model.",
                "[cs.SD]  10 Sep 2022With the above mentioned principles in mind, after several experiments, we opted for the Transformer\narchitecture [39], because it better captures the long-term structure of music [12].",
                "In particular, we want to\nmove towards collaborative and interactive control of the music components generated by the Transformer-based\narchitecture.",
                "The following sections introduce the design, implementation and preliminary experiments with a proto-\ntype architecture for generating personalized and adaptive music for games aligned with the above mentioned\nprinciples.",
                "2.2 Architecture of the Adaptive Music System\nThe architecture of (AMS)",
                "The comparison of our proposed model with the much more complete and robust AMS architecture is twofold.",
                "Furthermore, we use a deep learning architecture (Transformer) better suited to\ncapture long-term coherence in music.\nFigure 1: AMC architecture, reproduced from [15]\n3 Adaptability versus Continuity and other Design Issues\nA pure generative approach is some kind of ideal, as it could in principle combine personalization (learnt styles)\nwith real-time adaptation (to the game and players situation).",
                "In the following sub-sections, we\nwill describe and motivate various aspects and components of the architecture and of the generation process,\nnamely: the general design principles; the curation and pre-processing of the training musical examples; the way\nmusic generated is layered; the emotion model chosen to map the game play into some control of the generated\nmusic; the mapping discipline; the complete architecture; the implementation; and the preliminary evaluation.",
                "4.1 Design Principles\nAfter having at \frst experimented with a recurrent neural network architecture of type LSTM (part of Google's\nMagenta project library)",
                "[20], we selected the Transformer architecture for its ability to enforce consistency and\nstructure, by better handling long-term correlations.",
                "Transformer [39] is an important evolution of a Sequence-\nto-Sequence architecture (based on RNN Encoder-Decoder), where a variable length sequence is encoded into a\n\fxed-length vector representation which serves as a pivot representation to be iteratively decoded to generate\na corresponding sequence (see more details, e.g., in [8, Section 10.4]).",
                "For more details on the architecture, illustrated in Fig. 2, please see\nthe original article",
                "These layers are generated from the same learning corpus, but from di\u000berent seeds (starting sequences) and\nwith di\u000berent generation parameters (currently, we vary a temperature parameter that controls the determinism\nof the generation, for some more likely or more unpredictable result), depending on the controlling model (as\n4Figure 2: Transformer architecture, reproduced from [39]\nwill be presented in Section 4.5).",
                "4.6 Architecture\nThe \row logic of current architecture, illustrated in Fig. 6, is as follows:\n1.",
                "Figure 6: Final architecture \row\n4.7 Implementation\nTo optimize the music generation process, at least one music corresponding to each strategy is saved in memory.",
                "The architecture is designed as a server responsible for music generation, for various possible game clients, based\non game engines like Unity or Unreal, or speci\fc ones.",
                "4.8 Evaluation\nCurrent architecture has been tested with an emulated game model and with music generated from a corpus\nof ambient music.",
                "5.2 Interactive Coordination\nA more radical approach is to substitute the sequencer-like platform (currently, Ableton Live) by a more\ngeneral platform for interactive and collaborative control of musical components (being generated by our current\nTransformer-based architecture).",
                "It separates the macro-level coordination from the actual micro-level components, as for\narchitectural/coordination languages in software architectures",
                "The Skini platform\n9(whose architecture is shown in Fig.",
                "Figure 8: Skini architecture\n6 Conclusion\nIn this paper, we have presented an architecture, based on deep learning (more speci\fcally, the Transformer\narchitecture), for generating music for video games, personalized to the user musical preference.",
                "Our current architecture is a proof of concept, although it is complete and functional.",
                "We\nare currently working on the design of a next version architecture and its coupling with the coordination level\nbased on the Skini architecture.",
                "Software architecture { Perspectives on an emerging discipline ."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "Emotions in turn will modulate the selection among melodies\n(choosing the melodic theme assigned to currently highest activated concept or a\u000bect), with the instantiation of\ntheir characteristics being managed by a spreading activation model."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [
                "And, perhaps even more importantly,\nwe should seek a model to support instrumental layers used in video game composition (such as the practice\nof \\striping\", which is to record orchestral sections separately for future mixing according to the whims of the\ncomposer).",
                "4.3 Layering\nWe consider layers of music, analogous to the production of orchestral music for games [37], with currently up\nto 4 layers:\n\u20221st layer, the most conservative and neutral;\n\u20222nd layer, to add more excitement, e.g., though some additional instrument;\n\u20223rd and 4th layers, to intensify the immersion and the tension.",
                "It allows de\fning some kind of\n\\orchestral blueprint\" (actually, some cartography of possible paths) for activating various musical components\nof a piece of music.",
                "Fig. 7 shows an example of visual orchestral blueprint (musical \row) in Skini.\nFigure 7: Example of orchestral \row in Skini (Opus1 Piece by Bertrand Petit)."
            ],
            "electronic": [
                "Experimental Music: Composition with an Electronic Computer ."
            ],
            "pop": [
                "It recently became popular for such applications\nas: translation, text generation (e.g., the Generative Pre-trained Transformer 3 aka GPT-3 model), biological\nsequence analysis and music generation [12].",
                "From Pac-Man to Pop Music Interactive Audio in Games and New Media ."
            ],
            "Demo availability": [],
            "dataset": []
        },
        {
            "title": "WHAT IS MISSING IN DEEP MUSIC GENERATION? A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC",
            "architecture": [],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "Society for Music Information Retrieval\nConf., Bengaluru, India, 2022.to organizing principles in music, which are generally hi-\nerarchical and include sections, phrases and various kinds\nof patterns.",
                "Society for Music Information Retrieval\nConf. , vol.",
                "Society for\nMusic Information Retrieval Conf. , vol.",
                "Society for Music Information Retrieval\nConf. , 2014.",
                "[11] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer,\nI. Simon, C. Hawthorne, A. M. Dai, M. D. Hoff-\nman, M. Dinculescu, and D. Eck, \u201cMusic transformer,\u201d\narXiv preprint arXiv:1809.04281 , 2018.[12] S. Dai, Z. Jin, C. Gomes, and R. B. Dannenberg, \u201cCon-\ntrollable deep melody generation via hierarchical mu-\nsic structure representation,\u201d in Proc. of the 22nd Int.\nSociety for Music Information Retrieval Conf. , 2021.",
                "Society for Music Information\nRetrieval Conf. , 2021.",
                "Society for Music\nInformation Retrieval Conf. , Suzhou, China, 2017.",
                "Conference\non Music Perception and Cognition and the 8th Trien-\nnial Conf. of the European Society for the Cognitive\nSciences of Music , 2012, pp."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "[11,30] use objective metrics such as negative log-\nlikelihood, cross-entropy and prediction accuracy to com-\npare generated music with ground-truth human-composed\nmusic.",
                "But these metrics do not precisely correspond to\nhuman perception and are not reliable for musicality."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "Non-unique phrases (those that\nmatch other phrases) often occur in sequences such as\n\u201cAABBB\u201d in Figure 1 called sections , which are by def-\ninition separated by non-repeated or non-melodic phrases.",
                "Figure 3(c)(d) presents\ncounts of melodic pitch patterns analogous to the onset pat-\ntern counts.",
                "In Section 3.1, we have seen that most rhythmic and\nmelodic patterns in a phrase are repetitions.",
                "To calculate the song length, we use the\ntotal length of melodic phrases, omitting measures that are\nempty or have long rests.",
                "[15] E. Narmour et al. ,The analysis and cognition\nof melodic complexity:"
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "A STUDY OF REPETITION AND STRUCTURE IN POPULAR MUSIC\nShuqi Dai *\nCarnegie Mellon University\nshuqid@cs.cmu.eduHuiran",
                "Analyses of two\npopular music datasets (Chinese and American) illustrate\nimportant music construction principles: (1) structure ex-\nists at multiple hierarchical levels, (2) songs use repetition\nand limited vocabulary so that individual songs do not fol-\nlow general statistics of song collections, (3) structure in-\nteracts with rhythm, melody, harmony and predictability,\nand (4) over the course of a song, repetition is not random,\nbut follows a general trend as revealed by cross-entropy.",
                "A study of repetition and structure in popular\nmusic\u201d, in Proc. of the 23rd Int.",
                "We use objective data analysis to support the existence and\nsigni\ufb01cance of multiple levels of hierarchy in popular mu-\nsic.",
                "The main contribution of this work is a better under-\nstanding of the nature of repetition in popular music.",
                "For ex-\nample, listening experiments with reordered Classical and\nPopular music have shown that listeners are rather insensi-\ntive to restructuring, but these results are subtle and some-\nwhat ambiguous [17].",
                "Another popular trend is to use sequential models\nsuch as LSTMs and Transformers [11, 22, 27] to generate\nlonger music sequences, but they still struggle to generate\nrepetition and coherent structure on long-term time scales.",
                "Figure 1 : Structure hierarchy in pop music.",
                "For training and testing, we use a Chinese pop\nsong dataset POP909",
                "[36], which has 909 pop song per-\nformances in MIDI, and an American pop song dataset\nPDSA",
                "[37], in MusicXML, which has 348 American pop\nsongs originating from 1580 to 1924.",
                "3.1.1 Phrases and Sections\nResearchers [13] found two levels of structure in POP909:\nsections andphrases .",
                "For example, Figure 1 has an intro, two sections connected\nby a bridge transition, and an outro, which is a typical pop\nsong structure.",
                "Figure 4 : Average cross-entropy of diatonic pitch predic-\ntions over time within a phrase in POP909, using variable-\norder Markov models.",
                "We extract the phrase-level structure in PDSA using the\nalgorithm in [13], and use human-labeled structure in [13]\nfor POP909.",
                "PDSA has\n54 different half-note patterns, while POP909 has all 128\npossible patterns (onsets are quantized to 16th notes, and\nwe assume an initial onset).",
                "For POP909\nsongs, Figure 3(a) shows the average number of different\nonset patterns in phrases of different lengths (solid lines)\nand also the average number of patterns that occur only\nonce in the phrase (dashed lines).",
                "In summary, we \ufb01nd abundant evidence for repetition\nwithin phrases:\n\u2022 Compared to sampling onset patterns at random from the\nPOP909 distribution, real phrases have fewer distinct on-\nset patterns, and more onset patterns are repeated in the\nphrase (Fig 3(a)).",
                "Predictions are a linear combination of the back-(a) Including duplicated phrases in\nforeground training\n(b) Hold out repeated phrases in\nforeground training\nFigure 5 : Average entropy, cross-entropy and accuracy of\npredicting the \ufb01rst eight diatonic pitches in each phrase,\ntuning between foreground and background in POP909.\nground and foreground models.",
                "In Figure 6, we\ntrained on the entire dataset (background model) of melody\nFigure 6 : Average cross-entropy on diatonic pitches at dif-\nferent structure level positions in POP909 dataset predicted\nby background and foreground variable Markov models.\npitch sequences, holding out test songs; and also trained\non single songs (foreground model), holding out all phrase\nrepetitions to eliminate prediction by memorizing phrases,\nand holding out eight notes at a time for testing.",
                "POP909 supports this hypothesis:\nMore than 50% of the phrases repeat immediately, and al-\nmost all phrases repeat within a quarter of the song.",
                "At the song\nlevel as well, using the phrase repetition labels in POP909,\nwe found that for 79% of songs, 15% to 35% of their du-\nration consists of new material and the rest is repetition.",
                "There is a notice-Figure 7 :Left (green): Percentage of POP909 songs that\nhave phrase repetitions at different song locations.",
                "-entropy using variable-\norder Markov models on diatonic pitches in POP909 songs\nover time, and note the y-axis is inverted.",
                "8-\nbar phrases from V AE with real phrases in POP909.\nre\ufb02ected in pitch and rhythm.",
                "These differences from popular songs are striking.",
                "Figure 8 shows that there is no sig-\nni\ufb01cant differences between the entropy of pitch scale de-\ngree distributions at the start of phrases, middle of phrases\nand end of phrases, but we see signi\ufb01cant differences in\nthe real POP909 phrases.",
                "For example, the probabil-\nity of seeing the tonic at phrase end is 35% in real POP909\nphrases, while at the other positions it is around 20%.",
                "Following the logic in Figure 3, we count the rhythm\n(melody note onset) patterns for whole songs from Mu-\nsic Transformer and from POP909 itself.",
                "We trained the\nMusic Transformer on 80% of songs in POP909, using\n10% for validation and 10% for testing.",
                "We also analyzed half-note pitch patterns in 4-bar and\n8-bar phrases from the PDSA, from random pitch patterns\ndrawn from the entire PDSA distribution, and for the V AEFigure 9 : Analysis of note onset pattern vocabulary\nfrom randomly sampled POP909 patterns (yellow), Mu-\nsic Transformer songs (red), and POP909 songs (blue) as\na function of song length.",
                "We also compared trends in cross-entropy of the pitch se-\nquence over the course of songs from POP909 (Figure 7\nblue) and Music Transformer (Figure 11).",
                "POP909\ncross-entropy increases at around 20% of the song length,\nprobably due to the introduction of novel and contrasting\npatterns, and also around the end.",
                "POP909\u2019s average cross-\nentropy is greater than one bit per note except for a small\nregion around 90% of song length, while Music Trans-\nformer is below one on average for the last 30% of the\nsong, suggesting overly predictable sequences.",
                "Although we have focused on popular music, repetition\nand hierarchical structure seem to be ubiquitous in music.",
                "Pop music, with its nearly exact repetitions, seems easier\nto study than Classical music where we might expect more\nvariation, development and modulation, which make repe-\ntition less obvious.",
                "[13] S. Dai, H. Zhang, and R. B. Dannenberg, \u201cAuto-\nmatic analysis and in\ufb02uence of hierarchical structure\non melody, rhythm and harmony in popular music,\u201d in\nin Proc. of the 2020 Joint Conference on AI Music Cre-\nativity (CSMC-MuMe 2020) , 2020.",
                "[17] J. J. Rolison and J. Edworthy, \u201cThe role of formal struc-\nture in liking for popular music,\u201d Music Perception: An\nInterdisciplinary Journal , vol.",
                "[18] O. Julian and C. Levaux, Eds., Over and Over: Explor-\ning Repetition in Popular Music .",
                "[33] A. Elowsson and A. Friberg, \u201cAlgorithmic composition\nof popular music,\u201d in Proc. of the 12th Int.",
                "Wang, and R. B. Dannenberg, \u201cPer-\nsonalized popular music generation using imitation and\nstructure,\u201d arXiv preprint arXiv:2105.04709 , 2021.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset\nfor music arrangement generation,\u201d in Proc. of 21st\nInt."
            ],
            "Demo availability": [],
            "dataset": [
                "Analyses of two\npopular music datasets (Chinese and American) illustrate\nimportant music construction principles: (1) structure ex-\nists at multiple hierarchical levels, (2) songs use repetition\nand limited vocabulary so that individual songs do not fol-\nlow general statistics of song collections, (3) structure in-\nteracts with rhythm, melody, harmony and predictability,\nand (4) over the course of a song, repetition is not random,\nbut follows a general trend as revealed by cross-entropy.",
                "Mu-\nsic from recent music generation systems is analyzed and\ncompared to human-composed music in our datasets, often\nrevealing striking differences from a structural perspective.",
                "Another important effect of repetition is that song-\nspeci\ufb01c vocabulary of rhythm and pitch patterns is lim-\nited relative to what would be expected from the entire\ndataset.",
                "For training and testing, we use a Chinese pop\nsong dataset POP909",
                "[36], which has 909 pop song per-\nformances in MIDI, and an American pop song dataset\nPDSA",
                "Blue lines are real phrases in the dataset.",
                "Orange lines are sampled phrases constructed by choosing each pattern at random from the entire dataset distribution.",
                "This will of\ncourse be true necessarily if the entire dataset has a very\nlimited vocabulary, so as a baseline for comparison, we\nconstruct random phrases by sampling half-note onset pat-\nterns from the entire dataset distribution.",
                "This allows us to eval-\nuate whether patterns within phrases (blue) have similar\ndistributions to those of the entire dataset (orange).",
                "Again, we see that real song phrases have a\nlimited vocabulary compared to phrases assembled from\nrandom half-note units representing the entire dataset, and\nfewer real phrases go unrepeated.",
                "The vocabulary of pitch patterns within a song or phrase\nis also very limited compared to the whole dataset, im-\nplying pitch sequence repetitions within the phrase level.",
                "In Figure 6, we\ntrained on the entire dataset (background model) of melody\nFigure 6 : Average cross-entropy on diatonic pitches at dif-\nferent structure level positions in POP909 dataset predicted\nby background and foreground variable Markov models.\npitch sequences, holding out test songs; and also trained\non single songs (foreground model), holding out all phrase\nrepetitions to eliminate prediction by memorizing phrases,\nand holding out eight notes at a time for testing.",
                "The V AE generated\npatterns are signi\ufb01cantly more than the patterns in the real\nPDSA dataset, with p-value less than 10\u00005.\nmodel",
                "4.2 Discussion and New Directions\nRather than learning and reproducing general statistics of\ndatasets, we need to learn how songs strategically diverge\nfrom background or stylistic norms to create interest, sur-\nprise, and individuality.",
                "Zhang, M. Xu, S. Dai,\nG. Bin, and G. Xia, \u201cPop909: A pop-song dataset\nfor music arrangement generation,\u201d in Proc. of 21st\nInt."
            ]
        },
        {
            "title": "VIS2MUS: EXPLORING MULTIMODAL REPRESENTATION MAPPING FOR CONTROLLABLE MUSIC GENERATION",
            "architecture": [],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": []
        },
        {
            "title": "GENERATING MUSIC WITH SENTIMENT USING TRANSFORMER-GANS",
            "architecture": [
                "Due to the inherent instability of the adversarial pro-\ncess, several works have focused on improving the conver-\ngence and the quality of the samples generated by GANs\nvia new objective functions, regularization and normaliza-\ntion techniques, and model architectures.",
                "Here, we use\nthe same schedule as in [13], that is, 1=\u001c= (1=\u001cmin)n=N,\nwherenis the index of the current global optimization, N\nis the total number of steps and \u001cminis a hyperparameter\nwhich we chose to be 10\u00002.\n2.2 Transformers\nShortly after its presentation in 2017, the Transformer [14]\nbecame the most popular architecture in the \ufb01eld of Natural\nLanguage Processing (NLP), and it has also been success-\nfully applied to other areas, such as image recognition",
                "METHODS\n3.1 Architecture and Loss functions\nBoth the Generator and Discriminator are Transformers\nwith linear versions of the Attention Mechanism [18]."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [
                "This may be an indication that mu-\nsical aspects which have a great impact over arousal, such\nas velocity and tempo, are more easily understood by neu-\nral networks, while factors which have an impact over va-\nlence, such as modality or frequency of harmonic change\n[47], are more dif\ufb01cult to model for these systems."
            ],
            "society": [
                "Society for Music\nInformation Retrieval Conf., Bengaluru, India, 2022.affective content of a musical passage, being able to gen-\nerate musical pieces that project a desired emotional state.",
                "Kim, E. Schmidt, R. Migneco, B. Morton,\nP. Richardson, J. Scott, J. Speck, and D. Turnbull, \u201cMu-\nsic emotion recognition: A state of the art review,\u201d Pro-\nceedings of the 11th International Society for Music In-\nformation Retrieval Conference, ISMIR 2010 , 01 2010.",
                "[33] H. H. Tan and D. Herremans, \u201cMusic fadernets: Con-\ntrollable music generation based on high-level features\nvia low-level feature modelling,\u201d in Proc. of the Inter-\nnational Society for Music Information Retrieval Con-\nference , 2020.[34] L. Ferreira, L. Lelis, and J. Whitehead, \u201cComputer-\ngenerated music for tabletop role-playing games,\u201d in\nProceedings of the AAAI Conference on Arti\ufb01cial Intel-\nligence and Interactive Digital Entertainment , vol.",
                "[35] L. N. Ferreira and J. Whitehead, \u201cLearning to generate\nmusic with sentiment,\u201d Proceedings of the Conference\nof the International Society for Music Information Re-\ntrieval , 2019.",
                "Society for Music Information\nRetrieval Conf. , 2021.",
                "[43] C. Hawthorne, E. Elsen, J. Song, A. Roberts, I. Simon,\nC. Raffel, J. Engel, S. Oore, and D. Eck, \u201cOnsets\nand frames: Dual-objective piano transcription,\u201d in\nProceedings of the 19th International Society for\nMusic Information Retrieval Conference, ISMIR 2018,\nParis, France, 2018 , 2018.",
                "Dong, K. Chen, J. McAuley, and T. Berg-\nKirkpatrick, \u201cMuspy: A toolkit for symbolic music\ngeneration,\u201d in Proceedings of the 21st International\nSociety for Music Information Retrieval Conference\n(ISMIR) , 2020."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "This addition has a positive effect on the generated\nsamples, and we demonstrate, through evaluations of our\nmodel both via automatic metrics and human feedback,\nthat the proposed Transformer GAN obtains a performance\nthat competes with a current state-of-the-art model, even\nwhile having a smaller set of parameters and using a sim-\npler representation of music.",
                "For contemporary styles,\nlike Pop or Hip-Hop, where a rigid metrical grid is often\nfollowed, it is desirable to incorporate data about the rhyth-\nmic structure of the songs into the representation.",
                "To achieve this purpose, we used both the automatic evalu-\nation metrics proposed in [26,45] and a set of human eval-\nuation metrics to compare our GAN with the system that,\nas far as we know, corresponds to a state-of-the-art gen-\nerative model of symbolic music conditioned by sentiment\ncurrently available in the literature.",
                "For the automatic metrics, we chose Pitch Range (dis-\ntance between highest and lowest pitch), Number of Pitch\nClasses, and Polyphony (the average number of simultane-\nous notes).",
                "These metrics were calculated using the Muspy\nlibrary [46].",
                "As it can be seen, our model obtains a superior per-\nformance than that of the baseline and the pre-trained\nmodel on two of the three metrics.",
                "These results suggest that both\nthe proposed Transformer, and the Transformer GAN, are\ncompetitive with a state-of-the-art model with respect to\nthe four qualitative metrics.",
                "Overall, given the superior ratings of the Transformer\nGAN respective to the automatic metrics and its compet-\nitiveness with a state-of-the art model with respect to the\nhuman evaluations, and given the considerations above\nabout model size and representation, the Transformer GAN\nseems to be a promising model for music generation con-\nditioned by sentiment."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "[31] K. Zhao, S. Li, J. Cai, H. Wang, and J. Wang, \u201cAn emo-\ntional symbolic music generation system based on lstm\nnetworks,\u201d in 2019 IEEE 3rd Information Technology,\nNetworking, Electronic and Automation Control Con-\nference (ITNEC) ."
            ],
            "pop": [
                "Here, we use\nthe same schedule as in [13], that is, 1=\u001c= (1=\u001cmin)n=N,\nwherenis the index of the current global optimization, N\nis the total number of steps and \u001cminis a hyperparameter\nwhich we chose to be 10\u00002.\n2.2 Transformers\nShortly after its presentation in 2017, the Transformer [14]\nbecame the most popular architecture in the \ufb01eld of Natural\nLanguage Processing (NLP), and it has also been success-\nfully applied to other areas, such as image recognition",
                "The excerpts on\nthe dataset are from piano transcriptions of pop songs that\nwere labeled by its authors.",
                "For contemporary styles,\nlike Pop or Hip-Hop, where a rigid metrical grid is often\nfollowed, it is desirable to incorporate data about the rhyth-\nmic structure of the songs into the representation.",
                "The\nAILABS17k dataset [38] contains over 108hours of piano\ncovers of pop songs automatically transcribed by a state-of\nthe art piano transcription model",
                "H. Yang, \u201cEMOPIA: A multi-modal pop piano dataset\nfor emotion recognition and emotion-based music gen-\neration,\u201d in Proc.",
                "Yang, \u201cPop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions,\u201d in Proceedings of the 28th ACM\nInternational Conference on Multimedia , ser."
            ],
            "Demo availability": [],
            "dataset": [
                "The EMOPIA\ndataset [37] contains musical passages separated into four\nquadrants that each corresponds to a combination of pos-\nitive or negative arousal and valence.",
                "The excerpts on\nthe dataset are from piano transcriptions of pop songs that\nwere labeled by its authors.",
                "In the \ufb01rst place, it has\nto predict each item of each sequence from the real dataset\nbased on the previous elements, that is, it has to complete\npieces of already existing sequences.",
                "The second objec-\ntive of the network is to generate sequences that are similar\nto those of the real set from scratch, that is, without context\nfrom the real dataset, such that these sequences can fool\nthe Discriminator.",
                "There is\none of these couples for each class on the dataset and one\nfor the unlabeled sequences.",
                "First, there is a sin-\ngle feature unit that indicates if the passage originates from\nthe real or fake datasets and if it exhibits the desired char-\nacteristics provided by the conditional signal.",
                "3.2 Datasets\nWe used two datasets to train our models, mainly as a\nmeans to allow the networks to use a larger training cor-\npus.",
                "The\nAILABS17k dataset [38] contains over 108hours of piano\ncovers of pop songs automatically transcribed by a state-of\nthe art piano transcription model",
                "The EMOPIA dataset [37] was constructed ina similar fashion to the one above, but its songs were af-\nterwards labeled by the authors of the dataset according to\nperceived sentiment [2].",
                "The clips on this dataset amount\nto approximately 11hours.",
                "In this stage, the Generator was\ntrained simultaneously on both datasets, and taking into\nconsideration the difference in size between these datasets,\nto balance the training process, we alternated between op-\ntimization steps on randomly sampled batches from each\nset.",
                "The Discriminator worked with subsequences\nof length 16, and the training was done exclusively on the\nEMOPIA dataset [37].",
                "H. Yang, \u201cEMOPIA: A multi-modal pop piano dataset\nfor emotion recognition and emotion-based music gen-\neration,\u201d in Proc."
            ]
        },
        {
            "title": "WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning",
            "architecture": [
                "Here, we present WuYun, a\nknowledge-enhanced deep learning architecture for improving the structure of\ngenerated melodies, which \ufb01rst generates the most structurally important notes to\nconstruct a melodic skeleton and subsequently in\ufb01lls it with dynamically decorative\nnotes into a full-\ufb02edged melody.",
                "Numerous specialized architectures of the language model for music generation have demonstrated\npromising performance in generating long-range coherent melodies, including effective attention\nmechanisms (15, 16), enhanced memory networks (17\u201319), large-scale deep neural networks (20),\nand explicit musicality regularization (21).",
                "In this study, we propose WuYun, a hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning that incorporates the melodic skeleton as deep structural\nsupport to provide explicit guidance on the development direction of melody generation (Fig. 1A).",
                "At the stage of melody inpainting, we adopt a Transformer encoder\u2013decoder\narchitecture (39) to elaborate the melodic skeleton into a full-\ufb02edged melody by encoding the melodic\nskeleton as additional knowledge into the decoder to guide the melody generation process (Fig.",
                "To prove the effectiveness of the architecture, we evaluate WuYun on a publicly available melody\ndataset.",
                "The hierarchical\nskeleton-guided melody generation architecture effectively improves generated melodies\u2019 long-term\nstructure and musicality and outperforms other state-of-the-art methods by 0.51 on average on all\nsubjective evaluation metrics.",
                "Input Module(Embedding)Positional Encoding\nTransformer-XL\uff084 blocks\uff09Output Module(Classifier)Ca) Melodic Skeleton Generation ModuleInput(Melodic skeleton)RepresentationInput Module(Embedding)Encoder\uff084 blocks\uff09Decoder\uff084 blocks\uff09Recurrent Transformerb) Melodic Prolongation Generation ModuleRepresentationInput Module(Embedding)Input(Melody)RepresentationInput(Melodic skeleton)Output Module(Classifier)TrainA\nMelodic SkeletonMelody (example)\nMelodic skeletonSequence learning modelY1Y2Yn\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nX1X2Xn\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7Partial sequenceMelodic skeleton\u00b7\u00b7\u00b7\nX1X2Xn\u00b7\u00b7\u00b7EncoderY2Y3Yn\u00b7\u00b7\u00b7Y1Y2Yn-1\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7Decoder\n\u2026\nPredicted tokens\nPredicted tokens\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7Sequence-to-sequencelearning modelMelodyFigure 1: Architecture of WuYun.",
                "( C) Architecture details of WuYun.",
                "In the following, we elaborate on the\nmelodic skeleton extraction framework from rhythm and pitch dimensions and introduce the design of\nWuYun melody generation architecture that \ufb01rst constructs the melodic skeleton and then completes\nthe melody instead of sequentially generating a melody note-by-note at once.",
                "2D shows the tonal skeleton of\nthe \ufb01rst eight bars from the song \u201cHey Jude.\u201d\n2.3 Design of WuYun\nFigure 1C shows the diagram of the proposed hierarchical melody generation architecture called\nWuYun.",
                "Then, we design a hierarchical melody generation architecture with two generative modules\nresponsible for melodic skeleton construction and melody inpainting, respectively.",
                "In this subsection,\nwe will introduce the hierarchical melody generation architecture about how we generate the melodic\nskeleton and incorporate it to guide the melody generation process.",
                "The details about the MeMIDI\nsymbolic music representation method and the word embedding technique used in this architecture\u2019s\ninput module are described in the Materials and Methods section.",
                "At the stage of melody inpainting, we employ the recurrent Transformer-based\nencoder\u2013decoder architecture (18) in a sequence-to-sequence setup as the melody inpainting module\nto complete the melody conditioned on the melodic skeleton, i.e., \ufb01lling the missing information\nbetween the melodic skeleton notes.",
                "In this work, we focus on designing a hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, following the hierarchical organization principle of\nstructure and prolongation.",
                "Therefore, we used common language models in NLP to make the WuYun\narchitecture accessible.",
                "Each subject\nwas required to rate 18 musical pieces, which cost approximately 25 min.\nFigure 3A shows the mean opinion scores of WuYun architecture\u2019s melody generation performances\nwith nine different settings on the \ufb01ve subjective evaluation metrics from all subjects in the form of\nhistograms.",
                "This phenomenon can be explained by\nthe structure and prolongation proportion tradeoffs in the design of two-stage melody generation\narchitecture using an end-to-end learning framework.",
                "2.6 Comparisons with other melody generation methods\nTo prove the effectiveness of the proposed hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, we compared WuYun-RS (i.e., using the rhythmic\nskeleton setting) to \ufb01ve public SOTA Transformer-based melody generation models, namely, Music\nTransformer (15), Pop Music Transformer (17), Compound Word Transformer (19), Melons (33)\nand MeMIDI, that follow an end-to-end left-to-right note-by-note generative paradigm and treat\neach note equally.",
                "However, the original music representation of Music\n9Figure 3: Subjective evaluation results of the WuYun melody generation architecture based on different\nmelodic skeleton settings, and the other public melody generation models.",
                "(A) Subjective comparison of the\nperformance of the WuYun architecture based on different melodic skeleton settings in Experiment 1.",
                "The WuYun architecture with the rhythmic skeleton setting achieves the best performance in\nall melodic skeleton settings on all subjective evaluation metrics.",
                "However, there is still an obvious gap between the WuYun melody generation architecture\nand human-composed music, leaving room for improvement.",
                "On the\nother hand, although the Compound Word Transformer and Melons (Nos. 4 and 5) were inferior to\nWuYun-RS, their effective compound word representation and the linear Transformer as the backbone\narchitecture enable it to process multidimensional music information in one step simultaneously and\nobtain a better result among these \ufb01ve public SOTA melody generation models.",
                "Thus, combining the\nproposed knowledge-enhanced hierarchical skeleton-guided music generation architecture with more\nef\ufb01cient music representation methods and advanced language models can bring a better result for\nmelody generation tasks.",
                "3 Discussion\nThe methodology we have taken in designing WuYun, a hierarchical skeleton-guided melody genera-\ntion architecture based on knowledge-enhanced deep learning, combines music analysis theory and\nmusical psychology.",
                "4 Materials and Methods\n4.1 Details of dataset preprocessing\nWe evaluate the effectiveness of WuYun architecture on a commonly used and publicly available\nsymbolic melody dataset of Wikifonia (32, 33, 67).",
                "4.3 WuYun architecture\nHere, we brie\ufb02y elaborate on the con\ufb01guration details of the two Transformer-based generative\nmodules of WuYun architecture, i.e., the melodic skeleton generation module for the melodic skeleton\nconstruction stage and the melodic prolongation generation module for the melody inpainting stage.",
                "For reproducibility, we do not tweak the architecture of\nreferenced models so that our music generation architecture can be easily assembled with the public\nimplementation of Transformers.",
                "144.4 Training\nWe implemented the WuYun architecture with Pytorch (v1.7.1) (69).",
                "The parameters of the WuYun\narchitecture were optimized by minimizing the cross-entropy loss on a single NVIDIA GTX 2080-Ti\nGPU with 11 GB memory."
            ],
            "training parameters": [],
            "learning rate": [
                "Speci\ufb01cally, the training loss was minimized with the Adam optimizer\n(\f1= 0:9,\f2= 0:98), a learning rate of \"= 10\u00003, and dropout was applied with a ratio of 0.1."
            ],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "[31] S. Dai, Z. Jin, C. Gomes, R. B. Dannenberg, Controllable deep melody generation via hierarchical music\nstructure representation, in Proceedings of the 22nd International Society for Music Information Retrieval\nConference (ISMIR, 2021), pp. 143\u2013150."
            ],
            "copyright": [],
            "evaluation metrics": [
                "We demonstrate that WuYun can generate melodies with better\nlong-term structure and musicality and outperforms other state-of-the-art methods\nby 0.51 on average on all subjective evaluation metrics.",
                "The hierarchical\nskeleton-guided melody generation architecture effectively improves generated melodies\u2019 long-term\nstructure and musicality and outperforms other state-of-the-art methods by 0.51 on average on all\nsubjective evaluation metrics.",
                "2.4 Evaluation metrics\nSubjective and objective evaluations are the two essential aspects of evaluating the performance of\nmusic generation systems.",
                "Consequently, almost all the proposed objective\n7evaluation metrics are dif\ufb01cult to apply for comparing different music creation systems and lack\nsustainability for future development demands.",
                "We tried to calculate the averaging overlapped area\nof some musical feature distributions between generated musical pieces and ground-truth musical\npieces as the objective evaluation metrics like (18, 60) using the public evaluation toolbox.",
                "Each subject\nwas required to rate 18 musical pieces, which cost approximately 25 min.\nFigure 3A shows the mean opinion scores of WuYun architecture\u2019s melody generation performances\nwith nine different settings on the \ufb01ve subjective evaluation metrics from all subjects in the form of\nhistograms.",
                "The rhythmic skeleton setting (No. 3) achieved the best\nresult on all subjective evaluation metrics, followed by the tonal skeleton setting (No. 4).",
                "Among\nthe three types of melodic skeletons associated with rhythm (Nos. 1, 2, and 3), the melodic skeleton\ncomposed of a single type of accent (e.g., metrical accents or agogic accents (31)) has a large gap with\nthe rhythmic skeleton in richness, expectation, and overall quality and even surpassed by the random\nmelodic skeletons (Nos. 7, 8, and 9) on most subjective evaluation metrics.",
                "In this study, we chose the setting of the rhythmic skeleton (No. 3) that performed best on all\nsubjective evaluation metrics in this experiment as the default skeleton con\ufb01guration (denoted as\nWuYun-RS) for the next experiment to compare with other melody generation models.",
                "The WuYun architecture with the rhythmic skeleton setting achieves the best performance in\nall melodic skeleton settings on all subjective evaluation metrics.",
                "The average experiment time\ncost each subject about 2 h.\nFigure 3B shows the mean opinion scores and one-tailed t-test results of the different music generation\nsystems on the \ufb01ve evaluation metrics in the form of violin plots.",
                "Furthermore, WuYun-RS and WuYun-RRS demonstrate highly similar\nperformances in terms of the quality of generated melodies on all evaluation metrics."
            ],
            "metric": [
                "We demonstrate that WuYun can generate melodies with better\nlong-term structure and musicality and outperforms other state-of-the-art methods\nby 0.51 on average on all subjective evaluation metrics.",
                "The hierarchical\nskeleton-guided melody generation architecture effectively improves generated melodies\u2019 long-term\nstructure and musicality and outperforms other state-of-the-art methods by 0.51 on average on all\nsubjective evaluation metrics.",
                "Based on the listeners\u2019\nperception of tonal music, GTTM lists four hierarchical structure relationships from rhythm and pitch\ndimensions: grouping structure, metrical structure, time span reduction, and prolongational reduction.",
                "The rhythmic\nskeleton consists of the metrical accents, agogic accents on metrical accents, and agogic accents on syncopations\nin each measure.",
                "Metrical and rhythmic accents are the\ntwo main accents in the symbolic melody data.",
                "A metrical accent is an accent that falls on the strong\nbeat position within a measure.",
                "The metrical accent is periodic and cyclic, whose distribution pattern\ndepends on the type of meter.",
                "Note that a note cannot be both a metrical accent and a syncopation, which are\ncon\ufb02icted with each other.",
                "In this study, we extract the metrical accents, the agogic accents falling on the metrical accent, and\nthe agogic accents falling on the syncopation as the rhythmic skeleton notes, as illustrated in Fig.",
                "2B. Metrical accents are the foundation of other types of accent (34).",
                "Therefore, when a rhythmic accent and a metrical accent overlap, the rhythmic accent is generally\nmore perceptible and is the one that is preferred.",
                "For the tonal skeleton extraction method, we use the tension level as a metric to quantify the relative\nimportance of the pitch.",
                "6\u2022First, we used the position of the rhythmic skeleton notes as the boundary of the individual\ncontext because the metrical structure is the important basis of all hierarchical structure\ntypes (34).",
                "2.4 Evaluation metrics\nSubjective and objective evaluations are the two essential aspects of evaluating the performance of\nmusic generation systems.",
                "However, for the\nobjective evaluation, many efforts have been made to design quantitative metrics; there is not a set of\nconvincing and uni\ufb01ed metrics.",
                "Consequently, almost all the proposed objective\n7evaluation metrics are dif\ufb01cult to apply for comparing different music creation systems and lack\nsustainability for future development demands.",
                "We tried to calculate the averaging overlapped area\nof some musical feature distributions between generated musical pieces and ground-truth musical\npieces as the objective evaluation metrics like (18, 60) using the public evaluation toolbox.",
                "Therefore, before formal experiments, we conducted multiple rounds of discussions,\ntesting, and validation with musicians and nonmusicians regarding the above subjective evaluation\nmetrics and their descriptions until they could easily understand and grasp them.",
                "To ensure that\nthe recruited subjects have a common understanding of the metrics and scales in the questionnaire,\nwe also conducted evaluation training for them, including the explanation of subjective evaluation\nmetrics and preliminary experiments.",
                "All experimental settings\nand the proportion of melodic skeleton notes in the melody are described below:\n1.Downbeat only uses metrical accents as the melodic skeleton (32.8%).",
                "Each subject\nwas required to rate 18 musical pieces, which cost approximately 25 min.\nFigure 3A shows the mean opinion scores of WuYun architecture\u2019s melody generation performances\nwith nine different settings on the \ufb01ve subjective evaluation metrics from all subjects in the form of\nhistograms.",
                "The rhythmic skeleton setting (No. 3) achieved the best\nresult on all subjective evaluation metrics, followed by the tonal skeleton setting (No. 4).",
                "Among\nthe three types of melodic skeletons associated with rhythm (Nos. 1, 2, and 3), the melodic skeleton\ncomposed of a single type of accent (e.g., metrical accents or agogic accents (31)) has a large gap with\nthe rhythmic skeleton in richness, expectation, and overall quality and even surpassed by the random\nmelodic skeletons (Nos. 7, 8, and 9) on most subjective evaluation metrics.",
                "In contrast, a rigid melodic skeleton (i.e., including only one type\nof accent, especially metrical accents) reduces the quality of the generated melody and limits the\nmodels\u2019 performance.",
                "In this study, we chose the setting of the rhythmic skeleton (No. 3) that performed best on all\nsubjective evaluation metrics in this experiment as the default skeleton con\ufb01guration (denoted as\nWuYun-RS) for the next experiment to compare with other melody generation models.",
                "The WuYun architecture with the rhythmic skeleton setting achieves the best performance in\nall melodic skeleton settings on all subjective evaluation metrics.",
                "The average experiment time\ncost each subject about 2 h.\nFigure 3B shows the mean opinion scores and one-tailed t-test results of the different music generation\nsystems on the \ufb01ve evaluation metrics in the form of violin plots.",
                "Overall, WuYun-RS (No. 6) and WuYun-RRS (No. 7) outperformed the\nother \ufb01ve current SOTA end-to-end left-to-right note-by-note melody generation models on all metrics,\nincluding MusicTransformer (No. 1), Pop Music Transformer (No. 2), Compound Word Transformer\n(No. 4), Melons (No. 5), and MeMIDI (No. 3).",
                "Furthermore, WuYun-RS and WuYun-RRS demonstrate highly similar\nperformances in terms of the quality of generated melodies on all evaluation metrics.",
                "We\ncontend that a more precise and adaptable time grid is required to model a more expressive\nmetrical context, including the 32nd (18), 64th note, and even triplets.",
                "2.68\u00061.03 2.73\u00060.95\n9 Random75% 2.82\u00060.95 2.58\u00061.12 2.50\u00061.11 2.45\u00061.04 2.53\u00061.08\nTable S2: One-tailed t-test results between WuYun-RS and other music generation models on the \ufb01ve evaluation\nmetrics in experiment 2.\nModel Rhythm Richness Structure Expectation Overall\nMT"
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "Here, we present WuYun, a\nknowledge-enhanced deep learning architecture for improving the structure of\ngenerated melodies, which \ufb01rst generates the most structurally important notes to\nconstruct a melodic skeleton and subsequently in\ufb01lls it with dynamically decorative\nnotes into a full-\ufb02edged melody.",
                "Speci\ufb01cally, we use music domain knowledge\nto extract melodic skeletons and employ sequence learning to reconstruct them,\nwhich serve as additional knowledge to provide auxiliary guidance for the melody\ngeneration process.",
                "Our study provides a\nmultidisciplinary lens to design melodic hierarchical structures and bridge the\ngap between data-driven and knowledge-based approaches for numerous music\ngeneration tasks.",
                "Although melodies\nappear to be a simple linear succession of notes unfolding over time, the organizational structure of\nthe melodic notes is hierarchical, like a tree resulting in intricate long-distance dependencies (9, 10).",
                "Such a strategy requires\nrecognizing the group structure of the musical syntax in a melodic surface (e.g., phrases and sections)\nto extract music features for building a structure generation model.",
                "Nonetheless, inadequate music\nstructure boundary detection algorithms hinder the extraction of accurate melodic group structure.",
                "Conversely, little attention has been paid to the organizational logic of the deep structure beneath\nthe melodic surface, organized by different levels of structural importance among various musical\nevents (34\u201336) with the potential to enhance structured melody generation.",
                "In this study, we propose WuYun, a hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning that incorporates the melodic skeleton as deep structural\nsupport to provide explicit guidance on the development direction of melody generation (Fig. 1A).",
                "WuYun follows the hierarchical organization principle of structure and prolongation (35, 37), thus\ndividing traditional single-stage end-to-end melody generation into two stages: melodic skeleton\nconstruction and melody inpainting (Fig. 1B).",
                "At the stage of melodic skeleton construction, we \ufb01rst\nextract the most structurally important notes in a musical piece from rhythm and pitch dimensions\nas melodic skeletons on the basis of the music domain knowledge.",
                "We then train an autoregressive\ndecoder-only Transformer-based network (38) on the collected melodic skeleton data to construct\nnew melodic skeletons (Fig. 1C, a).",
                "We treat the melodic skeleton as the underlying framework of the\n\ufb01nal generated melody.",
                "At the stage of melody inpainting, we adopt a Transformer encoder\u2013decoder\narchitecture (39) to elaborate the melodic skeleton into a full-\ufb02edged melody by encoding the melodic\nskeleton as additional knowledge into the decoder to guide the melody generation process (Fig.",
                "Experimental results show that the generated melodic skeleton has comparable quality with\nthe real one extracted by our proposed melodic skeleton extraction framework.",
                "2Melodic skeletonMelodyMelodyDatabaseMelodic Skeleton ExtractionFramework\nPairingB\nStage 1: melodic skeleton constructionStage 2: melody inpainting\nMelodicSkeletonDatabase\nHierarchical structure analysis in music(Music domain knowledge)",
                "Input Module(Embedding)Positional Encoding\nTransformer-XL\uff084 blocks\uff09Output Module(Classifier)Ca) Melodic Skeleton Generation ModuleInput(Melodic skeleton)RepresentationInput Module(Embedding)Encoder\uff084 blocks\uff09Decoder\uff084 blocks\uff09Recurrent Transformerb) Melodic Prolongation Generation ModuleRepresentationInput Module(Embedding)Input(Melody)RepresentationInput(Melodic skeleton)Output Module(Classifier)TrainA\nMelodic SkeletonMelody (example)\nMelodic skeletonSequence learning modelY1Y2Yn\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nX1X2Xn\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7Partial sequenceMelodic skeleton\u00b7\u00b7\u00b7\nX1X2Xn\u00b7\u00b7\u00b7EncoderY2Y3Yn\u00b7\u00b7\u00b7Y1Y2Yn-1\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7Decoder\n\u2026\nPredicted tokens\nPredicted tokens\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7Sequence-to-sequencelearning modelMelodyFigure 1: Architecture of WuYun.",
                "The upper part of the \ufb01gure shows the basic shape of the melodic motion, and the low\npart of the \ufb01gure shows the melodic skeleton in the rhythm dimension.",
                "Every melody has an underlying melodic\nskeleton that provides structural support and connections among musical elements to guide the melodic motion.",
                "WuYun divides the melody generation process into melodic skeleton\nconstruction and melody inpainting stages following the hierarchical organization principle of structure and\nprolongation.",
                "At the melodic skeleton construction stage, the melodic skeleton extraction framework is proposed\nto extract the melodic skeleton in the rhythm and pitch dimensions by the hierarchical structure theory from\nmusic domain knowledge.",
                "A neural network for sequence learning trained on melodic skeletons can generate\nnovel ones.",
                "At the melody inpainting stage, another neural network for sequence-to-sequence learning would\n\ufb01ll the generated melodic skeleton into a full-\ufb02edged melody.",
                "WuYun is\ncomposed of a melodic skeleton generation module and a melodic prolongation generation module; the former\nis used for the melodic skeleton construction stage, and the latter is used for the melody inpainting stage with the\nguidance of the melodic skeleton.",
                "Inspired by the iterative mode of human composition guided by the principles of structure and\nprolongation, the whole process of melody creation can be seen as progressively \ufb01lling individual\ndecorative notes among the melodic skeleton; it is an effective modern composition technique that\nperfectly combines rules and composers\u2019 personality (35).",
                "In the following, we elaborate on the\nmelodic skeleton extraction framework from rhythm and pitch dimensions and introduce the design of\nWuYun melody generation architecture that \ufb01rst constructs the melodic skeleton and then completes\nthe melody instead of sequentially generating a melody note-by-note at once.",
                "2.2 Melodic skeleton extraction framework\nMusic theories present that there is an underlying identi\ufb01able framework beneath the melody surface\ncalled the melodic skeleton (35, 50).",
                "The melodic skeleton is composed of certain notes, which\nsound more structurally important from rhythm and pitch dimensions (34, 48) and are called the\nskeleton notes.",
                "The melodic skeleton\nserves as the crucial structural support of rhythm and harmony, indicating the direction of melody\ndevelopment.",
                "Knowledge of melodic skeleton information can help humans and machines better\n4Figure 2: Melodic skeleton extraction framework.",
                "5analyze, understand, and learn the logic of melodic hierarchy organization from surface to deep\nlayers.",
                "Here, we introduce a framework for melodic skeleton extraction based on the knowledge of music\ntheory (34, 35, 50\u201353) and music psychology (48, 49) to identify the dominant and subordinate\nrelationship of structural importance from rhythm and pitch dimensions between melody notes.",
                "The term \u201crhythmic cell\u201d de\ufb01nes as a \u201csmall rhythmic and melodic\ndesign that can be isolated or can make up one part of a thematic context\u201d (56).",
                "First, we convert the melody MIDI \ufb01les and their melodic skeletons into musical event\nsequences as the input data for model training using the MeMIDI symbolic music representation\nmethod.",
                "Then, we design a hierarchical melody generation architecture with two generative modules\nresponsible for melodic skeleton construction and melody inpainting, respectively.",
                "In this subsection,\nwe will introduce the hierarchical melody generation architecture about how we generate the melodic\nskeleton and incorporate it to guide the melody generation process.",
                "WuYun is designed to generate melodies in two stages hierarchically: melodic skeleton construc-\ntion and melody inpainting, instead of the dominant end-to-end left-to-right note-by-note melody\ngeneration paradigm.",
                "At the stage of melodic skeleton construction, we use the Transformer-XL\nmodel with only the decoder as the melodic skeleton generation module (19), which has the ad-\nvantage of remarkable performance in capturing long-term dependence.",
                "To develop the capacity\nof melodic skeleton construction, we trained the Transformer-XL model on the extracted melodic\nskeleton database.",
                "At the stage of melody inpainting, we employ the recurrent Transformer-based\nencoder\u2013decoder architecture (18) in a sequence-to-sequence setup as the melody inpainting module\nto complete the melody conditioned on the melodic skeleton, i.e., \ufb01lling the missing information\nbetween the melodic skeleton notes.",
                "In this work, the melody inpainting problem can be de\ufb01ned as\nfollows: given a melodic skeleton sequence Cs, generate an inpainted melody sequence Cm.",
                "The\nencoder maps the discrete input symbols of the melodic skeleton sequence Csto a high-dimensional\ncontinuous vector as conditional input into the decoder, and the decoder then generates an output\nsequenceCmin an autoregressive manner.",
                "The melodic skeleton sequence will be saved in the \ufb01nal\ngenerated melody.",
                "This method provides users an entry point to interact with the melody generation\nmodel by adjusting melodic skeleton notes between two stages to control the melodic motion.",
                "Therefore, we conducted two subjective evaluation experiments to evaluate the performance of our\nproposed WuYun, including different melodic skeleton settings in rhythm and pitch dimensions and\ncomparisons with public state-of-the-art (SOTA) music generation models.",
                "\u2022Structure: Whether the brain can feel the boundary of melodic phrases and the balance\namong melodic phrases\u2019 length.",
                "2.5 Model performance based on different melodic skeleton settings\nTo compare the effectiveness of variants of melodic skeleton extracted from rhythm and pitch\ndimensions, we comprehensively evaluated the performance of WuYun based on different settings of\nthe melodic skeleton.",
                "Furthermore, we added three control group settings of randomly selected notes\nwith different percentages as the melodic skeleton in order to verify the effectiveness of the proposed\nmelodic skeleton extraction method based on music domain knowledge.",
                "All experimental settings\nand the proportion of melodic skeleton notes in the melody are described below:\n1.Downbeat only uses metrical accents as the melodic skeleton (32.8%).",
                "2.Long Note only uses agogic accents as the melodic skeleton (27.4%).",
                "3.Rhythm uses rhythmic skeleton notes as the melodic skeleton (33.8%).",
                "4.Tonic uses tonal skeleton notes as the melodic skeleton (43.2%).",
                "5.Interaction uses the intersection of rhythmic skeleton notes and tonal skeleton notes as the\nmelodic skeleton (14.2%).",
                "6.Union uses the union of rhythmic skeleton notes and tonal skeleton notes as the melodic\nskeleton (62.8%).",
                "7.Random25% randomly selects 25% of melody notes as the melodic skeleton (25%).",
                "8.Random50% randomly selects 50% of melody notes as the melodic skeleton (50%).",
                "9.Random75% randomly selects 75% of melody notes as the melodic skeleton (75%).",
                "Generally, among all melodic\nskeleton settings, the proposed rhythmic and tonal skeleton based on music theory and psychological\nstudy performs better than other skeletons.",
                "Among\nthe three types of melodic skeletons associated with rhythm (Nos. 1, 2, and 3), the melodic skeleton\ncomposed of a single type of accent (e.g., metrical accents or agogic accents (31)) has a large gap with\nthe rhythmic skeleton in richness, expectation, and overall quality and even surpassed by the random\nmelodic skeletons (Nos. 7, 8, and 9) on most subjective evaluation metrics.",
                "In contrast, a rigid melodic skeleton (i.e., including only one type\nof accent, especially metrical accents) reduces the quality of the generated melody and limits the\nmodels\u2019 performance.",
                "We can preliminarily see that with the increased\npercentage of melodic skeleton notes, the performance of the two-stage melody generation went up\n\ufb01rst",
                "On the one hand, a low proportion of melodic skeleton notes makes it easier\nto train the melodic skeleton construction model in the \ufb01rst stage.",
                "However, the generated skeleton\nnotes will be too sparse to guide the second stage of melodic inpainting (such as the intersection\nskeleton setting, only 14.2%).",
                "On the other hand, if the proportion of melodic skeleton notes is too\nlarge, the training dif\ufb01culty and data dependency of the melodic skeleton construction model will\nincrease.",
                "The MeMIDI setting uses the MeMIDI data representation method like WuYun-\nRS and employs the Transformer-XL model without using the melodic skeleton for the melody\ngeneration task.",
                "Moreover, to prove the effectiveness of the generated melodic skeleton, we added\nthe setting of WuYun-RRS, skipped the melodic skeleton construction in the \ufb01rst stage, and directly\nused the real rhythmic skeleton as additional knowledge to guide the melody generation process\nof melody inpainting in the second stage.",
                "However, the original music representation of Music\n9Figure 3: Subjective evaluation results of the WuYun melody generation architecture based on different\nmelodic skeleton settings, and the other public melody generation models.",
                "(A) Subjective comparison of the\nperformance of the WuYun architecture based on different melodic skeleton settings in Experiment 1.",
                "The WuYun architecture with the rhythmic skeleton setting achieves the best performance in\nall melodic skeleton settings on all subjective evaluation metrics.",
                "This result\nindicates the effectiveness of the melodic skeletons generated via the melodic skeleton construction\nmodule.",
                "This also shows that designing clever\ndecorations for melodic skeletons is another dif\ufb01cult research problem, even for human composers.",
                "Unlike the dominant end-to-end left-to-right note-by-note melody generation\nparadigm, we use the hierarchical organization principle of structure and prolongation to decompose\nthe melody generation process into melodic skeleton construction and melody inpainting stages.",
                "We\nextract the most structurally important notes based on hearing sensitivity as melodic skeletons and\nincorporate them into the melody generation process as a deep structure to guide the model to learn\nthe hierarchical dependency structures among musical event sequences from the limited melody data\nwithout music boundary detection (31).",
                "In general, WuYun allows human users to edit the\ngenerated melodic skeleton and adjust its shape to guide and constrain the range of the decorative\nnotes at the next stage of melody inpainting.",
                "Thus, the proposed generation strategy based on the\n11hierarchical organization principle of structure and prolongation not only can maintain the long-range\ntonal coherence of generated melodies but also achieve control over the target of melodic motion by\nhuman users.",
                "Additionally, with WuYun and its melodic skeleton analysis framework, human users\ncan directly extract the skeleton from existing music compositions for music composition analysis or\nre-creation.",
                "Our study has some limitations, notably the performance of the melody inpainting model, except\nthat the quality of generated melodic skeleton may be poor; even if an original rhythmic skeleton\nis provided, the quality of the completed melody is still far from the real one.",
                "Another issue is how to effectively extract an organic\nmelodic skeleton from hierarchical musical structures combining two or more musical dimensions\n(e.g., rhythm and pitch) to further improve the structure of generated melodies.",
                "Additionally,\nwe expect to investigate other systematic music analysis theories and gain further psychological\nknowledge to analyze and compare the hierarchical levels of important musical events along different\nmusical dimensions for designing a more effective melodic skeleton extraction framework.",
                "\u2022Octave Transposition: All melodies are applied octave transposition to shift the pitch into\nthe range from C3 to C5 or are removed, which are out of the regular melodic pitch range\n(32).",
                "For simplicity, we do not use the Chord symbol in the melodic\nskeleton event sequence.",
                "4.3 WuYun architecture\nHere, we brie\ufb02y elaborate on the con\ufb01guration details of the two Transformer-based generative\nmodules of WuYun architecture, i.e., the melodic skeleton generation module for the melodic skeleton\nconstruction stage and the melodic prolongation generation module for the melody inpainting stage.",
                "We use an unconditional sequence learning model Transformer-XL for the melodic skeleton genera-\ntion module.",
                "Here, we used the\nmelodic skeleton data extracted from the training part of the Wikifonia dataset to train the melodic\nskeleton generation module.",
                "We use a conditional sequence-to-sequence model based on Transformer-based recurrent en-\ncoder\u2013decoder neural networks for the melodic prolongation generation module (18).",
                "We keep the same input module, output\nmodule, sampling method, length of training input tokens, and memory as same as the melodic\nskeleton generation module.",
                "For training the melodic prolongation generation module, we use the\nMeMIDI representations of the paired melodic skeleton and melody data as the encoder and decoder\ninput data, respectively.",
                "The mini-batches of the input data for the melodic skeleton generation module and the melodic\nprolongation generation module were 20 and 44, respectively.",
                "Subjective evaluation scores of generated melodies based on different melodic skeleton settings in\nExperiment 1 (mean \u0006standard deviation)."
            ],
            "harmonic complexity": [],
            "expressiveness": [
                "Accents can be expressed in various ways to increase musical\nexpressiveness and add character to the movement of music."
            ],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "1 Introduction\nAutomatic music generation is one of the popular multidisciplinary research topics in generative\nart and computational creativity (1), which has achieved revolutionary advances in various arti\ufb01cial\nintelligence-generated content applications by utilizing deep learning techniques (2, 3), including\ninteractive music production collaboration tools (4, 5), video background music generation (6), music\neducation (7), and music therapy (8).",
                "We\narrived at a similar conclusion as PopMNet (32) that a better result of objective evaluation does not\nmean better structure and musicality of generated music.",
                "2.6 Comparisons with other melody generation methods\nTo prove the effectiveness of the proposed hierarchical skeleton-guided melody generation architecture\nbased on knowledge-enhanced deep learning, we compared WuYun-RS (i.e., using the rhythmic\nskeleton setting) to \ufb01ve public SOTA Transformer-based melody generation models, namely, Music\nTransformer (15), Pop Music Transformer (17), Compound Word Transformer (19), Melons (33)\nand MeMIDI, that follow an end-to-end left-to-right note-by-note generative paradigm and treat\neach note equally.",
                "Additionally, the MIDI quantization level of the Pop MusicTransformer, Compound\nWord Transformer, and Melons only considered the 16th note time grid.",
                "Overall, WuYun-RS (No. 6) and WuYun-RRS (No. 7) outperformed the\nother \ufb01ve current SOTA end-to-end left-to-right note-by-note melody generation models on all metrics,\nincluding MusicTransformer (No. 1), Pop Music Transformer (No. 2), Compound Word Transformer\n(No. 4), Melons (No. 5), and MeMIDI (No. 3).",
                "2.84\u00060.89 2.68\u00060.95 2.75\u00060.87 2.67\u00060.87 2.71\u00060.88\n6 WuYun-RS 3.13\u00060.88 3.07\u00060.87 3.13\u00060.86 3.02\u00060.92 3.00\u00060.87\n7 WuYun-RRS 3.20\u00060.81 3.11\u00060.85 3.15\u00060.88 3.00\u00060.96 3.02\u00060.88\n8 Human 3.54\u00060.82 3.65\u00060.76 3.68\u00060.89 3.55\u00060.92 3.57\u00060.84\nMT, PMT, and CWT stand for Music Transformer, Pop Music Transformer, and Compound Word Trans-\nformer, respectively.",
                "Yang, Pop music transformer: Beat-based modeling and generation of expressive pop piano\ncompositions, in Proceedings of the 28th ACM International Conference on Multimedia (MM, 2020), pp.",
                "Ren, J. He, X. Tan, T. Qin, Z. Zhao, T. Liu, Popmag: Pop music accompaniment generation, in\nProceedings of the 28th ACM International Conference on Multimedia (MM, 2020), pp.",
                "[32] J. Wu, X. Liu, X. Hu, J. Zhu, Popmnet:",
                "Generating structured pop music melodies using neural networks.",
                "3:43\u000210\u000075:37\u000210\u000092:44\u000210\u0000126:50\u000210\u000062:21\u000210\u000010\nPMT 2:18\u000210\u000081:21\u000210\u000091:20\u000210\u000062:55\u000210\u000061:13\u000210\u00007\nMeMIDI 1:88\u000210\u000063:09\u000210\u000066:86\u000210\u000072:31\u000210\u000056:29\u000210\u00007\nCWT 3:31\u000210\u000048:47\u000210\u000043:02\u000210\u000042:03\u000210\u000031:69\u000210\u00003\nMelons 4:60\u000210\u000031:54\u000210\u000038:02\u000210\u000042:12\u000210\u000037:41\u000210\u00003\nWuYun-RRS 0.26 0.36 0.42 0.42 0.41\nMT, PMT, and CWT stand for Music Transformer, Pop Music Transformer, and Compound Word Trans-\nformer, respectively.\n19"
            ],
            "Demo availability": [],
            "dataset": [
                "To prove the effectiveness of the architecture, we evaluate WuYun on a publicly available melody\ndataset.",
                "We randomly selected ten melodies from the evaluation dataset for the listening materials.",
                "Therefore, in this experiment,\nwe applied the 16th note time grid as the MIDI quantization level to the melody dataset for all music\ngeneration models.",
                "4 Materials and Methods\n4.1 Details of dataset preprocessing\nWe evaluate the effectiveness of WuYun architecture on a commonly used and publicly available\nsymbolic melody dataset of Wikifonia (32, 33, 67).",
                "The Wikifonia dataset contains thousands of\nlead sheets in MusicXML format.",
                "Here, we describe the procedure below to clean\nup noisy data and arti\ufb01cial errors since the dataset is user-generated.",
                "We set one chord per beat\nand unify the chord representation of the Wikifonia dataset using the chord dictionary as\ndescribed in the following subsection.",
                "\u2022Chord\nTo cover the chord types in the Wikifonia dataset, we use a more comprehensive chord\nevent list.",
                "Here, we used the\nmelodic skeleton data extracted from the training part of the Wikifonia dataset to train the melodic\nskeleton generation module.",
                "Acknowledgements\nThanks to Huawei Technologies Co., Ltd for the help in dataset collection and comments."
            ]
        },
        {
            "title": "An investigation of the reconstruction capacity of stacked convolutional autoencoders for log-mel-spectrograms",
            "architecture": [
                "An autoencoder\n(AE) is a type of neural architecture composed of an encoder\nand a decoder.",
                "The architecture was evaluated\nby applying Inverse-STFT (ISTFT) on the generated frames\ndeveloping also a deterministic method for reconstructing the\nphase.",
                "A. Autoencoders\nAn autoencoder is a neural architecture composed of an\nencoder and a decoder [2] trained together in an unsupervised\nmanner.",
                "To expand the initial architecture to a deep neural autoen-\ncoder, a similar approach is adopted.",
                "Their architectureFig. 1.",
                "Illustration of a stacked convolutional autoencoders architecture for the reconstruction of the log-mel-spectrogram.",
                "[17] F. Girosi, M. Jones, and T. Poggio, \u201cRegularization Theory and Neural\nNetworks Architectures,\u201d Neural Computation , vol."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "[8] A. Sarroff and M. A. Casey, \u201cMusical audio synthesis using autoen-\ncoding neural nets,\u201d Proceedings of the International Society for Music\nInformation Retrieval Conference (ISMIR2014) , October 2014.",
                "[21] S. S. Stevens, J. V olkmann, and E. B. Newman, \u201cA scale for the\nmeasurement of the psychological magnitude pitch,\u201d The journal of the\nacoustical society of america , vol. 8, no. 3, pp. 185\u2013190, 1937.",
                "[26] F. Roche, T. Hueber, M. Garnier, S. Limier, and L. Girin, \u201cMake\nthat sound more metallic: Towards a perceptually relevant control of\nthe timbre of synthesizer sounds using a variational autoencoder,\u201d\nTransactions of the International Society for Music Information Retrieval\n(TISMIR) , vol. 4, pp."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "In addition, we introduce an\nevaluation metric to measure the similarity between the original\nand reconstructed samples.",
                "Our approach\nis based on the accuracy of the generated frequencies as it\npresents a signi\ufb01cant metric for the perception of harmonic\nsounds.",
                "In Section V we present the evaluation\nmethods used and we introduce a novel evaluation metric.",
                "(E)\nDecoder based on symmetric encoder.",
                "Another metric applied to\nmeasure the accuracy of the generated spectrogram was the\nstructural similarity index (SSIM) which also demonstrated\nas a suf\ufb01cient initial indicator for the comparison between\nthe two spectrograms.",
                "However, these two objective metrics\ncannot accurately capture the quality of the spectrogram.",
                "In order to evaluate the generated spectrograms, we devel-\noped a metric that highlights the signi\ufb01cance of harmonics in\ntimbre synthesis.",
                "Recall =IdFreq\nOrigFreq(9)\nPrecision and recall are metrics to compute the relevance\nbetween the retrieved and relevant values.",
                "F1 score demonstrates the harmonic\nmean of precision and recall indicating a more accurate metric\nfor the overall system.",
                "According to the\nmetrics, a more compressed representation is able to gener-\nate more accurate log-mel-spectrograms.",
                "[25] P. Esling, A. Chemla-Romeu-Santos, and A. Bitton, \u201cGenerative timbre\nspaces: Regularizing variational auto-encoders with perceptual metrics,\u201d"
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "The sounds were acoustic, electronic, or synthetic\nand could belong in different categories as per their velocity\nor acoustic quality.",
                "513\u2013516,\nInstitute of Electrical and Electronics Engineers, 1985."
            ],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "[3] have\ndemonstrated promising results in extracting information from\nrelatively big and complex datasets into a latent space.",
                "C. Regularization Techniques\nOne of the most considerable issues with deep neural\nnetworks is that they tend to over\ufb01t a training dataset.",
                "Regularization can be applied only on the weights W\n(kernel regularization ), or only on the bias term b(bias\nregularization ), or on the output layer y=Wx+b(activity\nregularization ).\n3) Data Augmentation: An attempt to increase the training\ndataset by alternating the original samples.",
                "In image processing, data augmentation\ncan be achieved by \ufb02ipping, scaling, or shifting the original\nimages [18] while in audio processing, common techniques\ninclude noise injection, time shifting, or speed alternation [20].\n4) Early Stopping: A validation dataset is used to calculate\nthe loss function after each epoch.",
                "E XPERIMENTS\nA. Dataset\nFor the conducted experiments, we used a subsample of\nthe NSynth dataset2, which is a dataset of four-second mono-\nphonic notes.",
                "The dataset was split into training, validation,\nand testing as 80/10/10.",
                "The experiments\nwere conducted on a Tesla P100 GPU using the TensorFlow\nlibrary3.\n2https://magenta.tensor\ufb02ow.org/datasets/nsynth\n3https://www.tensor\ufb02ow.org/V. E VALUATION\nTo evaluate the effectiveness of a generative network, many\nmethods have been proposed.",
                "However, training the autoencoder in the whole\nNSynth dataset conditioned by the pitch can expand the\nvariety of the produced sounds but also creates an additional\nperplexity which can reduce the performance of the network."
            ]
        },
        {
            "title": "Byte Pair Encoding for Symbolic Music",
            "architecture": [
                "Indeed, most recent models are based on the\nTransformer architecture (Vaswani et al., 2017).",
                "Model and training\nAs we speci\ufb01cally focus on sequential models, we exper-\niment with the state of the art deep learning architecture\nfor most NLP tasks at the time of writing, the Transformer(Vaswani et al., 2017) architecture."
            ],
            "training parameters": [],
            "learning rate": [
                "We use a one cycle learning rate scheduler:\nthe initial learning rate is close to 0 and gradually grows for the 30% \ufb01rst steps to 5e\u00006,1e\u00006and5e\u00007for the generators,\nclassi\ufb01er pre-training and classi\ufb01er \ufb01ne-tuning respectively, then slowly decreases down to 0."
            ],
            "batch size": [
                "The batch size is set to 16 for the generator, and 24 for the classi\ufb01er."
            ],
            "ethical": [],
            "impact": [
                "We experiment\non music generation and composer classi\ufb01cation,\nand study the impact of BPE on how models learn\nthe embeddings, and show that it can help to in-\ncrease their isotropy, i.e., the uniformity of the\nvariance of their positions in the space.",
                "Furthermore, Section 7 provides an additional\nstudy on the impact of BPE on how the models learn the\nembeddings.",
                "The decoding step time, i.e.,\nthe time of the conversion of tokens to a MIDI \ufb01le, is almost not impacted by BPE."
            ],
            "society": [
                "In\nProceedings of the 20th International Society for Mu-\nsic Information Retrieval Conference, ISMIR 2019,\nDelft, The Netherlands, November 4-8, 2019 , pp. 685\u2013\n692, 2019.",
                "In Extended Abstracts for\nthe Late-Breaking Demo Session of the 22nd Interna-\ntional Society for Music Information Retrieval Con-\nference , 2021.",
                "In Extended Ab-\nstracts for the Late-Breaking Demo Session of the 23nd In-\nternational Society for Music Information Retrieval Con-\nference , 2022.",
                "In Transactions of the Interna-\ntional Society for Music Information Retrieval , vol-\nume 5, pp.",
                "In Proceedings of the 18th In-\nternational Society for Music Information Retrieval\nConference, ISMIR 2017, Suzhou, China, October\n23-27, 2017 , pp.",
                "In Proceedings of the 21rd\nInternational Society for Music Information Retrieval\nConference , Bengalore, India, December 2022.",
                "In Extended abstracts\nfor the Late-Breaking Demo Session of the 16th Inter-\nnational Society for Music Information Retrieval Con-\nference , 2015."
            ],
            "copyright": [],
            "evaluation metrics": [
                "Section 4 describes our experimental settings and\nSection 5 describes the evaluation metrics that we use for\nthe experimental evaluation.",
                "5. Evaluation metrics\nGenerative models are often evaluated with automatic met-\nrics on the generated results."
            ],
            "metric": [
                "Section 4 describes our experimental settings and\nSection 5 describes the evaluation metrics that we use for\nthe experimental evaluation.",
                "5. Evaluation metrics\nGenerative models are often evaluated with automatic met-\nrics on the generated results.",
                "Language models are often assessed\nwith BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) or\nother metrics that compare generated results with reference\nsentences.",
                "It exists no reference-free metric measuring\nits quality or \ufb01delity.",
                "Metrics with reference such as BLEU\nmay be suited for machine translation tasks, but remains\nirrelevant for open-ended generation, such as in our case.",
                "With this motivation,\nwe introduce a new metric we called Tokenization Syntax\nErrors (TSE).",
                "Metrics of generated results.",
                "The results of all metrics are reported in Table 1.",
                "Kilgour, K., Zuluaga, M., Roblek, D., and Shar-\ni\ufb01, M. Fr \u00b4echet audio distance: A reference-\nfree metric for evaluating music enhancement\nalgorithms."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [
                "Kermarec, M., Bigo, L., and Keller, M. Improving tokeniza-\ntion expressiveness with pitch intervals."
            ],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "Hadjeres, G., Pachet, F., and Nielsen, F. DeepBach:\na steerable model for Bach chorales generation.",
                "Liang, F. T., Gotham, M., Johnson, M., and Shotton,\nJ. Automatic stylistic composition of bach chorales\nwith deep LSTM."
            ],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "strategies can be split in two categories: 1) embedding pool-\ning strategies such as Compound Word (Hsiao et al., 2021)\n(CPWord ),Octuple (Zeng et al., 2021) or PopMag (Ren\net al., 2020); 2) token combination strategies such as in\nMuseNet (Payne, 2019) or LakhNES (Donahue et al., 2019).",
                "Datasets\nWe experiment with two datasets: POP909 (Wang et al.,\n2020b) and GiantMIDI (Kong et al., 2021).",
                "The POP909 dataset (Wang et al., 2020b) is composed of\n909 piano tracks of Pop musics, with aligned MIDI and\naudio versions.",
                "Normalized distributions of the token types of the BPE\ntokens, per BPE factor for the POP909 dataset.\n0",
                "2k 4k 6k 8k 10k 12k 14k\nVocabulary size2.02.53.03.54.0Avg. token combinationsPOP909 TSD\nPOP909 REMI\nGiantMIDI TSD",
                "The\nPOP909 dataset being smaller than GiantMIDI, it naturally\nleads to a higher maximum number of combinations as the\nlatter is more diverse.",
                "Overall (\")\nPOP909 TSD 0.66 \u00b1 0.13 0.84 \u00b1 0.12 0.69 \u00b1 0.14",
                "No BPE 1.0 \u00b1 1.8 13.6 \u00b1 8.0 - 0.59 \u00b1 0.08 0.82 \u00b1 0.10 0.64 \u00b1 0.09 0.00 0.00\nBPE\u00024 0.2 \u00b1 0.9 21.9 \u00b1 19.9 - 0.65 \u00b1 0.07 0.82 \u00b1 0.10 0.74 \u00b1 0.08 0.24 0.19\nBPE\u000210 0.5 \u00b1 2.2 13.4 \u00b1 14.6 - 0.64 \u00b1 0.07 0.78 \u00b1 0.12 0.74 \u00b1 0.07 0.53 0.42\nBPE\u000220 0.8 \u00b1 2.1 12.8 \u00b1 11.0 - 0.62 \u00b1 0.07 0.79 \u00b1 0.11 0.70 \u00b1 0.09 0.20 0.31\nBPE\u000250 22.4 \u00b1 24.0 4.4 \u00b1 5.3 - 0.56 \u00b1 0.07 0.70 \u00b1 0.12 0.62 \u00b1 0.11 0.02 0.02\nBPE\u0002100 21.5 \u00b1 40.2 35.6 \u00b1 56.0 - 0.54 \u00b1 0.08 0.66 \u00b1 0.14 0.63 \u00b1 0.10 0.00 0.00\nPVm 6.1 \u00b1 6.6 6.9 \u00b1 9.3 - 0.59 \u00b1 0.08 0.78 \u00b1 0.12 0.73 \u00b1 0.08 0.01 0.06\nPVDm 23.6 \u00b1 19.3 0.2 \u00b1 0.7 - 0.43 \u00b1 0.09 0.57 \u00b1 0.19 0.54 \u00b1 0.12 0.00 0.00\nPOP909 REMI 0.66 \u00b1 0.13 0.84 \u00b1 0.12 0.69 \u00b1 0.14",
                "In particular, big jumps\nof maximum number of combinations, e.g. from 14 to 27\nfor POP909 Remi , indicate that two already big BPE tokens\nrepresent the most recurrent succession.",
                "Number of tokens sampled and not sampled by generative\nmodels, respectively right and left separated by j.\nStrategy POP909 TSD POP909 Remi GiantMIDI",
                "Generator POP909 TSD POP909 Remi GiantMIDI TSD GiantMIDI Remi\nNo BPE 0.09 0.14 0.08 0.09\nBPE\u00024 0.02 0.04 0.02 0.02\nBPE\u000210 0.12 0.11 0.02 0.07\nBPE\u000220 0.13 0.05 0.02 0.02\nBPE\u000250 0.02 0.01 0.01 0.01\nBPE\u0002100 0.01 0.01 0.01 0.00\nPVm 0.02 0.02 0.01 0.02\nPVDm 0.00 0.00 0.00 0.00\nCPWord - 0.04 - 0.08\nOctuple - 0.04 - 0.02\nClassi\ufb01er TSD (\") Remi (\") TSD Large ( \")",
                "The estimations are\nmore accurate when N\u001dd, as more samples populate allByte Pair Encoding for Symbolic Music\nGen. POP909 TSD\nnoBPEbpe4bpe10 bpe20 bpe50bpe100PVmPVDm020406080DimensionlPCA\nMLE\nMOM\nTLE\nT woNN",
                "FisherS Gen. POP909 Remi\nnoBPEbpe4bpe10bpe20bpe50bpe100PVmPVDm\nCPWordOctuple0204060lPCA\nMLE\nMOM\nTLE\nT woNN\nFisherS Gen. GiantMIDI TSD\nnoBPEbpe4bpe10 bpe20 bpe50bpe100PVmPVDm0510152025lPCA\nMLE\nMOM\nTLE\nT woNN\nFisherS Gen. GiantMIDI Remi\nnoBPEbpe4bpe10bpe20bpe50bpe100PVmPVDm\nCPWordOctuple020406080\nlPCA\nMLE\nMOM\nTLE\nT woNN\nFisherS\nClasmall TSD\nnoBPEbpe4bpe10 bpe20 bpe50bpe100PVmPVDm020406080Dimension\n200300400500600700lPCA\nMLE\nMOM\nTLE\nT woNN",
                "Gen. POP909 no BPE\n Gen. POP909 BPE\u000220\nClasmall GiantMIDI",
                "Pop music transformer:\nBeat-based modeling and generation of expressive pop\npiano compositions.",
                "Popmag:",
                "Pop music accompaniment generation.",
                "URL https://openreview.\nnet/forum?id=ByxY8CNtvr .\nWang, Z., Chen, K., Jiang, J., Zhang, Y ., Xu, M., Dai,\nS., Bin, G., and Xia, G. Pop909: A pop-song dataset\nfor music arrangement generation.",
                "B. Data downsampling\n0 1 2 3 4 5 6 7\nduration0.00.20.40.60.81.01.21.4densityDataset\nPOP909\nGiantMIDI\n0 20 40 60 80 100 120\nvelocity0.0000.0050.0100.0150.0200.0250.030densityDataset\nPOP909\nGiantMIDI\nFigure 7.",
                "Distributions of the note durations and velocities of the POP909 and GiantMIDI datasets.",
                "time (sec)\nPOP909 TSD\nNo BPE 139 17.81 \u00b1 4.12 - 0.04 \u00b1 0.02 0.01 \u00b1 0.02\nBPE\u00024 556 9.71 \u00b1 2.12 -45.50 0.20 \u00b1 0.05 0.02 \u00b1 0.02\nBPE\u000210 1390 8.05 \u00b1 1.75 -54.80 0.44 \u00b1 0.10 0.02 \u00b1 0.02\nBPE\u000220 2780 6.95 \u00b1 1.53 -60.99 0.77 \u00b1 0.18 0.02 \u00b1 0.02\nBPE\u000250 6950 5.84 \u00b1 1.28 -67.20 1.59 \u00b1 0.37 0.02 \u00b1 0.02\nBPE\u0002100 13.9k 5.33 \u00b1 1.16 -70.10 2.72 \u00b1 0.63 0.02 \u00b1 0.02\nPVm 747 12.72 \u00b1 2.92 -28.59 0.03 \u00b1 0.01 0.01 \u00b1 0.01\nPVDm 14.1k 7.63 \u00b1",
                "0.02 \u00b1 0.01 0.01 \u00b1 0.01\nPOP909 Remi\nNo BPE 152 18.06 \u00b1 4.12 - 0.03 \u00b1 0.02 0.01 \u00b1 0.01\nBPE\u00024 608 10.55 \u00b1 2.26 -41.61 0.21 \u00b1 0.05 0.02 \u00b1 0.02\nBPE\u000210 1520 8.85 \u00b1 1.90 -51.00 0.47 \u00b1 0.11 0.02 \u00b1 0.02\nBPE\u000220 3040 8.01 \u00b1 1.74 -55.64 0.86 \u00b1 0.19 0.02 \u00b1 0.02\nBPE\u000250 7600 7.32 \u00b1 1.58 -59.46 1.97 \u00b1 0.43 0.02 \u00b1 0.02\nBPE\u0002100 15.2k 6.70 \u00b1 1.43 -62.92",
                "Figure 10 shows the pairwise cosine similarity of the learned embedding vectors, for the TSD andRemi representation on\nthe POP909 dataset.",
                "POP909\n100101102\nDimension0.00.20.40.60.81.0Singular valuenoBPE\nbpe4\nbpe10\nbpe20\nbpe50\nbpe100\nPVm\nPVDm\nnoBPE adj.",
                "Pairwise cosine similarity matrix of learned embedding of the generative models, on the POP909 dataset.",
                "UMAP 3d representations of the embeddings of generative models with the POP909 dataset."
            ],
            "Demo availability": [],
            "dataset": [
                "This work aims at closing this gap by shedding light on the\nresults and performance gains of using BPE:\n\u2022We experiment on two public datasets (Wang et al.,\n2020b; Kong et al., 2021), with two base tokenizations,\non which BPE is learned with several vocabulary sizes,\non the generation and composer classi\ufb01cation tasks,\nand show that it improves the results;\n\u2022We compare BPE with other sequence reduction tech-\nniques introduced in recent research;\n\u2022We study the geometry of the learned embeddings, and\nshow that BPE can improve their isotropy;\n\u2022We show some limits of BPE, such as on the proportionarXiv:2301.11975v1",
                "Algorithm 1 Learning of BPE pseudo-code\nRequire: Base vocabularyV, target vocabulary size N,\ndatasetX\n1:whilejVj<N do\n2: Finds=ft1;t2g2V2, fromX, the most recurrent\ntoken succession\n3: Add a new token tinV, mapping to s\n4: Substitute every occurrence of sinXwitht\n5:end while\n6:returnV\nBPE is nowadays largely used in the NLP \ufb01eld as it allows\nto encode rare words and segmenting unknown or com-\nposed words as sequences of sub-word units (Sennrich et al.,\n2016).",
                "In\nthis context, BPE can allow to represent a note, or even a\nsuccession of notes, that is very recurrent in the dataset, as\na single token.",
                "Experimental settings\nThis section details the experimental protocol by describing\nthe models, the training and the datasets used along with the\nspeci\ufb01c tokenization processes.",
                "We split datasets\nin two subsets: one only used for training and updating\nthe models, one for validation to monitor trainings, that is\nalso used to test the models after training.",
                "These subsets\nrepresent respectively 65% and 35% of the original datasets.",
                "Datasets\nWe experiment with two datasets: POP909 (Wang et al.,\n2020b) and GiantMIDI (Kong et al., 2021).",
                "The POP909 dataset (Wang et al., 2020b) is composed of\n909 piano tracks of Pop musics, with aligned MIDI and\naudio versions.",
                "The GiantMIDI dataset (Kong et al., 2021) is composed\nof 10k piano MIDI \ufb01les, transcribed from audio to MIDI\nwithout downbeat and tempo estimation.",
                "Considering the com-\nplexity of its content, we make the assumption that it is a\ndif\ufb01cult dataset for a model to learn from.",
                "We perform data augmentation on the pitch dimension on\nboth datasets.",
                "Normalized distributions of the token types of the BPE\ntokens, per BPE factor for the POP909 dataset.\n0",
                "The distribution for\nthe GiantMIDI dataset are showned in Appendix C.\nFigure 4 shows the evolution of the average number of non-\nBPE token combinations represented by the BPE tokens.",
                "The\nPOP909 dataset being smaller than GiantMIDI, it naturally\nleads to a higher maximum number of combinations as the\nlatter is more diverse.",
                "Sim stands for similarity, the best results are\nthe closest to the datasets.",
                "These numbers,\ncorrelated with the model, dataset sizes and overall token\ndistribution of the dataset, might help to choose an optimal\nvocabulary size.",
                "Composer classi\ufb01cation\nComposer classi\ufb01cation is performed with the top-10 most\npresent composers of the GiantMIDI dataset.",
                "We also plan to experiment with larger model, dataset and\nvocabulary sizes, hoping to \ufb01nd guidelines for choosing an\noptimum vocabulary size.",
                "Giantmidi-\npiano: A large-scale midi dataset for classical\npiano music.",
                "URL https://openreview.\nnet/forum?id=ByxY8CNtvr .\nWang, Z., Chen, K., Jiang, J., Zhang, Y ., Xu, M., Dai,\nS., Bin, G., and Xia, G. Pop909: A pop-song dataset\nfor music arrangement generation.",
                "B. Data downsampling\n0 1 2 3 4 5 6 7\nduration0.00.20.40.60.81.01.21.4densityDataset\nPOP909\nGiantMIDI\n0 20 40 60 80 100 120\nvelocity0.0000.0050.0100.0150.0200.0250.030densityDataset\nPOP909\nGiantMIDI\nFigure 7.",
                "Distributions of the note durations and velocities of the POP909 and GiantMIDI datasets.",
                "Figure 7 shows the distributions of velocity and duration values of the notes from the two datasets we use.",
                "Normalized distributions of token types per BPE factor for the GiantMIDI dataset.",
                "The tokenization time could be\ndecreased if performed by a faster compiled language such as Rust or C. The Figure 8 complements the Figure 3, with the\nGiantMIDI dataset.",
                "Figure 10 shows the pairwise cosine similarity of the learned embedding vectors, for the TSD andRemi representation on\nthe POP909 dataset.",
                "Pairwise cosine similarity matrix of learned embedding of the generative models, on the POP909 dataset.",
                "UMAP 2d representations of the embeddings of classi\ufb01er models pre-trained with the GiantMIDI dataset and TSD tokenization.",
                "UMAP 3d representations of the embeddings of generative models with the POP909 dataset."
            ]
        },
        {
            "title": "A Symbolic-domain Music Generation Method Based on Leak-GAN",
            "architecture": [
                "[14] based on \nnetwork architecture, which allows the discriminator to leak th e \nextracted features from higher level to the generator to help t he \nguidance more distant, thus solving the problem of sparsity in \nguiding signals while training.",
                "To alleviate this situation, the LeakGAN model does not only provide generator with discriminator\u2019s information, it additionally applies a hierarchical reinforcement learning architecture to the generator in order to coordinate the leaked information with \ngeneration process of \nG\uf071.  \nWe introduce a MANAGER module and a WORKER \nmodule of the generator, which both start from an all-zero \nhidden state, denoted as 0Wh and 0Mh.",
                "FB \n2) GAN Setting \nWe choose LSTM to be the architecture of both \nMANAGER and WORKER in the generator."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [
                "Furthermore, we take the music theory into account to test the generated data on two  \nmusic theory evaluation metrics, the Smooth-saltatory \nprogression (SSP) comparison and Note-level mode test.",
                "And then, several objective \nevaluation metrics are designed to judge the quality of \ngenerated music."
            ],
            "metric": [
                "To prove good quality of the \ngenerated pieces, we conduct comparative test on LSTM model \nand evaluate the randomly-sampled music on five metrics in the \nlight of statistics and music theory.",
                "With a novel way of \nrepresenting music data, we conduct a certain number of \nexperiments based on the idea of taking the generation problem \nas a sequential decision-making process [15], using evaluation \nmetrics of both statistics and music theory.",
                "In terms of these \nmetrics, our model shows improvements to a certain extent \ncompared to other models.",
                "B. Training Settings \n1) Objective Metrics \nFor the generated music, we choose three mathematical \nstatistics metrics [21], which are Wilcoxon Test, Mann-Whitney U(MWU) Test and Kruskal-Wallis H(KWH) Test respectively, to be our evaluation index.",
                "Furthermore, we take the music theory into account to test the generated data on two  \nmusic theory evaluation metrics, the Smooth-saltatory \nprogression (SSP) comparison and Note-level mode test.",
                "The \nspecific explanation of these five metrics is presented in Tabl e \n1.",
                "TABLE I.  EXPLANATION OF METRICS  \nName Explanation Type \nWilcoxon \nTest  \nReflection of how close the piece is to realistic \nmusic.",
                "We use the five objective metrics mentioned before as \nevaluation scores.",
                "OBJECTIVE EVALUATION PERFORMANCE  \nMetrics LeanGAN LSTM Type \nWilcoxon \nTest  0.89105026  0.60805510 CB \nMWU Test 0.86661874  0.65296427 CB \nKWH)",
                "And then, several objective \nevaluation metrics are designed to judge the quality of \ngenerated music."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Based on the understanding  \nof how pop music is composed, Dong et al.",
                "Dataset \nAs we mainly focus on melody generation, the POP909[19] \npop music dataset is the best choice at our knowledge, as its music melody track can be explicitly distinguished and directly  \nderived.",
                "The reason we choose the length of 150 is that in our representation of pop song melodies, the verse and chorus of a song are approximately 150-word sequence.",
                "Pop909:",
                "A pop-song dataset for music arrangement genera tion."
            ],
            "Demo availability": [],
            "dataset": [
                "Cutting \nAs the length of one complete song may be too long for our \ngeneration model, we cut long text by piece according to analysis results of the experimental dataset.",
                "Dataset \nAs we mainly focus on melody generation, the POP909[19] \npop music dataset is the best choice at our knowledge, as its music melody track can be explicitly distinguished and directly  \nderived.",
                "After our processing, the dataset consists of 11792 melody \ndata with an average length of 210-word.",
                "We randomly sample three tenths of the dataset as the test \ndata and the remaining seven tenths as the train data, on which  \nwe run the adversarial training pretrained by MLE for 200 \nepochs.",
                "We train the 8 layers LSTM model with the same dataset for 500 epochs.",
                "A pop-song dataset for music arrangement genera tion."
            ]
        },
        {
            "title": "AI Music Therapist: A Study on Generating Specific Therapeutic Music based on Deep Generative Adversarial Network Approach",
            "architecture": [
                "Therefore, three dimensions need to be discussed in this \nstudy: whether the generated content is melody or \naccompaniment; whether single-track or multi-track music is \ngenerated; and whether the network architecture is CNN \n(convolutional neural network) or LSTM (recurrent neural \nnetwork), or GAN (generative adversarial neural network) \nwith CNN as the underlying architecture, which performs \nwell in various image video tasks."
            ],
            "training parameters": [],
            "learning rate": [
                "The GAN superparameters are as follows: the maximum \nnumber of iterations was set to 50,000, the batch size was set \nto 64, the initial learning rate was set to 0.001, and the Adam \noptimizer (betal=0.5, beta2=0.9) was chosen."
            ],
            "batch size": [
                "The GAN superparameters are as follows: the maximum \nnumber of iterations was set to 50,000, the batch size was set \nto 64, the initial learning rate was set to 0.001, and the Adam \noptimizer (betal=0.5, beta2=0.9) was chosen."
            ],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "Therefore, the introduction of notes in chronological order is \nnot necessarily harmonious or melodic, which is the problem \nto be addressed in this project."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "2022 IEEE 2nd International Conference on Electronic Technology, Communication and Information (ICETCI) \nAI Music Therapist: A Study on Generating \nSpecific Therapeutic Music based on Deep \nGenerative Adversarial Network Approach \nYurui Hou \nWalnut Hill School for The Arts \nNatick, MA 01760, United States \nyurui.hou2023 @walnuthillarts.org",
                "International Conference on Electronic Technology, Communication and Information (ICETCI)"
            ],
            "pop": [
                "Wang Yuhua et al. played pop songs or \nlight music to conscious patients during surgery, and \nobserved the changes of their heart rate and blood pressure, \nand scored their anxiety."
            ],
            "Demo availability": [],
            "dataset": [
                "paragraph3 |   : paragraph | \n  \nphrase | | phrase 2 | phrase 3 phrase 4     \n    \n      \n| pixel1 | pixel2 |   \nFig. 7. Music composition \nE. Introduction to the Data Set \nThe data set is divided into two parts, one is the public \npiano dataset, and the other is the multiple self-selected \npiano pieces.",
                "The Lakh Pianoroll Dataset (LPD) is a collection of \n174,154 multitrack piano rolls derived from the Lakh MIDI \nDataset (LMD).",
                "The final dataset has a total of 127,731 bars, and \nthe goal of the model is to generate a five-track piano roll of \nfour bars.",
                "The network was trained 20, 40, 60, 80, 100, and 200 \ntimes on the dataset, and the reference values and the results \nof 100 and 200 times were sliced randomly in two segments \nfor comparison.",
                "GAN multi-track generation results \nFigure 10(a) shows the output of a random sequence in \nthe dataset, with the horizontal and vertical coordinates \nindicating time and notes, respectively."
            ]
        },
        {
            "title": "An intelligent music generation based on Variational Autoencoder",
            "architecture": [
                "The main architecture of the three networks used \nin this section has been added to the convolutional neural \nnetwork.",
                "The \nprimary architecture for the three networks used in this \nsection is CNN."
            ],
            "training parameters": [],
            "learning rate": [
                "The momentum of the Adam optimizer was 0.5, the \nlearning rate for E and G was 0.0005, and D was 0.0001."
            ],
            "batch size": [
                "All models were trained with minimum batch random \ngradient descent (SGD) with a minimum batch size of 64."
            ],
            "ethical": [],
            "impact": [],
            "society": [
                "In: 15th International Society for \nMusic Information Retrieval Conference Late Breaking and Demo \nPapers, pp. 84\u201393.",
                "In \nInternational Society for Music Information Retrieval, pag-es 255\u2013\n261, 2016. \n \n398\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "The VAE consists of an \nencoder \u074d\u0c12\u123a\u0754\u0201\u0756\u123b  approximating a posteriori \u074c\u123a\u0756\u0201\u0754\u123b  and a \ndecoder \u074c\u0c0f\u123a\u0754\u0201\u0756\u123b  of parametric likelihood \u074c\u123a\u0754\u0201\u0756\u123b ."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "[10] established a \nbacktracking Backtracking specification language (BSL), \nwhich is used to implement CHORAL, a rule-based expert \nsystem that can construct a four-part chorus with Bach style \nfor the monophonic main melody, and has certain practical \nvalue.",
                "Au expert system for harmonizing chorales in the style of J.S. \nBach."
            ],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "V. EXPERIMENTAL ANALYSIS  \nCombining the theories of melody rule model, harmony \nrule model and rhythm rule model, different styles of music \nhave been successfully realized, such as pop, classical, jazz, \nmelody + chord, etc.",
                "Pop style music generation \n \nFigure4."
            ],
            "Demo availability": [],
            "dataset": [
                "Experimental data \nThe data used in this paper is the classical piano MIDI \ndataset, which contains music files of different formats for \nall classical music.",
                "[14] toolkit to process the individual MIDI files in the \ndataset, marked the time intervals and note types to obtain \nrhythm sequences."
            ]
        },
        {
            "title": "APE-GAN: A Novel Active Learning Based Music Generation Model With Pre-Embedding",
            "architecture": [
                "[4] used a generative adversarial architecture\nto combine two RNNs and resulted in more complexity in\nthe generated musics and increased intensity span similarity\nbetween the generated musics and real musics.",
                "In this work, we reference MuseGAN as our baseline\narchitecture.",
                "Transformer Architecture",
                "[8]\nTransformer is a novel architecture introduced by the paper\n\u201cAttention Is All You Need\u201d."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "This paper\nused an evaluation metric (Kullback\u2013Leibler divergence, or KL\ndivergence) based on similarity of music sequences generated by\nsimilar or different meanings textual inputs.",
                "To evaluate APE-GAN more objectively, we\nmeasure APE-GAN\u2019s descriptive ability by comparing the\nmusic sequences generated by different textual inputs with\nthe metric of KL divergence."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "In other words, most of the time, a\nhuman can still effortlessly distinguish generated musics from\nthe real ones, either from the melodic or rhythmic perspective.",
                "Through analysis of\nthe pianorolls, we can \ufb01nd that the sequences are valid\nmusics, because all the samples illustrated have clear melodic\nlines, distinctions between voice parts, reasonable temporal\nstructures, and variations in note lengths."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "\u000fWe use active learning technology to ef\ufb01ciently train\nour APE-GAN to achieve high performance under the\n123\nICEICT 2021 \nIEEE 4th International Conference on Electronic Information and Communication Technology\n978-1-6654-3203-0/21/$31.00 \u00a92021 IEEE\nXi'an, China \u2022",
                "August 18-20, 20212021 IEEE 4th International Conference on Electronic Information and Communication Technology (ICEICT) | 978-1-6654-3203-0/21/$31.00",
                "Can GAN originate new electronic dance music\ngenres?\u2013Generating novel rhythm patterns using GAN with Genre\nAmbiguity Loss."
            ],
            "pop": [
                "This method\nuses BERT to obtain embedding vector, thus provides prior and\nartistic thoughts from humans so that our model can achieve\na higher level of creativity than most of the popular methods.",
                "Before Transformers, Long-\nShort-Term-Memory (LSTM)-based models are very popular\nin constructing Seq2Seq models because the LSTM model can\nmap sequences of data to vectors while selectively remember-\ning the parts of the sequences it \ufb01nds important.",
                "A mu-\nsically plausible network for pop music generation."
            ],
            "Demo availability": [],
            "dataset": [
                "After using Lakh\nPianoroll Dataset to train APE-GAN, the similar meaning textual\ninputs result in outputs with KL divergence approximate to 0,\nwhile different meaning textual inputs result in outputs with KL\ndivergence approximate to 1.\nKeywords \u2014Music Generation; Generative Adversarial Net-\nworks(GAN); BERT; Active Learning; KL Divergence\nI. I NTRODUCTION",
                "These \u201cuncertain\u201d data will be selected from the\ncomplete dataset and delivered to the oracle, or humans, who\nwill label these data via online interface.",
                "After labelling, these\ndata will be sent back to the original dataset to update it.",
                "The source data we used is the cleansed version\nof Lakh Pianoroll Dataset.",
                "The dataset provides 21,425\npianorolls in the format of .npz"
            ]
        },
        {
            "title": "Automatic Music Generation System based on RNN Architecture",
            "architecture": [
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n294\n \n978-1-6654-7657-7/22/$31.00 \u00a92022 IEEE  Auto\nmatic Music Generation System based on RNN \nArchitecture \n \nSandeep Kumar \nDepartment of Computer Science & \nEngineering \nKoneruLakshmaiah Educational \nFoundation  \nVaddeswaram, Andhra Pradesh, India \ner.sandeepsahratia@gmail.com \n \nShilpa Rani \nDepartment of Computer Science & \nEngineering \nNeil Gogte Institute of Technology \nHyderabad, India \nshilpachoudhary1987@gmail.com \n \n KeerthiGudiseva \nDepartment of Computer Science & \nEngineering \nKoneruLakshmaiah Educational \nFoundation  \nVaddeswaram, Andhra Pradesh, India \nkeerthigudiseva0611@gmail.com",
                "We \ndiscussed the architecture and approach in the third section.",
                "13 Muhammad Nadeem et \nal., 2019 LSTM  Nottingham \nMusicDataba\nse The majority of participants enjoyed listening to the music created by \nthe proposed musical data structure and RNN architecture.",
                "[14] Dungan, Belinda M., and Proceso L. Fernandez, \u201cNext Bar Predictor: \nAn Architecture in Automated Music Generation,\u201d In IEEE \nInternational Conference on Communication and Signal Processing \n(ICCSP), pp. 109-113, 2020.",
                "[17] Kumar S, Shilpa Rani, Arpit Jain, Chaman Verma, Maria \nSimonaRaboaca, Zolt\u00e1nIll\u00e9s and BogdanConstantinNeagu, \u201cFace \nSpoofing, Age, Gender and Facial Expression Recognition Using \nAdvance Neural Network Architecture-Based Biometric System, \u201d \nSensor Journal, vol."
            ],
            "training parameters": [],
            "learning rate": [
                "6. \n \nD. Learning Rate \n \nThe learning rate is a hyper-parameter that determines \nhow much to alter the model each time the model weights \nare updated in response to the predicted error.",
                "A high \nlearning rate speeds up the learning process but results in a \nless-than-ideal final set of weights.",
                "Although training will \ntake significantly longer, a slower learning rate might enable \nthe model to learn a more ideal or globally ideal set of \nweights."
            ],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "[17] Kumar S, Shilpa Rani, Arpit Jain, Chaman Verma, Maria \nSimonaRaboaca, Zolt\u00e1nIll\u00e9s and BogdanConstantinNeagu, \u201cFace \nSpoofing, Age, Gender and Facial Expression Recognition Using \nAdvance Neural Network Architecture-Based Biometric System, \u201d \nSensor Journal, vol."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [
                "Dataset A GAN-based model that was trained on a dataset of Bach's \norchestral symphonies produced the desired outcomes."
            ],
            "electronic": [
                "[10] Chen, Hongyu, Qinyin Xiao, and Xueyuan Yin, \u201cGenerating music \nalgorithm with deep convolutional generative adversarial networks,\u201d \nIn 2nd IEEE International Conference on Electronics Technology \n(ICET), pp.",
                "[11] Jiang, Tianyu, Qinyin Xiao, and Xueyuan Yin, \u201cMusic generation \nusing the recurrent bidirectional network,\u201d In 2nd IEEE International \nConference on Electronics Technology (ICET), pp.",
                "[15] Lang, Runnan, Songsong Wu, Songhao Zhu, and Zuoyong Li, \n\u201cSSCL: Music Generation in Long-term with Cluster Learning,\u201d In \n4th IEEE Information Technology, Networking, Electronic and \nAutomation Control Conference (ITNEC), pp. 77-81, 2020.",
                "[29] Misra, N. R., Kumar, S., & Jain, A, \u201cA review on E-waste: Fostering \nthe need for green electronics,\u201d In International Conference on \nComputing, Communication, and Intelligent Systems (ICCCIS), pp. \n1032-1036, 2021."
            ],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "The device is \ncomposed of a set of piano MIDI records from the MAESTRO \ndataset that are used to build song segments.",
                "Using a selection of piano MIDI files from the \nMAESTRO dataset, we can learn how to train a model.",
                "We described the findings of benchmark datasets in section \n4, and in the following subsections, we evaluated by \ncontrasting the existing methods with the suggested method.",
                "From the MAESTRO dataset, we can also \nlearn how to train a model using a group of piano MIDI \nfiles.",
                "Figures 9 and 10 show \nthe results for faster computation times using RW-hybrid for the \n30musicS dataset after two iterations.",
                "Four \nlayers total ( 3) Network of convolutional neurons: 2000 rows in the \ndataset Distribution of the dataset 70:20:10 50 pixels per second \nsquare spectrogram slices in the input layer 6 layers (4 Conv max \npools and 1 completely)",
                "WaveNet, \nWaveGlow, and the \nneural-source-filter \n(NSF) model NSynth  and \nMAESTRO \nDataset",
                "8 Brandon Royal,  et al. \n2020 preprocessing and \nreconstruction \ntechnique hash based Nottingham \ndataset Method Average (song 1) 4.14  Medeot et al.",
                "2022 2nd International Conference on Technological Advancements in Computational Sciences (ICTACS)  \n \n296 \n 9 Sarthak Agarwal et al., \n2018 PRECON-LSTM  Nottingham \nMus\nic \nDatabase  Sco\nreMeanMean Realness \nArtif3.120 2.747 \nGen \nHum3.613 3.516 \nComp  \n10 AdvaitMaduskar et al. \n2020 Autoregressive GAN \nmodel Bach\u2019s \nmusical \nsymphonies \ndataset Outlined a generation model for note sequence generation using the \nGAN framework.",
                "11 Tianyu Jiang et al., \n2019 Bidirectional LSTM \nnetwork  Classical \nPiano \nDataset Bidirectional LSTM network was presented to produce harmonic \nmusic.",
                "Dataset A GAN-based model that was trained on a dataset of Bach's \norchestral symphonies produced the desired outcomes.",
                "RNN Structure I\nV. RESULT AND DISCUSSION  \n \nA. D\nataset \n \nMore than 200 hours of paired audio and MIDI \nrecordings from the International Piano-e-first Competition's \nten years are included in the MAESTRO dataset.",
                "16-bit \nPCM stereo) \n \nB. Training and testing of the Dataset \n \nBy removing notes from the MIDI documents, the \ntraining dataset is produced.",
                "C. Epochs \n \nA hyper-parameter that regulates how frequently the \nlearning algorithm iterates through the training dataset is the \nnumber of epochs.",
                "The underlying model parameters have \nbeen updated once every epoch for each sample in the \ntraining dataset.",
                "The musical \ninstrument comprises of a set of piano MIDI files from the \nMAESTRO dataset that serve as the basis for the song \nportions.",
                "[9] Agarwal, Sarthak, VaibhavSaxena, VaibhavSingal, and Swati \nAggarwal, \u201cLstm based music generation with dataset preprocessing \nand reconstruction techniques,\u201d In IEEE Symposium Series on \nComputational Intelligence (SSCI), pp. 455-462, 2018."
            ]
        },
        {
            "title": "Development of Application Software for Generating Music Composition Inspired by Nature Using Deep Learning",
            "architecture": [
                "Figure 1 Illustration of (a) GAN (b) Variational En coder \n \nb) Variational Auto encoder: It is the simple feed forward network, in which the neurons are \nstacked in the layered architecture."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "2.   Formulating the Musical notes corresponding to the collected dataset: The earlier \nproposed methodology",
                "[10] will be adopted to collect the features from the natur al \ndataset and are further used to obtain the musical notes corresponding to the fea tures \ncollected."
            ]
        },
        {
            "title": "Evaluating Deep Music Generation Methods Using Data Augmentation",
            "architecture": [
                "The tiered architecture of\nthe SampleRNN allows for different computational focus to be\napplied to different levels of abstraction of the audio, which\nallows long term dependencies to be modelled ef\ufb01ciently.\nJukebox.",
                "We use a classi\ufb01er architecture [25] that came fourth in the\nMediaEval 2019 competition with respect to macro-averaged\nPR-AUC.",
                "This analysis depends on the behaviour of the classi\ufb01er, so\nfuture work should explore the effect of different classi\ufb01er\narchitectures."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [
                "One hypothesis for\nthis behaviour is that these are both priming based generation\nmethods: the \ufb01nal generated sample may deviate signi\ufb01cantly\nfrom the priming sample class, impacting the augmentation\nbehaviour in this numerous class setting."
            ],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "Recent work [12] proposes simple,\nmusically informed metrics to evaluate the musicality of gen-\nerated music.",
                "However these metrics do not capture abstract\nqualities of music such as its emotion or mood.",
                "We measure classi\ufb01er performance according to the three\nmetrics that were used in the MediaEval 2019 competition:\nF1-score, area under the precision-recall curve (PR-AUC),\nand the area under the Receiver Operator Characteristics\ncurve (ROC-AUC).",
                "We adopt two means of averaging each\nmetric: a) micro-averaged, which is the harmonic mean of the\noverall metric scores and b) macro-averaged, which performs\nan unweighted average of class-speci\ufb01c metric scores.",
                "We\nevaluate performance on the test partition of each dataset with\nrespect to micro and macro averaged metrics, calculated in the\nsame manner as the MediaEval 2019 competition submission.",
                "We observe from Table III(a) that DDSP\u2019s\nsamples yielded a consistent increase in performance with\nrespect to micro and macro averaged metrics; a quantitative\nindication that these samples contain meaningful information.",
                "Only Jukebox samples yield consistent\nperformance increase with respect to micro-averaged metrics,implying that performance was unequal across classes.",
                "[10] N. Chen, A. Klushyn, R. Kurle, X. Jiang, J. Bayer, and P. Smagt,\n\u201cMetrics for deep generative models,\u201d in International Conference on\nArti\ufb01cial Intelligence and Statistics ."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [
                "In order to\ngain insights respective to which types of music the generation\nmodels can generate, we utilise a second label set: this is\nan almost fully overlapping subset of the \ufb01rst, on which we\nperformed a more coarse relabelling based on a psychology-\nbased interpretation of the original labels."
            ],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "This is the \ufb01rst attempt at\naugmenting a music genre classi\ufb01cation dataset with conditionally\ngenerated music.",
                "We investigate the classi\ufb01cation performance\nimprovement using deep music generation and the ability of\nthe generators to make emotional music by using an additional,\nemotion annotation of the dataset.",
                "Finally, we\nperform the same experiments on an almost comprehensive\nsubset of our dataset with a relabelling, which we introduce,\npertaining to coarser arousal/valence emotion classes.",
                "MTG-J AMENDO DATASET",
                "For our experiments we use the MTG-Jamendo dataset [21],\nwhich is a large collection of labelled, high-quality commercial\nmusic.",
                "Distribution of total duration of music for each emotional class for\nthe dataset used in this study.",
                "We\nevaluate performance on the test partition of each dataset with\nrespect to micro and macro averaged metrics, calculated in the\nsame manner as the MediaEval 2019 competition submission.",
                "the same hyperparameters reported in its submission paper\n[25].\nA. Augmentation Policy\nFor each generative method, we generate an equal duration\nof music per class, with the total length of generated music\nequal to 5 % of the duration of the train split of each dataset\n(8 hours for mood/theme, 7.85 hours for emotional).",
                "We trained\nSampleRNN and DDSP on the training partition of each\ndataset and used performance on the validation partition to\ntune hyperparameters.",
                "C ONCLUSION & F UTURE WORK\nFigure 3 reveal that no model could generate music with\nmeaningful features for allclasses of each dataset.",
                "A gener-\native model for raw audio,\u201d arXiv preprint arXiv:1609.03499 , 2016.[6] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-Z. A. Huang,\nS. Dieleman, E. Elsen, J. Engel, and D. Eck, \u201cEnabling factorized\npiano music modeling and generation with the MAESTRO dataset,\u201d in\nInternational Conference on Learning Representations , 2019.",
                "[21] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra,\n\u201cThe mtg-jamendo dataset for automatic music tagging,\u201d in Machine\nLearning for Music Discovery Workshop, International Conference on\nMachine Learning (ICML 2019) , Long Beach, CA, United States,\n2019."
            ]
        },
        {
            "title": "Generating Music Algorithm with Deep Convolutional Generative Adversarial Networks",
            "architecture": [
                "GAN Network architecture."
            ],
            "training parameters": [],
            "learning rate": [
                "Training with the adam optimizer, and the learning rate is \npreferably 0.002.",
                "This may be mitigated by the nets\u2019 respective learning rates."
            ],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "/g120 \n\u6fcb\u6fbb\u6fb5\u6fc2\u6fae\u6f94 Wasserstein GAN[12], solve problems such \nas mode collapse, improve learning stability, and \nprovide meaningful learning curves useful for debugging and hyperparametric searches.",
                "The discriminator is almost symmetrical with the generator."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "* Corresponding author: Qinyin Xiao(xiaoqinyin@msn.com) \n/g24/g26/g252019 2nd International Conference on Electronics Technology\n978-1-7281-1618-1/19/$31.00 \u00a92019 IEEE\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "A complete music according to the performance mode, the instrument can be generally separated into: keyboard, wind, string, percussion, electronic."
            ],
            "pop": [
                "First, considering that we are expected to generate some \nPop music.",
                "Popular music is mainly composed of the above instruments, and other less common instruments are classified as others."
            ],
            "Demo availability": [],
            "dataset": [
                "E\nXPERIMENT\nA. Data Set \nThe mentioned above dataset from The Lakh MIDI \nDataset1, which is a collection of 176,581 unique MIDI files, \n45,129 of which have been matched and aligned to entries in the Million Song Dataset.",
                "the same token, pretraining the discriminator \nagainst dataset before we start training the generator will establish a clearer gradient."
            ]
        },
        {
            "title": "Generating Music with Emotions",
            "architecture": [
                "Specifically, our\nELMG consists of the following three parts: 1) Lyric and\nmelody generator: a novel encoder-decoder architecture that\ncan generate lyric and melody by accepting a small piece\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "In this work, we propose a novel encoder-decoder\narchitecture for lyric and melody generation.",
                "The architecture of the proposed lyric-melody generator is\nshown in Figure 3, which is a sequential encoder-decoder\nmodel trained end-to-end to compose lyrics and melodies.",
                "The architecture of the proposed ELMG system."
            ],
            "training parameters": [],
            "learning rate": [
                "The learning rate and\nnumber of epochs are the same with bidirectional LSTM.",
                "The loss function\nis optimized by Adam optimizer with initial learning rate of\n1e-4 and decayed after every epoch."
            ],
            "batch size": [
                "The model fine-\ntuned for 10 epochs with the warm-up proportion as 0.1 and\nbatch size as 16.",
                "The\nbatch size is set to 64, 32, 16 for datasets with length 20, 50,\n100 respectively."
            ],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "The EMOPIA\ndataset consists of 1,078 clips from 387 piano solo perfor-\nmances.",
                "Therefore, we think the deep learning\nmodel trained on EMOPIA cannot be used to annatate our\ndataset because of the following reasons: 1) There\u2019s a domain\ngap between piano solo performances and pop songs\u2019 melodies\nin our dataset."
            ],
            "choral": [
                "DeepBach [20]\nis proposed by Hadjeres et al., which used a dependency\nneural network and a Gibbs-like sampling procedure to gen-\nerate Bach\u2019s four parts chorales.",
                "[20] G. Hadjeres, F. Pachet, and F. Nielsen, \u201cDeepbach: a steerable model\nfor bach chorales generation,\u201d in International Conference on Machine\nLearning."
            ],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "In human life, the contem-\nporary pop music is often used to express and share\nemotions.",
                "Therefore, we think the deep learning\nmodel trained on EMOPIA cannot be used to annatate our\ndataset because of the following reasons: 1) There\u2019s a domain\ngap between piano solo performances and pop songs\u2019 melodies\nin our dataset.",
                "Yang,\n\u201cEmopia: A multi-modal pop piano dataset for emotion recognition\nand emotion-based music generation,\u201d arXiv preprint arXiv:2108.01374,\n2021."
            ],
            "Demo availability": [],
            "dataset": [
                "There is no existing large-scale music datasets with the annotation\nof human emotion labels.",
                "In this paper, we propose an\nannotation-free method to build a new dataset where each sample\nis a triplet of lyric, melody and emotion label (without requiring\nany labours).",
                "Specifically, we first train the automated emotion\nrecognition model using the BERT (pre-trained on GoEmotions\ndataset) on Edmonds Dance dataset.",
                "We then train the encoder-decoder based model to\ngenerate emotional music on that dataset, and call our overall\nmethod as Emotional Lyric and Melody Generator (ELMG).",
                "The framework of ELMG is consisted of three modules: 1) an\nencoder-decoder model trained end-to-end to generate lyric and\nmelody; 2) a music emotion classifier trained on labeled data\n(our proposed dataset); and 3) a modified beam search algorithm\nthat guides the music generation process by incorporating the\nmusic emotion classifier.",
                "Given the advent of large-\nscale music datasets, e.g., LMD-full MIDI",
                "Dataset",
                "[5] and\nreddit MIDI dataset",
                "While training a deep model with the sense\nof \u201cmood\u201d categories (i.e., human emotions) is challenging,\ndue to the fact that most music datasets do not have emotion\nlabels.",
                "Though in the literature there are small datasets, they\nare limited to learn the mapping (from emotions to music).",
                "[13] built a music dataset VGMIDI com-\nposed of 95 labelled piano pieces and 728 unlabelled pieces,\nand trained a deep generative network to generate music with\na given emotion.",
                "In 2020, they expanded this VGMIDI dataset\nfrom 95 to 200 labelled pieces and presented a model called\nBardo Composer based on GPT-2",
                "[15] proposed a symbolic music dataset EMOPIA\nthat includes 1,078 music clips from 387 songs with Valence-\nArousal emotion labels.",
                "We aim\nfor the large-scale manual-labour-free dataset and the music\ngenerator for not only melodies but also lyrics with specific\nemotions.",
                "To this end, we first build the lyric-melody dataset with\nemotion labels.",
                "We use the songs with English lyrics se-\nlected from the LMD-full MIDI dataset [5] and reddit MIDI\ndataset",
                "The pipeline includes a few steps: 1) cutting each\nsong into lyric segments with fixed length; 2) fine-tuning a\nBert [16] model on Edmonds Dance dataset [17]; and 3)\nusing the result model to annotate the segments.",
                "We elaborate\nthe dataset construction in Section III.",
                "Beside of building the dataset, we design the Emotional\nLyric and Melody Generator (ELMG) system, which to our\nbest knowledge, is the first attempt to automatically and\nsimultaneously generate lyric and melody with a specific given\nemotion using deep learning.",
                "\u2022We build large-scale paired lyric-melody dataset with\nautomatic emotion labels consisting of 11,528 MIDI\nsongs.",
                "With the advent of large music\ndatasets, deep learning models have recently achieved high-\nquality results in music composition tasks.",
                "However, it is too expensive\nto manually annotate emotion labels for music datasets, which\ncauses great difficulties for music generation tasks conditioned\non emotions.",
                "They also built a new music dataset\nwith manually emotion labels called VGMIDI, which consists\nof 95 labelled piano pieces and 728 unlabelled pieces.",
                "[24]\nto generate music with a specific emotion and the VGMIDI\ndataset was extended to 200 labelled data.",
                "More recently, Hung et\nal.built an emotion-labeled symbolic music dataset called\nEMOPIA",
                "They also verified that the proposed dataset can be used\nfor generating music conditioned on emotions.",
                "Nevertheless,\nexisting music datasets with emotion labels are both smallin size.",
                "Therefore, we create a new large-scale paired lyric-\nmelody dataset with emotion labels for generating harmonious\nmusic that can evoke emotions.",
                "In recent years, with the ad-\nvent of music datasets with lyrics, deep learning was also\nresearched for mining musical knowledge between lyrics and\nmelodies.",
                "There is no large-scale music dataset with emotion labels\npublicly available for emotion-conditioned music generation.",
                "In this work, we build a paired lyric-melody music dataset,\nthe details of the new dataset used to generate lyric and\nmelody with emotions are introduced in this section.",
                "There\nare many different ways to represent music for deep learning,\nthe form of music representation in this work is introduced\nin Section III-A. The basic information of the paired lyric-\nmelody English songs dataset is introduced in Section III-B.\nThe method that we used to annotate music is introduced in\nSection III-C. The detailed analysis of the annotated dataset\nis given in Section III-D.\nA. Data Representation\nInspired by Yu et al.",
                "14, NO. 8, AUGUST 2021 3\nTABLE I\nEXAMPLES FROM GOEMOTIONS DATASET .",
                "B. Data Collection\nThe dataset used in our work comes from two large-\nscale MIDI music datasets: LMD-full MIDI dataset [5] and\nreddit MIDI dataset",
                "There are 176,581\ndifferent MIDI files in the LMD-full dataset, but most of them\ndo not contain lyrics.",
                "In this work we only use the music\ndata with English lyrics, so only 7,497 MIDI files are selected\nfrom the LMD-full dataset.",
                "Similarly, the reddit MIDI dataset\ncontains 130k different MIDI files but only 4,031 with English\nlyrics are selected.",
                "Altogether there are 11,528 MIDI files in\nour dataset.",
                "TABLE II\nEXAMPLES FROM EDMONDS DANCE DATASET .",
                "left me broken and\nbruised but now i know that you were wrong...sadness,\ndisgust,\nanger\nTABLE III\nCLASSIFICATION RESULTS (%) OFBERT MODELS TRAINED\nONGOEMOTIONS DATASET AND EDMONDS DANCE\nDATASET ,TESTED ON EDMONDS DANCE DATASET .",
                "\u201cB OTH\u201d\nMEANS FIRST TRAINED ON GOEMOTIONS DATASET AND\nTHE FINE -TUNED ON EDMONDS DANCE DATASET .",
                "Train dataset Acc Precision Recall F1 score\nGoEmotions 52.44 45.50 55.26 49.93\nEdmonds Dance 77.90 81.82 80.67 81.23",
                "Both 79.02 82.85 81.88 82.31\nC. Data Annotation\nFor the above large-scale dataset, manually labelling emo-\ntions expressed in music by humen is expensive.",
                "Therefore, in\nthis work we exploit the deep learning models to automatically\nannotate the paired lyric-melody dataset.",
                "There are many\ndatasets that can be used to train the annotator, such as large-\nscale social media or dialog datasets with emotion labels [32],\nrelatively small-scale lyric datasets for lyric emotion classifi-\ncation [17], [33], [34] and small-scale emotion-labelled music\ndatasets without lyric [13], [15].",
                "The largest human\nannotated dataset for text sentiment classification is GoEmo-\ntions",
                "Table I shows illustrative samples of GoEmotions dataset,\neach sample text has one or more corresponding labels.",
                "The advantage of GoEmotions dataset is its large scale, but\nthe disadvantage is that there\u2019s a domain gap between Reddit\ncomments and song lyrics.",
                "(a) Pitch distribution of the whole dataset.",
                "(b) Duration distribution of the whole dataset.",
                "(c) Rest distribution of the whole dataset.",
                "Melody distribution of the collected dataset.",
                "(a), (b) and (c) show the distribution of pitch, duration and rest of the whole dataset respectively.",
                "There\u2019s some relatively small-scale lyric datasets manually\nlabelled according to human emotions.",
                "Recently, Edmonds et\nal.constructed Edmonds Dance dataset [17], which consists of\nlyrics retrieved from 524 English songs.",
                "As shown in Table II,\nthere\u2019s 8 emotion categories in the Edmonds Dance Dataset\nand each song has one or more corresponding labels.",
                "Same\nas GoEmotions, the 8 categories are grouped into positive,\nnegative or ambiguous:\n\u2022positive: anticipation, joy, trust\n\u2022negative: anger, disgust, fear, sadness\n\u2022ambiguous: surprise\nIn order to have a common model for emotion classifi-\ncation, we train Bert-base [16] models on GoEmotions and\nEdmonds Dance Dataset.",
                "Bert stands for Bidirectional Encoder\nRepresentations from Transformers [36], which has been pre-\ntrained on large-scale natural language datasets and given\nstate-of-the-art results on a wide variety of natural language\nprocessing tasks.",
                "Because there\u2019s no domain gap between\nEdmonds Dance Dataset and our dataset, we randomly select\n1/10 data from the Edmonds Dance Dataset as test data.",
                "The\nexperimental results are shown in Table III, to our surprise, the\nBert model trained on GoEmotions dataset has relatively worse\nperformance for lyric emotion classification.",
                "However, the Bert\nmodel directly trained on Edmonds Dance Dataset achieves\nbetter performance, despite the in-domain dataset is magnitude\nsmaller than out-of-domain dataset.",
                "In addition, pre-training\nthe Bert model on GoEmotions dataset and then fine-tuningthe model on Edmonds Dance Dataset can slightly improve\nthe classification accuracy of song lyrics.",
                "In order to answer this question, we\ntrain deep learning models on the EMOPIA dataset [15] and\nevaluate if they can be used on our dataset.",
                "The EMOPIA\ndataset consists of 1,078 clips from 387 piano solo perfor-\nmances.",
                "We train a bidirectional LSTM with self-attention to classify\nthe music clips according to their valence, and achieves 83.3%\ntest accuracy on EMOPIA dataset.",
                "Then, this model are used\nto classify the melodies of our dataset, the results are shown\nin Table IV, we can see that the classification results are\ncatastrophically unbalanced, even though the training data in\nEMOPIA dataset is balanced.",
                "Therefore, we think the deep learning\nmodel trained on EMOPIA cannot be used to annatate our\ndataset because of the following reasons: 1) There\u2019s a domain\ngap between piano solo performances and pop songs\u2019 melodies\nin our dataset.",
                "2) EMOPIA is a small-scale dataset.",
                "14, NO. 8, AUGUST 2021 5\nTABLE IV\nANNOTATION RESULTS OF THE BI-LSTM TRAINED ON\nEMOPIA FOR OUR DATASET ,ALL SEGMENTS ARE\nLABELLED TO HIGH-VALENCE OR LOW-VALENCE .",
                "LengthAnnotationsTotal\nHigh-valence Low-valence\n20 25,743 77,797 103,540\n50 7,069 36,833 43,902\n100 2,699 20,163 22,862\nTABLE V\nANNOTATION RESULTS FOR OUR DATASET ,ALL SEGMENTS\nARE LABELLED TO POSITIVE ,NEGATIVE OR UNLABELLED .",
                "Then, the Bert model trained on GoEmotions dataset and then\nfine-tuned on Edmonds Dance Dataset is used to annotate\nthese music segments.",
                "Table V shows the annotation results for our dataset, from\nwhich we can see that about 64% are labelled, and the number\nof positive segments is larger than the number of negative\nsegments.",
                "Examples form the annotated dataset are shown\nin Table VI.",
                "Detailed quantitative comparison of melody\ndistributions is shown in Table VII, it shows that the pitch,\nduration and rest distributions of positive and negative samples\nare pretty similar to the whole dataset.",
                "Firstly, syllable-level and word-level\nskip-gram models are trained on the whole dataset, which aim\nat mapping each English word and syllable to a vector [39].",
                "VI\nEXAMPLES FROM ANNOTATED DATASET .",
                "INCLUDE THE WHOLE DATASET (WD),\nPOSITIVE AND NEGATIVE .",
                "[39] trained on the whole lyrics dataset, we keep\nmost of the hyper-parameter settings in [28] for training the\nskip-gram models: tokens context window c= 7, negative\nsampling distribution parameter \u03b1= 0.75, and the learning\nrate is set to 0.03 with a gradually decay.",
                "Algorithm 1 Emotional Lyric and Melody Generator\nRequire: labelled and unlabelled dataset XlandXu, required\nemotion e, piece of seed lyric m\n1:Initialize word embedding Ew\n2:Initialize syllable embedding Es\n3:forx\u2208Xl\u222aXudo\n4: Update EwandEs\n5:end for\n6:Initialize lyric and melody generator G\n7:forx\u2208Xl\u222aXudo\n8: Update G\n9:end for\n10:Initialize music sentiment classifier C\n11:forx\u2208Xldo\n12: Update C\n13:end for\n14:y\u2190EBS(G, C, m, e )\n15:return y,Ew,Es,G,C\ndenoted as Ew(\u00b7)andEs(\u00b7)respectively.",
                "where Eis the number of emotions in the dataset and ||\nrepresents the concatenation operation.",
                "TABLE VIII\nEMOTION CLASSIFICATION ACCURACY (%) OFLSTM AND\nTRANSFORMER ON DIFFERENT DATASETS WITH DIFFERENT\nLENGTH .",
                "datasetsLength\n20 50 100\nBidirectional LSTM 99.8 99.9 99.9\nSelf-attention Transformer 100.0 99.9 99.9\nA. Emotion Classifier",
                "As shown in Table V, the number of positive\nsamples is larger than the number of negative samples, so over-\nsampling method is used to overcome the imbalance problem\nof the dataset, which means repeatedly using negative samples\nin every epoch, so that the ratio of positive and negative\nsamples in the training data is close to 1:1.",
                "Table VIII shows the emotion\nclassification accuracy of all datasets created in Section III-C,\nfrom it we can see that both the LSTM and Transformer based\nmodels can successfully classify the datasets.",
                "Therefore we\ncan use the classifier trained on labelled data of the datasets\nin EBS algorithm.",
                "B. Music Generation\nThe lyric-melody generator is an encoder-decoder model\ntrained end-to-end on the unlabelled datasets.",
                "The\nbatch size is set to 64, 32, 16 for datasets with length 20, 50,\n100 respectively.",
                "We\nalso implement AutoNLMC on our dataset for comparison,\nwhich is a sequence to sequence model consists of one encoder\nand multiple decoders proposed in [30].",
                "It demonstrates the Transformer has\nstronger learning ability and can better fit the dataset.",
                "Then, we generate lyrics and melodies by using the EBS\nalgorithm introduced in Section IV-C. We use 5 different seed\nlyrics: \u201cI give you my\u201d, \u201cbut when you told me\u201d, \u201cif I was\nyour man\u201d, \u201cI have a dream\u201d, \u201cwhen I got the\u201d and different\ngenerators trained on various datasets (length = 20, 50, 100)\nwith various skip-gram models (dimension = 10, 50, 100, 128).",
                "Distributions of ground-truth melody and generated melody on the testing dataset.",
                "We invited volunteers to evaluate the music data selected from\nthe ground-truth dataset, music segments generated by GRU\nbased model and Transformer based model.",
                "This demonstrates that emotions in our dataset are mainly\nconveyed by lyrics and the ELMG system proposed by us\nsuccessfully learned to generate lyrics to represent the required\nemotion.",
                "We can see that both GRU based generator and Transformer\nbased generator can successfully generate music segments of\nalmost the same high quality as the training dataset.",
                "We think\nthat the quality of the dataset is the bottleneck of our ELMG\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "The ELMG system has the potential to generate music\nwith higher quality if a better dataset is given.\nVI.",
                "In this paper, we construct a large-scale paired lyric-\nmelody dataset with emotion labels and propose Emotional\nLyric and Melody Generator (ELMG) system for emotion-\nconditioned music generation.",
                "Firstly, we find that dataset\nannotators trained on in-domain data are more reliable than\nmodels trained on out-of-domain data.",
                "Then, both GRU and\nTransformer based encoder-decoder network trained on our\ndataset successfully learned to compose lyric and melody.",
                "The new dataset created in this work only has single track\nin the melody and the emotion annotator only focus on the\nlyric.",
                "The quality of the dataset limits the effectiveness of our\nproposed model.",
                "Collect large-scale polyphonic music dataset\nwith emotion labels is a valuable further work for us.",
                "[5] https://colinraffel.com/projects/lmd/.\n[6] https://www.reddit.com/r/datasets/.\n[7] H.-W. Dong, W.-Y .",
                "Yang,\n\u201cEmopia: A multi-modal pop piano dataset for emotion recognition\nand emotion-based music generation,\u201d arXiv preprint arXiv:2108.01374,\n2021.",
                "[32] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen, G. Nemade,\nand S. Ravi, \u201cGoemotions: A dataset of fine-grained emotions,\u201d arXiv\npreprint arXiv:2005.00547, 2020.",
                "[33] E. C \u00b8 ano and M. Morisio, \u201cMoodylyrics: A sentiment annotated lyrics\ndataset,\u201d in Proceedings of the 2017 International Conference on Intelli-\ngent Systems, Metaheuristics & Swarm Intelligence, 2017, pp."
            ]
        },
        {
            "title": "Generating Music with Generative Adversarial Networks and Long Short-Term Memory",
            "architecture": [
                "D. Potential reasons and further improvements \nWith the increase of training times, the generated WAV files \nare broken, all of these clues point in one direction: there ma y \nbe some problems with the GAN model architecture."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "However, due to the long cycle, high cost and complicated process, the traditional manual creation method has been unable to meet the society\u2019s increasing demand for music, and the use of computers for music generation will become a popular composition trend in the future.",
                "REFERENCES  \n[1] Hiller Jr L A, Isaacson L M. Musical composition with a high speed digital \ncomputer[C]//Audio Engineering Society Convention 9.",
                "Audio \nEngineering Society, 1957."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "TABLE 4 STRUCTURE OF LSTM  MODEL  \nLayer Type LSTM LSTM Dense \nOutput Shape (None, \n6, 128) (None, \n128) (None, 1) \nParametric Number 66560 131584 129 \nTotal Parametric \nNumber 198273 \nTrainable Parametric \nNumber 198273",
                "Non-trainable Parametric \nNumber 0"
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "The author found that the melodies generated by this model were  2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI) | 978-1-6654-3881-0/21/$31.00 \u00a92021 IEEE | DOI: 10.1109/CEI52496.2021.9574491\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "MIDI is a technical standard that describes a communications protocol, \ndigital interface, and electrical connectors that connect a wid e \nvariety of electronic musical instruments, computers, and \nrelated audio devices for playing, editing, and recording music  \n[11]."
            ],
            "pop": [
                "However, due to the long cycle, high cost and complicated process, the traditional manual creation method has been unable to meet the society\u2019s increasing demand for music, and the use of computers for music generation will become a popular composition trend in the future.",
                "These genres are blues, classical, country, disco, hip  \nhop, jazz, metal, pop, reggae and rock and all of them are wav \nfiles."
            ],
            "Demo availability": [],
            "dataset": [
                "Here, the sample rate of the music in our dataset is 44. 1 \nkHz.",
                "With these pieces of music, the dataset can be generated by \nsplitting the music into small segments of n samples.",
                "Figure 6 Workflow  \nDataset size and the number of the training epoch are two \nkey elements that played a decisive role in the success or fail ure \nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "With the same dataset, the training epoch had been \nincreased from 3 epochs to 4, 5, 6, 7, 8, 40 and 50 epochs ( Table \n3)."
            ]
        },
        {
            "title": "Generation of Music With Dynamics Using Deep Convolutional Generative Adversarial Network",
            "architecture": [
                "DCGAN was chosen to be the deep \nlearning architecture used in this paper.",
                "There are many deep learning architectures used for music \ngeneration system.",
                "[1, 2] is the most \ncommonly used architecture in music generation system.",
                "Generator Architecture \nC. Discriminator \nFor the discriminator, it was designed in the reversed \norder of the generator.",
                "Discriminator Architecture \nD. Model Training"
            ],
            "training parameters": [],
            "learning rate": [
                "Hyperparameter \nHyperparameter Values \nLearning Rate 5e-6 \nNumber of Epochs 100K \nBatch Size 32 \nAdam_decay_rate_1 0.5 \n \nE. Data Post-processing \nTo return to original piano-roll before data-\npreprocessing"
            ],
            "batch size": [
                "Hyperparameter \nHyperparameter Values \nLearning Rate 5e-6 \nNumber of Epochs 100K \nBatch Size 32 \nAdam_decay_rate_1 0.5 \n \nE. Data Post-processing \nTo return to original piano-roll before data-\npreprocessing"
            ],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "The participants \ndescribed the excerpts as \u201cquite melodic\u201d and \u201cactive and \nlively\u201d.",
                "These responses indicate that the music generation \nsystem can generate interesting harmonic progression and \nmelodic sequences."
            ],
            "harmonic complexity": [],
            "expressiveness": [
                "Good flow and dynamics  \nSome no tes end abruptly  More dynamics and m usicality  \n \nThe excerpts rating on expressiveness are found in Figure \n7."
            ],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "With the piano-roll data \nrepresentation, Deep Convolutional Generative Adversarial \nNetwork (DCGAN) learned the data distribution from the \ngiven dataset and generated new data derived from the same \ndistribution.",
                "It can capture and \nlearn the data distribution given a dataset and generating a \nsample from the same distribution.",
                "Piano-roll Representation \n B. Dataset \nThe Oracle Hip Hop Sample Pack from Cymatics"
            ]
        },
        {
            "title": "Monophonic Music Generation With a Given Emotion Using Conditional Variational Autoencoder",
            "architecture": [
                "In [35], Valenti et al. presented the architecture for\nmusic generation that is based on an adversarial autoencoder.",
                "Available:\nhttp://arxiv.org/abs/1704.01444\n[34] G. Hadjeres, F. Nielsen, and F. Pachet, ``GLSR-VAE: Geodesic latent space\nregularization for variational autoencoder architectures,'' in Proc."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "The generated examples were evaluated with two methods, in the \u001crst using metrics for\ncomparison with the training set and in the second using expert annotation.",
                "Testing how the use of\nthe baseline model (CVAE CDense) and the proposed model\n(CVAECGRU) affects the obtained metrics for the generated\nVOLUME 9, 2021 129093J. Grekow, T. Dimitrova-Grekow: Monophonic Music Generation With Given Emotion\nFIGURE 6.",
                "B. EVALUATION OF RESULTS USING METRICS\nTo evaluate the generated music sequences, they were tested\nusing the following metrics",
                "Four metrics were calculated for each\ngenerated example.",
                "The same metrics were also calculated\nfor the training set.",
                "Comparing the distributions of the values\nof these metrics allowed us to assess whether the generated\n\u001cles have the speci\u001cc emotions.",
                "Table 3 presents the mean and standard deviation ( \u001b)\nof the metrics obtained from the music generated with the\nproposed and baseline models, and from music used as a\ntraining set.",
                "The mean and \u001bvalues obtained\nfrom the music generated with the proposed model are\ncloser to the values obtained from the training set, especially\nwhen it comes to the metrics pitch range and n pitches\nused.\nTABLE 3.",
                "Mean and standard deviation ( \u001b) of the metrics obtained from\nthe generated and training sets labeled with emotions e1-e4.",
                "Distributions of the calculated metrics for the generated\n(proposed model) and the training set labeled by emotion\nare shown in Figs. 11, 12, 13 and 14.",
                "Box plots of the metric pitch range for the generated and training data sets labeled with emotions e1-e4.\nand angry (e2) use more varying sounds than sequences\nwith emotions sad (e3) and relaxed (e4).",
                "We could conclude\nthat the pitch range andn pitches used metrics are suitable\nfor distinguishing emotions on the arousal axis of Russell's\nemotion model.",
                "We see an inverse distribution of values using\nthepitch in scale C minor rate metric (Fig. 14), where \u001cles\nwith emotions e2 and e3 have greater values than e1 and e4.",
                "It could be concluded that the pitch in scale C major rate and\npitch in scale C minor rate metrics are suitable for distin-\nguishing emotions on the valence axis of Russell's emotion\nmodel.",
                "To compare the statistics of the obtained value distributions\nfor the individual metrics, the Kolmogorov-Smirnov (KS)\nstatistic [51] was calculated to determine whether two distri-\nbutions differ (Tables 4, 5, 6 and 7).",
                "Box plots of the metric n pitches used for the generated and training data sets labeled with emotions e1-e4.",
                "Box plots of the metric pitch in scale C major rate for the generated and training data sets labeled with emotions e1-e4.",
                "Box plots of the metric pitch in scale C minor rate for the generated and training data sets labeled with emotions e1-e4.",
                "The most sim-\nilar sets for each metric were recorded with an increment\nof 1 (or 0.5 in the case of two winners) in the matrix.",
                "The sum of the horizontal lines in Table 8 is 4.0 as the\nsimilarities were counted for four metrics.",
                "Kolmogorov-Smirnov statistic between distributions of\nmetric pitch range obtained from the generated and training sets\nlabeled with emotions e1-e4.",
                "Kolmogorov-Smirnov statistic between distributions of\nmetric n pitches used obtained from the generated and training\nsets labeled with emotions e1-e4.",
                "Kolmogorov-Smirnov statistic between distributions of\nmetric pitch in scale C major rate obtained from the generated\nand training sets labeled with emotions e1-e4.",
                "Kolmogorov-Smirnov statistic between distributions of\nmetric pitch in scale C minor rate obtained from the generated\nand training sets labeled with emotions e1-e4.",
                "Their values, different from 0, show that the\nseparately used metrics are not as sensitive as they should\nbe and these elements could be interpreted as errors .",
                "Hence,\nwe can make one more interesting observation, the metric-set\nused in this evaluation is much more sensitive to arousal\nthan to valence.",
                "The generated music using the proposed model\naccording to the presented metrics is more similar to the\nmusic of J.S. Bach, which was used as a training set.",
                "This proves that even\nthe use of a non-recursive model as a baseline in our experi-\nment made sense because it showed changes in the obtained\nmetrics for different emotions, which is a very interesting\nobservation.",
                "The evaluation concerned the same \u001cles as during the\nevaluation using the metrics (Section VI-B), i.e. each model\nwas assessed using 80 music sequences, generated 20 for\neach of the four emotions (e1, e2, e3, e4).",
                "Also, in\nthe case of the baseline model, the smaller similarities of the\ngenerated examples to the original melodies was re\u001dected in\nthe metrics in Section VI-B (Table 9).",
                "After conducting both evaluations (using metrics and\nexpert opinions), we can see that using additional objective\nmetrics to evaluate the model (Section VI-B) is helpful in\nthis case."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "This collection mostly\nincludes chorales (382) as well as several other composi-\ntions, 410 pieces in total.",
                "[4] G. Hadjeres, F. Pachet, and F. Nielsen, ``DeepBach: A steerable model for\nBach chorales generation,'' in Proc."
            ],
            "orchestral": [],
            "electronic": [
                "TEODORA DIMITROVA-GREKOW received the\nPh.D. degree in electronics and automation from\nVienna University of Technology, Austria, in 1997."
            ],
            "pop": [
                "Being an important basis for\nhuman-machine interaction success, emotion recognition is\nalso a popular \u001celd of exploration."
            ],
            "Demo availability": [],
            "dataset": [
                "The training dataset was extracted\nfrom video game soundtracks in MIDI format, a part of which\nwas annotated according to a two-dimensional model that\nrepresents emotion using valence-arousal.",
                "Section III\ndescribes the phases of building a music dataset and the\nemotion model used in the experiments.",
                "MUSIC DATASET\nA. PREPARING OF SYMBOLIC MUSIC DATASET\nThe \u001crst phase of building a music generating system is build-\ning or selecting a database with musical compositions.",
                "In [39], Dong et al. studied\nkey mode distributions of different music datasets, among\nothers (Lakh MIDI Dataset, Wikifonia Lead Sheet Dataset,\nHymnal Dataset, J. S. Bach music21 Dataset).",
                "They found\nthat key mode distributions (minor, major) in most databases\nwere rather imbalanced, with the exception of the J. S. Bach\nmusic21 Dataset, where the occurrence of major compo-\nsitions is equal to 56% in relation to the whole.",
                "A fairly\neven key mode distribution of compositions is important\nwhen creating a database in which emotions will be assessed,\ntherefore the J. S. Bach music21 Dataset was selected as the\nstarting database for building the training set.",
                "The music generation system created in this work should\ngenerate monophonic sequences, therefore the original\nJ. S. Bach music21 Dataset underwent several transforma-\ntions (Fig. 1).",
                "Transformations of J. S. Bach music21 dataset.",
                "Another transformation is the limitation of the music exam-\nple length to four bars and the selection of pieces only\nin a 4/4 time signature, which prevail in the J. S. Bach\nmusic21 Dataset, but which resulted in a reduction in the\nnumber of examples in the dataset.",
                "Another transformation concerned the keys of the exam-\nples, which vary greatly in the J. S. Bach music21 Dataset.",
                "B. DATASET ANNOTATION",
                "Since the music generation system will learn using mono-\nphonic melodies, all MIDI \u001cles from the dataset have been\nencoded into pitch-based representation using the MusPy\nToolkit.",
                "The length of each example from the dataset corresponds\nto four bars in a 4/4 time signature, which is four quarter\nnotes per bar, making a total of 16 quarter notes.",
                "The shortest\nnote value in the dataset is sixteenth notes, and therefore\nexamples with sixteen notes were discretized.",
                "Thus, each\nMIDI \u001cle from the dataset was encoded into a pitch-based\nrepresentation with 64 time steps.",
                "After processing the MIDI dataset, the number of different\npitch notes was reduced to 29, which after adding rest and\nhold tokens gives a total of 31 different tokens in a sequence,\nwhich were additionally one-hot encoded.",
                "C. EVALUATION OF RESULTS USING EXPERT OPINIONS\nThe same method that was used to label the training dataset\n(Section III-B), i.e. asking the same three music experts\nwith a university music education to annotate the emotion of\nthe generated music \u001cles, was used as a second method of\nevaluating the generated music sequences.",
                "[14] M. Khateeb, S. M. Anwar, and M. Alnowami, ``Multi-domain feature\nfusion for emotion classi\u001ccation using DEAP dataset,'' IEEE Access ,\nvol."
            ]
        },
        {
            "title": "Music Deep Learning: A Survey on Deep Learning Methods for Music Processing",
            "architecture": [
                "For this purpose,\nvaluable information is extracted using MIR techniques and\nthen different DL architectures are usually tested [6].\nFig.",
                "DL METHODS FOR MIR\nThe DL architectures that are most frequently employed for\nMIR tasks are: i) Recurrent Neural Networks (RNNs), and\nii)Convolutional Neural Networks (CNNs).",
                "In Table I, the\nmost common used DL architectures applied on MIR tasks\nare summarized.",
                "I\nDL METHODS FOR MIR\nDL Architectures Applications Research Paper\nRNNs Feature extraction [11] - [14]\nLSTMs Emotion prediction [10]\nCNNs Feature extraction [16] - [25], [27]\nUnsupervised Learning Sound representations",
                "[14] different variants of RNN\narchitectures are employed in order to tackle such problems.",
                "In [27] attention augmented CNNs were trained to\nrecognize musical instruments, outperforming the classical\nCNN architectures.",
                "In Table II, the most common used DL\narchitectures applied on MG tasks are summarized.",
                "TABLE II\nDL METHODS FOR MG\nDL Architectures Applications Research Paper\nRNNs Music generation",
                "Classical\nRNN architectures have been tested on various MG tasks [30]\n- [35].",
                "Although\nattention mechanisms seem to promise better results in both\nMIR and MG, it is likely that standalone architectures will\nnot outperform the current ones.",
                "On the contrary, combined\narchitectures which leverage the individual characteristics of\neach model are going to dominate the field in the near future.",
                "The different\nDeep Learning models and a review of the state-of-the-art\narchitectures were thoroughly surveyed, while future research\ndirections were highlighted.",
                "\u201cCNN architectures for large-scale audio classifica-\ntion.\u201d"
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "Proceedings of\nthe 18th International Society for Music Information Retrieval Confer-\nence, 621\u2013627. https://doi.org/10.5281/zenodo.1417327",
                "International Society for Music Information Retrieval (ISMIR).",
                "21 st International Society for Music Informa-\ntion Retrieval Conference (ISMIR), Aug 2020, Toronto, Canada."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "DeepBach: a steerable\nmodel for bach chorales generation."
            ],
            "orchestral": [],
            "electronic": [
                "[8] N. Ndou, R. Ajoodha and A. Jadhav, \u201dMusic Genre Classification:\nA Review of Deep-Learning and Traditional Machine-Learning Ap-\nproaches,\u201d 2021 IEEE International IOT, Electronics and Mechatronics\nConference (IEMTRONICS), 2021, pp. 1-6, doi: 10.1109/IEMTRON-\nICS52119.2021.9422487."
            ],
            "pop": [
                "Attention mechanism [26] has gained much popularity re-\ncently.",
                "B. GANs\nAnother popular approach in the field of MG is the use of\nGANs.",
                "The authors of [54]\npropose Pop Music Transformer to compose pop piano music,\nachieving better rhythmic structure than other models.",
                "Pop Music Transformer:\nBeat-based Modeling and Generation of Expressive Pop Piano Com-\npositions."
            ],
            "Demo availability": [],
            "dataset": [
                "Singal and S. Aggarwal, \u201dLSTM based Music\nGeneration with Dataset Preprocessing and Reconstruction Techniques,\u201d\n2018 IEEE Symposium Series on Computational Intelligence (SSCI),\n2018, pp."
            ]
        },
        {
            "title": "Music Generation using Deep Generative Modelling",
            "architecture": [
                "But this \narchitecture is trained  on a fixed number of chords and is \nthus unable t o create a more diverse combination of notes.",
                "2. Representation of musical waveforms  \nHere, we have used random interpolation to generate music on \na scaled  GAN  architecture."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "[2] H. W. Dong, W. Y. Hsiao, L. C. Yang and Y. H. Yang, \u201cMuseGAN\ndemonstration of a convolutional GAN based model for generating multi-track \npiano-rolls.,\u201d in International Society of Music Information Retrieva\nConference , 2017."
            ],
            "copyright": [
                "Furthermore, the \nproject will also act to detect plagiarism and act as a \ndevice to resolve copyright issues in the music industry."
            ],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "[3] A. Shin and L. Crestel, \u201cMelody Generation for Pop Music via word\nrepresentation of musical properties,\u201d arXiv, vol."
            ],
            "Demo availability": [],
            "dataset": [
                "The following results, as seen in Figures 2,3 and 4, were \nachieved when a GAN based model was trained on a dataset \nconsisting of Bach\u2019s musical symphonies.",
                "Pitch vs Time representation of dataset used for random interpolation  \nFollowing the training of the model using this data, the time \ninterval for random interpolation between instruments was set \nas 5 (sec/instrum ent) and constant Q -spectrogram of the \ngenerated music was obtained.  \nFig.",
                "Constant -Q spectrogram of generated music  \nThus, from our dataset, we were able to randomly subsample, \ninterpolate and generate 656 samples."
            ]
        },
        {
            "title": "Music Generation using Deep Learning with Spectrogram Analysis",
            "architecture": [],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": []
        },
        {
            "title": "Music Generation with AI technology: Is It Possible?",
            "architecture": [
                "[0 0\n1 10 0\n0 0\n0\n10\n10\n00\n0]  \nAlso, while holding a note is not the same as replaying a note, \nwe need to distinguish these two events, so \ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66  is also \nneeded which looks similar to \ud835\udc61\ud835\udc5d\ud835\udc59\ud835\udc4e\ud835\udc66. \n2) Architecture \nBiaxial LSTM generates polyphonic music by modeling \nevery note in every time step as a probability, using all previous \ntime steps and all notes already generated in the current time step \nas the condition.",
                "\ud835\udc73\ud835\udc85\ud835\udc9a\ud835\udc8f\ud835\udc82\ud835\udc8e\ud835\udc8a\ud835\udc84\ud835\udc94 =\u2211\ud835\udc95\ud835\udc91\ud835\udc8d\ud835\udc82\ud835\udc9a(\ud835\udc95\ud835\udc85\ud835\udc9a\ud835\udc8f\ud835\udc82\ud835\udc8e\ud835\udc8a\ud835\udc84\ud835\udc94 \u2212\ud835\udc9a\ud835\udc85\ud835\udc9a\ud835\udc8f\ud835\udc82\ud835\udc8e\ud835\udc8a\ud835\udc84\ud835\udc94 )\ud835\udfd0 \n3) Architecture",
                "The main difference of the architecture of DeepJ model \ncompared with Biaxial LSTM is that we use style conditioning  \nat each layer.",
                "Architecture of DeepJ  \n1267\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO.",
                "7. Flow chart of pre -processing  \n6) Architecture  \nGANs  implements adversarial learning mainly by \nconstructing two sub -network generators and discriminators.",
                "For the three models, although they are all cutting -edge \nresearch in the field of AI -generated music, they all have very \ndifferent model architectures, which results in different music \nbeing generated by each of them."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "However, in this paper the authors give some metrics tha t can be \nused to quantitatively evaluate the goodness of computer \ngenerated music.",
                "Human \nevaluation is also a more convincing way of evaluation than pure \nmetrics, because music is an abstract art that can be felt by \nhumans, while  data indicators maybe can  prove the complexity \nof music, but cannot replace human feelings."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "They also fail to include a large range of important \nfeatures in music, such as dynamics, rhythmic patterns and \nmelodic contour.",
                "To extract as many melodic, harmonic \nand arpeggio features as possible in the music, the authors used \na transposed convolutional neural network (Transposed CNN), \na structure that extracts features by reducing the number of \nchanne ls and increasing the matrix size."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [
                "However, theoretically it can be \nextended to generate other types of music, only the model needs \nto be modified in the future."
            ],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                ", \ud835\udc18 \ud835\udfcf,\ud835\udfd0, \ud835\udc18\ud835\udfcf,\ud835\udfcf) \n12652022 IEEE5th InternationalConference onElectronics Technology (ICET)",
                "IEEE 5th International Conference on Electronics Technology (ICET) | 978-1-6654-8508-1/22/$31.00 \u00a92022"
            ],
            "pop": [
                "MuseGAN \nis an innovational approach based on Generative Adversarial \nNetwork that can be used to generate pop music with multiple \ntracks.",
                "For instance, in pop music, there is more \nthan one instrument playing at the same time and have their own \nmelody and rhythmic patterns.",
                "Both of these two models are related to \nmusic style, while the main difference is, the DeepJ model is \napplicable to all musi c styles, but the MuseGAN model is mostly \nused to generate pop music which has multiple instruments and tracks.",
                "4) Apply GAN into pop music generation  \nPop music is usually composed of multiple \ninstruments/tracks.",
                "Due to the existence of a long -term independent  structure of \nmultitrack pop music, the authors used bars as the most basi c \nunit for model training.",
                "In the pre -processing process for multiple tracks of popular \nmusic, the authors used the musPy tool previously developed by \ntheir team to binarize the music tracks into a binarized image \nmatrix.",
                "The MuseGAN model, on the other \nhand, is mainly targeted at modern pop music and usually \ncontains a variety of instruments such as drums, bass, guitar, \norchestra and piano.",
                "2) MuseGAN model  \nThe goal of Mu seGAN is to generate pop music of multiple \ntracks in piano -roll format.",
                "The \nMuseGAN model is another style -related model that is used to \ngenerate music with multiple instrument tracks, normally used \nto generate modern pop music.",
                "Compared with DeepJ model, the MuseGAN model is \nmore suitable for generating pop music w ith multiple \ninstruments and tracks, and can be used in the future for game \nsoundtracks, etc."
            ],
            "Demo availability": [],
            "dataset": [
                "In DeepJ model, we use a dataset of 23 different composers \nfrom different classical music periods.",
                "In the process of data pre -processing, authors found that the \ntracks in original dataset tend to play only a few notes in the \nentire songs.",
                "The authors stored the pre -processed data in Lakh Piano \nroll Dataset (LPD), and the subsequent LPD -5-matched dataset \nsaved data with higher confidence so that to improve the training \nof the model.",
                "In more technical terms, the goal of the generator is \nto generate samples that match the data distribution of the \ntraining dataset.",
                "Therefore, the features considered in the \ndatasets are also different.",
                "the research objectives, features in datasets and the \nevaluation methods taken by DeepJ model and the MuseGAN \nmodel.",
                "C. Features  in Datasets  \n1) DeepJ model  \nThe dataset of DeepJ includes MIDI files of pieces \ncomposed by 23 well -known composers in the three major \nperiods of class ical music (Baroque period, classical period, and \nromantic period).",
                "Besides, in this dataset, the temporal \nresolution was set as 24 time steps for one beat, thus, the concept \nof velocity was ignored."
            ]
        },
        {
            "title": "Music Generation with Bi-Directional Long Short Term Memory Neural Networks",
            "architecture": [
                "This paper is organized as follows: Section II describes, in \nbrief, the literature survey; Section III explains the network \narchitecture which includes the deep learning models \nimplemented; the methodology of implementation is \ndiscussed in section IV; Section V shows the results of the \nsurvey conducted to compare the computer -generated music \nwith human compositions; Section VI finally concludes the \npaper.",
                "Bretan et al. have used autoencoder s for predicting the \nnext four beats in a musical sequence by comparing a variety \nof these architectures and their objective functions.",
                "As mentioned in the methodology section, the \narchitecture of the suggested model is based on two \nbidirectional LSTM layers functioning as an encoder -decoder \nsystem with a self -attention layer between them to effectively \nutilize long sequences from the first b i-directional LSTM and \nformulate a weighted input for the second one.",
                "This approach to composition renders a simple LSTM \nbased architecture inadequate, and hence  our proposed system \nimplements bi -directional LSTM .",
                "1.   Representation of Bi -Directional LSTM  \n \nBidirectional LSTM layers are stacked in the proposed \nsystem forming a model where the output of one layer serves \nas the input for another, which significantly increases the \nperformance of this architecture by providing a cascading \ncumulative effect while processing the data [14].",
                "To integrate this approach, \nattention -based architecture is used.",
                "The training step consists of the architecture specified \nearlier in the form of a deep learning network using bi -lstm \nand self -attention layers.",
                "Further, the proposed \nmodel is able to overcome the drawbacks of the other \narchitectures such as GANs and Auto -Encoders by not being \nsusceptible to mode -collapse or generating overly repetitive \nmusic respectively."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [
                "PARAMETERS USED IN THIS WORK  \nParameter  Value  \nEpochs  4 \nBatch Size  16 \nSequence Length  50 \nAttention Regularizer Weight  0.00001  \nFrame per second  5 \nOptimizer  NAdam  \nGraphical Processing Unit (GPU)"
            ],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "They have \nused Information Rate as a metric to measure the performance \nof a network [12] .",
                "However, one of the challenges in this domain is the lack of quantifiable metrics, which is why we \nhave utilized a survey for judging the quality of the results."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [
                "This paper works with the generation of homophonic \nmusic for which the piano channel of the input MI DI file is \nconverted to a piano roll format and is read over two axes."
            ],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "International Conference on \nElectronics Technology (ICET), 2019, pp."
            ],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "It involves \ntraining  the model  on a large  dataset  of musical  input  and \nusing it to produce similar music with slight variations.",
                "In this paper,  the training  is done  using  the MIDI  and \nAudio  Edited  for Synchronous  Tracks  and Organization \n(MAESTRO) dataset  [15].",
                "It  consists  of audio recordings in \nMIDI  format  and has a duration  of 172 hours  which  is \nsignificantly  more  than other  datasets  having  similar  music.",
                "Two separate LSTM models were developed in \n[11] for training dynamics and tempo separately over a dataset \nof Chopin\u2019s mazurkas.",
                "Despite a smaller dataset, the model \nsuccessfully recognized the real -world dependencies .",
                "A recurrent \nneural network -based model is trained on the MAESTRO \ndataset [20].",
                "The dataset used to train the network consists of 200 piano \nsongs from the MAESTRO dataset provided by Magenta \n[17].",
                "In addition, \nthe length of the output layer is equal to the total number  of \nthe unique combinations of notes present in the dataset.",
                "[17] Mateusz Modrzejewski, Mateusz Dorobek & Przemys\u0142aw Rokita, \n\u201cApplication of Deep Neural Networks to Music Composition Based \non MIDI Datasets and Graphical Representation,\u201dArtificial \nIntelligence and Soft Computing, 2019, Volume 11508, ISBN : 978 -3-\n030-20911 -7 \n[18] Zhang, Z., Luo, C., and Yu, J. Towards the gradient vanishing, \ndivergence mismatching and mode collapse of generative adve rsarial \nnets.",
                "[20] Curtis Hawthorne , Andriy Stasyuk , Adam Roberts , Ian Simon , Cheng -\nZhi Anna Huang , Sander Dieleman , Erich Elsen , Jesse Engel , Douglas \nEck, \u201cEnabling Factorized Piano Music Modeling and Generation with \nthe MAESTRO Dataset,\u201d 2019 International Conference on Learning \nRepresentations(ICLR) , 2019, arXiv:1810.12247"
            ]
        },
        {
            "title": "RE-RLTuner: A topic-based music generation method",
            "architecture": [
                "As Long Short-Term Memory (LSTM) networkarchitectures excel in modeling sequential information indata, LSTM based music generation method is proposed\n1CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems,\nShenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\nShenzhen 518055, China.",
                "We use the deep Q learning(DQN) reinforcement learning\narchitecture."
            ],
            "training parameters": [],
            "learning rate": [
                "p(a|s)generated based\non the probability of action for Q network \u03c9\nirespectively\nfor each topic corresponding weights, p(note|topici)for\nthe corresponding topic generated on the probability of thenote.\u03b3,\u03b8,Q(s,a;\u03b8)represent learning rate of reinforcement\nlearning, current strategy of reinforcement learning and aselection under the current policy respectively.",
                "In the training of RL, the learning rate of the Q network is0.01, the discount rate of reward is 0.95, and the update rateof the target Q network is 0.01."
            ],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "[8] Sturm B, Santos J F, Korshunova I. Folk music style modelling by\nrecurrent neural networks with long short term memory units[C]//16thInternational Society for Music Information Retrieval Conference.2015."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "This algorithm is designed to generate a melodic,harmonic and diverse machine part of the music responseto the human part."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [
                "In addition, they were also asked to evaluate how closeeach of the three types of music samples to the music fromthe dataset."
            ],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Carrying the duration and pitch in-formation of music, symbolic representation has simplercomputation characteristics and therefor it is popular amongresearchers[12]."
            ],
            "Demo availability": [],
            "dataset": [
                "MODEL DETAIL\nA. Dataset\nIn the \ufb01eld of neural network based music generation,\nmusic data format can be divided into audio and symbolrepresentations.",
                "We use the Nottingham dataset, a dataset offolk songs, pitch range from 53 to 88, which we quanti\ufb01edinto 4/4 beats.",
                "A. Objective Evaluation\nTo verify the validity of the model, we adopted the\nfollowing indexes for quantitative analysis as in [5], [12].\nPC/bar PI IOI Auto-lag1 Auto-lag2 Auto-lag3\ndataset 1.51 25.7 1.49 0.88 0.77 0.67\nLSTM 2.73 20.0 13 -0.41 0.44 -0.37\nRLTuner 3.6 28.02 18 -0.03 -0.03 -0.03\nRE-RLTuner 2.9 17.5 11 \u22120 .10 0.003 \u22120",
                "We found that the improved methodis better than RLTuner in PI, IOI, and correlation indexescompared to those of the dataset.",
                "We randomly chose four music\nsegments from each of the LSTM, RLTuner, Re-RLTuner,and a piece of original music from the dataset.",
                "We askedvolunteers to listen to the original music from the dataset andbased on that to rate extracted music samples individually.",
                "In addition, they were also asked to evaluate how closeeach of the three types of music samples to the music fromthe dataset."
            ]
        },
        {
            "title": "Some Reflections on the Potential and Limitations of Deep Learning for Automated Music Generation",
            "architecture": [
                "We are going\nto overview the models and techniques behind thelatest works in this \ufb01eld, highlighting the strengths\nand weaknesses of their proposed architectures in\nSection II.",
                "This similarity\nis the reason Convolutional Neural Networks, the\nmost common and successful deep architecture for\nimages, can be applied to music.",
                "This architecture has demon-\nstrated it\u2019s effectiveness in language modeling [8],\ntext generation and translation [9], as well as with\nimage captioning [10] and generation [11].",
                "The autoencoder architecture consists of two\nspecular networks called encoder and decoder.",
                "V AEs have been used for music generation in [19]\nwhere the authors also experimented with a stacked\narchitecture.",
                "Both models are based on CNNs,\nbut while MidiNet uses a fairly classical CNN\narchitecture, MuseGan uses an unprecedented model\nwith a strong hierarchy that is based not on the\nsingle not but on the bar as a unit, featuring a bar\ngenerator controlled by a phrase generator.",
                "It has to be noted that\nGANs are still affected by the issues of LSTM and\nCNN if those architecture are part of the generator\nand discriminator networks."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "Yang, \u201cMidiNet: A\nconvolutional generative adversarial network for symbolic-\ndomain music generation,\u201d in Proceedings of the 18th\nInternational Society for Music Information Retrieval Con-\nference (ISMIR\u20192017), Suzhou, China, 2017."
            ],
            "copyright": [
                "While the\nmusic create many not be very compelling the result\nis pleasing enough for a short soundtrack and the\nreal value is in lifting the burden of copyright\ninfringement from small creators."
            ],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "Bach-styled\nChorales generated in [14] and Irish folk music\ngenerated in [15] are an example of how a having\na corpus with rigid structural rules makes it easier\nfor the model to generate convincing results, even\nthough expert ears can still identify them as arti\ufb01cial\ndue to some dubious style decision or technical\nerror.",
                "[14] G. Hadjeres, F. Pachet, and F. Nielsen, \u201cDeepBach: a Steer-\nable Model for Bach chorales generation,\u201d arXiv preprint\narXiv:1612.01010, 2016."
            ],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "This notation was introduced\nin the early 90s to share Irish folk tunes on the\nInternet and has since become the most popular\nformat alternative to midi (it even has a MIME type).",
                "An example of pianoroll notationAnother popular representation is the so-called\nPianoroll, taking its name from the cylinders in\nold automatic player pianos.",
                "They gained popularity in 2016\nwith the release of \u201dDaddy\u2019s Car\u201d a track in style\nof the Beatles generated by their software; while the\npress described as the rise of the AI composers the\ntruth is that the model generated the sheet music,\nin a style that sound like a mishmash of all the\nBeatles different styles, and this was then arranged\nand performed by the musician Beno \u02c6\u0131t Carr \u00b4e.",
                "Available:\nhttp://arxiv.org/abs/1704.01279\n[28] M. Marchini, F. Pachet, and B. Carr \u00b4e, \u201cRethinking Re\ufb02exive\nLooper for structured pop music,\u201d in Proceedings of the\nInternational Conference on New Interfaces for Musical\nExpression, 2017, pp."
            ],
            "Demo availability": [],
            "dataset": [
                "One way to overcome this is to impose a\nconstraint on the structure of the music, both at the\narchitectural level and in the dataset.",
                "Nonetheless this shows how, with the right\ndataset and constraints, these kind of models can\n28\nAuthorized licensed use limited to: b-on: UNIVERSIDADE DO MINHO."
            ]
        },
        {
            "title": "The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation",
            "architecture": [
                "It makes more\nsense to embed the notion of music structure into the model\narchitecture and generative procedure.",
                "As far as we know,\nthis is the \ufb01rst attempt in applying WaveNet to symbolic music\ngeneration (The name of WaveNet implies its usage on audio\napplications, but in theory the temporal-CNN architecture can\nalso be used for symbolic generation).",
                "An important variation is the bidirectional architecture.",
                "C. LSTM Architecture",
                "D. W aveNet Architecture\nWaveNet proposed to use a stack of dilated temporal convo-\nlution layers",
                "The WaveNet architecture, reproduced from [15].",
                "The overall architecture and a\nstack of dilated convolutions is shown in Fig. 4 and Fig.",
                "Also, unfortunately,\nthe current WaveNet architecture is restricted to one-way\nmusic generation."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "[5] C. Z. Huang, T. Cooijmans, A. Roberts, A. Courville and D. Eck,\nCounterpoint by Convolution, The 18th International Society for Music\nInformation Retrieval Conference, 2017.",
                "[8] G. Medeot, S. Cherla, K. Kosta, M. McVicar, S. Abdalla, M. Selvi,\nE. Rex and K. Webster, StructureNet: INDUCING STRUCTURE IN\nGENERATED MELODIES, The 19th International Society for Music\nInformation Retrieval Conference, 2018.",
                "A Uni\ufb01ed Approach to Symbolic and Audio Represen-\ntations, The 16th International Society for Music Information Retrieval\nConference, 2015."
            ],
            "copyright": [],
            "evaluation metrics": [
                "To\ndate, most of the evaluation metrics for neural music models\nwere done in terms of immediate prediction error, incapable\nof capturing longer terms salience structures."
            ],
            "metric": [
                "To\ndate, most of the evaluation metrics for neural music models\nwere done in terms of immediate prediction error, incapable\nof capturing longer terms salience structures."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "This effectively combines aspect of different\ntime scales of chords and melody in music, learns simultane-\nously temporal delayed dependencies between melody over\npast and next two bars, and also learns harmonic-melodic\n772019 International Workshop on Multilayer Music Representation and Processing (MMRP)",
                "During the\ngeneration process, the chord condition is given at each time\nstep for a guiding purpose, and the \ufb01nal output shall both keep\nthe melodic \ufb02ow and interacts with chords."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "[3] Liang and Feynman, BachBot: Automatic composition in the style of\nBach chorales, University of Cambridge, 2016.",
                "[4] G. Hadjeres and F. Pachet, B DeepBach: a Steerable Model for Bach\nchorales generation, Proceedings of the 34th International Conference\non Machine Learning, PMLR 70:1362-1371, 2017.",
                "[11] H. Hermann, F. Johannes and M. Wolfram, HARMONET: A Neural Net\nfor Harmonizing Chorales in the Style of J.S.Bach, Proceedings of the\n4th International Conference on Neural Information Processing Systems,\npp. 267287, 1991."
            ],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "E XPERIMENTS\nA. Dataset",
                "Bene\ufb01t from the short-term memory structure\nand the explicit input, it is natural for the LSTM model to\ncapture innate structures in the dataset."
            ]
        },
        {
            "title": "PopMNet: Generating structured pop music melodies using neural networks",
            "architecture": [
                "3.Architecture of SGN.",
                "4.Architecture  of MGN.",
                "The architecture  of MGN is illustrated  in Fig.4."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [
                "The batch size was 50.",
                "The batch size was 64 and the weight  decay  was 10\u22125."
            ],
            "ethical": [],
            "impact": [],
            "society": [
                "[6]L.-C. Yang, S.-Y. Chou, Y.-H. Yang, Midinet:  a convolutional  generative  adversarial  network  for symbolic-domain  music generation,  in: Proceedings  of \nthe 18th International  Society  for Music Information  Retrieval  Conference,  Suzhou,  China, 2017.",
                "[19]G. Medeot,  S. Cherla, K. Kosta, M. McVicar,  S. Abdallah,  M. Selvi, E. Newton-Rex,  K. Webster,  Structurenet:  inducing  structure  in generated  melodies,  in: \nThe 19th International  Society  for Music Information  Retrieval  Conference,  2018, pp."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "Then l(u, v)\ndenotes  the structure  metric  of the model  on the dataset.",
                "If udenotes  the distribution  of a single  melody,  then l(u, v)de-\nnotes the structure  metric  of this melody.",
                "Table2shows  different  models\u2019  structure  metrics.",
                "Structure  metrics  of PopMNet  and PopMNet-NC  \nwere similar,  suggesting  that the structure  of the generated  melodies  was mainly  brought  by the guidance  of the generated  \nmelody  structure  instead  of the chord  progression.",
                "So far no convincing  metric  can measure  the \nquality  of melodies.",
                "We did not use it as a metric  to evaluate  models  because  we found  that a higher  likelihood  on the training  set or the \nevaluation  set does not imply  better  performance  by a preliminary  behavior  experiment  by the \ufb01rst author.",
                "In experiments,  subjects  were asked  to rate melodies  on a scale from 0 to 5 on the following  four metrics:\n\u2022Pleasure:  do you like the melody?\n\u2022Reality:  is the melody  composed  by a human?\n\u2022Smoothness:  is the transition  between  notes smooth?\n\u2022Integrity:  does the melody  have a clear structure?",
                "These  metrics  are similar  to the metrics  used in previous  studies [ 5,6].",
                "Table 2\nStructure  metrics  of melodies  generated by  different  models.",
                "PopMNet  outperformed  the four existing  models  on \nall metrics  by a large margin  (Pleasure:  p \u22640.0043;  Reality:  p \u22640.00054;  Smooth:  p \u22640.0011;  Integrity,  p \u22649.00 \u00d710\u22125; \none-tailed  t-test,  N=170).",
                "3 .98\u00b10.89\nscores  of the PopMNet  and PopMNet-Real  were not signi\ufb01cantly  different  (p \u22650.34 on  all metrics;  two-tailed  t-test,  N=\n170).",
                "PopMNet  outperformed  \nthe four existing  models  on all metrics  by a large margin  (Pleasure:  p \u22643.73 \u00d710\u22125; Reality:  p \u22649.86 \u00d710\u22126; Smooth:  \np \u22641.19 \u00d710\u22125; Integrity,  p \u22647.21 \u00d710\u22124; one-tailed  t-test,  N=80).",
                "The average  scores  of the PopMNet  and PopMNet-\nReal were not signi\ufb01cantly  different  (p \u22650.738 on  all metrics;  two-tailed  t-test,  N=80).",
                "We calculated  the correlation  between  structure  metrics  and human  evaluation  scores  of all 50 melodies  evaluated  in \nSection 5.4.5(see Fig.10).",
                "A strong  correlation  (r=-0.757)  was found  between  the repetition  metric  and the evaluation  \nscore.",
                "However,  only a weak correlation  (r=-0.128)  was found  between  the rhythmic  sequence  metric  and the evaluation  \nscore.",
                "9.The structures  of 24 melodies  with highest (a)  and lowest (b)  average  scores, averaged  across subjects  and the four described  metrics.",
                "Correlation  between  structure  metrics  and average  scores of melodies."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [
                "Many types of music  such as pop music  have high-level  units such as phrases  and periods  \u2014 a \nphrase  consists  of several  bars and a period  consists  of several  phrases."
            ],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "GenJam  is a genetic  algorithm-based  model  that pro-\nduces  jazz solos over a given chord  progression [ 12].",
                "[12]J.A. Biles, et al., GenJam:  a genetic  algorithm  for generating  jazz solos, in: International  Computer  Music Conference,  vol."
            ],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Arti\ufb01cial Intelligence 286 (2020) 103303\nContents lists available at ScienceDirect\nArti\ufb01cial  Intelligence\nwww.elsevier.com/locate/artint\nPopMNet:  Generating  structured  pop  music  melodies  using  \nneural  networks\nJian Wua, Xiaoguang Liub, Xiaolin Hua,\u2217, Jun Zhua\naInstitute  for Arti\ufb01cial  Intelligence,  Beijing National  Research  Center for Information  Science and Technology  (BNRist),  the State Key Laboratory  \nof Intelligent  Technology  and Systems,  and Department  of Computer  Science and Technology,  Tsinghua  University,  Beijing 100084,  China\nbLingDongYin  Technoloy  Co., Ltd., Beijing, 100084,  China\na r t",
                "However,  generating  pop music melodies  with well organized  structures  remains  to be \nchallenging.",
                "In this paper, we present  a melody  structure-based  model  called PopMNet  \nto generate  structured  pop music melodies.",
                "PopMNet  \nconsists  of a Convolutional  Neural  Network (CNN)-based  Structure  Generation  Net (SGN) \nand a Recurrent  Neural  Network (RNN)-based  Melody  Generation  Net (MGN).",
                "Fig.1shows  the melody  of a pop song \u201cSimple  Love\u201d.",
                "Speci\ufb01cally,  we consider  two important  relations  \u2014 repetition and sequence , which  play critical  \nroles in the formation  of pop music  melody  structures",
                "1.A piece of melody of \u201cSimple Love\u201d, which is a pop song by Chinese singer Jay Chou, released on 14 September 2001.",
                "We propose  a melody  structure-based  melody  generation  model,  called  PopMNet , where  the structure  is characterized  \nby repetition  and sequence  between  pairs of bars.",
                "Experiments  show that melodies  \ngenerated  by our PopMNet  have better  structures  compared  to many existing  models.",
                "Many types of music  such as pop music  have high-level  units such as phrases  and periods  \u2014 a \nphrase  consists  of several  bars and a period  consists  of several  phrases.",
                "PopMNet  shares  a similar  \nidea with the above  models:  generates  high-level  features  (melody  structure)",
                "Our models\nThe PopMNet  and its two variants  were used in the experiments:\n\u2022PopMNet: the model  proposed  in this paper.",
                "\u2022PopMNet-Real: same as PopMNet  but the melody  structures  used in the melody  generation  are extracted  from the \ndataset.",
                "\u2022PopMNet-NC: the PopMNet  without  chord  progressions  as a condition.",
                "while melodies  generated  by PopMNet  and Music  Transformer  contained  clear structures.",
                "One observation  \nis that the melodies  generated  by PopMNet  and Music  Transformer  often contain  long-term  relations,  which  is indicated  \nby red or black dots towards  the lower  left of the adjacency  matrices  (e.g., the \ufb01rst and fourth  column  in Figs.7d,7e).",
                "The percents  of PopMNet  and Music  Transformer  are close to the real data.",
                "The percents  of repetition  and \nrhythmic  sequence  of MidiNet  and LookbackRNN  are comparable  with those of PopMNet.",
                "The distributions  of PopMNet  and Music  Transformer  were \nsimilar  to the distributions  of real melody  structures  (Fig.8).",
                "Structure  metrics  of PopMNet  and PopMNet-NC  \nwere similar,  suggesting  that the structure  of the generated  melodies  was mainly  brought  by the guidance  of the generated  \nmelody  structure  instead  of the chord  progression.",
                "In this experiment,  we evaluated  \ufb01ve models,  PopMNet,  PopMNet-real,  AttentionRNN,  LookbackRNN,  and \nMidiNet.",
                "Repetition Rhythmic sequence No relation\nReal data 29.06% 32.64% 38.30%\nAttentionRNN 0.42% 24.73% 74.85%\nLookbackRNN 9.13 % 37.78% 53.09%MidiNet 18.75% 18.752% 62.29%Music Transformer 35.64% 25.15% 39.21%\nPopMNet 27.05% 28.91% 44.04%\nPopMNet-Real 31.45% 31.33% 37.22%PopMNet-NC 26.28% 29.45% 44.27%\nin the revision  phase  and it would  be costly  to redo this experiment  by incorporating  a new model.",
                "Transformer 2.597 0.848\nPopMNet 1.743 0.484\nPopMNet-Real 1.565 0.665\nPopMNet-NC 1.738 0.562",
                "PopMNet  outperformed  the four existing  models  on \nall metrics  by a large margin  (Pleasure:  p \u22640.0043;  Reality:  p \u22640.00054;  Smooth:  p \u22640.0011;  Integrity,  p \u22649.00 \u00d710\u22125; \none-tailed  t-test,  N=170).",
                ".18\u00b11.16\nMidiNet 2 .70\u00b11.36 2 .57\u00b11.39 2 .81\u00b11.32 2 .93\u00b11.34\nPopMNet",
                "3 .50\u00b11.00 3 .40\u00b11.02 3 .46\u00b11.07 3 .64\u00b11.03\nPopMNet-Real 3",
                "MidiNet 2 .02\u00b11.10 1 .70\u00b10.97 2 .19\u00b11.16 2 .21\u00b11.20\nPopMNet",
                "3 .39\u00b10.83 3 .10\u00b11.02 3 .26\u00b11.05 3 .33\u00b11.03\nPopMNet-Real 3 .31\u00b11.08",
                ".01\u00b11.16 3 .10\u00b11.97 3 .02\u00b11.23\nPopMNet",
                "3 .98\u00b10.89\nscores  of the PopMNet  and PopMNet-Real  were not signi\ufb01cantly  different  (p \u22650.34 on  all metrics;  two-tailed  t-test,  N=\n170).",
                "PopMNet  outperformed  \nthe four existing  models  on all metrics  by a large margin  (Pleasure:  p \u22643.73 \u00d710\u22125; Reality:  p \u22649.86 \u00d710\u22126; Smooth:  \np \u22641.19 \u00d710\u22125; Integrity,  p \u22647.21 \u00d710\u22124; one-tailed  t-test,  N=80).",
                "The average  scores  of the PopMNet  and PopMNet-\nReal were not signi\ufb01cantly  different  (p \u22650.738 on  all metrics;  two-tailed  t-test,  N=80).",
                "In this experiment,  we evaluated  PopMNet  and Music  Transformer.",
                "Besides  melodies  generated  by PopMNet  and Music  Transformer,  10 melodies  sampled  from the dataset  were also evaluated.",
                "PopMNet  performed  signi\ufb01cantly  better  than Music  Transformer  (Pleasure:  p =2.25 \u00d710\u22125; Reality:  p =0.0059;  Smooth:  \np =0.0038;  Integrity,  p =0.0070;  one-tailed  t-test,  N=180), but performed  worse  than human  (Pleasure:  p \u22642.66 \u00d710\u22128; \nReality:  p \u22641.87 \u00d710\u221210; Smooth:  p \u22641.34 \u00d710\u22126; Integrity,  p \u22644.91 \u00d710\u221212; one-tailed  t-test,  N=180) (Table 5).",
                "Discussion\nWe present  the PopMNet  to generate  structured  pop music  melodies,  which  integrates  melody  structure  into the gen-\neration  process.",
                "In experiments,  \nwe compared  PopMNet  with four existing  models  AttentionRNN,  LookbackRNN,  MidiNet,  and Music  Transformer.",
                "The results  \nindicate  that PopMNet  can generate  higher  quality  melodies  with clear structures  than",
                "This is only the \ufb01rst step towards  generating  rich and compelling  pop music  melodies.",
                "First, only repetition  and rhythmic  \nsequence  between  bars were considered  as structures,  while real, human-composed  pop music  melodies  contain  much more \ncomplex  relations  between  melody  segments,  which  will certainly  be studied.",
                "[5]H. Zhu, Q. Liu, N.J. Yuan, C. Qin, J. Li, K. Zhang, G. Zhou, F. Wei, Y. Xu, E. Chen, Xiaoice  band: a melody  and arrangement  generation  framework  \nfor pop music, in: Proceedings  of the 24th ACM SIGKDD  International  Conference  on Knowledge  Discovery,  Data Mining,  New York, NY, USA, 2018, \npp.",
                "[8]R. Middleton,  \u2018Play it again sam\u2019: some notes on the productivity  of repetition  in popular  music, Pop.",
                "[16]H. Chu, R. Urtasun,  S. Fidler, Song from pi: a musically  plausible  network  for pop music generation,  in: International  Conference  on Learning  Repre-\nsentations  (ICLR), 2017, workshop  track."
            ],
            "Demo availability": [],
            "dataset": [
                "Unfortunately,  an analysis  on our dataset  shows  that only 3.35% of pairs have the tonal sequence  relation  \u2014 too few to learn \nan accurate  model.",
                "Fig.5a shows  \nthe adjacency  matrices  of some melody  structures  in our dataset.",
                "Statistics  on our dataset  shows  that about  99.83%  \nof pitches  are between  C2 to C5.",
                "Dataset\nMIDI \ufb01les typically  represent  chords  with notes and it is di\ufb03cult  to distinguish  between  chord  progressions  and melodies.",
                "The dataset  is public  available.1\n5.2.",
                "Graph  visualization\nFig.5visualizes  the structures  of several  melodies  in the dataset  and the structures  generated  by SGN.",
                "We extracted  the structure  \nof the \ufb01rst 32 bars of each lead sheet in the dataset  using Algorithm 1, which  resulted  in 3,859 structures  in total.",
                "\u2022PopMNet-Real: same as PopMNet  but the melody  structures  used in the melody  generation  are extracted  from the \ndataset.",
                "The AttentionRNN,  LookbackRNN,  and MidiNet  were trained  on our dataset  with the same setting  in their original  papers.",
                "The Music  Transformer  was also trained  on our dataset  but the setting  of the model  is a little bit different  from the original  J. Wu et al. /",
                "Despite  efforts  in tuning  its \nhyperparameters,  we failed to obtain  satisfactory  results  with MusicVAE  after training  it on our dataset.",
                "To obtain  good \nresults,  MusicVAE  may require  a large dataset  for training.",
                "In the original  work [3], the training  dataset  consisted  of 1.5 \nmillion  MIDI \ufb01les.",
                "The chord  progressions  and primer  melodies  were randomly  sampled  from lead sheets  in the \ndataset.",
                "Let udenote  the distribution  of melodies  generated  by a model  and vdenote  the distribution  of the dataset.",
                "Then l(u, v)\ndenotes  the structure  metric  of the model  on the dataset.",
                "Among  the four existing  models,  Music  Transformer  per-\nformed  the best in capturing  repetitions  in the dataset.",
                "Table 1\nThe percentages  of relations  in the melodies  in the dataset  and the melodies  \ngenerated  by different  models.",
                "Besides  melodies  generated  by PopMNet  and Music  Transformer,  10 melodies  sampled  from the dataset  were also evaluated.",
                "One reason  is that almost  all structures  of generated  melodies  contained  many rhythmic  sequences  (Figs.7, 9) and \ntheir distributions  had small wasserstein  distances  to the distribution  of the rhythmic  sequence  of the dataset  (Table2).J. Wu et al. /"
            ]
        },
        {
            "title": "Singability-enhanced lyric generator with music style transfer",
            "architecture": [
                "Section 3 describes\nthe lyrics style transfer generation modules, system architecture, model\nconstruct, transfer algorithm, etc. Section 4 describe the datasets and\nevaluation methodology used in experiments and discusses experimen-\ntal design and results.",
                "[4] showed that these architectures can be applied and\nadapted for natural language generation, comparing a number of ar-\nchitectural and training schemes.\n2.2.",
                "Generative pretrained transformer\nThe GPT-2 architecture is very similar to the Transformer model de-\ncoder structure.",
                "The Google Brain\nteam introduced the Transformer [5], which incorporates an encoder\u2013\ndecoder architecture to create a sequence to sequence (Seq2Seq) model\nwithout using convolutional (CNN) or recurrent (RNN) neural net-\nworks.",
                "Fig. 2-2 shows\nthe Transformer encoder and decoder architecture.",
                "2-3 shows GPT-2 architecture.",
                "The framework applies GPT-2 with several conditions, known as a\nconditional GPT-2, with architecture as shown in Fig. 2-4.",
                "Architecture of Transformer\u2019s encoder and decoder",
                "Typical GPT-2 architecture",
                "[10] offers a range of tools to process human natural\nlanguage and simplify text analysis, collated from research by the\nStanfordNLP Group, including part of speech tagger, named entity\nrecognizer, dependency parsing, etc. StanfordNLP is built using highly\naccurate neural networks, allowing efficient training and evaluation\nusing your own marker data, and these modules are based on the\nPyTorch architecture.",
                "This paper used a multilayer RNN encoder and\nmultilayer RNN with attention for decoding, creating a style transfer\nspecific architecture.",
                "2-9 shows the overall architecture.",
                "[15] proposed a simple AWD-\nLSTM neural language model architecture and pretrained weights for\nstyle-specific text generation.",
                "Overall architecture for Jhamtani et al.",
                "Jin et al. proposed a multitask\nframework that adopts Seq2Seq based on Transformer architecture to\nsummarize styles and introduces style-guided encoder attention into the\nmulti-head attention module.",
                "Fig. 2-12\nshows the proposed model architecture.",
                "System architecture of lyrics style transfer generation\nThis study demonstrates the effectiveness of the GPT-2 model in\ngenerating stylized lyrics.",
                "3-1 shows the overall architecture for the proposed method,\nwith more detailed description in subsequent sections.",
                "Model architecture proposed by Gao et al.",
                "Model architecture proposed by Jin et al.",
                "The model architecture of Syed et al."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [
                "Three\nparameters were defined as follows\n\u2022\ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc58: only k tokens with the highest probability are kept;\n\u2022\ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc5d: retain the cumulative probability \u2265\ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc5d; and\n\u2022\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc56\ud835\udc61\ud835\udc60 : the shape of logits distribution, the product of batch size\nand vocabulary size.",
                "For example, suppose we set \ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc5d= 0.95,\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc56\ud835\udc61\ud835\udc60 = 5(i.e., batch size =1\nand vocabulary size =5), with probabilities [0.62,0.17,0.15,0.03,0.03],\nand hence cumulative probability =",
                "Batch size\nwas used to specify how many lyrics versions the model generates, with\nlater lyrics generally performing better."
            ],
            "ethical": [],
            "impact": [],
            "society": [
                "G. Meseguer-Brocal, A. Cohen-Hadria, G. Peeters, Dali: A large dataset of\nsynchronized audio, lyrics and notes, automatically created using teacher-student\nmachine learning paradigm, in: Proceedings of 19th International Society for\nMusic Information Retrieval Conference, ISMIR, 2018, September."
            ],
            "copyright": [],
            "evaluation metrics": [
                "Evaluation metrics\nIn general, it is difficult to judge the quality of lyrics generated\nautomatically by a computer.",
                "In the end, we conducted experiments with two\nevaluation metrics, including automatic and human evaluation."
            ],
            "metric": [
                "Evaluation metrics\nIn general, it is difficult to judge the quality of lyrics generated\nautomatically by a computer.",
                "[25],\nthis study used the automatic metrics of overlap score.",
                "Table 4-1 shows the five metrics used\nto evaluate the final lyrics, including thematic (T), structural (S),\noriginality (O), meaningfulness (M) and Fitness (F) scores; and Figs. 4-1\nand 4-2 show the interface for the web based survey for lyrics transfer\ngeneration.",
                "The generated stylized lyrics include elements from\nthe key sentence and the generated sentences are meaningful and\n44J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig. 4-2. Web based performance metrics questionnaire.",
                "Table 4-1\nFive metrics for human evaluation.",
                "Metric Question Reference\nThematic (T)",
                "Model Metrics Rater Mean \u00b1Std\n1 2 3\nGPT-2Thematic (T) 4.4 4.5 3.9 4.2667 \u00b10.2625\nStructural (S) 1.6 1.9 1.9 1.8000 \u00b10.1414\nOriginality (O) 4.5 4.0 4.1 4.2000 \u00b10.2160\nMeaningfulness (M) 3.1 3.0 3.8 3.3000 \u00b10.3559\nFitness (F) 1.4 1.9 1.8 1.7000 \u00b10.2160\nGPT-2+DPThematic (T) 3.5 4.0 4.0 3.8333 \u00b10.2357\nStructural (S) 3.9 4.1 3.8 3.9333 \u00b10.1247\nOriginality (O) 3.0 3.5 3.6 3.3667 \u00b10.2625\nMeaningfulness (M) 3.2 3.7 3.2 3.3667 \u00b10.2357\nFitness (F) 3.1 3.0 3.5 3.2000 \u00b10.2160\nGPT-2+DP+RMThematic (T) 3.8 3.6 3.9 3.7667 \u00b10.1247\nStructural (S) 4.3 4.4 3.7 4.1333 \u00b10.3091\nOriginality (O) 3.5 3.5 3.2 3.4000 \u00b10.1414\nMeaningfulness (M) 2.9 3.4 3.4 3.2333 \u00b10.2357\nFitness (F) 4.7 3.8 3.8 4.1000 \u00b10.4243Table 4-4\nResults of human evaluation (Rock2Pop).",
                "Model Metrics Rater Mean \u00b1Std\n1 2 3\nGPT-2Thematic (T) 4.3 4.5 4.4 4.4000 \u00b10.0816\nStructural (S) 1.7 1.5 2.3 1.8333 \u00b10.3399\nOriginality (O) 4.1 3.9 4.3 4.1000 \u00b10.1633\nMeaningfulness (M) 3.4 3.6 3.9 3.6333 \u00b10.2055\nFitness (F) 1.5 1.2 1.7 1.4667 \u00b10.2055\nGPT-2+DPThematic (T) 3.6 3.7 4.4 3.9000 \u00b10.3559\nStructural (S) 3.6 3.9 3.5 3.6667 \u00b10.1700\nOriginality (O) 3.5 3.1 4.5 3.7000 \u00b10.5888\nMeaningfulness (M) 3.7 3.8 3.0 3.5000 \u00b10.3559\nFitness (F) 3.5 3.2 3.4 3.3667 \u00b10.1247\nGPT-2+DP+RMThematic (T) 4.1 3.5 4.5 4.0333 \u00b10.4110\nStructural (S) 4.4 3.8 4.3 4.1667 \u00b10.2625\nOriginality (O) 3.8 3.6 3.6 3.6667 \u00b10.0943\nMeaningfulness (M) 3.4 3.3 3.5 3.4000 \u00b10.0816\nFitness (F) 4.1 4.1 3.9 4.0333 \u00b10.0943\n50J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig. 4-8.",
                "In the end, we conducted experiments with two\nevaluation metrics, including automatic and human evaluation."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "For\nexample, suppose the original lyrics have a \u2018\u2018pop\u2019\u2019 style and the goal\nis to generate lyrics with a \u2018\u2018rock\u2019\u2019 style.",
                "Experiments were conducted on an English music\ndataset containing pop and rock music.",
                "This study collected lyrics from five genres: pop, country,\nrock, rap, and reggae; with the songs collected spanning different years.",
                "For example,\nsuppose pop song lyrics were used as training data, then and all the data\nis combined into a single text, splitting each lyric with < |endoftext |>.",
                "The different lyric styles\nuse different expressions and lyricism, e.g. pop lyrics tend to emphasize\nromantic love, whereas rock lyrics tend to emphasize social or political\naspects.",
                "The different styles often use different words, e.g. pop songs\nusually contain \u2018\u2018love\u2019\u2019, \u2018\u2018feel\u2019\u2019, \u2018\u2018live\u2019\u2019, \u2018\u2018heart\u2019\u2019, and \u2018\u2018tell\u2019\u2019; whereas rock\nsongs usually contain \u2018\u2018oh\u2019\u2019, \u2018\u2018never\u2019\u2019, \u2018\u2018burn\u2019\u2019, and \u2018\u2018rock\u2019\u2019.",
                "We had some\nlimitations song lyric dataset size, so we focused on pop and rock styles,\ncombining pop and rock lyric data to form a text file with over 8000\n40J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "This study captured pop, rap, country, rock, and reggae lyric styles\nfrom the Genius website [22], which is non-parallel data, to overcome\nsparse data problems when generating different lyric styles.",
                "The top\n60 songs were then captured by sorting them according to artist\u2019s song\npopularity.",
                "Thus, pop\nstyle and rock style models were generated by fine-tuning GPT-2.",
                "Suppose we have two lyric text datasets X = {x(1),x(2),\u2026,x(m)}\n(original lyrics) and Y = {y(1),y(2),\u2026,y(n)}(output lyrics after GPT-2\nprocessing), with styles SxandSy, respectively (e.g. Sxis pop style and\nSyis rock style).",
                "Suppose pop music style lyrics are transferred to rock music.",
                "Then\nX(structure template) is the provided pop music lyrics and Yis the\ntarget rock music lyrics.",
                "We chose a sample of 100 stylized\nlyrics texts generated by the proposed system for each transfer task\nfor users to cross-rate, with 50 each being pop to rock and rock to\npop conversion, respectively.",
                "Fig. 4-3 and Fig. 4-6 show stylized lyrics using\nGPT-2 from pop to rock and rock to pop, respectively.",
                "We set the key\nsentence as \u2018\u2018Bury every word I\u2019ve said in the city of the dead and drown\nthis masterpiece in red\u2019\u2019 (rock style) and conditionally transferred from\npop to rock style.",
                "The generated results are divided into two\nparts, one is Pop to Rock (Pop2Rock) and the other is Rock to Pop\n(Rock2Pop).",
                "A total of 100 songs in the experimental section, 50\nPop to Rock and 50 Rock to Pop, will be evaluated for each of\nthe three different approaches.",
                "Method Style transfer from rock\n(Pop2Rock)Style transfer from pop\n(Rock2Pop)\nAverage of 50 tracks\n(Mean \u00b1Std)Average of 50 tracks\n(Mean \u00b1Std)\nGPT-2 0.2893 \u00b10.0674 0.3039 \u00b10.0702\nGPT-2 + DP 0.6683 \u00b10.1351 0.6962 \u00b10.0996\nGPT-2 + DP + RM 0.5500 \u00b10.1137 0.5798 \u00b10.0774\nGPT-2 yields 0.2893 \u00b10.0674 in Pop2Rock and 0.3039 \u00b10.0702\nin Rock2Pop, which is the lowest overlap value among all methods,\nindicating that the GPT-2 model is capable of generating original song\ntexts.",
                "In Pop2Rock, GPT-2+DP has an overlap of 0.6683 \u00b10.1351, and\nin Rock2Pop, GPT-2+DP has an overlap of 0.6962 \u00b10.0996, which\nis even higher than GPT-2 alone because GPT-2+DP draws on the\nstructure of the original pop or rock lyrics, thus increasing the overlap\nscore, but the overlap is within acceptable limits and the content still\nretains the results produced by GPT-2.",
                "GPT-2+DP+RM was modified for each concluding\nphrase of the GPT-2+DP results, yielding 0.5500 \u00b10.1137 in Pop2Rock\nand 0.5798 \u00b10.0774 in Rock2Pop, with overlap scores even lower\nthan GPT-2+DP, indicating that some rhyming words can be effectively\nsubstituted.",
                "The result of the GPT-2 processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2+DP+RM processing of the lyrics from Pop to Rock.",
                "The result of the GPT-2 processing of the lyrics from Rock to Pop.",
                "The result of the GPT-2+DP processing of the lyrics from Rock to Pop.\nTable 4-3\nResults of human evaluation (Pop2Rock).",
                "Model Metrics Rater Mean \u00b1Std\n1 2 3\nGPT-2Thematic (T) 4.4 4.5 3.9 4.2667 \u00b10.2625\nStructural (S) 1.6 1.9 1.9 1.8000 \u00b10.1414\nOriginality (O) 4.5 4.0 4.1 4.2000 \u00b10.2160\nMeaningfulness (M) 3.1 3.0 3.8 3.3000 \u00b10.3559\nFitness (F) 1.4 1.9 1.8 1.7000 \u00b10.2160\nGPT-2+DPThematic (T) 3.5 4.0 4.0 3.8333 \u00b10.2357\nStructural (S) 3.9 4.1 3.8 3.9333 \u00b10.1247\nOriginality (O) 3.0 3.5 3.6 3.3667 \u00b10.2625\nMeaningfulness (M) 3.2 3.7 3.2 3.3667 \u00b10.2357\nFitness (F) 3.1 3.0 3.5 3.2000 \u00b10.2160\nGPT-2+DP+RMThematic (T) 3.8 3.6 3.9 3.7667 \u00b10.1247\nStructural (S) 4.3 4.4 3.7 4.1333 \u00b10.3091\nOriginality (O) 3.5 3.5 3.2 3.4000 \u00b10.1414\nMeaningfulness (M) 2.9 3.4 3.4 3.2333 \u00b10.2357\nFitness (F) 4.7 3.8 3.8 4.1000 \u00b10.4243Table 4-4\nResults of human evaluation (Rock2Pop).",
                "The result of the GPT-2+DP+RM processing of the lyrics from Rock to Pop.\nFig. 4-9. Results of human evaluation (Pop2Rock).",
                "Results of human evaluation (Rock2Pop)."
            ],
            "Demo availability": [],
            "dataset": [
                "Then the rock lyrics dataset\nis trained for model migration and the lyric text is modified using a\npost-processing module to ensure that every line and word in the lyrics\nmatches the audio.",
                "Section 3 describes\nthe lyrics style transfer generation modules, system architecture, model\nconstruct, transfer algorithm, etc. Section 4 describe the datasets and\nevaluation methodology used in experiments and discusses experimen-\ntal design and results.",
                "However, GPT-2 is a very large Transformer based lan-\nguage model that requires training on a large dataset.",
                "The pre-training dataset\ncontains 8 million web pages collected by crawling qualified outbound\nlinks from Reddit.",
                "They divided the data into three style\ndatasets and split style data into sentences for training examples.",
                "[20] proposed an author stylized rewriting language\nmodel (StyleLM) with two parts: unsupervised pretraining with a Trans-\nformer based language model on a large English dataset cascaded\nthem into an encoder\u2013decoder like framework; and author specific\nfine-tuning with denoising Auto-Encoder loss (DAE loss), allowing the\ndecoder to push the target author\u2019s style while rewriting the encoder\ninput text, as shown in Fig.",
                "Experiments were conducted on an English music\ndataset containing pop and rock music.",
                "Model training and fine-tuning\nThe proposed model was trained on a lyrics dataset collected by\nscraping outbound links on the Genius lyrics website, which is the\nworld\u2019s largest collection of song lyrics and crowdsourced musical\n39J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "The dataset included 2024, 1204, 2159, 2299, and\n266 songs in each genre, respectively.",
                "Basically,\nwe take a pretrained model and train it with a specific song style lyricsdataset, leading the model towards generating the specific lyric style.",
                "We had some\nlimitations song lyric dataset size, so we focused on pop and rock styles,\ncombining pop and rock lyric data to form a text file with over 8000\n40J.-W. Chang, J.C. Hung and K.-C. Lin Computer Communications 168 (2021) 33\u201353\nFig.",
                "Process to re-train GPT-2.\nlines as the training dataset.",
                "The next step was to encode the dataset,\nencoding generates the file.",
                "Abbreviation Name Sentence Representation\n\ud835\udc5b\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc57 Nominal subject Tim defeated Amy nsubj (defeated, Tim)\n\ud835\udc5b\ud835\udc60\ud835\udc62\ud835\udc4f\ud835\udc57\ud835\udc5d\ud835\udc4e\ud835\udc60\ud835\udc60 Passive nominal subject Amy was defeated by Tim nsubjpass (defeated, Amy)\n\ud835\udc4e\ud835\udc5a\ud835\udc5c\ud835\udc51 Adjectival modifier Sam eats red meat amod (meat, red)\n\ud835\udc5b\ud835\udc5a\ud835\udc5c\ud835\udc51 Noun modifier Filled up with water nmod (filled, water)\n\ud835\udc4e\ud835\udc51\ud835\udc63\ud835\udc5a\ud835\udc5c\ud835\udc51 Adverb modifier Genetically modified food advmod (modified, genetically)\n\ud835\udc51\ud835\udc5c\ud835\udc4f\ud835\udc57 Direct object She gave me a raise dobj (gave, raise)\n\ud835\udc50\ud835\udc5c\ud835\udc5a\ud835\udc5d\ud835\udc62\ud835\udc5b\ud835\udc51 Compound nouns Wait at this bus stop compound (bus, stop)\ndataset including many unique songs accompanied by metadata, i.e.,\ngenre, artist, year, album, and song title.",
                "Each style was fine-tuned to the pretrained model using the\nlyric dataset, and trained according to the original lyrics.",
                "Suppose we have two lyric text datasets X = {x(1),x(2),\u2026,x(m)}\n(original lyrics) and Y = {y(1),y(2),\u2026,y(n)}(output lyrics after GPT-2\nprocessing), with styles SxandSy, respectively (e.g. Sxis pop style and\nSyis rock style).",
                "The dataset is non-parallel, i.e., data does not contain\npairs (x(m),y(n))that describe the same content.",
                "Dataset\nSong lyrics are widely available across the internet in the form\nof user-generated content.",
                "Two large-scale datasets are used in our\nexperiments.",
                "Two large-scale datasets are used in this experiment.",
                "The\nfirst dataset is a large corpus of 7952 English lyrics crawled from the\nGenius lyrics website and used to fine-tune the lyric style conversion\ngeneration model.",
                "The second dataset is the audio of the original lyrics,\nand the DALI dataset [24] was chosen for this study, which is a large\ndataset of audio full sections synchronized with audio, lyrics and notes,\nwith lyrics and notes (of the vocal melody) aligned in time.",
                "Due to its\ndata integrity, the dataset for this study is composed of 5358 songs with\nEnglish lyrics, each of which contains audio, lyrics and midi notes.",
                "G. Meseguer-Brocal, A. Cohen-Hadria, G. Peeters, Dali: A large dataset of\nsynchronized audio, lyrics and notes, automatically created using teacher-student\nmachine learning paradigm, in: Proceedings of 19th International Society for\nMusic Information Retrieval Conference, ISMIR, 2018, September."
            ]
        },
        {
            "title": "Human, I wrote a song for you: An experiment testing the influence of machines\u2019 attributes on the AI-composed music evaluation",
            "architecture": [],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [
                "However, participants \u2019 awareness of the \ncomposition process (i.e., whether the music was composed by a human \ncomposer versus an AI) had no significant impact on their perception of \nthe music piece (e.g., affective response, general attitude, and mean -\ningfulness)."
            ],
            "society": [
                "Related to the question \nof authorship, a music society has also moved forward in recognizing an \nAI virtual composer as the official author of its work (Kaleagasi, 2017 ).",
                "Role theory \nAs machines substitute human labor, their roles in society should be \nconsidered.",
                "I feel that the future society will be dominated by autonomous machines.",
                "In Audio engineering society convention 9.",
                "Audio Engineering Society.",
                "New Media & \nSociety.",
                "In International society for music \ninformation retrieval conference (pp. 324\u2013331)."
            ],
            "copyright": [
                "The discussion is also closely related to other social and legal \nissues associated with copyright (Ihalainen, 2018 ; Hristov, 2020 ) and \nhuman labor replacement (Frey & Osborne, 2017 ).",
                "For instance, if an AI music generator creates a song, some people might \nthink the AI generator should possess its copyright, while others believe \nit should be given to the programmer.",
                "Autonomy may matter more than intentions in terms of un-\nderstanding machine creativity because computer creativity often \nsparks debate about intellectual properties, such as copyright (Ihalai -\nnen, 2018 ).",
                "Artificial intelligence and the copyright survey.",
                "Computer creativity: Artificial intelligence and copyright.",
                "Artificial intelligence \nand music: Open questions of copyright law and engineering praxis."
            ],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "It explains \nhow social positions and expectations determine characteristic social \nbehavior patterns and focuses on performances or behaviors in a given \nsetting when defining roles, not the performer \u2019s attributes (Biddle, 1986 ; \nSolomon et al., 1985)."
            ],
            "choral": [
                "Apart from GANs and Transformer, a hierarchical recurrent \nneural network (HRNN), which Deep Bach uses, is based on \npseudo-Gibbs sampling in order to produce notes in the style of Bach \nchorales, providing more techniques to create music (Hadjeres et al., \n2017 ; Wu et al., 2020 ).",
                "Deepbach: A steerable model for \nbach chorales generation."
            ],
            "orchestral": [],
            "electronic": [
                "In this case, people will see this machine less as a musician but rather as \na musical instrument, such as software that electronic dance music \n(EDM) musicians use.",
                "Journal of Broadcasting & Electronic Media, 64(4), 566\u2013591. https://doi. \norg/10.1080/08838151.2020.1835136 \nYang, L., Chou, S., & Yang, Y. (2017)."
            ],
            "pop": [
                "Generative Adversarial Networks \n(GANs) have been a popular structure type for developing creative machines."
            ],
            "Demo availability": [],
            "dataset": []
        },
        {
            "title": "Rethinking musicality in dementia as embodied and relational",
            "architecture": [],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [
                "It further highlights the\nethical imperative to fully support musicality through institutional policies, structures and practices.",
                "It further highlights the ethical imperative to\nfully support musicality through institutional policies, structures and\npractices.",
                "This model has already\nbeen applied to explicate an ethic of sexuality that o \ufb00ers an important\nalternative to the positivist legacy of bioethical principles in the \ufb01eld of\ndementia ( Grigorovich & Kontos, 2016 ;Kontos, Grigorovich, et al.,\n2016 ).",
                "Flourishing in this context occurs when embodied selfhood is\nsupported in and through the creation of enabling environments and\nrelational practices \u2013or corporeal-ethical spaces \u2013that support embo-\ndied forms of communication and meaningful engagement(Macpherson, 2016).",
                "Guiding visually impaired walking groups: Intercorporeal ex-\nperience and ethical sensibilities."
            ],
            "impact": [
                "Research on the impact of music programs is dominated by studies\nthat evaluate music as a therapeutic tool to achieve instrumental out-\ncomes ( DeNora & Ansdell, 2014).",
                "However,\nit remains unclear what therapies are most e \ufb03cacious, and\nthere is little consensus on the generative mechanisms that account for\nthe impact that music has on targeted outcomes ( Chang et al., 2015 ;\nDeNora & Ansdell, 2014 ;Spiro, 2010 ).",
                "The purpose of the study was to explore the\nimpact of elder-clowning on residents of a long-term care home and to\nexplore the relational and aesthetic dimensions of elder-clowning that\nsupport residents' engagement."
            ],
            "society": [
                "Relational citizenship brings a new and critical di-\nmension to the discourse on music, ageing, and the body in contemporary society.",
                "We argue that relational citizenship brings a new and\ncritical dimension to the discourse on music, ageing, and the body incontemporary society.",
                "Funding statement\nThis work was supported by a Canadian Institutes of Health\nResearch Operating Grant (MOP \u2013114953) and the Alzheimer Society of\nCanada and the Institute of Aging (Canadian Institutes of Health\nResearch) (Award #03-07).",
                "Ageing & Society, 31 (1), 70 \u201392.",
                "Ageing & Society, 23 (1), 99 \u2013114.\nHughes, J. C. (2001).",
                "Ageing & Society, 24 (6), 829 \u2013849.\nKontos, P. (2006).",
                "Journal of the American Geriatrics Society, 64 (2), 347 \u2013353.\nKontos, P., Miller, K. L., & Kontos, A. P. (2017).",
                "Disability & Society, 16 (3), 377 \u2013392.\nReynolds, L., Innes, A., Poyner, C., & Hambidge, S. (2016).",
                "Ageing and Society, 20 , 389 \u2013411.",
                "Ageing\n& Society, 29 (07), 1041 \u20131063 .P. Kontos, A. Grigorovich Journal of Aging S tudies 45 (2018)"
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [
                "In this sense, musicality is tantamount to theexistential expressiveness of the body that emerges from our active and\nresponsive propensity towards the world."
            ],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "Neither could their movements be mistaken for\npure imitation, particularly if we consider that Abe gave a solo per-\nformance and, while the elder-clowns o \ufb00ered musical accompaniment,\nit was Betty who produced the improvised lyrics."
            ],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Further, musicality in the context of dementia abounds in popular\nculture and empirical discourse ( Cuddy & Du \ufb03n, 2005; Oppenheimer,\n2005 ;Pickles & Jones, 2006).",
                "These were\nparticularly popular with the residents.",
                "Abe would frequently sing a popular Yiddish song repeating the one\nverse he knows over and over."
            ],
            "Demo availability": [],
            "dataset": []
        },
        {
            "title": "deepsing: Generating sentiment-aware visual stories using cross-modal music translation",
            "architecture": [],
            "training parameters": [],
            "learning rate": [
                "Gradient descent can be\nused to fit the translation model, i.e., \ud835\udee5\ud835\udc16= \u2212\ud835\udf02\ud835\udf15\ue238\n\ud835\udf15\ud835\udc16, where\ud835\udf02is the\nlearning rate and the notation \ud835\udc16is used to refer to the parameters of\nthe translation model, which is typically a deep neural network.",
                "The sound attribute\nestimator was trained for 30+20 training epochs using a learning rate\nof10\u22124\u221510\u22125.",
                "The visual attribute estimator was pretrained on the\nImagenet dataset and then fine-tuned (after replacing the last regression\nlayer) for 50+10 epochs using the same learning rates ( 10\u22124\u221510\u22125).",
                "The translation model\nwas trained for 200 epochs with a learning rate of 0.001."
            ],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "We also define\na divergence metric \ue230(\ud835\udc2d1,\ud835\udc2d2)for measuring the dissimilarity between\ntwo attribute vectors \ud835\udc2d1,\ud835\udc2d2\u2208R\ud835\udc41\ud835\udc4e.",
                "an audio segment \ud835\udc31,\ngenerate an appropriate image \ud835\udc32so as:\n\ud835\udc32= arg\ud835\udc32min\ue230(\ud835\udc53\ud835\udc4e(\ud835\udc31),\ud835\udc54(\ud835\udc32)), (1)\nfor an appropriately defined divergence metric \ue230(\u22c5).",
                "114059\n6N. Passalis and S. Doropoulos\nTable 1\nEvaluating the matching between the target sentiment and the generated\nsentiment using two different metrics (mean absolute error (MAE) and\nmean precision on the valence and arousal).",
                "Finally, employing\nmore advanced methods for aligning the attribute spaces of the audio\nattribute estimator and visual attribute estimator, e.g., Zhu, Zhuang,\nand Wang (2019 ), while at the same time employing diversity metrics,\nsuch as entropy, to increase the diversity of the generated content, is\nexpected to further improve the quality of the visual stories."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "For example, arc diagrams have been proposed to provide\nsimple visualizations of repetitions of musical phrases in songs ( Wat-\ntenberg , 2002 ), while other approaches attempted to visualize the\nstructure in music by capturing the chord progression ( Bergstrom,\nKarahalios, & Hart , 2007 ), concurrent tones ( Ciuha et al. , 2010 ), as\nwell as melodic and harmonic patterns ( Snydal & Hearst , 2005 )."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Genre Control Proposed\nMAE Precision MAE Precision\nBlues 0.787 50.38 0.581 73.33\nClassical 1.138 49.95 0.734 89.25\nCountry 0.707 49.54 0.561 68.17\nDisco 0.921 50.13 0.741 58.75\nHiphop 0.760 49.65 0.697 56.33\nJazz 0.722 49.30 0.529 78.63\nMetal 0.918 50.38 0.893 47.04\nPop 0.885 48.54 0.669 62.50\nReggae 0.738 50.25 0.524 67.92\nRock 0.752 50.75 0.652 59.12\nAverage 0.833 49.89 0.658 66.10\n4.2.",
                "Uehara, Misa, & Itoh, Takayuki Pop music visualization based on acoustic features and\nchord progression patterns applying dual scatterplots."
            ],
            "Demo availability": [],
            "dataset": [
                "Instead of directly training GANs for music visualization,\nwhich would be very challenging given the lack of appropriate datasets,\nwe propose employing an efficient cross-modal translation approach\nthat allows for translating the audio sentiment space into the visual\nsentiment space, through any pre-trained GAN model.",
                "These estimators can be trivially fit-\nted using annotated datasets for the corresponding domains.",
                "Please refer to Section 4 for more details on the\ndatasets and models used for training these estimators.",
                "It is worth noting that this choice was\nlargely dictated by the current availability of open datasets for training\nsentiment-related attribute extractors.",
                "The visual attribute estimator was pretrained on the\nImagenet dataset and then fine-tuned (after replacing the last regression\nlayer) for 50+10 epochs using the same learning rates ( 10\u22124\u221510\u22125).",
                "For the visual space we\nemployed the statistics of the corresponding training dataset, while\nfor the audio space we use song-level z-score normalization, except\nfrom the third song used for the conducted qualitative experiments,\nwhere using the statistics of the whole dataset led to a wider variety of\ngenerated concepts.",
                "Experimental results\nFirst, we provide quantitative results using the GTZAN dataset\n(Sturm, 2012), along with the results of the conducted user study.",
                "The GTZAN dataset, which contains music segments of 10\ndifferent music genres was used.",
                "The proposed\nmethod was applied on all data provided by the GTZAN dataset and the\nagreement between the audio sentiment and the sentiment expressed\nby the generated images was measured.",
                "Among the most important limitations is the lack of large-scale\nsentiment datasets that can be used for training the aforementionedsentiment extractors.",
                "Using EEG-based methods ( Gauba, Kumar, Roy,\nSingh, Dogra, & Raman , 2017 ) is a very promising future research\ndirections for easily collecting such large sentiment datasets with rel-\natively low effort.",
                "Exploring MIDI datasets.",
                "An analysis of the GTZAN music genre dataset."
            ]
        },
        {
            "title": "ComposeInStyle: Music composition with and without Style Transfer",
            "architecture": [
                "The GAN architecture of the style transfer step\nis built out of the GAN architecture of the second step.",
                "The compositions generated from each architecture\nis finally evaluated by the common pre-trained classifier of the first step.",
                "Using the latest state of the art architectures, a hybrid model is\ndeveloped which capture the entire objective of this paper in a nutshell.",
                "Since\nthere is no exactly relevant work on music composer style transfer,\ncomparison has been done by implementing the model architecture in\na contextually similar work on genre style transfer.",
                "The entire setup\nis same except the model architecture of the comparison paper.",
                "This\ngives an opportunity to compare and contrast the advantages of our\nproposed hybrid style transfer architecture as compared to another style\ntransfer model in a recent work by Brunner, Wang, Wattenhofer, and\nZhao (2018b ).",
                "The architecture has been\ndescribed later in the paper in details.",
                "Another\npaper (Zhu, Park, Isola, & Efros, 2017) introduces the architecture\nfor cycle GAN which is also a significant advancement in the field\nof generative networks.",
                "The models vary in\narchitecture and the authors also extend it to generate accompanying\ntracks for human composed music.",
                "Encoder/Decoder architecture has\nbeen used along with adversarial training to learn music represen-\ntations and skip connections have been used for pitch information.",
                "CNN in Siamese architecture have been used as the\nmodel which has been evaluated using KNN classifiers.",
                "Architecture of the hybrid model\nThe hybrid model in Fig. 2 visualizes the architectures of the\nresearch objectives in a nutshell.",
                "The style transfer GAN architecture focuses on keeping the\nFig.",
                "Architecture of using Vanilla GAN for generating compositions from noise in\nStep 2.\ncontent of composition by composer A and outputs the melody in the\nstyle of the preferred composer B of choice.",
                "Architecture of the vanilla GAN\nThe audio melodies thus generated are in MIDI format which is then\nconverted into raw audio format (wav).",
                "Description of GAN architectures \ud835\udc3a\ud835\udc34,\ud835\udc3a\ud835\udc35,\ud835\udc37\ud835\udc34and\ud835\udc37\ud835\udc35\nIn step 2, paired vanilla GAN has been trained to generate multi-\ntrack polyphonic music.",
                "In this step, composer specific melody isExpert Systems With Applications 191 (2022) 116195\n6S. Mukherjee and M. Mulimani\nFig. 4. High level generator architecture as used in the proposed hybrid model.",
                "5. High level discriminator architecture as used in the proposed hybrid model.\ngenerated from noise.",
                "The overall architecture of a single GAN consists\nmainly of 2 parts: Generator and Discriminator 4.",
                "The architecture of each of the\nindividual components is described in detail below.",
                "Individual Architecture of a single GAN:\n1.Generator: The generator which is used here for step 2 com-\nprises of the following high-level components:",
                "The main parts of the architecture are detailed below.",
                "(1)Generator: Generator works similar to the encoding\u2013decoding\narchitecture.",
                "Full style transfer architecture",
                "6. Low level generator architecture as used in the proposed hybrid model.",
                "Residual network architecture as used in the proposed hybrid model.",
                "8. Low level discriminator architecture as used in the proposed hybrid model.",
                "Complete style transfer model architecture.",
                "The configurations used for the training of the style transfer model\narchitecture are:\n1.",
                "One can note that since there is no recent works on music composer\nspecific composition generation and style transfer, comparison has\nbeen done strictly on the model architectures.",
                "Key differences between the 2 model architectures\nThe main differences between the proposed hybrid style transfer\nmodel and the comparison model are given in Table 8.",
                "Since here\ncomparison is done architecture wise (due to unavailability of work\non composer style transfer), Table 8 captures in detail the major\ndifferences between the 2 models in the next sections.",
                "The style transfer architecture has been trained with only 100\nsongs from each composer.",
                "Vol. 3807 , In Advanced signal processing algorithms, architectures, and\nimplementations (pp. 637\u2013645)."
            ],
            "training parameters": [],
            "learning rate": [
                "Other parameters such as learning rates and other training pa-\nrameters are also kept constant for both the cases."
            ],
            "batch size": [
                "Batch size: 4\n4.6."
            ],
            "ethical": [],
            "impact": [
                "With the recent boom of ML/AI, the impact has\nbeen so huge that the nature of jobs for human beings are about to\nchange.",
                "Music is such a creative gift that has endless impact on human\nbeings.",
                "This would help in explaining which feature had more impact in the\nprediction of composer classes for each basic model.",
                "From the figures, it is observed that Feature 8 which is one of the\nchromagram feature is having a commendable impact on the classifica-\ntion of all 3 composer classes for all the basic models.",
                "For the Decision\nTree model, feature 8 has highest impact for determining the composer\nclasses Liszt and Schubert (see Supplementary Figure S1).",
                "However,\nfeature 3 stands out of the rest in terms of the highest impact it has\non classifying the composer class Chopin.",
                "For KNN model on the other\nhand, feature 8 is seen to have the highest impact value for determining\nthe classes of composers Liszt and Chopin (see Supplementary Figure\nS2).",
                "Class Description\nClass 0 Liszt\nClass 1 Chopin\nClass 2 Schubert\n6 is seen to have the highest impact (see Supplementary Figure S2).",
                "Using Random Forest classifier, feature\n8 seems to have highest impact for the classification of composer\nclasses Liszt and Schubert as shown in Supplementary Figure S4.",
                "Classifiers SVM with linear and radial\nkernels show similar feature importance in terms of classifying com-\nposers Liszt and Schubert as shown in Supplementary Figures S5 and\nS6 with feature 8 as having the highest impact.",
                "The only difference\nis seen during classification of composer Chopin where feature 4 and\n6 seem to have the highest impact value using linear SVM and radial\nSVM respectively.",
                "4.7.2.1 Composer class wise drill down\nIn the composer level Shapley values representation, Supplementary\nFigures S7 to S24 shows the impact and importance of each feature\non the classification task for each composer.",
                "As it is seen, feature 8 is\nhaving consistently high impact in determining the composer classes.",
                "Feature 8 seem to consistently dominate the rest of the features in\nterms of impact value and importance as seen in Supplementary Figures\nS7, S8, S9, S10, S11 and S12.",
                "For composer Chopin however, Supplementary Figures S13 to S18\ndoes not show any consistently high impact feature.",
                "Feature 19 in\nSupplementary Figure S13 is seen to have the highest impact for\nDecision Tree model.",
                "On the other hand, features 4, 1 and 16 is seen to\nhave the highest impact on the model outputs for KNN, Gaussian Naive\nBayes and Random Forest respectively as in Supplementary Figures\nS14, S15 and S16.",
                "Feature 4 seem to have the highest impact on the\nmodel output using SVM with linear and radial kernels as shown in\nSupplementary Figures S17 and S18.",
                "For composer Schubert, Supplementary Figures S19 to S24 shows\nfeature 6 as the consistent high impact feature for all the models except\nDecision Tree as seen in S20, S21, S22, S23 and S24.",
                "Although the scenario remains the same\nin case of ensemble models as well, the proposed hybrid model is seen\nperforming better than the comparison model, however the ensemble\ntechnique seems not to have a significant impact on the classifier\nperformances.",
                "Although tremendous research efforts are being put towards improv-\ning the impact of machines in various scientific fields, relatively less\nresearch has been done towards bringing the impact of machines in the\ncreative fields like music."
            ],
            "society": [
                "In\nInternational society for music information retrieval conference (ISMIR) (pp. 747\u2013754).",
                "In International society for music information\nretrieval conference .",
                "In International society for music\ninformation retrieval conference (ISMIR) (pp. 190\u2013196).",
                "IEEE Computer Society.",
                "IEEE Computer Society.",
                "In International society for music information retrieval conference (ISMIR)\n(pp. 524\u2013530).",
                "In International society for music information retrieval\nconference late breaking and demo papers (pp. 84\u201393).",
                "International Society for Optics and Photonics.",
                "In International society\nfor music information retrieval conference (ISMIR) (pp. 324\u2013331).",
                "IEEE Computer Society."
            ],
            "copyright": [],
            "evaluation metrics": [
                "Human evaluation along with instrument activity detection has been\nused as the evaluation metrics for this step."
            ],
            "metric": [
                "In\nthe field of classification also, some of the recent works like Zhou, Li,\nand Mitri (2016) compares and contrasts the effectiveness of various\nclassification models using classification rate and Cohen\u2019s kappa as the\nmetrices.",
                "Human evaluation along with instrument activity detection has been\nused as the evaluation metrics for this step.",
                "Human\nevaluation is used as the only metric for evaluation.",
                "Accuracy is chosen as the metric for evaluation in this case.",
                "(2)Discriminator: The discriminator consists of 5 Conv layers with\nvarious types of activations as shown in Fig. 8 like leaky Rec-\ntiLinear Unit (ReLU) and Parametric RectiLinear Unit (PReLU).",
                "Metric Proposed Hybrid Style Transfer Model Comparison model\nGenerators Paired Vanilla GAN (2 GANs) and\nCycle GAN (2 GANs)Cycle GAN (only 2 GANs)\nDiscriminators Discriminator for composer A,\nDiscriminator for composer BDiscriminator for composer A,\nDiscriminator for composer B,\nDiscriminator for composer A\n(mixed) and\nDiscriminator for composer B\n(mixed)",
                "A multimodal approach to song-level style identification in\npop/rock using similarity metrics."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "In particular, the melodic aspect (T3) of the style transferred mu-\nsic clips rated highly positively.",
                "Technical quality T1: How successful is the composition after style transfer from A to\nB?\nT2: Do the harmonic elements of the style transferred version B\nshow technical correctness concerning the style of composer B?\nT3: Do the melodic elements of the style transferred version B show\ntechnical correctness concerning the style of composer B?\nExpressiveness E1:"
            ],
            "harmonic complexity": [],
            "expressiveness": [
                "In this work, CAT\nevaluates the quality of music in terms of creativity, technical quality,\nand expressiveness using domain experts.",
                "Technical quality T1: How successful is the composition after style transfer from A to\nB?\nT2: Do the harmonic elements of the style transferred version B\nshow technical correctness concerning the style of composer B?\nT3: Do the melodic elements of the style transferred version B show\ntechnical correctness concerning the style of composer B?\nExpressiveness E1:",
                "E1 and E2: Questions correspond to expressiveness criteria."
            ],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "Electronics ,9(3),\n424."
            ],
            "pop": [
                "The machine learning models which focus on clas-\nsifying music based on genre like classical, jazz, pop, rock and others,\ndo not focus on the composer specific styles.",
                "The same composer\ncan compose music in all styles like jazz, pop, blues, etc. dependingon the requirement.",
                "It shows that it works better\nthan Hard Thresholding (HT) and Bernoulli Sampling (BS) methods\npopularly used.",
                "Nakamura, Shibata, Nishikimi, and Yoshii (2019)\nperforms genre based style conversion (say classical to pop) using\nunsupervised models.",
                "The authors\nconvert MIDI music from one genre to another (say jazz to pop) which\nis evaluated using neural style classifiers.",
                "Wang and Tzanetakis (2018)\nperforms a singing style investigation on pop music using variants of\nneural network.",
                "Popular 4/4 time signature signifies 1 bar, contains 4 beats and\neach beat can contain 1/4 note, 2/8 note or 4/16 note (Brunner et al.,\n2018b).",
                "Audio thumbnailing of popular music using\nchroma-based representations.",
                "A multimodal approach to song-level style identification in\npop/rock using similarity metrics."
            ],
            "Demo availability": [],
            "dataset": [
                "The paper focuses on 3 composer maestros Liszt, Chopin and Schubert taken from\nthe Maestro dataset.",
                "The highest accuracy obtained is\n80% for composer classification using the maestro dataset, 77.27% for the classification of the generated style\ntransferred version of a composition into the target composer class using the pre-trained classifiers.",
                "116195\n3S. Mukherjee and M. Mulimani\nNow the dataset consisting of 100 songs of each of the 3 composers,\nare classified using various well-known ML models like:\n\u2022Basic Models: Decision Tree, K-Nearest Neighbors (KNN), Gaus-\nsian Naive Bayes (GNB), Support Vector Machine (SVM) using\nboth linear and radial kernels, Random Forest.",
                "The GAN is\ntrained by divide and conquer by dividing the dataset with overlap and\nthen concatenating the generated output again with overlap in order\nto minimize information loss.",
                "Dataset\nThe dataset used here consists of 3 composers named Liszt, Chopin\nand Schubert from the Maestro dataset ( Hawthorne et al. , 2019 ).",
                "The\nMIDI files from Maestro v2 dataset is taken, converted to wav format,\npreprocessed and features are extracted from them.",
                "Step 1: Audio data in the MIDI format is taken from the Maestro\ndataset ( Hawthorne et al. , 2019 ) and features are extracted from them.",
                "The dataset: The dataset of 3 composers (Liszt, Chopin and\nSchubert) taken from the Maestro dataset (Hawthorne et al.,\n2019) are used to train both the networks.",
                "(2018b) performs on the dataset after\nevaluation using the classifiers trained in step 1.\n4.7.4.",
                "The dataset used here is the Maestro dataset (Hawthorne\net al., 2019).",
                "Enabling factorized piano music modeling and generation with the\nMAESTRO dataset."
            ]
        },
        {
            "title": "The algorithmic composition for music copyright protection under deep learning and blockchain",
            "architecture": [
                "Theyalsoproposedagenerativearchitecture\nthatcombinedthesemodelswiththepredictionsofacousticclas-\nsifiers."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [
                "[31] S.T\u00f6nnissen,F.Teuteberg,Analysingtheimpactofblockchain-technology\nfor operations and supply chain management: An explanatory model\ndrawn from multiple case studies, Int."
            ],
            "society": [
                "Withthedevelopmentofthesociety,music\nhasbeendevelopinginaccordancewiththelaw,especiallyin\nthecontinuousenrichmentandapplicationofmusic\u2019smelody,\nrhythm,harmony,polyphony,structure,form,andmusic."
            ],
            "copyright": [
                "AppliedSoftComputing112(2021)107763\nContents lists available at ScienceDirect\nAppliedSoftComputing\njournal homepage: www.elsevier.com/locate/asoc\nThealgorithmiccompositionformusiccopyrightprotectionunder\ndeeplearningandblockchain\nNanaWanga,HuiXub,FengXuc,\u2217,LeiChengd\naDepartment of Physics, Harbin University, Harbin, 150080, China\nbCollege of Education, University of Perpetual Help System DALTA, Manila, Philippines\ncAcademy of Music, Chuzhou University, Chuzhou, 239000, China\ndConservatory of Music, Moscow State Normal University, Moscow 101-135, Russia\na r t",
                "i n f o\nArticle history:\nReceived8March2021\nReceivedinrevisedform23July2021\nAccepted24July2021\nAvailableonline12August2021\nKeywords:\nAlgorithmiccomposition\nBlockchain\nCopyrightprotection\nDeeplearning\nNeuralnetworka b s t",
                "r a c t\nTo strengthen music copyright protection effectively, a new deep learning neural network music\ncompositionneuralnetwork(MCNN)isproposed.",
                "Then,thedigital\nmusic copyright protection system based on blockchain is constructed from three perspectives of\nconfirmingright,usingright,andprotectingright.",
                "It is proved that the music\ncopyright protection model based on block chain can ensure that the copyright owners of works\nobtaincorrespondingeconomicbenefitsfromvariousdistributionchannels,whichishelpfultobuild\naharmoniousmusicmarketenvironment.",
                "Inshort,theinnovationofthisstudyisreflectedinthatit\nfillsinthegapofdetailedcomparativestudyofthedifferencesintheapplicationofdifferentmodels,\nrealizestheframeworkofmusiccopyrightprotectionsystem,andprovidesconvenientconditionsfor\ncomposers.",
                "However,italsohighlights\nsomeproblemsinthefieldofdigitalcopyrightmanagementand\nprotection[7].Thecurrentdigitalmusiccopyrightmanagement\nsystemismainlycentralized.",
                "Therefore,digitalcopyrightdisputesfrequentlyoccur,and\ninfringementshaveincreasinglybecomeasharppainpointfor\ntheInternetculturalindustry[8].",
                "Many experts and scholars have analyzed the research in\nthefieldofmachine-assistedcreationandcopyrightprotection.",
                "Theprotectionofmusiccopyrightisrealizedbyencoding\n\u2018\u2018smart\u2019\u2019contractstolicensetheuseofworks[12].Grfdanand\nErsoy(2021)proposedamusicwalletmodelbasedonblockzincir\nforsafelyandlegallylisteningtoaudiofiles.",
                "There\nare many strategies for music copyright protection, but there\nisnosystemframeworkthatisimplemented,whichmakesit\ndifficult for these technologies to be applied in practice.",
                "I.Intheintroduction,thepractical\nsignificanceandvalueofthisresearcharenoted,andthescientific\nissuesofmusiccreationandmusiccopyrightareclarified.",
                "II.In\ntheliteraturereview,currentresearchoncomputeralgorithmic\ncompositionandmusiccopyrightprotectionissummarized.",
                "V.\nIntheresults,theconstructedmodelanddigitalcopyrightsystem\naretested,furtherverifyingtheeffectivenessofthemodeland\nsystemproposedinthisarticle.",
                "IV.Acompletemusiccopyrightprotection\nsystemisbuiltusingblockchain.",
                "Music copyright\nThere are many studies on the adoption of blockchain in\nthe management of digital music, but most of them were at\ntheinitialalgorithmprogrammingstage[21].Jianetal.(2016)\npointedoutthedilemmafacedbydigitalcopyrightprotection\ninthecurrentInternetera,analyzedtheblockchainitselfand\nitsmechanism,andpointedoutthecurrentdevelopmentstatus\nofblockchain.",
                "Theworkalsodiscussedtheadvantagesandex-\nistingproblemsofapplyingblockchaintothedigitalcopyright\nprotectionsystem[22].Qureshietal.(2020)pointedoutthatthe\ntraditionaldigitalcopyrightsystemadoptedacentralizedstorage\nmethod,andcarefullyanalyzedtheshortcomingsanddrawbacks\nofChina\u2019straditionaldigitalcopyrightmanagementmethods.",
                "In\nresponsetotheaboveproblems,aconceptualschemeforap-\nplyingblockchaintocopyrightprotectionisputforward,anda\nfeasibilityanalysisoftheschemeismade[23].Caietal.(2018)\npointedoutthechallengesfacedbythecurrentdigitalcopyright\nprotectionmechanism.",
                "Theoriginandessenceofblockchainand\nthe overall data structure of blockchain were introduced, the\nissuesinadoptionofblockchainindigitalcopyrightprotection\nwereintroduced,andabriefoverviewofadoptionexampleswas\ngiven[24].FengandWei(2017)pointedoutthatdigitalcopyright\nwasnowfacingnumerousdilemmas\u2014thepublic\u2019sawarenessof\ncopyrightprotectionwasweak,infringementproblemsoccurred\nrepeatedly,andthefoundationofbasicvaluewasalsoweak.",
                "Inaddition,theyalso\nsummeduptherisksofapplyingblockchain-technicalproblems,\nmarket-orientedoperationproblems,andcompatibilityproblems\nofcopyrighttheory[25].",
                "Thisagreementwassuitableforthecopyrightprotectionofdig-\nitalimages,includingtheidentificationofthecopyrightofnew\nworksandtheencryptionsignatureofthecopyrightownerof\nthework.",
                "Theanalysiscontentofmostforeigncopyright-relatedbooksis\nrelativelycomprehensive,includingallcopyrighttypesinvolved\nintheproduction,circulation,andfinalconsumptionofmusic,\nbutsomewillfocusonacertaintypeofmusiccopyright.\n2.3.",
                "Manyproblemsin\nthefieldofdigitalmusiccopyrightprotectionhavebecomein-\ncreasinglyprominent.",
                "Ontheone\nhand,itisurgenttosolvetheproblemsofdigitalmusiccopyright.",
                "Ontheotherhand,theapplicationofblockchaininthefieldof\ndigitalmusiccopyrighthasincreasinglyattractedtheattention\nofthemusicindustryandcopyrightindustry.",
                "Therefore,howto\nusedeeplearningtobuildanappropriatecompositionsystem\nandblockchaintorealizetheprotectionofmusiccopyrightis\nofimportantreferencevaluetopromotethedevelopmentofthe\nmusicindustry.",
                "On the other hand,\nthepublicintheinvisibleeasytocausealotofunintentional\ninfringementofdigitalmusiccopyrightevents.",
                "OntheInternet,it\nisthefrequentoccurrenceofinfringementthatmakescopyright\nremedyparticularlydifficult.",
                "Accordingto\nthedefinition,blockchainisdifficulttotamperwith,decentral-\nized, safe, and efficient, and the application of the blockchain\ntotheprotectionofdigitalmusiccopyrightplaysanimportant\nrole.",
                "Therefore,ontheonehand,itisnecessarytoreducethecostof\ncopyrighttransactionandreducethepossibilitythattherelative\npartyofthetransactionwilltransferthecosttothemusiccreator\ntodamagehisinterests.",
                "Ontheotherhand,itisnecessaryto\nenhancethediscoursepowerofmusiccreatorsinthecopyright\ntransactioninordertoobtainthebenefitsconsistentwiththeir\npsychologicalexpectations.",
                "Construction of copyright protection system\n4.2.1.",
                "Throughblockchain,thepersonalinformation,timeinforma-\ntion,digitalmusiccontent,andotherinformationofthecopyright\nowneroftheworkarestoredineachblock.",
                "Then,\nthrough the smart contract, the copyright owner of the work\ncanreceivetheuser\u2019sautomaticpaymentthroughthesystem\nFig. 11.Copyrightprotectionsystem-rightconfirmationstage.\ninthefirsttime.",
                "Right using stage\nWhenthetraditionaldigitalmusiccopyrightisused,thevar-\niousservicesareroughlythesame,anditisnecessarytofully\nexplorethepathofrightusinginChina\u2019sdigitalmusiccopyright\nmanagementapplicationsystem.",
                "Fromforeigncompanymanage-\nmentmodels,manywaysandpathsof\u2018\u2018blockchain+copyright\u2019\u2019\narefoundintherightusingsystem,whichisadoptedinadvertis-\ningandmusicindustrychainIPincubation,aswellasothertypes\nofprojects,tofullyexpandthesinglerightapplicationsystem.",
                "Theconceptionofapply-\ningblockchaintotherightusingstageofChina\u2019sdigitalmusic\ncopyrightmanagementapplicationsystemisimplemented.",
                "Right protection stage\nMusicworksinformationisenteredintotheblockchainfor\nstorageafteritisgenerated,andcopyrightinformationisjointly\nsupervisedbyeachnode.",
                "Therefore,theadoptionofblockchainin\nthemanagementofmusiccopyrightprotectioninChinacanim-\nprovethecurrentmusiccopyrightprotectionsystem.",
                "12.Copyrightprotectionsystem-right-usestage.",
                "13.Copyrightprotectionsystem-rightsprotectionstage.",
                "Theblockchain-based\ndigital music copyright protection system proposed can accu-\nratelycompleterequestsunderconcurrentoperationsofmultiple\nuserswhileensuringahighprocessingspeed.\n5.5.",
                "Comparative analysis of copyright protection performance\nFig.24showsthecomparisonresultsoftheproposedalgo-\nrithmwithotherblockchainplatforms.",
                "Therefore,thealgorithmhashigherperformance,which\ncan avoid the low throughput state to a certain extent, and\nismoresuitableforapplicationindigitalcopyrightprotection\nsystems.",
                "Applied Soft Computing 112 (2021) 107763\nFig. 23.Performanceverificationofcopyrightprotectionsystem.",
                "Fig. 24.Comparativeanalysisofcopyrightprotectionperformance.\n5.6.",
                "Comparison results of performance of different algorithms\nInFig.26,theresultsofthisalgorithmarecomparedwith\nVAE\u2013GAN,GAN,andRL-RNN.Fromtheexperimentalresults,the\nmusic samples generated by MCNN network have the highestTable 3\nMusiccopyrightsecurityanalysisresults.",
                "Security analysis of copyright protection\nIn Table 2, the model proposed is analyzed through music\nsafetyperformance.",
                "The\nmaximummusicprotectionrateoftheproposedmodelisashigh\nas82.33%.Therefore,theproposedmodelhashighefficiencyof\nmusiccopyrightprotection(seeTable3).",
                "Basedontheactual\noperatingstatusofdigitalmusiccopyrightmanagementinChina,\naprimaryapplicationsystemthatappliesblockchaintodigital\nmusiccopyrightmanagementinChinaisconceived.",
                "Theactual\nutilityofblockchainfordigitalmusiccopyrightmanagementin\nChinaisprovedtobeconducivetothedevelopmentofdigital\nmusiccopyrightmanagementinChina.",
                "Althoughthisworkhas\nconstructed a composition model and copyright system, there\narestillmanyshortcomings.",
                "Secondly,therearefewcompaniesinChinathat\nactuallyapplyblockchaintodigitalmusiccopyrightmanagement,\nand the application system concept is still in the ideal stage.",
                "[8] M.Finck,V.Moscon,Copyrightlawonblockchains:Betweennewforms\nof rights administration and digital rights management 2.0, IIC-Int. Rev.\nIntell.",
                "[12] B. Bod\u00f3, D. Gervais, J.P. Quintais, Blockchain and smart contracts: the\nmissing link in copyright licensing?",
                "[13] Grfdan. R., M. Ersoy, Blockchain-based music wallet for copyright\nprotectioninaudiofiles,J.Comput.",
                "[22] W.Jian,G.Li,Z.Jingning,Digitalcopyrightprotectionbasedonblockchain\ntechnology,RadioTel.Inform.7(2016)60\u201362.",
                "Wang,Theapplicationofblockchaintechnology\nindigitalcopyright,DEStechTrans.",
                "[25] Z. Feng, Z. Wei, Analysis of digital copyright protection based on block\nchaintechnology,Sci.Technol.Law1(2017)10\u201319.",
                "[26] A.Tresise,J.Goldenfein,D.Hunter,Whatblockchaincanandcan\u2019tdofor\ncopyright,2018,pp.362\u2013373."
            ],
            "evaluation metrics": [],
            "metric": [
                "Amongwhich,thedatalayeristhecoread-\nvantageofblockchain,whichencapsulatestheblockchainstruc-\nture in the blockchain system and provides functions such as\nasymmetric encryption and time stamping for data."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [
                "Themelodyof\nmusicisadoptedtomeasurethefluencyandnaturalnessofthe\nmusic,harmonycanmeasurethecomfortofthemusic,thesense\nof the music can express the integrity of the music, and the\nintensitycanhighlighttheexpressivenessofthemusic."
            ],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "Introduction\nWiththeprogressanddevelopmentofscienceandtechnology\nandthewidespreadapplicationofelectronicequipment,digital\nimagehasbecomeanindispensableinformationmediuminsocial\nproductionandpeople\u2019slife."
            ],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "Algorithm performance test\nToverifythesuperiorityoftheMCNNalgorithm,experiments\nareconductedbasedontheCPMGdataset,andtheresultsare\ncompared with three music generation algorithms: VAE\u2013GAN,\nSeqGAN,andRL-RNN.VAE\u2013GANaddstheencodinganddecoding\nprocesstotheGANnetwork."
            ]
        },
        {
            "title": "Towards a Deep Improviser: a prototype deep learning post-tonal free music generator",
            "architecture": [],
            "training parameters": [],
            "learning rate": [
                "We optimised models on the AlgorithmicCorpus and then solely tuned and \ufb01ne-tuned (varying the\nlearning rate) these for the Improvised Corpus, since these\nresults were adequate for our purpose, and we were notdetermined as yet on utterly optimising all models.",
                "We used robust scaling [to address the\nasymmetric distributions of temporal values (shown later inFig. 3)] and the very different ranges of the individual\nparameters, and then, we undertook some hyperparametertuning (notably to trim down to the minimal size nets\neffective for our corpora), together with limited parameter\n\ufb01ne-tuning using sequenced learning rates."
            ],
            "batch size": [],
            "ethical": [
                "Compliance with ethical standards\nConflict of interest The authors declare they have no conflict of\ninterest."
            ],
            "impact": [],
            "society": [],
            "copyright": [
                "The\n\ufb01rst item was an extract of composer Morton Feldman\u2019s\nPiano Four Hands (1957\u20131958, from the Etcetera\nKTC2015 CD performed by Roger Woodward), the second\nan extract of a Deep Improviser product and the third taken\nfrom an algorithmic piece included in the corpus it hadlearned (the latter two extracts are provided as supple-\nmentary material, while the \ufb01rst cannot be published here\nfor reasons of copyright, but is available)."
            ],
            "evaluation metrics": [],
            "metric": [
                "Natural Computing Applications Forum 2018\nAbstract\nTwo modest-sized symbolic corpora of post-tonal and post-metrical keyboard music have been constructed: one algo-\nrithmic and the other improvised.",
                "Keywords Deep learning /C1Music /C1Post-tonal /C1Post-metrical /C1Improvisation\n1 Introduction\nCould a deep learning model of music function as a free\nimprovising partner?",
                "[ 1,2]\nand often post-metrical (that is, mostly it lacks hierarchical\nrepetitive rhythmic structures)",
                "By the same\ntoken, a free improviser may choose, for example, to adopt\na tonal or metrical posture, or engage in blues-oriented\nphrases, at any time.",
                "Thus, free improvisation is not con-\ngruent with post-tonal and/or post-metrical music, but at\nthe least includes them.",
                "[ 27] produces music closely akin to Irish Folk\nmusic, with clear tonal and metrical features very much in\ncommon with it.",
                "To accommodate post-metrical features, our modelalso needs to permit continuous variation in the event\nduration (be it chord or note) and inter-onset interval (ioi)\nbetween events (time of event onset does not so clearlyrepresent this feature, but rather requires differencing to\ngenerate it).",
                "In sum, our system should be able to accommodatetonality or continuous pitches, and metricality or a time\ncontinuum, together with the conceivable intermediates.",
                "3 Creating multi-hand keyboard corpora\nWe constructed two keyboard corpora for the training ofour initial models, since we can \ufb01nd no prior symboliccorpus with the objectives and features we required (post-\ntonal and post-metrical).",
                "We used robust scaling [to address the\nasymmetric distributions of temporal values (shown later inFig. 3)] and the very different ranges of the individual\nparameters, and then, we undertook some hyperparametertuning (notably to trim down to the minimal size nets\neffective for our corpora), together with limited parameter\n\ufb01ne-tuning using sequenced learning rates.",
                "[ 37] are useful\nin adopting a similar approach to tonal and metrical music,\nthey are not applicable here because speci\ufb01c chord voic-\nings rarely recur (though individual notes of course do),and the whole vector of p1\u201310, v,d, ioi essentially never\nrecurs exactly, partly because of the continuous parametersNeural Computing and Applications (2020)",
                "While we intend to grapple fully with this in due\ncourse, for the time being we allowed ourselves a prelim-inary informal test: 21 researchers listened to three\nunidenti\ufb01ed items of post-tonal and post-metrical keyboard\nmusic in a group setting.",
                "We will consider transforma-tions based on cumulative density functions, identi\ufb01cation\nof repetitions and geometrical structures within perceptu-\nally grounded representations as possible means of reduc-ing the complexity of those data [ 44,45], particularly for\nfurther analyses of outputs.",
                "Enders W (2004) Applied econometric time series, 2nd edn.\nWiley, Hoboken\n35."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "Several of these algorithmic pieces were multi-strand in nature, that is, they have multiple simultaneous\nmelodic strands as in chamber and orchestral music, as\nFig. 1 Musical representation in the form of a single input matrix.",
                "Linking melodic expectation to expressiveperformance timing and perceived musical tension."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "Dean RT, Bailes F (2016) Relationships between generated\nmusical structure, performers\u2019 physiological arousal and listener\nperceptions in solo piano improvisation."
            ],
            "choral": [],
            "orchestral": [
                "Several of these algorithmic pieces were multi-strand in nature, that is, they have multiple simultaneous\nmelodic strands as in chamber and orchestral music, as\nFig. 1 Musical representation in the form of a single input matrix."
            ],
            "electronic": [
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material).",
                "Essentially, it is the nonlinear acti-\nvation components within neural nets, together with a\npossibly large number of input features, and the potentialElectronic supplementary material The online version of this\narticle ( https://doi.org/10.1007/s00521-018-3765-x ) contains\nsupplementary material, which is available to authorized\nusers.\n&Roger T. Dean\nroger.dean@westernsydney.edu.au\n1MARCS Institute for Brain, Behaviour and Development,\nWestern Sydney University, Sydney, Australia\n2austraLYSIS, Sydney, Australia\n3Department of Computing, Goldsmiths, University of\nLondon, London, UK\n123Neural Computing and Applications (2020)",
                "An audio excerpt of\none piece from the Algorithmic Corpus and another fromthe Seed Piece (to provide an example of improvised\nkeyboard playing) are provided within Electronic Supple-\nmentary Material, realised using the Pianoteq physicalsynthesis piano.",
                "Manning Electronic\nAdvanced Publication, Shelter Island\n21."
            ],
            "pop": [
                "In other words, it is\noften very different from common practice Western music,\nor pop and rock (as illustrated in Online Resource 3\namongst Electronic Supplementary Material)."
            ],
            "Demo availability": [],
            "dataset": []
        },
        {
            "title": "From artificial neural networks to deep learning for music generation: history, concepts and trends",
            "architecture": [
                "Since a few years, there are a large number of\nscienti\ufb01c papers about deep learning architectures andexperiments to generate music, as witnessed in [ 3].",
                "In\n[18], Graves analyzes the application of recurrent neural\nnetworks architectures to generate sequences (text and\nmusic).",
                "Then, we analyze successively: basic types of\narchitectures and strategies in Sect. 7, various ways to\nconstruct compound architectures in Sect.",
                "8and some\nmore re\ufb01ned architectures and strategies in Sect. 9.",
                "An example is the experiment\nconducted by the YACHT dance music band with theMusicVAE architecture\n8from the Google Magenta Project\n[39].",
                "The\nunderlying architecture, named Coconet [ 27], has been\ntrained on a dataset of 306 Bach chorales.",
                "9.5, but, in this section, we will at \ufb01rst consider a\nmore straightforward architecture, named MiniBach\n9[3,\nSection 6.2.2].",
                "The architecture, shown in Fig. 2, is feedforward (the\nmost basic type of arti\ufb01cial neural network architecture) for\na multiple classi\ufb01cation task: to \ufb01nd out the most likely\nnote for each time slice of the three counterpoint melodies.",
                "4.1 Todd\u2019s Time-Windowed and conditioned\nrecurrent architectures\nThe experiments by Todd in [ 61] were one of the very \ufb01rst\nattempts at exploring how to use arti\ufb01cial neural networks\nto generate music.",
                "Although the architectures he proposed\nare not directly used nowadays, his experiments and dis-cussion were pioneering and are still an important source of\ninformation.",
                "He named his \ufb01rst design the Time-\nWindowed architecture, shown in Fig. 4, where a sliding\nwindow of successive time periods of \ufb01xed size is con-sidered (in practice, one measure long).",
                "2 MiniBach architecture\nand encoding\n14In that respect, the Time-Windowed model is analog to an order 1\nMarkov model (considering only the previous state) at the level of a\nmelody measure.",
                "7.2.1 ), in which the output\nis explicitly reentered (recursively) into the input of the\narchitecture, in Todd\u2019s Sequential architecture the reen-trance is implicit because of the speci\ufb01c nature of the\nrecurrent connexions: the output is reentered into the\ncontext units while the input\u2014the plan melody\u2014isconstant.",
                "6.\n4.1.1 Influence\nTodd\u2019s Sequential architecture is one of the \ufb01rst examples\nof using a recurrent architecture and an iterative strategy\n20\nfor music generation.",
                "4 Time-Windowed architecture.",
                "5 Sequential architecture.",
                "Adapted from [ 61]\n17This is a peculiar characteristic of this architecture, as in recent\nstandard recurrent network architecture recurrent connexions are\nencapsulated within the hidden layer (as shown in Sect. 7.2).",
                "19Actually, as an optimization, Todd proposes in the following of his\ndescription to pass back the target (training) values and not the outputvalues.20These and other types of architectures and generation strategies are\nmore systematically analyzed in Sects.",
                "5and7.Neural Computing and Applications (2021) 33:39\u201365 43\n123extra input, named plan, which represents a melody that the\nnetwork has learnt, could be seen as a precursor of con-\nditioning architectures, where a speci\ufb01c additional input is\nused to condition (parametrize) the training of the\narchitecture.21",
                "\u2019\u2019\nThus, these early designs may be seen as precursors of\nsome recent proposals:\n\u2013 Hierarchical architectures, such as MusicVAE [ 53]\n(shown in Fig.",
                "7and described in Sect. 9.3); and\n\u2013 Architectures with multiple time/clocks, such as Clock-\nwork RNN [ 32] (shown in Fig. 8) and SampleRNN",
                "his described initial experiment [ 35], the architecture\nis a conventional feedforward neural network architecture\nused for binary classi\ufb01cation, to classify \u2018\u2018well-formed\u2019\u2019\nmelodies.",
                "Adapted from\n[61]\n21An example is to condition the generation of a melody on a chord\nprogression, in the MidiNet architecture [ 67] to be described in\nSect.",
                "\u2019\u2019\nThe idea of an attention mechanism , although not yet\nvery developed, may be seen as a precursor of attentionmechanisms in deep learning architectures: at \ufb01rst as an\nadditional mechanism to focus on elements of an input\nsequence during the training phase [ 15, Section 12.4.5.1],\nnotably for translation applications, until being proposed as\nthefundamental and unique mechanism (as a full alterna-\ntive to recurrence or convolution) in the Transformerarchitecture [ 64], with its application to music generation,\nnamed MusicTransformer [ 28].\nFig. 7 MusicVAE architecture.",
                "Reproduced from [ 53] with\npermission of the authors\nFig. 8 Clockwork RNN\narchitecture.",
                "But, as dis-\ncussed in Sect. 9, novel types of architectures have also\nbeen proposed.",
                "Before that, we will introduce a conceptual\nframework in order to help at organizing, analyzing and\nclassifying various types of architectures, as well as varioususages of arti\ufb01cial neural networks for music generation.",
                "\u2013Architecture the nature of the assemblage of processing\nunits (the arti\ufb01cial neurons) and their connexions.",
                "\u2013Strategy : the way the architecture will process repre-\nsentations in order to generate\n24the objective while\nmatching desired requirements.",
                "10 Creation by\nre\ufb01nement\u2014Architecture andstrategy\n23Systems refers to various proposals (architectures, systems and\nexperiments) about deep learning-based music generation surveyedfrom the literature.24It is important to highlight that, in this conceptual framework, by\nstrategy we only consider the generation strategy , i.e., the strategy to\ngenerate musical content.",
                "A strategy for training an architecture couldbe quite different and is out of direct concern in this classi\ufb01cation.46 Neural Computing and Applications (2021) 33:39\u201365\n123Note that these \ufb01ve dimensions are not com-\npletely orthogonal (unrelated).",
                "Select a type(s) of architecture andcon\ufb01gurate it;\n4.Train thearchitecture with the examples ;\n5.",
                "6 Representation\nThe choice of representation and its encoding is tightly\nconnected to the con\ufb01guration of the input and the output\nof the architecture, i.e., the number of input and output\nnodes (variables).",
                "Although a deep learning architecturecan automatically extract signi\ufb01cant features from the data,\nthe choice of representation may be signi\ufb01cant for the\naccuracy of the learning and for the quality of the gener-\nated content.",
                "6.1 Phases and types of data\nBefore getting into the choices of representation to beprocessed by a deep learning architecture, it is important to\nidentify the main types of data to be considered, dependingon the phase (training or generation):\n\u2013Training data , the examples used as input for the\ntraining;\n\u2013Generation (input) data , used as input for the gener-\nation (e.g., a melody for which an accompaniment willbe generated, as in the \ufb01rst example in Sect. 3); and\n\u2013Generated (output) data , produced by the generation\n(e.g., the accompaniment generated), as speci\ufb01ed by theobjective.",
                "Architectures that process the raw signal are sometimes\nnamed end-to-end architectures.",
                "The WaveNet\narchitecture [ 47], used for speech generation for the Google\nassistants, was the \ufb01rst to prove the feasibility of sucharchitectures.",
                "26Indeed, at the level of processing by a deep network architecture,\nthe initial distinction between audio and symbolic representation boils\ndown, as only numerical values and operations are considered.",
                "The encoding of a representation (of a musical content)\nconsists in the mapping of the representation (composed of\na set of variables , e.g., pitch or dynamics) into a set of\ninputs (also named input nodes orinput variables ) for the\nneural network architecture.",
                "The advantage of one-hot\nencoding is its robustness against numerical operations\napproximations (discrete versus analog), at the cost of ahigh cardinality and therefore a potentially large number of\nnodes for the architecture.",
                "33The \ufb01gure also illustrates that a piano roll could be straightfor-\nwardly encoded as a sequence of one-hot vectors to construct theinput representation of an architecture, as, e.g., shown in Fig.",
                "2.48 Neural Computing and Applications (2021) 33:39\u201365\n1237 Main basic architectures and strategies\nFor reasons of space limitation, we will now jointly introduce\narchitectures and strategies.34For an alternative analysis\nguided by requirements (challenges), please see [ 4].\n7.1 Feedforward architecture\nThe feedforward architecture35is the most basic and\ncommon type of arti\ufb01cial neural network architecture.",
                "In Todd\u2019s Time-Windowed architecture in Sect.",
                "7.2 Recurrent architecture\nArecurrent neural network (RNN) is a feedforward neural\nnetwork extended with recurrent connexions in order to\nlearn series of items (e.g., a melody as a sequence of notes).",
                "Todd\u2019s Sequential architecture in Sect.",
                "As pointed out in\nSect. 4.1, in modern recurrent architectures, recurrent\nconnexions are encapsulated within the hidden layer, whichallows an arbitrary number of recurrent layers (as shown in\nFig. 13).",
                "7.2.1 Recursive strategy\nThe \ufb01rst music generation experiment using current state of\nthe art of recurrent architectures, the LSTM (Long Short-\nTerm Memory [ 25]) architecture, is the generation of blues\nchord (and melody) sequences by Eck and Schmidhuber in[8].",
                "Another interesting example is the architecture by\nSturm et al. to generate Celtic melodies [ 59].",
                "36The feedforward architecture and the feedforward strategy are\nnaturally associated, although, as we will see in some of the nextsections, other associations are possible.",
                "By sampling a pitch followingthe distribution generated recursively by the architecture,\n41\nwe introduce stochasticity in the process of generation andthus content variability in the generation.\n8 Compound architectures\nFor more sophisticated objectives and requirements, com-\npound architectures may be used.",
                "We will see that, from an\narchitectural point of view, various types of combination42\nmay be used:\n8.1 Composition\nSeveral architectures, of the same type or of different types,\nare composed, e.g.:\n\u2013 A bidirectional RNN, composing two RNNs, forward\nand backward in time, e.g., as used in the C-RNN-GAN\n[44] (see Fig. 16) and the MusicVAE [ 53] (see Fig. 7\nand Sect. 9.3) architectures; and\n\u2013 The RNN-RBM architecture [ 1], composing an RNN\narchitecture and an RBM architecture.",
                "8.2 Refinement\nOne architecture is re\ufb01ned and specialized through someadditional constraint(s), e.g.:\n\u2013 An autoencoder architecture (to be introduced in\nSect. 9.1), which is a feedforward architecture with\nFig.",
                "15,G ]has around one\nchance in two of being selected and A ]one chance in four.\n42We are taking inspiration from concepts and terminology in\nprogramming languages and software architectures [ 57], such as\nre\ufb01nement ,instantiation ,nesting andpattern [13].50 Neural Computing and Applications (2021)",
                "33:39\u201365\n123one hidden layer with the same cardinality (number of\nnodes) for the input layer and the output layer; and\n\u2013 A variational autoencoder (VAE) architecture, which is\nan autoencoder with an additional constraint on the\ndistribution of the variables of the hidden layer (see\nSect. 9.2), e.g., the GLSR-VAE architecture [ 20].\n8.3 Nesting\nAn architecture is nested into the other one, e.g.:\n\u2013 A stacked autoencoder architecture,43e.g., the Dee-\npHear architecture [ 60]; and\u2013 A recurrent autoencoder architecture (Sect. 9.3), where\nan RNN architecture is nested within an autoencoder,44\ne.g., the MusicVAE architecture [ 53] (see Sect. 9.3).",
                "8.4 Pattern\nAn architectural pattern is instantiated onto a given archi-\ntecture(s),45e.g.:\n\u2013 The anticipation-RNN architecture [ 19] that instantiates\ntheconditioning pattern46onto an RNN with the output\nof another RNN as the conditioning input; and\n\u2013 The C-RNN-GAN architecture [ 44], where the GAN\n(Generative Adversarial Networks) pattern (to be\nintroduced in Sect. 9.4) is instantiated onto two RNN\narchitectures, the second one (discriminator) beingbidirectional (see Fig. 16); and\n\u2013 The MidiNet architecture [ 67] (see Sect. 9.4), where\ntheGAN pattern is instantiated onto two convolu-\ntional\n47feedforward architectures, on which a condi-\ntional pattern is instantiated.",
                "Therefore, it is also named an RNN\nEncoder\u2013Decoder architecture.",
                "45Note that we limit here the scope of a pattern to the external\nenfolding of an existing architecture.",
                "46Such as introduced by Todd in his Sequential architecture\nconditioned by a plan in Sect.",
                "47Convolutional architectures are actually an important component\nof the current success of deep learning and they recently emerged as\nan alternative, more ef\ufb01cient to train, to recurrent architectures [ 3,\nSection 8.2].",
                "A convolutional architecture is composed of a succes-\nsion of feature maps and pooling layers [ 15, Section 9][ 3, Sec-\ntion 5.9].",
                "However, we do not detail convolutional architectureshere, because of space limitation and of nonspeci\ufb01city regarding mu-sic generation applications.",
                "Neural Computing and Applications (2021) 33:39\u201365 51\n1238.5 Examples\nFigure 17illustrates various examples of compound\narchitectures and of actual music generation systems.",
                "8.6 Combined strategies\nNote that the strategies for generation can be combined too,\nalthough not in the same way as the architectures: they are\nactually used simultaneously on different components of\nthe architecture.",
                "7.2.2 ,\ntherecursive strategy is used by recursively feedforward-\ning current note into the architecture in order to produce\nnext note and so on, while the sampling strategy is used at\nthe output of the architecture to sample the actual note\n(pitch) from the possible notes with their respectiveprobabilities.",
                "9 Examples of refined architectures\nand strategies\n9.1 Autoencoder architecture\nAnautoencoder is a re\ufb01nement of a feedforward neural\nnetwork with two constraints: (exactly) one hidden layer\nand the number of output nodes are equal to the number of\ninput nodes.",
                "An early example of this strategy is the use of the Dee-\npHear nested (stacked) autoencoder architecture to gener-\nate ragtime music according to the style learnt [ 60].",
                "9.2 Variational autoencoder architecture\nAlthough producing interesting results, an autoencoder\nsuffers from some discontinuity in the generation when\nexploring the latent space.49Avariational autoencoder\n(VAE)",
                "16 C-RNN-GAN architecture with the D(iscriminator)",
                "17 A tentative illustration\nof various examples andcombination types (in color\nfonts) of compound\narchitectures (in black boldfont) and systems (in black\nitalics font) (color \ufb01gure online)\nFig. 18 (left) Autoencoder\narchitecture.",
                "(right) Stacked\nautoencoder (order-2)\narchitecture\nFig.",
                "Another issue is that the semantics (meaning) of the\ndimensions captured by the latent variables is automati-\ncally \u2018\u2018chosen\u2019\u2019 by the VAE architecture in function of thetraining examples and the con\ufb01guration and thus can only\nbe interpreted a posteriori .",
                "[ 68].9.3 Variational recurrent autoencoder (VRAE)\narchitecture\nAn interesting example of nested architecture (see\nSect. 8.3) is a variational recurrent autoencoder (VRAE).",
                "The motivation is to combine:\n\u2013 The variational property of the VAE architecture for\ncontrolling the generation; and\n\u2013 The arbitrary length property of the RNN architecture\nused with the recursive strategy.54\nAn example (also hierarchical) is the MusicVAE archi-tecture [ 53] (shown in Fig. 7, with an example of con-\ntrolled generation in Fig. 21).",
                "9.4 Generative adversarial networks (GAN)\narchitecture\nAn interesting example of architectural pattern is the\nconcept of Generative Adversarial Networks (GAN) [ 16],\nas illustrated in Fig.",
                "The architecture, illus-\ntrated in Fig. 23, follows two patterns: adversarial (GAN)\nand conditional (on history and on chords to condition\nmelody generation).",
                "22 Generative adversarial networks (GAN) architecture.",
                "55Please refer to [ 67]o r[ 3, Section 6.10.3.3] for more details about\nthis sophisticated architecture.",
                "This incremental instantiation strategy has been used in\nthe DeepBach architecture [ 21] for generation of Bach\nchorales.",
                "The compound architecture,56shown in Fig. 25,\ncombines two recurrent and two feedforward networks.",
                "As\nopposed to standard use of recurrent networks, where asingle time direction is considered, DeepBach architecture\nconsiders the two directions forward in time and back-\nwards in time.",
                "Coconet [ 27], the architecture used for implementing the\nBach Doodle (introduced in Sect. 3), is another example of\nthis approach.",
                "It uses a Block Gibbs sampling algorithm forgeneration and a different architecture (using masks to\nindicate for each time slice whether the pitch for that voice\nis known, see Fig. 27).",
                "MidiNet architecture.",
                "Reproduced from [ 67] with permission of the authors\n56Actually this architecture is replicated 4 times, one for each voice\n(4 in a chorale).57The two bottom lines correspond to metadata (fermata and beat\ninformation), not detailed here.56 Neural Computing and Applications (2021) 33:39\u201365\n123An example of counterpoint accompaniment generation is\nshown in Fig.",
                "An example of application to music is the generation\nalgorithm for the C-RBM architecture [ 34].",
                "9.7 Other architectures and strategies\nResearchers in the domain of deep learning techniques for\nmusic generation are designing and experimenting with\nvarious architectures and strategies,61in most cases com-\nbinations or re\ufb01nements of existing ones, or sometimes\nFig.",
                "25 DeepBach architecture for the soprano voice prediction.",
                "Reproduced from [ 21] with\npermission of the authors\n58The architecture is convolutional (only) on the time dimension, in\norder to model temporally invariant motives, but not pitch invariantmotives which would break the notion of tonality.\n59Because of space limitation, and the fact that RBMs are not\nmainstream, we do not detail here the characteristics of RBM (see,e.g., [ 15, Section 20.2] or [ 3, Section 5.7] for details).",
                "However, there is no guarantee that combining a\nmaximal variety of types will make a sound and accurate\narchitecture.62Therefore, it is important to continue to\ndeepen our understanding and to explore solutions as well\nas their possible articulations and combinations.",
                "Control is necessary to inject constraints (e.g., tonality,\nrhythm) in the generation, as witnessed by the C-RBMarchitecture (see Sect. 9.6).",
                "Some challenge is that a deep\nlearning architecture is a kind of black box; therefore, some\ncontrol entry points (hooks) need to be identi\ufb01ed, such as:\nFig.",
                "27 Coconet Architecture.",
                "28 C-RBM Architecture generation algorithm.",
                "Music generated by a\ndeep learning architecture may be very pleasing for less\nthan a minute but usually starts to be boring after a little\nwhile because of the absence of a clear sense of direction.",
                "Structure imposition is a \ufb01rst direction, as in C-RBM, or by\nusing hierarchical architectures as in Music-VAE.",
                "A notable attempt has been proposed for creating\npaintings in [ 9], by extending a GAN architecture to favor\nthe generation of content dif\ufb01cult to classify within exist-ing styles and therefore favoring the emergence of new\nstyles.",
                "Meanwhile, as discussed in Sect. 2.2, we believe\nthat is more interesting to use deep learning architectures toassist human musicians to create and construct music, than\npursuing purely autonomous music generating systems.",
                "9.5.11 Conclusion\nThe use of arti\ufb01cial neural networks and deep learning\narchitectures and techniques for the generation of music (as\nwell as other artistic contents) is a very active area of\nresearch.",
                "Architecture An (arti\ufb01cial neural network) architecture\nis the structure of the organization of computational units\n(neurons), usually grouped in layers, and their weightedconnexions.",
                "Examples of types of architecture are:\nfeedforward (aka multilayer perceptron), recurrent\n(RNN), autoencoder and generative adversarial networks(GAN).",
                "Architectures process encoded representations\n(in our case of a musical content) which have been\nencoded.",
                "Autoencoder A speci\ufb01c case of arti\ufb01cial neural network\narchitecture with an output layer mirroring the input\nlayer and with one hidden layer.",
                "Compound architecture An arti\ufb01cial neural network\narchitecture which is the result of some combination of\nsome architectures.",
                "Conditioning architecture The parametrization of an\narti\ufb01cial neural network architecture by some condition-ing information (e.g., a bass line, a chord progression...)\nrepresented via a speci\ufb01c extra input, in order to guide\nthe generation.",
                "The function used\nfor measuring the distance between the prediction by an\narti\ufb01cial neural network architecture (y \u02c6) and the actual\ntarget (true value y).",
                "Creation by re\ufb01nement strategy A strategy for gener-\nating content based on the incremental modi\ufb01cation of a\nrepresentation to be processed by an arti\ufb01cial neural\nnetwork architecture.",
                "It is used as a cost(loss) function for a classi\ufb01cation task to measure the60 Neural Computing and Applications (2021) 33:39\u201365\n123difference between the prediction by an arti\ufb01cial neural\nnetwork architecture (y \u02c6) and the actual target (true value\ny).",
                "Dataset The set of examples used for training an\narti\ufb01cial neural network architecture.",
                "Decoder feedforward strategy A strategy for generat-\ning content based on an autoencoder architecture in\nwhich values are assigned onto the latent variables of the\nhidden layer and forwarded into the decoder componentof the architecture in order to generate a musical content\ncorresponding to the abstract description inserted.",
                "An arti\ufb01cial\nneural network architecture with a signi\ufb01cant number of\nsuccessive layers.",
                "Encoding The encoding of a representation consists in\nthe mapping of the representation (composed of a set of\nvariables, e.g., pitch or dynamics) into a set of inputs\n(also named input nodes or input variables) for the neuralnetwork architecture.",
                "End-to-end architecture An arti\ufb01cial neural network\narchitecture that processes the raw unprocessed data\u2014\nwithout any pre-processing, transformation of represen-\ntation or extraction of features\u2014to produce a \ufb01nal\noutput.",
                "Feedforward The basic way for a neural network\narchitecture to process an input by feedforwarding theinput data into the successive layers of neurons of the\narchitecture until producing the output.",
                "Feedforward architecture It is the most basic and\ncommon type of arti\ufb01cial neural network architecture.",
                "Generative adversarial networks (GAN) A compound\narchitecture composed of two component architectures,the generator and the discriminator, who are trained\nsimultaneously with opposed objectives.",
                "Hidden layer Any neuron layer located between the\ninput layer and the output layer of a neural networkarchitecture.",
                "The \ufb01rst layer of a neural network\narchitecture.",
                "In deep learning architectures, vari-\nables within a hidden layer.",
                "Layer A component of a neural network architecture\ncomposed of a set of neurons.",
                "A type of recurrent\nneural network architecture with capacity for learning\nlong-term correlations and not suffering from the\nvanishing or exploding gradient problem during thetraining phase.",
                "Multilayer perceptron (MLP) A feedforward neural\narchitecture composed of successive layers, with at leastone hidden layer.",
                "Also named Feedforward architecture.",
                "Neuron The atomic processing element (unit) of an\narti\ufb01cial neural network architecture, inspired by thebiological model of a neuron.",
                "Weights\nwill be adjusted during the training phase of the neuralnetwork architecture.",
                "Node The atomic structural element of an arti\ufb01cial\nneural network architecture.",
                "Objective The nature and the destination of the musical\ncontent to be generated by a neural network architecture.",
                "The name comes from digital circuits, one-hot referringto a group of bits among which the only legal (possible)\ncombinations of values are those with a single high (hot)\n(1) bit, all the others being low (0).Output layer The last layer of a neural network\narchitecture.",
                "Parameter The parameters of an arti\ufb01cial neural\nnetwork architecture are the weights associated witheach connexion between neurons as well as the biases\nassociated with each layer.",
                "Perceptron One of the \ufb01rst arti\ufb01cial neural network\narchitectures, created by Rosenblatt in 1957.",
                "Pooling For a convolutional architecture, a data dimen-\nsionality reduction operation (by max, average or sum)for each feature map produced by a convolutional stage,\nwhile retaining signi\ufb01cant information.",
                "This is the basis of a recurrent\nneural network (RNN) architecture.62 Neural Computing and Applications (2021) 33:39\u201365\n123Recurrent neural network (RNN)",
                "A type of arti\ufb01cial\nneural network architecture with recurrent connex-ions and memory.",
                "Single-step feedforward strategy A strategy for gener-\nating content where a feedforward architecture processes\nin a single processing step a global temporal scoperepresentation which includes all time slices.",
                "Strategy The way the architecture will process repre-\nsentations in order to generate the objective whilematching desired requirements.",
                "Time slice The time interval considered as an atomic\nportion (grain) of the temporal representation used by anarti\ufb01cial neural network architecture.",
                "Time step The atomic increment of time considered by\nan arti\ufb01cial neural network architecture.",
                "The long short-term memory (LSTM) architecture solved the problem.",
                "Software architecture: perspectives on\nan emerging discipline."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [
                "Compliance with ethical standards\nConflict of interest The authors declared that they have no conflict of\ninterest."
            ],
            "impact": [
                "Section 4analyzes in\ndepth some pioneering works in neural networks-basedmusic generation from the late 1980s and their later impact."
            ],
            "society": [
                "Proceedings of the 18th international society for\nmusic information retrieval conference (ISMIR 2017).",
                "In: Proceedings of the 18th international society formusic information retrieval conference (ISMIR 2017)."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "The output layer actually mirrors the input\nlayer, creating its peculiar symmetric diabolo (or sand\ntimer) shape aspect, as shown in the left part of Fig. 18."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [
                "In\n[51], Pons presents a short historical analysis of the use of\nneural networks for various types of music applications\n(that we expand in depth)."
            ],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "An example is the way he wascomposing chorales by designing and applying (with talent) counter-point rules to existing melodies.40 Neural Computing and Applications (2021) 33:39\u201365\n123learn very well musical style from a given corpus and to\ngenerate new music that \ufb01ts into this style.",
                "The\nunderlying architecture, named Coconet [ 27], has been\ntrained on a dataset of 306 Bach chorales.",
                "Therefore, the dataset is\nconstructed by extracting all possible 4 measures long\nexcerpts from the original 352 chorales, also transposed inall possible keys.",
                "After training on several examples, generation can take\nplace, with an example of chorale counterpoint generated\nfrom a soprano melody shown in Fig.",
                "1 Example of chorale generation by Bach Doodle.",
                "3 Example of a chorale counterpoint generated by MiniBach\nfrom a soprano melody\nFig.",
                "This incremental instantiation strategy has been used in\nthe DeepBach architecture [ 21] for generation of Bach\nchorales.",
                "Reproduced from [ 67] with permission of the authors\n56Actually this architecture is replicated 4 times, one for each voice\n(4 in a chorale).57The two bottom lines correspond to metadata (fermata and beat\ninformation), not detailed here.56 Neural Computing and Applications (2021) 33:39\u201365\n123An example of counterpoint accompaniment generation is\nshown in Fig.",
                "An example is a chorale with 3 voices(alto, tenor and bass) matching a soprano melody.",
                "Hadjeres G, Pachet F, Nielsen F (2017) DeepBach: a steerable\nmodel for Bach chorales generation."
            ],
            "orchestral": [],
            "electronic": [
                "6.2.2 SymbolicThe main symbolic formats used are:\n\u2013MIDI\n28\u2014It it is a technical standard that describes a\nprotocol based on events, a digital interface and connec-\ntors for interoperability betwee n various electronic musi-\ncal instruments, softwares and devices [ 43].",
                "A tech-\nnical standard that describes a protocol, a digital\ninterface and connectors for interoperability between\nvarious electronic musical instruments, softwares anddevices.",
                "Hiller LA, Isaacson LM (1959) Experimental music: composition\nwith an electronic computer."
            ],
            "pop": [
                "An example of the use of GAN for generating music is\nthe MidiNet system [ 67], aimed at the generation of single\nor multitrack pop music melodies."
            ],
            "Demo availability": [],
            "dataset": [
                "The\nunderlying architecture, named Coconet [ 27], has been\ntrained on a dataset of 306 Bach chorales.",
                "Therefore, the dataset is\nconstructed by extracting all possible 4 measures long\nexcerpts from the original 352 chorales, also transposed inall possible keys.",
                "Once trained on this dataset, the systemmay be used to generate three counterpoint voices corre-\nsponding to an arbitrary 4 measures long melody provided\nas an input.",
                "Dataset The set of examples used for training an\narti\ufb01cial neural network architecture."
            ]
        },
        {
            "title": "Conditional hybrid GAN for melody generation from lyrics",
            "architecture": [
                "InMaskGAN, [ 18] proposes the actor-critic GAN\narchitecture that uses reinforcement learning to\ntrain the generator, where the in-\ufb01lling techniquemay alleviate mode collapse.",
                "1 Architecture of\nconditional hybrid GAN3194 Neural Computing and Applications (2023) 35:3191\u20133202\n123yp\nt\u00fe1/C24softmax \u00f0ot\u00de: \u00f03\u00de\nHere, softmax \u00f0ot\u00derepresents the multinomial distribution\non the set of all possible MIDI numbers.",
                "Therefore, to evaluate our proposed architecture, we\nuse Self-BLEU in [ 30] to measure the diversity of gener-\nated samples and maximum mean discrepancy (MMD) in\n[31] to measure the quality of generated samples.",
                "During the adversarial training,\nSelf-BLEU values of our C-Hybrid-GAN architecturereach the peak around 45 epochs, decrease until 100\nepochs, and then approach to the stability."
            ],
            "training parameters": [],
            "learning rate": [
                "Initially, the generator network is pre-\ntrained with the MLE objective for 40 epochs using a\nlearning rate of 10/C02.",
                "And then, adversarial training is\nperformed for 120 epochs with a learning rate of 10/C02for\nboth the generator and discriminator."
            ],
            "batch size": [
                "The batch size is set to 512 and\na maximum temperature bmax\u00bc1000 is used during the\nadversarial training."
            ],
            "ethical": [],
            "impact": [
                "On the other hand, due to the\nquantization error, the generated music attributes could beassociated with an improper discrete-valued music attri-\nbute, which would lead to a negative impact on melody\ngeneration.",
                "As Gumbel-Softmaxand RMC are mainly involved in the proposed C-Hybrid-\nGAN, their impacts are further investigated as the ablation\nstudy, and the results of TBC-LSTM-MLE and TBC-LSTM-GAN are shown in Table 2.\n4.1 Experimental setup\nWe use the Adam [ 32] optimizer with b1\u00bc0:9 and b2\u00bc\n0:99 and perform gradient clipping if the norm of the\ngradients exceeds 5."
            ],
            "society": [],
            "copyright": [],
            "evaluation metrics": [
                "Through extensive experiments using evaluation metrics, e.g., maximum mean\ndiscrepancy, average rest value, and MIDI number transition, we demonstrate that the proposed C-Hybrid-GAN outper-forms the existing methods in melody generation from lyrics."
            ],
            "metric": [
                "Through extensive experiments using evaluation metrics, e.g., maximum mean\ndiscrepancy, average rest value, and MIDI number transition, we demonstrate that the proposed C-Hybrid-GAN outper-forms the existing methods in melody generation from lyrics.",
                "In TextGAN, [ 19] utilizes a kernelized\ndiscrepancy metric to map high-dimensional latent\nfeature distributions of real and synthetic sentences,\nwith the aim of mitigating the model collapse.",
                "The con\ufb01guration of generator anddiscriminator is summarized in Table 1.4.2 Diversity evaluation of generated sequences\nWe use the Self-BLEU score by [ 30] as a metric to measure\nthe diversity of melodies generated by our proposed model.",
                "We calculate the BLEU\nscore for every generated melody and de\ufb01ne the averageBLEU score as the Self-BLEU metric.",
                "In addition, for\nmetrics on temporal attributes such as average rest value\nand the number of notes without rest, C-Hybrid-GAN isalso closest to the ground truth.",
                "Besides metrics discussed in Table 2, the distribution of\nthe transitions between MIDI numbers is a very important\nattribute for quantitatively measuring generated melodies.",
                "Mean values\narelrs\u00bc1:3666, lrn\u00bc1:3692,\nandlrns\u00bc1:3679, respectively\nTable 2 Metrics evaluation of attributes\nGround truth C-LSTM-GAN C-Hybrid-MLE C-Hybrid-GAN TBC-LSTM-MLE TBC-LSTM-GAN\n2-MIDI repetitions 7.4 9.7 6.8 6.5 9.1 10.6\n3-MIDI repetitions 3.8 2.2 2.8 2.7 2.1 3.0MIDI span 10.8 7.7 12.7 12.0 13.7 12.1Unique MIDI number 5.9 5.1 6.0 6.1 6.2 6.1Average rest value 0.8 0.6 1.4 0.7 1.1 0.8Non-rest note number 15.6 16.7 12.7 16.1 12.7 15.9Song length 43.3 39.2 60.9 39.1 51.0 41.4\nFig.",
                "Following the existing works, threekinds of subjective measurements are used as evaluation\nmetrics: (1) how about the entire melody?"
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "Monteith K, Martinez TR, Ventura D (2012) Automatic genera-\ntion of melodic accompaniments for lyrics."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "The gradient of the generator lossoloss G\nohG\ncannot be back propagated to the generator via the dis-\ncriminator, and hence generator parameters hGcannot be\nupdated."
            ],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "Large-scale Chinese language lyrics-melody\ndataset was built to evaluate the proposed learning model.",
                "In our initial work by [ 1], we not only built a large dataset\nconsisting of 12,197 MIDI songs each with paired lyricsand melody, but also have veri\ufb01ed the feasibility of melody\ngeneration from lyrics by LSTM-based deep generative\nmodel [ 8].",
                "In our dataset, the number of distinct MIDI numbersis 100.",
                "The melody-lyrics dataset in [ 1] is utilized in\nour experiment, which contains 13,251 sequences, eachconsisting of 20 syllables aligned with the triplet of music\nattributes { y\np\nt;yd\nt;yr\nt}.",
                "The dataset is split into training,\nvalidation and test sets with the ratio of 8:1:1.",
                "2 Training curves of self-\nBLEU scores on testing dataset\nFig.",
                "Average note duration distance between generatedsequences and sequences from ground truth dataset is\ncalculated and shown in Fig.",
                "Average rest duration\ndistance between generated sequences and sequences from\nground truth dataset is shown in Fig. 7.",
                "4 Training curves of MMD\nscores on testing dataset3198 Neural Computing and Applications (2023) 35:3191\u20133202\n123melodies generated by our model C-Hybrid-GAN,\nC-Hybrid-MLE, and C-LSTM-GAN.",
                "Data Availability The datasets generated during and/or analyzed\nduring the current study are available in [ 1] repository, https://github."
            ]
        },
        {
            "title": "Scene2Wav: a deep convolutional sequence-to-conditional SampleRNN for emotional scene musicalization",
            "architecture": [
                "The proposed architecture, shown in Fig. 2, consists of three modules.",
                "Dataset distribution:\ntrain and test, positive and\nnegative emotion scoresPositive Negative Total\nTrain 1,778 1,404 3,182\nTest 369 772 1,141\nTotal 2,147 2,176 4,3231798 MultimediaToolsandApplications(2021)80: 1793\u20131812Fig.2 Proposed model\narchitecture for Scene2Wav.",
                "This model is also\ntrained by end-to-end manner with architecture differing only in the decoder module.",
                "6 Baseline model architecture for ConvSeq2Seq consisting of two modules: emotional visual feature\nextraction with CNN, and sequence encoding and decoding with an Encoder-Decoder Deep RNN framework\n5.2 Examplesofgeneratedsamples\nExperimental results are showcased in Figs."
            ],
            "training parameters": [],
            "learning rate": [
                "The encoder is a 2-layer deep GRU-RNN with 128 hidden units, trained for 20\nepochs, with ASGD Optimizer, learning rate of 0.001, momentum of 0.98, weight decay of\n1e\u22125, and scheduler of 0.8."
            ],
            "batch size": [
                "Lastly, in the proposed model, is the conditional SampleRNN\ndecoder, a 2-layer deep RNN with 1,024 hidden units, batch size of 128, and quantization\nlevelqof 256, corresponding to a per-sample bit depth of 8."
            ],
            "ethical": [],
            "impact": [],
            "society": [
                "In: Proceedings of the 18th international society for music information retrieval conference\n33.",
                "International\nSociety for Optics and Photonics\n40."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "5.3 Humanevaluation:qualitativemetric\nThe most widely accepted measure of performance for researchers in this field to evaluate\nthe quality of generated audio is through human evaluation feedback.",
                "5.4 Perceptualaudiometric:quantitativemetric\nThere is currently no widely accepted quantitative metric to measure the quality of audio\ngenerated by machine learning models.",
                "al [ 14]h a v e\ntaken steps in that direction by proposing a perceptual audio metric (PAM) for perceptual\nassessment in audio.",
                "The advantage of this method, and the reason why we are using it here,\nis the metric\u2019s good correlation with human evaluators, achieved by training a deep neural\nnetwork with crowdsourced human judgments.",
                "Note, however,\nthat PAM is still a distance metric nonetheless, meaning that it\u2019s extremely dependent on\nthe relationship between evaluated audio and target audio.",
                "Table 3shows that our model obtains better and more consistent scores\nin the perceptual audio metric when compared to the baseline model, for both negative and\npositively inclined cases.",
                "We then analyze the detected chords in the generated\nmusics, both from our proposed model Scene2Wav and the baseline model ConvSeq2Seq.1807 MultimediaToolsandApplications(2021)80: 1793\u20131812Table3 Perceptual audio metric (PAM) used to evaluate the quality of generated music in Figs.",
                "This is a distance metric, so lower values are desirable.",
                "A differentiable perceptual\naudio metric learned from just noticeable differences."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "Lee\nmholee@gmail.com\nGwenaelle Cunha Sergio\ngwena.cs@gmail.com\n1School of Electronics Engineering, Kyungpook National University, 80 Daehakro,\nBukgu, Daegu 41566, South KoreaMultimediaToolsandApplications(2021)80:1793\u20131812\nPublishedonline: 2020 September10and in all modern societies, visual arts and music are intimately intertwined [ 17].",
                "(2016-0-00564,1810 MultimediaToolsandApplications(2021)80: 1793\u20131812Development of Intelligent Interaction Technology Based on Context Awareness and\nHuman Intention Understanding) (50%) and Electronics and Telecommunications Research\nInstitute (ETRI) grant funded by the Korean government"
            ],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "Others have used the IAPS dataset to train a Sup-\nport Vector Classifier to classify 1D emotion in paintings, successfully demonstrating the\npotential of machines deriving emotion from images [ 40], or used it to collect \u201cdescriptive\nemotional category data\u201d with the aim of identifying images that evoke one emotion more\nthan others [ 16].",
                "Researchers and developers can have access to our code,1\nincluding data pre-processing and proposed model, and replicate it with their own dataset.",
                "Section 3describes the dataset and pre-preocessing steps.",
                "On top of excelling in the mentioned\ntasks, another advantage of using this network is the fact that it takes as input minimally\nprocessed images, abolishing the need of hand-crafted features and making it general for a\nplethora of datasets.",
                "This model hierarchically combines sample-level modules as multilayer perceptrons\nand frame-level modules as RNNs in order to capture long-term dependencies in the\ntemporal sequences, and it does so on three different datasets.",
                "3 Datasetandpre-processing\nOne of the challenges in this work is finding an appropriate dataset with all the required\ninformation needed for our task.",
                "The dataset is divided in such a way as to guarantee disjoint\ntrain and test datasets, presented in the top and bottom half of the table respectively, so as\nto prevent overfitting of the model.",
                "The pre-processing of the dataset2consists of splicing the data (scene, audio, and\nemotion scores) into chunks of 3 seconds, duration that was kept short due to the high-\ndimensional characteristic of audio signals.",
                "Dataset distribution:\ntrain and test, positive and\nnegative emotion scoresPositive Negative Total\nTrain 1,778 1,404 3,182\nTest 369 772 1,141\nTotal 2,147 2,176 4,3231798 MultimediaToolsandApplications(2021)80: 1793\u20131812Fig.2 Proposed model\narchitecture for Scene2Wav.",
                "6.\n5 Resultsanddiscussion\n5.1 Trainingcon\ufb01guration\nBoth the proposed and baseline models are trained on the previously discussed 3 second\nspliced dataset (see Section 3) and both use the same CNN and Encoder RNN with heuris-\ntically determined configurations.",
                "5.5.1 Extendedemotionevaluation\nIn this section, we extend the emotion analysis on longer samples obtained with the same\ndataset and also with short samples obtained with the additional DEAP dataset [ 11].",
                "We first show emotion evaluation on samples of\n10 seconds duration obtained with the same dataset used in this paper, the COGNIMUSE\ndataset [ 46].",
                "Emotion evaluation on short and long samples,\nand even samples obtained from a different dataset, show that Scene2Wav is better able to\nproduce music with the same emotion as the scene.",
                "Future works include increasing the size of\nthe dataset, improving longer-term dependency modeling for longer audios, and including\nmore emotion classes, such as considering arousal and adding a neutral class."
            ]
        },
        {
            "title": "Attentional networks for music generation",
            "architecture": [
                "Regularly used architecture of LSTM units have a cell andthree regulators.",
                "81:5179\u201351893 Proposedmethodology\nFigure 2outlines the proposed architecture for music generation.",
                "81:5179\u20135189Fig.2 Architecture diagram\nrequired to add 12 to its pitch value.",
                "Browne CB (2001) System and method for automatic music generation using a neural network\narchitecture.",
                "Framewise phoneme classification with bidirectional lstm and other\nneural network architectures."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [
                "1 a) Sheet Music of the song: \u201cThe Last Farewell\u201d by Roger Whittaker b) Musical Notes (Extracted\nfrom MIDI file) of the song: \u201cThe Last Farewell\u201d5181 Multimedia Tools and Applications (2022) 81:5179\u20135189Table1 Batch construction for the JAZZ ML ready MIDI dataset: Batch size 64 characters\nBatch-1"
            ],
            "ethical": [],
            "impact": [
                "In 2009 when the AI winter ended the deeplearning works started influencing and impacting the field of music and audio AI industry."
            ],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "MSE metric ismore correlated to the inherent harmony structures between the original melody and themusic generated by the network."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "In this work, we propose a deep learning based music generation\nmethod in order to produce old style music particularly JAZZ with rehashed melodic struc-\ntures utilizing a Bi-directional Long Short Term Memory (Bi-LSTM) Neural Network with\nattention.",
                "In [ 15], authors stated that\nLSTMs are able to capture the medium-scale melodic structure in music pretty well.",
                "In this work, we utilize an end to end pipeline based on Bi-LSTM network with an atten-\ntion module to produce old style Jazz music with rehashed melodic structures automaticallywithout any human intervention.",
                "In order to circumvent this issue, the two possiblesolutions are as follows: 1. to construct music with melodic beat, increasingly complexstructure, and using a wide range of notes counting speckled notes, longer harmonies andrests.",
                "[11] are also effectively utilized in generating melodic notes where themodel consists of two networks, generator that is responsible for generating random infor-mation and discriminator that is responsible for assessing created arbitrary information forrealness against the original data.",
                "It doesn\u2019t store songs like sound formats, however it stores data that is equipped forproducing future melodic notes.",
                "To achieve our objective of generating old style music with rehashed melodicstructure, we have utilized Bi-LSTM network with attention layer [ 22].",
                "Given the ongoing trends in AI inmusic industry, we envisage that the current work presents progressively complex modelsand information portrayals that successfully captures the fundamental melodic structure."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "Table 1shows the batch construction for the used dataset.",
                "1 a) Sheet Music of the song: \u201cThe Last Farewell\u201d by Roger Whittaker b) Musical Notes (Extracted\nfrom MIDI file) of the song: \u201cThe Last Farewell\u201d5181 Multimedia Tools and Applications (2022) 81:5179\u20135189Table1 Batch construction for the JAZZ ML ready MIDI dataset: Batch size 64 characters\nBatch-1",
                "Thetypical length of each MIDI file in the dataset ranges from 1 to 4774.",
                "4 Experimentationandresults\n4.1 Datasetdiscription\nWe used Jazz ML ready MIDI dataset2to train our model.",
                "The dataset comprises of 818\ndiverse Jazz music melodies.",
                "The dataset comprises of:\n\u2013 The list of notes extracted from the midi file,\n\u2013 Number of notes and\u2013 List of unique notes for each midi file.",
                "We have divided the dataset into training/validation and test splits in the ratio 80:20."
            ]
        },
        {
            "title": "Monophonic music composition using genetic algorithm and Bresenham\u2019s line algorithm",
            "architecture": [],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "We have considered\nmelodic and rhythmic aspect of music in this work, thereby creating monophonic music.",
                "\u2013 Then the melodic interval (d ) is calculated for the remaining points in set using Eq.",
                "dk=max(|Ab1k\u2212Ab2k+1|,|Ab2k\u2212Ab1k+1|) (7)\nwhere Ab1kandAb2krefers to the absolute value of kthnote in parent 1 and parent 2\nrespectively and dkis the melodic interval for kthpoint in set C.",
                "\u2013 Finally the point having minimum melodic interval is chosen as the crossover point for\nthe selected pair of parents.",
                "While proposed methodgenerates new compositions with creative exploration of the search space.\n\u2013 Proposed method vs existing evolutionary approaches: The existing methods on\ngenerating monophonic music mostly work in two directions: creating compositionsidentical to the given reference [ 24,28,37] or generating new melodic ideas"
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [
                "Music can have different\ntextures, namely monophonic, homophonic and polyphonic.",
                "While homophonic and poly-\nphonic textures also include harmony along with melody and rhythm."
            ],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "[ 3] designed GenJam to generated jazz solos using interactive GA.",
                "[3 ] Interactive GA for melody (Jazz solos).",
                "Biles JA et al (1994) Genjam: a genetic algorithm for generating jazz solos."
            ],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "The process starts with a randomly generated population of individuals.",
                "This makes it a good choice for music composition wherethe individuals of population can be represented by note sequences.",
                "Literaturereview\nIn recent years, algorithmic music composition has become very popular among computer\nresearchers [ 16].",
                "However, the user has to listen to each sample thoroughly to givefeedback which limits the population size and number of generations.",
                "81:26483\u201326503Table1 Some popular approaches for music composition\nReference Composition Approach Remark\nBiles et al.",
                "Rhythm and Uses hierarchical population representation\nother elements taken from progression file\nJohanson et al.",
                "Although there\nare other durations possible but they are less popular and seldom used.",
                "The motif pro-\nvided by user is included in the chromosomes while generating initial population.",
                "Thepopulation is evaluated based on fitness functions.",
                "If the criterion is satisfied, then out-put melody is generated; else, the population is passed for the next step.",
                "Further, selection,crossover and mutation operations are performed to generate new population.",
                "Example of chromosome with user\u2019s motif\n4.2.2 Initialpopulation\nThe population is initialized with 50 chromosomes.",
                "4.2.3 Evaluation\nThe fitness function for evaluating the population is defined in Eq.",
                "If the fitness of i\nthgenome is Fi,\nthe probability of its selection is defined as:\npi=Fi/summationtextN\ni=0Fi(5)\nwhere N denotes number of genomes in the population.\n\u2013 Survivor Selection: The (\u03bc+\u03bb)-Evolutionary Strategy is used for survivor selection.",
                "This crossed population is then passed for three different26494 Multimedia Tools and Applications (2022) 81:26483\u201326503Fig.9 (\u03bc+\u03bb)-Strategy for Survivor Selection\nmutation operators as shown in Fig. 9.",
                "4.2.6 Mutation\nMutation operators are used to maintain diversity in the population.",
                "Size of the population is 50, and rest of the parameters are samein both algorithms.",
                "After 20 generations, the average fitness of the population is calculated.",
                "The size of population is 50, and rest of the\noperators and parameters are same in both algorithms.",
                "The average fitness of population iscalculated after 20 generations."
            ],
            "Demo availability": [],
            "dataset": [
                "[ 2] Generative RNN model for sheet music Uses dataset in ABC music notation26488 Multimedia Tools and Applications (2022) 81:26483\u201326503recurrent unit (GRU) to generate convincing monophonic melodies.",
                "[ 2] proposed generative RNN models to build a music gener-\nator using Seq2Seq and Character RNN, which is trained by Abc formatted music datasetwith approximately 34,000 songs of different genres.",
                "These methods generate music that is akin to the music in the dataset."
            ]
        },
        {
            "title": "Polyphonic music generation generative adversarial network with Markov decision process",
            "architecture": [
                "These algorithms rely\non the experimental enumeration of the discriminator/generator architectures to find betternetwork architecture settings, but they do not completely solve the training difficulties and thelack of diversity among the generated samples."
            ],
            "training parameters": [],
            "learning rate": [
                "a t eo f Gis a gradient descent\n\u03b8 \u03b8\u00fe/C11hr\u03b8J\u03b8\u00f0\u00de \u00f0 13\u00de\nwhere /C11hrepresents the learning rate."
            ],
            "batch size": [],
            "ethical": [],
            "impact": [
                "81:29865 \u201329885displacement impacts both macro/C0F1 and macro/C0F1D, the difference between them\ndecreases."
            ],
            "society": [
                "Presented at the 19th International Society for Music Information Retrieval\nConference (ISMIR), Paris, France, Sept. 23 \u201327\n10.",
                "Presented at the 18th International Society for Music Information\nRetrieval Conference (ISMIR), Suzhou, China, 23 \u201327\n11."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "A temporally coherent, volumetric GAN for\nsuper-resolution fluid flow."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "Conklin D, Gasser M, Oertl S (2018) Creative chord sequence generation for electronic dance music."
            ],
            "pop": [
                "MuseGAN V1 uses convolution in the generator and discrim-\ninator, which can generate multi-channel pop/rock music from scratch or can accompany a\ntrack provided by the user."
            ],
            "Demo availability": [],
            "dataset": [
                "For example, it is difficult for a classic GAN to create a new melody outside thetraining dataset, and it is difficult to break through the shackles of melody and tone in that\ndataset.",
                "By learning the characteristics of and mapping between various independentmelody sequences in a polyphonic music dataset, this model can generate polyphonic musicthat is closer to real-world music, effectively resolving the issues concerning the continuity ofpolyphonic music sequences and ensuring the diversity of generated samples \u2014making the\nmodel more powerful for unsupervised music sequence processing.",
                "Thus, the generatedpolyphonic music will not be destroyed with the growth of the sequence; better quality musicwill be generated; the shackles of the dataset will be broken; and melodies outside the datasetcan be created.",
                "3 Experimental results and analysis\n3.1 Dataset composition and preprocessing\nThe research team have constructed 6,947 original datasets composed of polyphonic music\n[19] to serve as the dataset for the polyphonic music generation model.",
                "The file format for the\ndataset is music instrument digital interface (MIDI)",
                "After the team loaded a MIDI file in the dataset, each note in the filewas parsed into a list of start time, duration, tone, and speed.",
                "Original music\n(MIDI files)\nPreprocessing\nVector \nsequencesToken \nsequencesVector \nsequencesPreprocessingMIDI files\nMelody\nchord\nToken Token\nTrain Valid\nFig. 5 Complete processing flow chart of original music datasetMultimedia Tools and Applications (2022)",
                "Figure 10shows the various styles of polyphonic\nmusic output by the model using the dataset created for this paper.",
                "macro/C0P\u00bc1\nnXn\ni\u00bc1Pi \u00f017\u00de\nmacro/C0R\u00bc1\nnXn\ni\u00bc1Ri \u00f018\u00de\nmacro/C0F1\u00bc2/C2macro/C0P/C2macro/C0R\nmacro/C0P\u00femacro/C0R\u00f019\u00de\nIn this paper, all polyphonic music sequences in the original dataset are \u201ctrue\u201dsamples, while\nnoise sequences are \u201cfalse\u201dsamples.",
                "During the experiments, the research team input the original polyphonic music dataset into\nthe proposed music generation model to obtain the polyphonic music generated by thealgorithm in this paper.",
                "After completing the above steps, the team used the polyphonic music sequence in\nthe original dataset and the formulas for precision and recall to obtain the comparison testresults of the two models, as shown in Table 2.",
                "It should be noted that, after the neural network, the output music sequence will produce a\nvariety of new notes, so it is not possible to determine the one-to-one correspondence betweenthe music sequence in the original dataset and the generated music sequence.",
                "81:29865 \u201329885 29881the innovation of neural networks for music generation and will not be limited to the melody or\ntone in the original music dataset.",
                "This study also relies on changing the dataset of the input model for the style change of musicproduction.",
                "Adding MDP and MCTS to the model preventsthe generated music from being constrained to the melody and tone in the original dataset,making the generated music more original.",
                "At the same time, the research team has built a new\ndataset, in which all data files are in MIDI format.",
                "LSTM based music generation with dataset prepro-\ncessing and reconstruction techniques\n2."
            ]
        },
        {
            "title": "A combination of multi-objective genetic algorithm and deep learning for music harmony generation",
            "architecture": [
                "However recently, the neural networks, and evolutionary algorithms have been more\nwidely used in the AMG literature, such as conditional rhythms generation of drum sequences\nwith neural networks [ 21], and using a Hierarchical Recurrent Neural Network (HRNN) for\nmelody generation [ 35] or combining two types of music generation models, namely symbol-\nic, and raw audio models based on the WaveNet architecture [ 22].",
                "The architecture of theLSTM network is shown in Fig.",
                "The architecture of the Bi-LSTM network2426 Multimedia Tools and Applications (2023) 82:2419\u20132435trained neural network model with opinions of music experts, and the trained Bi-LSTM neural\nnetwork model with opinions of regular listeners.",
                "Long short-term memory recurrent neural network\narchitectures for melody generation."
            ],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [],
            "society": [
                "In proceedings of the annual meeting of the cognitive science society (Vol. 31, no. 31)\n3. Agres K, Herremans D, Bigo L, Conklin D (2017)"
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "The science of harmony often refers to the verticalaspect of music and is distinct from horizontal and melodic motion."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [
                "However recently, the neural networks, and evolutionary algorithms have been more\nwidely used in the AMG literature, such as conditional rhythms generation of drum sequences\nwith neural networks [ 21], and using a Hierarchical Recurrent Neural Network (HRNN) for\nmelody generation [ 35] or combining two types of music generation models, namely symbol-\nic, and raw audio models based on the WaveNet architecture [ 22]."
            ],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "McVicar M, Fukayama S, Goto M (2014) AutoLeadGuitar: automatic generation of guitar solo phrases in\nthe tablature space."
            ],
            "choral": [
                "Chorale\ngeneration is one of the most popular works of music generation in terms of harmony andproduces very structured music."
            ],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "In the meantime, music may be one of the popular options because of its ability to evoke emotions.https://doi.org/10.1007/s11042-022-13329-6\n*Maryam Majidi\nMry20mj@gmail.com\nRahil Mahdian Toroghi\nMahdian.t.r@gmail.com\n1Faculty of Media Technology and Engineering, Iran Broadcasting University, Tehran, IranPublished online: 25 June 2022Multimedia Tools and Applications (2023) 82:2419\u20132435Digital advances have also changed the shape of m usic composing.",
                "AMG models can be classified into these groups: Markov model-based methods [ 3,6,9,\n24,28\u201330], approaches based on music rules and regulations [ 4,8,18,20], neural network-\nbased models [ 1,2,5,11,15,21,22,25,26,32,35], methods based on evolutionary\noptimization algorithms, and population-based [ 12,16,17,19,31,34], and algorithms based\non local search [ 7,14].",
                "Furthermore, the idea and\ninnovative scientific contribution of study [ 34], design a computer program called GenDash\nthat employs evolutionary computation in the composing music and the MetaCompose music\ngenerator uses a novel combinatorial evolutionary technique with Feasible-Infeasible Two-Population (FI-2POP) for effective melody generation [ 31].",
                "More sophis-\nticated deep learning models such as recursive n eural networks have become popular.",
                "Chorale\ngeneration is one of the most popular works of music generation in terms of harmony andproduces very structured music.",
                "Therefore, in the proposed method, because the population is enough, by carefully\nencoding the chromosomes, selecting an appropriate objective function and algorithm param-eters, the genetic algorithm will be a desirable option for the core of the proposed method.",
                "Objective FunctionStart\nGenerating Initial \nPopulation randomly\nCalculating the Fitness \nof each music piece\nSelection\nCrossover\nMutation\nReplacementMusic Grammar\nBi-LSTM 1\nBi-LSTM 2\nEnd+\nConver ged?Music\nScore\nFig. 1",
                "At each iteration of\nGA1, the best chromosomes in the population are selected based on minimum violation of therules and the maximum similarity to a human-made polyphonic music database.",
                "GrammarGenetic 1Calculate the probability\nof occurrence of notes in the\nprimar ydata setGenerating initial\npopulation of chromosomes\nbased on probabilities\nCalculating the grammatical\nfitness of each chromosomesusing the objective functionSelection\nbased roulette\nwheel\nDatabase 1( e x p e r t\naudience) and\n2 (ordinary audience)ScoreMusic\nNeural Network\nMusic\nScoreComparison\nprocess\n+-\nTarget valuesMinimize\nerror and\nadjust weightsInput OutputAudience listening\nGenetic 2Calculate the probability\nof occurrence of notes in the\nprimar ydata setGenerating initial\npopulation of chromosomes\nbased on probabilitiesCalculating the fitness of\neach chromosomes using the\nobjective functionSelection\nbased roulette\nwheelCrossover\n&\nMutation+Objective FunctionCrossover\n&\nMutationReplacement\nOr\nEnd\n\u00d7wError\nReplacement\nOr\nEnd\nFig.",
                "In fact, the use of rules individuallyTable 1 Parameters Settings\nParameter GA 1 GA 2\nNumber of Iterations 3600 3600\nPopulation Size 15 15\nCrossover rate 0.5 0.5Mutation rate 0.1 0.1\nObjective Function Grammar Grammar & Human\n00.250.50.7511.25\n20 40 60 80 100GA 1 GA 2Music Generation TimeTime (Sec)\nNumber of  Notes\nFig."
            ],
            "Demo availability": [],
            "dataset": [
                "Lstm based music generation with dataset preprocess-\ning and reconstruction techniques."
            ]
        },
        {
            "title": "A Style-Specific Music Composition Neural Network",
            "architecture": [
                "The architecture consists of a generator, adiscriminator and a conditional network with four convolutional layers to de\ufb01ne the har-mony direction.",
                "Therefore, the neural network architecture with the best performance in the trainingdata has been the MCNN model, evaluated in the iteration from 0 to 3500 time steps.",
                "Sak H, Senior A, Beaufays F (2014) Long short-term memory recurrent neural network architectures for\nlarge scale acoustic modeling."
            ],
            "training parameters": [],
            "learning rate": [
                "We employ a random gradientdescent with 0.9 momentum and remain the learning rate at 0.001."
            ],
            "batch size": [
                "Algorithm 1 Training Process of LSTM Generator\nInput: preprocessed training data are fed into the network\nOutput: model parameters of neural network\n1: Input training data, set iteration times, batch size, the number of hidden layer\u2019s cells and network layers.2:"
            ],
            "ethical": [],
            "impact": [],
            "society": [
                "In: Proceedings of international conference on information society, London, pp 486\u2013491\n5."
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "Through the establishment\nof an expert review panel, the performance of different models in generating classical musicwas evaluated from the following \ufb01ve aspects:\n1231908 C. Jin et al.\nFig. 8 ROC curve of style classi\ufb01er for 4 models\n\u2013Melody The melody of classical music sounds more balanced and symmetrical.",
                "Li Z, Tang J (2015) Weakly supervised deep metric learning for community-contributed image retrieval."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "Biles JA (1994) GenJam: a genetic algorithm for generating jazz solos."
            ],
            "choral": [
                "An expert system for harmonizing chorales in the style of J. S. Bach.",
                "Hadjeres G, Pachet F, Nielsen F (2017) Deepbach: a steerable model for bach chorales generation."
            ],
            "orchestral": [],
            "electronic": [
                "In: Proceedings of international conference on computer,communications and electronics, pp 501\u2013504\n40."
            ],
            "pop": [
                "Styles include classical, pop, jazz, rock and R&B etc.",
                "The symbolic model trained and generated at the note level is currently apopular method.",
                "[ 21] introduces MidiNet, which combines GAN network\nand CNN network to generate popular melody."
            ],
            "Demo availability": [],
            "dataset": [
                "According to the different characteristics of intelligent composition, it can be divided into:\nrandom generation composition, rule-based knowledge system composition, mathematic-based composition, music grammar composition, and genetic algorithm composition [ 5].T o\na certain extent, these methods can meet the basic requirements of automatic composition,but as for the melody\u2019s structure, can not meet the constant changes of music datasets.",
                "123A Style-Speci\ufb01c Music Composition Neural Network 1895\n2.2 Symbol Model Based on Neural Networks\nNowadays, existing intelligent music composition method can be broadly classi\ufb01ed into two\ntypes according to format variety of training datasets: an original audio model and a symbolmodel (MIDI).",
                "The parameters of generator are trained by the dataset, and actor\u2013critic (AC) network is usedto make \ufb01ne-tuning.",
                "We employed the matched subset of the Lakh MIDI dataset (LMD) and Classical Piano MidiPage dataset as the training dataset, Lakh MIDI dataset [ 47] provides a rich collection of real-\nworld MIDI \ufb01les and some associated meta-data.",
                "Classical Piano Midi Page dataset collectedclassical music sets of composers from the eighteenth century to the nineteenth century,including works of Bach, Beethoven, Chopin and other 25 composers.",
                "In the experiment, weselect 2000 music samples of Classical Piano Midi Page dataset with speci\ufb01c composers\u2019styles in MIDI format [ 48] as test dataset, and each sample was a single orbital with an average\ntime of 2\u20134 min. To meet the requirements of the experiment, each sample was divided into20 s to obtain more than 20,000 classical music samples.",
                "As is shown in Fig. 6, midi \ufb01les in the dataset were transformed into sequences.",
                "All the models adopt 1024 neurons and have trained the same datasets."
            ]
        },
        {
            "title": "Self-Supervised Music Motion Synchronization Learning for Music-Driven Conducting Motion Generation",
            "architecture": [
                "An overview of the two-\nstage training procedure can be found in Algorithm 1.\n4.3 Network Architectures\nAs shown in Fig.3, four neural networks are involved\nin our approach: a music encoder Emusic, a motion en-\ncoderEmotion , a generator G, and a discriminator D.",
                "Although\nthe architectures of these studies[57{60]appear similarto that of our proposed approach, there are three fun-\ndamental di\u000berences between them."
            ],
            "training parameters": [],
            "learning rate": [
                "As suggested in [19], our\nM2S-GAN is trained by the RMSprop optimizer[64]with a learning rate of 0.000 5.",
                "M2S-Net and other com-\nparable models are trained by the Adam optimizer[65]\nwith a learning rate of 0.001."
            ],
            "batch size": [
                "The batch size of the\ncontrastive and generative learning stage is 10 and 3 re-\nspectively."
            ],
            "ethical": [],
            "impact": [
                "6.5 Impact of Di\u000berent Negative Pairs\nAs addressed in Subsection 4.5, hard negatives\nshould have the advantage over easy and super-hard\nnegatives for M2S learning.",
                "In our experiment, although\nthe super-hard negatives under-perform hard negatives\nin M2S learning, the training process under super-hard\nnegatives is reasonably smooth, as shown in Fig.9.6.6 Impact of Training Set Scale\nWe next conduct another experiment to demon-\nstrate the necessity of discarding the MSE loss: we\ntrain the MSE model and our proposed M2S-GAN us-\ning di\u000berent scales of the training set."
            ],
            "society": [
                "the 19th International Society for Music\nInformation Retrieval Conference , September 2018, pp.218-\n224.",
                "the 20th International Society for Music Information\nRetrieval Conference , November 2019, pp.894-899.",
                "the 20th International Society for Music Infor-\nmation Retrieval Conference , November 2019, pp.115-122.\nDOI: 10.5281/zenodo.3527753."
            ],
            "copyright": [],
            "evaluation metrics": [
                "3) We conduct extensive experiments on Conductor-\nMotion100, using both standard evaluation metrics and\nseveral newly designed metrics.",
                "6.2 Evaluation Metrics\nSince learning music-driven conducting motion\ngeneration is a very new task, there are few existing\nmetrics available for measuring the outcomes."
            ],
            "metric": [
                "In\nthe subsequent generative stage, the previously learned\nmusic representations provide semantic information for\nthe motion generator, while the motion representations\nare used to calculate a proposed perceptual training\nmetric named sync loss.",
                "3) We conduct extensive experiments on Conductor-\nMotion100, using both standard evaluation metrics and\nseveral newly designed metrics.",
                "6.2 Evaluation Metrics\nSince learning music-driven conducting motion\ngeneration is a very new task, there are few existing\nmetrics available for measuring the outcomes.",
                "Notably\nthese metrics require a feature encoder, which is typ-\nically obtained by classi\fcation pre-training; however,\nthere are no available class labels for the conducting\nmotions.",
                "Therefore, to evaluate the quality of conduct-\ning motions more e\u000bectively, we propose several new\nmetrics in addition to the existing metrics, as detailed\nin the below.",
                "To provide a better understanding of\nthe characteristics of these metrics, we illustrate the\nchanges in metric values under spatial-temporal pertur-\nbation in Fig.5.",
                "Temporal perturba-\n6\u25cbhttps://pytorch.org, Apr. 2022.Fan Liu et al. : Self-Supervised Music Motion Synchronization Learning 549\n0.040Metric Value0.030\n0.020\n0.010\n0.000\n0.80.91.01.11.2\nAmplitude Scaling FactorTime Stretch Factor2.00\n1.50\n1.00\n0.50\n0.00\nMetric Value3.0\n2.0\n1.0\n0.0\n0.80.91.01.11.2\nAmplitude Scaling FactorTime Stretch Factor2.00\n1.50\n1.00\n0.50\n0.00\nMetric Value\n0.80.91.01.11.2\nAmplitude Scaling FactorTime Stretch Factor2.002.00\n1.501.50\n1.001.00\n0.500.50\n0.000.00\nMetric Value1.4\n1.0\n0.6\n0.2\n0.8\n0.91.01.11.2\nAmplitude Scaling FactorTime Stretch Factor2.00\n1.50\n1.00\n0.50\n0.00\nMetric Value0.150\n0.100\n0.050\n0.000\n0.80.91.01.11.2\nAmplitude Scaling FactorTime Stretch Factor2.00\n1.50\n1.00\n0.50\n0.00\u03a410-1\n\u03a410-1\n0.80.91.01.11.2\nAmplitude Scaling FactorTime Stretch Factor2.00\n1.50\n1.00\n0.50",
                "0.00Metric Value4\n3\n2\n1\n0(b) (a)\n(d) (c)\n(f) (e)\nFig.5.",
                "Changes in metric values under spatial-temporal perturbation.",
                "6.2.1 Mean Squared Error\nMean squared error (MSE) is the most straight-\nforward way to measure how close the generated mo-\ntion is to the ground truth, and has thus been widely\nused as an evaluation metric by existing audio-to-\nmotion translation work[3,4,29,30,32,43].",
                "The only ex-\nisting learning-based music-driven conducting motion\ngeneration method, KHMM[12], also adopts a mean ab-\nsolute error (MAE)-like metric.",
                "How-\never, it would not be suitable to use the sync loss as\nan evaluation metric, since our proposed M2S-GAN\ndirectly minimizes it."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "Tempo: ~130 BPM\n0.001 0.000 0.002 0.003 0.004 0.005 0.006\nPiano solo\n100 200 300 400 500 600 7002.5\n2.0\n1.5\n1.0\n0.5\n0.0Frequency (Hz)2.5\n2.0\n1.5\n1.0\n0.5\n0.0Frequency (Hz)\nPiano solo\n100 200 300 400 500\n0.82 Hz, 49.2 BPM\nMvt.",
                "Tempo: ~170 BPM\n0.000 0.002 0.004 0.006 0.008\n 100 200 300 40050 150 250 3502.5\n2.0\n1.5\n1.0\n0.5\n0.0Frequency (Hz)2.5\n2.0\n1.5\n1.0\n0.5\n0.0Frequency (Hz)Motion Contour Motion Contour Motion Contour\nPiano Solo\n1000 200 300 400 500 600 700\nPiano Solo\n0.0 0.5 1.0 1.5 2.0\n\u03a41040.010\n0.008\n0.006\n0.004\n0.002\n0.0000.004\n0.003\n0.002\n0.001\n0.000\nPiano Solo\n100 0 200 300 400 500\nPiano Solo\n0.2 0.6 1.0 1.4 0.0 0.4 0.8 1.2 1.6 1.8\n\u03a41040.010\n0.008\n0.006\n0.004\n0.002\n0.0000.003\n0.002\n0.001\n0.000\n100 0 200 300 40050 150 250 350\n 0.2 0.6 1.0 0.0 0.4 0.8 1.2\n\u03a41040.010\n0.008\n0.006\n0.004\n0.002\n0.0000.004\n0.003\n0.002\n0.001\n0.000Time (s)",
                "Both strength contour and summed motion speed can recognize the piano solo section in Mvt.1 and Mvt.2.552 J. Comput.",
                "Fig.6(c) and Fig.6(d) present the examples\nof summed motion speed and strength contour, respec-\ntively, from which the piano solo sections can be clearly\nidenti\fed."
            ],
            "choral": [],
            "orchestral": [
                "Although\nrecent studies have successfully generated motion for singers, dancers, and musicians, few have explored motion generation\nfor orchestral conductors.",
                "2 Related Work\n2.1 Music-Driven Conducting Motion\nGeneration\nMusic-driven conducting motion generation involves\nthe generation of skeleton sequences of an orchestral\nconductor according to a given piece of music.",
                "[25] Dansereau D G, Brock N, Cooperstock J R. Predicting\nan orchestral conductor's baton movements using machine\nlearning.",
                "[51] Huang Y, Chen T, Moran N, Coleman S, Su L. Identifying\nexpressive semantics in orchestral conducting kinematics."
            ],
            "electronic": [],
            "pop": [
                "For its part, our proposed approach has two\nlearning stages: we \frst obtain an optimal M2S-Net,\nand then apply it to M2S-GAN.\n5.2 Sync Loss vs Perceptual Loss\nPerceptual loss[61]is a popular choice in many ill-\nposed image manipulation tasks.",
                "In a study of music-driven dance\ngeneration, Ren et al.[1]proposed pose perceptual loss,\nwhere a motion encoder pre-trained on dance genre\nclassi\fcation (distinguishing ballet, pop, and hip-hop\ndance) was used as the perceptual loss network."
            ],
            "Demo availability": [],
            "dataset": [
                "To verify the e\u000bectiveness of our method,\nwe construct a large-scale dataset, named ConductorMotion100, which consists of unprecedented 100 hours of conducting\nmotion data.",
                "In addition, we \fnd that existing conducting mo-\ntion datasets are too small to train a generative deep\nlearning model.",
                "Thus, we collect and construct a large-\nscale conducting motion dataset.",
                "The constructed dataset, named\nConductorMotion100, has 100 hours of conducting mo-\ntion data and aligned Mel spectrograms.",
                "Its scale\nsigni\fcantly exceeds that of existing conducting mo-\ntion datasets.",
                "2) We collect and construct a large-scale conduct-\ning motion dataset, ConductorMotion100, based on\nadvanced object detection and pose estimation tech-\nniques.",
                "ConductorMotion100 contains 100 hours of\nconducting motion and aligned music data; its scale\nsigni\fcantly exceeds that of existing conducting motion\ndatasets.",
                "Both the dataset ConductorMotion100 and the ex-\nperimental codes are open-sourced1\u25cb.",
                "3 Data Preparation\nDue to the scale of existing conducting motion\ndatasets being insu\u000ecient to train a deep genera-\ntive model, we construct a large-scale conducting mo-\ntion dataset, named ConductorMotion100, by deploy-\ning pose estimation on conductor view videos of con-\ncert performance recordings collected from online video\nplatforms.",
                "As\nshown in Table 1, its scale far exceeds that of exist-\ning conducting motion datasets.",
                "To facilitate related\nresearch, the dataset is made public4\u25cb.",
                "Comparison on the Scale of Conducting Motion Datasets\nYear Dataset Length (min)\n2013 Saras\u0013 ua et al.[48]120.0\n2013 Dansereau et al.[25]0.5\n2014 Saras\u0013 ua and Guaus[49]250.0\n2017 Karipidou et al.[50]36.0\n2019 Huang et al.[51]180.0\n2019 Lemouton et al.[52](IDEA dataset) 56.0\n2021 Ours (ConductorMotion100) 6 000.0\n4\u25cbhttps://github.com/ChenDelong1999/VirtualConductor, Mar. 2022.Fan",
                "Therefore, we \frst annotate a small object detec-\ntion dataset, Concert300, and \fne-tune a pre-trained\nYOLO-V3[20]to recognize which human is the conduc-\ntor.",
                "Formally, the ConductorMotion100 dataset can be\ndescribed asD=f(Xi;Yi)gN\ni=1, where Xi=fxtgTx\ni\nt=1\nandYi=fytgTy\ni\nt=1are thei-th Mel spectrogram and\nconducting motion sequence respectively.",
                "In Fig.2 ,\nwe present the visualization of a sample ( X;Y) in the\nConductorMotion100 dataset; the sample corresponds\nto the \fnal part of Tchaikovsky's 1812 Overture.",
                "Visualization of a sample in the ConductorMotion100\ndataset.",
                "4.2 Overview of Proposed Approach\nFormally, given the ConductorMotion100 dataset D,\nour goal is to learn a music encoder Emusic and a gener-\natorGto predict a motion sequence from a given Mel\nspectrogram Xand random zsampled from a normal\ndistribution, i.e., ^Y=G(Emusic(X);z).",
                "Training Procedure of M2S-Net and M2S-GAN\nInput : datasetD=f(Xi;Yi)gN\ni=1; loss function weights \u0015adv,\u0015sync,wGP;\nOutput : trained music encoder Emusic , generator G;\n/",
                "During training,\nthe positive and negative pairs are automatically sam-\npled from the dataset.",
                "In addition, their method is computationally ine\u000ecient,\nespecially when facing our large-scale ConductorMo-\ntion100 dataset.",
                "knowledge of the music motion relationship is learned\nfrom the large dataset by the model itself.",
                "In addition, we construct a large-scale conducting\nmotion dataset, ConductorMotion100, which contains\nan unprecedented 100 hours of conducting motion and\ncorresponding music data.",
                "The ConductorMotion100\ndataset enables M2S-GAN to learn rich music seman-\ntics.",
                "Since the scale of ConductorMotion100 is also\nlarger than many datasets for music information re-\ntrieval (MIR) tasks, in future, we will also validate the\ne\u000bectiveness of using it as a pretraining dataset for MIR\ntasks, such as beat tracking and tempo estimation.",
                "On our collected ConductorMo-\ntion100 dataset, the proposed method achieved 0.049\nRDE and 2.046 SCE, outperforming all the compared\nmethods.",
                "[52] Lemouton S, Borghesi R, Haapam\u007f aki S, Bevilacqua F, Fl\u0013 ety\nE. Following orchestra conductors: The IDEA open move-\nment dataset."
            ]
        },
        {
            "title": "Integration of a music generator and a song lyrics generator to create Spanish popular songs",
            "architecture": [],
            "training parameters": [],
            "learning rate": [],
            "batch size": [],
            "ethical": [],
            "impact": [
                "In order to check the impact of the seed words used for \nthe generation on the lyrics, Table\u00a0 5 presents the validation \nnumbers according to the semantic domain.",
                "This suggests that the selection of the seeds does not have an impact on the meaning only, but also on the rhythm, even if indirectly, because seeds will constrain the semantic network, which is tightly related to the gram-mar."
            ],
            "society": [
                "Keywords Computational creativity\u00a0\u00b7 Music generation\u00a0\u00b7 Lyrics generation\n1 Introduction\nPopular music has had a great influence in society, since it \nhas been transmitted orally from generation to generation.",
                "In: Audio engineering society conference: 2019 AES international conference on immersive and interactive audio, audio engineer -\ning society\nZhu H, Liu Q, Yuan NJ, Qin C, Li J, Zhang K, Zhou G, Wei F, Xu Y, \nChen E (2018)"
            ],
            "copyright": [],
            "evaluation metrics": [],
            "metric": [
                "Even though poetic text typically follows metrical constraints, song lyrics must address these more tightly because lyrics aim to be sung.",
                "It is based on the dot system\u00a0(Lerdahl and Jackendoff 1983), which sets the metrical accents of each beat inside a bar, and thus the strengths of each note, according to their position."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "Briefly, they reflect that, on the one hand, the melodies transmit a feeling of Spanish popular music, and on the other hand, the text of the lyrics is related to the topics analyzed, and the rhythm follows the melodic aspects of the music.",
                "The results of the \nanalysis are used as the training corpus by the Markov mod-els, which will generate music in the popular style, further described in Sect.\u00a0 3.3.\n3.1  Music retrieval\nThe resulting platform aims at the generation of music based on the features of the Spanish popular music, where, accord-ing to ethnomusicologists\u00a0(Manzano\u00a0Alonso 2001), three main factors should be considered:\n\u2013 Melodic behavior: Unlike the tonal music, which is based \nin minor and major modes, Spanish popular music makes use of seven modes, each built from the seven notes of the natural scale, starting from different notes.",
                "The melodic behavior consists of continuous chromatizations and instabilities.",
                "Unlike classical music, in popular music the harmonic tension and the use of the chords degrees are not particularly relevant, as it does not follow harmonic rules; they are only used according to the melodic course.",
                "More precisely, the melodic line could be played by a piano, the lyrics were shown below in text, and there was also a picture with the score with lyrics included, as shown in Fig.\u00a0 5.",
                "The 60% of the Fig. 5  Screenshot of the valida-\ntion form to analyze melody \nand lyrics\n4432 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\nusers considered that the music adapts to the popular music \nstandards well or very well, as does the melodic rhythm, where the 54% of the users evaluated this item as good or very good.",
                "For this purpose, a new listening test was performed to retrieve users\u2019 opinion on the quality of musi-cal features such as the sound, the rhythm and the melodical behavior, and the quality of the rhythm and semantics of the lyrics.",
                "Diputa-\ncion de Burgos\nMonteith K, Martinez TR, Ventura D (2012) Automatic generation of \nmelodic accompaniments for lyrics."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Vol.:(0123456789)1 3Journal of Ambient Intelligence and Humanized Computing (2020) 11:4421\u20134437 \nhttps://doi.org/10.1007/s12652-020-01822-5\nORIGINAL RESEARCH\nIntegration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create \nSpanish popular songs\nMar\u00eda\u00a0Navarro\u2011C\u00e1ceres1 \u00a0\u00b7 Hugo\u00a0Gon\u00e7alo\u00a0Oliveira2\u00a0\u00b7 Pedro\u00a0Martins2\u00a0\u00b7 Am\u00edlcar\u00a0Cardoso2\nReceived: 7 December 2018 / Accepted: 19 February 2020 / Published online: 11 March 2020 \n\u00a9 Springer-Verlag GmbH Germany, part of Springer Nature 2020",
                "This work develops ETHNO-MUSIC, an intelligent system that gen-erates melodies based on popular music.",
                "ETHNO-MUSIC generates melodies with Markov models, which learns from a corpus of Spanish popular music.",
                "Briefly, they reflect that, on the one hand, the melodies transmit a feeling of Spanish popular music, and on the other hand, the text of the lyrics is related to the topics analyzed, and the rhythm follows the melodic aspects of the music.",
                "Keywords Computational creativity\u00a0\u00b7 Music generation\u00a0\u00b7 Lyrics generation\n1 Introduction\nPopular music has had a great influence in society, since it \nhas been transmitted orally from generation to generation.",
                "Unlike classical music, Spanish popular music is always \nlinked to a functionality, meaning the purpose for which the melody was conceived.",
                "Consequently, most of the existing repertoire include the use of lyrics, which in fact, is one of the most important factors in the popular culture.",
                "Although the popular melody usually follows the lyrics structure and rhythm, it also hap-pens to the contrary, lyrics that are adapted to a melody cre-ated beforehand.",
                "This case is specially interesting to analyze what kind of words are chosen to be part of the lyrics, what topics are more common and how they adapt to the popular tone.",
                "Years of ethnomusicological education are often required \nto master the idiosincracies of the modal system and under -\nstand the vocal music in a popular context.",
                "Additionally, these proposals usually generate music following the lyrics structure, whereas in \n * Mar\u00eda Navarro-C\u00e1ceres \n maria90@usal.es\n1 Expert Systems and\u00a0Applications Laboratory (ESALab), \nDepartment of\u00a0Computer Sciences, Universidad de Salamanca, Salamanca, Spain\n2 CISUC, Department of\u00a0Informatics Engineering, University of\u00a0Coimbra, 3004-504\u00a0Coimbra, Portugal4422 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\nthis work we pretend to adapt lyrics once the melody is con-\nstructed and cannot be changed, as it commonly occurs in the popular tradition.",
                "Moreover, the initiatives have usually focused on the generation of music of a rather tonal/classi-cal character and, to date, there has been no relevant studies addressing the generation of Spanish popular music.",
                "For this purpose, we present a system capable to generate \nmusic and lyrics whithin a popular context.",
                "The contribu-tion is precisely how the popular music is integrated in two systems and adapted to generate popular songs successfully.",
                "For this purpose, new melodies are generated following the style of original Spanish popular songs.",
                "MMs are trained in a \ncorpus and then used to generate a new melody that fits the style of popular songs.",
                "Tra-la-lyrics has been adapted to the Spanish language \nthrough an augmented semantic network and new line tem-plates, collected automatically from the lyrics of Spanish popular songs.",
                "To imitate this behavior and set the generation domain, seed words were carefully selected according to the common sets that appear in popular songs.",
                "Once the melodies are generated, a listening test was \ndeveloped to evaluate the musical quality according to the Spanish popular music standards and to collect the users\u2019 opinion about the usefulness of the system to interact with them and generate music.",
                "The evaluators had to score on the one hand, the quality of the melody, sound and rhythm according to the Spanish popular music standards.",
                "Section\u00a0 3 describes the gen-\neration process to create popular melodies.",
                "However, they are usually \nthought to generate classical or jazz music automatically, and not as a guide to generate popular songs.",
                "Recently, com-panies as Google have been significantly developed projects 4423 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nsuch as Magenta (Project 2018), which uses convolutional \nneural networks to create melodies.",
                "Although other techniques could have been adopted for both music and lyrics generation, we decided to tackle this goal with two systems that were familiar to us \u2014 ETHNO-MUSIC and Tra-la-Lyrics\u00a02.0 \u2014 and focus on their integration for the generation of Spanish popular songs.",
                "Other reasons that lead to this decision include the nature of ETHNO-MUSIC, developed with Spanish popular music in mind, and the ease of integrating and adapting Tra-la-Lyrics\u00a02.0 to this domain, given that it is built on top of PoeTryMe, a flexible poetry generation platform, already adapted to many different sce-narios, including the generation in three languagues\u00a0(Gon-\u00e7alo\u00a0Oliveira et\u00a0al. 2017): Portuguese, Spanish and English.",
                "3  System description\nFigure\u00a0 1 gives an overview of the overall system to create \nSpanish popular music.",
                "The system that generates music, ETHNO-MUSIC, \nis provided with a memory to store beforehand different melodies in the popular music style.",
                "These music files are retrieved from a wide variety of popular music melodies 4424 M.",
                "The results of the \nanalysis are used as the training corpus by the Markov mod-els, which will generate music in the popular style, further described in Sect.\u00a0 3.3.\n3.1  Music retrieval\nThe resulting platform aims at the generation of music based on the features of the Spanish popular music, where, accord-ing to ethnomusicologists\u00a0(Manzano\u00a0Alonso 2001), three main factors should be considered:\n\u2013 Melodic behavior: Unlike the tonal music, which is based \nin minor and major modes, Spanish popular music makes use of seven modes, each built from the seven notes of the natural scale, starting from different notes.",
                "\u2013 Rhythm: Popular music usually resorts to uniform beat due to the syllabic text used in the lyrics, or their func-tionality, created to dance or to follow rituals.",
                "Generally, it is very difficult to understand the specific fea-tures that some songs can share in popular music, since this type of music has been in constant evolution due to its char -\nacteristic grammar and its oral transmission from generation to generation.",
                "In order to obtain this information and preserve popular \nmusic, some ethnomusicologists have visited different geo-graphical zones, transcribing folk songs that people sing in rituals or traditional holidays.",
                "By analyzing many of the melodies collected over the \nyears by different ethnomusicologists and in different songs, we can draw some conclusions in this regard that allow us to discern the popular music.",
                "According to some authors (Schindler 1991), in Spanish popular music, work, love \nsongs and lullabies share features related to the key sig-nature, rhythmic patterns and general sonority or tonality, which makes them very interesting to use as a corpus in the development of the learning model.",
                "But to identify the popular music, we should not only analyze the particular duration or pitch.",
                "Unlike classical music, in popular music the harmonic tension and the use of the chords degrees are not particularly relevant, as it does not follow harmonic rules; they are only used according to the melodic course.",
                "Draw -\ning on these properties and also inspired by the concept of viewpoints exposed by Whorley et\u00a0al. (2013), the following features of the popular songs were selected:\n\u2013 Pitch: musical note.",
                "MemoryRetrieve \nFeature s\nSongs\nMarkov \nModel \nTraining\nCorpus \nRetrieval\nGeneration \nProcess\nNew MelodyListening \nProcess\nInteraction \nProcess\nFig. 1  Overview of ETHNO-MUSIC4425 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n\u2013 Time signature: it is a rate that represents the time signa-\nture of the melody.",
                "In our case, the popular music does not have so many musical resources as classical music, for example, which makes the number of states of the MM to decrease.",
                "Typically, musical and text phrases in Spanish popular \nmusic are quite short in order to make them easier for people to learn and start singing right away.",
                "The generation of popular Spanish songs results from the integration of two creative systems: ETHNO-MUSIC, in charge of generating melodies, and Tra-la-Lyrics\u00a0(Gon-\u00e7alo\u00a0Oliveira 2015), in charge of generating suitable lyrics.",
                "As we described in the previous section, music is gener -\nated based on the features of Spanish popular music, which include the rhythm and the sonority.",
                "Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n4.1.4  Line generation\nPoeTryMe has line generation modules for producing text \nfragments in Portuguese, English and Spanish, based on a semantic network and a grammar with templates, tightly connected to the relation types in the network.",
                "Moreover, for the creation of the grammar, a set of about 280 Spanish popular song lyrics, transcribed for this purpose, was exploited, in addition to the 400 poems, thus increasing variation in the produced text.",
                "To complete the adaptation, lyrics had to be generated with seed words related to concepts typically invoked in Spanish popular lyrics.",
                "On the other hand, we aim to validate the musical and lyrics results, meaning the vocal songs generated can follow the style of the Spanish popular\u00a0music.",
                "Consequently, the musical results should also be acceptable for our potential users or listeners from a popu-lar musical point of view.",
                "L\u00f3pez-Ortega and L\u00f3pez-Popa (2012) discuss the quality of the system theoretically applying some creative concepts, such as deliberation and spontaneity.",
                "For the generation of the music, 280 popular songs were \nselected.",
                "In a second stage, we produced several songs to perform a listening test and retrieve a subjective evaluation of the popular music and the lyrics generated.",
                "Finally, in order to \nknow the popular features that they contain and how they were generated, some of the examples produced are shown and analyzed in Section\u00a0 5.3.\n5.1  Analyzing the\u00a0interaction with\u00a0the\u00a0user\nIn the original ETHNO-MUSIC, during the generation pro -\ncess, each iteration of the system consists of adding of a new note in the melody assisted by the mouse position and the MM, and it is iterated until the user decides to stop.",
                "In percentages, for this listening test we have a relative standard error of 21.18%, which can be considered admissible for such sub-jective tests.4429 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nThe listening tests always include some level of subjec-\ntivity, as usually depends on the culture or even the mood \nof the people involved.",
                "To minimize the subjectivity of our evaluation, we looked for expert users in Spanish popular music.",
                "These objec-tive features include similarity between the music generated and the Spanish popular music style, similarity of lyrics, or significance, always from the popular music perspectives.",
                "These experts were asked if they think the melodies generated follows the standards of popular music according to the following items:\n\u2013 Melody: how pleasant is the melody?\n\u2013 Sound: how well does the melody, in some way, give a \nfeeling of the popular style of the songs?\n\u2013 Rhythm of the melody: how well does the rhythm suit the popular music style?",
                "The statistical analysis suggests that the sys-\ntem can generate melodies with a good musical quality and that captures the style of the Spanish popular melodies.",
                "The rhythm is quite diffi-cult to capture, even when the user tries to make changes throught the device, as the the melodies in popular music often use very regular figures and it is difficult to extract new ones.",
                "5.2  Analyzing the\u00a0melody and\u00a0lyrics\nFor the generation of lyrics, melodies were split into phrases using \nminP=8 and maxP=16 , because Spanish popular \nsongs typically use lines of 8 or 16 syllables.",
                "Given their commonality among Spanish popular music, the domains of sleep \u00a0(common in lullabies) and love\u00a0(used in love songs) \nwere used for lyrics generation, and represented by the fol-lowing groups of seed words:\n\u2013 Sleep: dormir, cuna, beb\u00e9, pa\u00f1al, noche, mam\u00e1, coco, \nsue\u00f1o, so\u00f1ar, estrellas, luna (in English, to sleep, cradle, \nbaby, diaper, night, mummy, poo, dream, to dream, stars, moon);Table 1  Statistics resulting from \nthe listening test results/u1D7122 MeMo\nMelody 6.238e \u2212044 4\nSound 2.136e \u2212024 4\nRhythm 5.641e \u2212034 3Table 2  Final statistics after the users finished testing the system\nEasy to use Interface Control \nQualityOverall Ratings\nMode 4 3 4 4\nMedian 4 3 4 34430 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\n\u2013 Love: amor, novia, moza, mozo, bella, belleza, feliz, \nalcoba, morena, guapa, sonrisa, ojos, bonito, bonita  \n(in English, love, girlfriend, girl, lad, beautiful, beauty, \nhappy, bedroom, brunette, pretty, smile, eyes, pretty).",
                "However, our proposal includes important novelties, such as the use of Spanish language and the use of a new corpus of folk songs, unlike the rest of the works, which are centered in English classical and pop music.",
                "Additionally, we have to take into account that the Mi mode is more frequent than La mode in Spanish popular music.",
                "Among the possible options, Spanish popular music commonly has times of 6/8, 4/4, 3/4 and 2/4.",
                "A total of 17 users with musical knowledge about popular \nmusic (more than 4 years of experience in popular music, or students of Musicology) were asked to answer and online form with questions about the melodies generated and how they follow the standards of popular music according to the sonority and the rhythm.",
                "Sound: how well does the melody, in some way, give a \nfeeling of the popular style of the songs?",
                "Rhythm of the melody: how well does the rhythm suit the popular music style?\n4.",
                "Lyrics and music generation X X X\nIncludes folk songs X \u2212 \u2212\nGenerating music before lyrics X \u2212 \u2212\nLearning Machines X X X\nUse of Markov models X \u2212 X\nInteraction with the users \u2212 \u2212 X\nUse of Spanish language X \u2212 \u22124431 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\n5.",
                "We expect the system to reflect the perceptual quality of \nthe melodies according to the popular songs style.",
                "The 60% of the Fig. 5  Screenshot of the valida-\ntion form to analyze melody \nand lyrics\n4432 M.\u00a0Navarro -C\u00e1ceres et al.\n1 3\nusers considered that the music adapts to the popular music \nstandards well or very well, as does the melodic rhythm, where the 54% of the users evaluated this item as good or very good.",
                "We selected those ones that present different popular features, like different time signatures, scales and lyrics.",
                "All the melodies generated share representative features of popular music, such as the constant repetition of pitches and the limited tes-situra.",
                "The melody represents a good example of popular music, as the median and modes obtained in the listening test of 4, even if it is not very pleasant according to the evalua-tions, with a mode and a median of 3.\nLyrics for this song were generated with the sleep-related \nseeds, which is clear by the presence of words like cuna\u00a0(cra-dle), dormir\u00a0(to sleep), sue\u00f1o\u00a0(dream) or so\u00f1ar\u00a0(to dream), \nfrom the seed set, and also other related words, such as som-noliento\u00a0(sleepy), dormido\u00a0(asleep), durmiendo\u00a0(sleeping), \nor sentir \u00a0(to feel, an hypernym of to dream).",
                "Exceptions occur in the word som-no-lien-to and in one of the occurrences of so-\u00f1ar, for which the two last Table 4  Overall validation \nresults for the 10 assessed songsItem Rating Mo Md\n1 2 3 4 5\nMelody 0 13 42 47 18 4 4\nSound 3 4 41 59 13 4 4\nRhythm (Melody) 0 8 47 52 13 4 4\nRhythm (Lyrics) 2 8 45 41 24 3 4\nSubject 5 12 25 53 24 4 4\nText Meaning 20 6 32 50 11 4 4\nOverall Quality 1 14 52 41 12 3 3\nTable 5  Validation of the lyrics for the 10 assessed songs, according \nto the semantic domain\nItem Sleep Love\nMo Md Mo Md\nRhythm (lyrics) 3 3 4 4\nSubject 4 4 4 4\nText meaning 4 4 4 4\nOverall quality 3 3 3 34433 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nFig.",
                "However, the melody behaves like the popular music, with rests in the second and third degrees, a typical feature of modal (and popular) music, in this case, of the Mi Mode.",
                "This happens for a series of reasons, including that this pattern has eight Fig. 8  Song in La mode, with \nSpanish lyrics on the domain \n\u2018amor\u2019\u00a0(love), including rough English translation\nyall\u00b4\u0131triste cantaba, disc\u00b4\u0131puloys eguido r\nnos\u00b4equ\u00b4eama rte, amo r\nyall\u00b4\u0131triste cantaba, disc\u00b4\u0131puloys eguido r\natusarasyalta r\namant eyamo rand there wassadly singin g\ndisciple and follo wer\nand there wassadly singin g\ndisciple and follo wer\ntoyouraltarsand altar\nlover and love\n4435 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nsyllables, a common length for musical phrases in Spanish \npopular music and half the value of minP, which becomes closer to minP with its variable part\u00a0(in this case, moza y menor ); and it can be used for three different semantic \nrelations.",
                "Additionally, it presents an integration of ETHNO-MUSIC and Tra-La-Lyrics to add lyrics to the melodies also following the style of the Spanish popular music.",
                "However, despite the users\u2019 indications, we do not avoid to follow the standards of the popular music in the generation process.",
                "Evaluation aimed to demonstrate that both the melody \nand the lyrics share common features with the original Span-ish popular music.",
                "Although there are some open issues in current state of \nthe art, the main goal of this work was the application of different techniques to create popular songs, which is a nov -\nelty by itself.",
                "Finally, the use of MMs as a complement to help the users through a musical composition has been scored as quite interesting, as the users feel included in the composition process, and they are an essential part to create new melodies, preserving the Spanish popular music style.",
                "Given the importance of lyrics in popular music for eth-\nnomusicologist studies, a deeper analysis of popular songs and their lyrics should be addressed in order to improve the vocabulary and semantics of the lyrics generator.",
                "The MIT Press, Cambridge\nL\u00f3pez-Ortega O, L\u00f3pez-Popa SI (2012) Fractals, fuzzy logic and \nexpert systems to assist in the construction of musical pieces.",
                "Bolet\u00edn \nInformativo de la Fundaci\u00f3n Juan March 204:3\u201318\nManzano\u00a0Alonso M (2001) Cancionero popular de burgos.",
                "M\u00fasica y poes\u00eda popular de Espa\u00f1a y Portugal.",
                "Centro de Cultura Tradicional4437 Integration of\u00a0a\u00a0music generator and\u00a0a\u00a0song lyrics generator to\u00a0create Spanish popular songs  \n1 3\nScirea M, Togelius J, Eklund P, Risi S (2016)",
                "Xiaoice band: A melody and arrangement genera-tion framework for pop music."
            ],
            "Demo availability": [],
            "dataset": []
        },
        {
            "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
            "architecture": [
                "ReghunathandRajan EURASIPJournalonAudio,Speech,andMusic\nProcessing         (2022) 2022:11 \nhttps://doi.org/10.1186/s13636-022-00245-8\nEMPIRICAL RESEARCH OpenAccess\nTransformer-basedensemblemethod\nformultiplepredominantinstruments\nrecognitioninpolyphonicmusic\nLekshmiChandrikaReghunath*and RajeevRajan\nAbstract\nMultiplepredominantinstrumentrecognitioninpolyphonicmusicisaddressedusingdecisionlevelfusionofthree\ntransformer-basedarchitecturesonanensembleofvisualrepresentations.",
                "Weexperimentedwith\ntwotransformerarchitectureslikeVisiontransformer(Vi-T)andShiftedwindowtransformer(Swin-T)fortheproposed\ntask.",
                "Awavegenerative\nadversarialnetwork(WaveGAN)architectureisalsoemployedtogenerateaudiofilesfordataaugmentation.",
                "[ 10]\nanalyzed the architecture of Han et al. in order to for-\nmulateanefficientdesignstrategytocapturetherelevant\ninformation about timbre.",
                "Ourmodelisderivedfrom\n[29]withsomesignificantchangesasdescribedin\nSection4,anditoutperformstheexistingmodels,\nincluding[ 1].Theefficacyoftransformermodelsand\nattentionmechanismsaredemonstratedby\ncomparisonwithCNNandDNNarchitectures.",
                "Theprincipleofautocorrelationis\nusedtoestimatethetempoateverysegmentinthenovelty\nfunction[ 37].Autocorrelationtempogramsarecomputed\nwithlibrosa.feature.tempogram using a 2048 point FFT\nwindowandahopsizeof512.\n4 Modelarchitectures\n4.1 DNN\nA DNN framework on musical texture features (MTF)\nis experimented with to examine the performance of\ndeeplearningmethodologyonhandcraftedfeatures.",
                "4.2 CNN\nCNN uses a deep architecture with repeated convolu-\ntions followed by max-pooling.",
                "The last convolutional layers used 3 \u00d73\nfilters as later layers reveal more specific and complex\n1https://librosa.org/doc/latest/tutorial.htmlReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page5of14\nFig.2(a)BlockdiagramoftheproposedmethodofVisiontransformer,(b)Internalarchitectureoftransformerencoder\npatterns and final layers activations help to recognize the\npredominant instruments from accompaniments.",
                "Figure 2shows the architecture of our\nproposedmethod.",
                "Unlike other\ntransformers Swin-T [ 29] has a hierarchical architecture\nandhaslinearcomputationalcomplexitythroughthepro-\nposedshiftedwindow-basedself-attentionapproach.",
                "Figure 3shows the architecture of our\nproposed method.",
                "Figure3(b) shows the internal architecture of the Swin-T\nblock.",
                "All audio files in the IRMAS dataset are\nFig.3(a)BlockdiagramoftheproposedmethodofSwintransformer,(b)InternalarchitectureofSwin-TblockReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page7of14\nin a 16-bit stereo .wav format with a sampling rate of\n44,100 Hz.",
                "In WaveGAN architecture, the transposed convolution\noperation is modified to widen its receptive field.",
                "Thediscriminatorisalsomodifiedsimilarly,usinglength-\n25 filters in one dimension and increasing stride from\ntwo to four which results in WaveGAN architecture [ 43].The transposed convolution in the generator produces\ncheckerboard artifacts [ 43].",
                "6.3 Effectoftransformerarchitectureandattention\nThe instrument-wise F1 scores for all the Mel-\nspectrogram experiments are shown in Fig.",
                "Ontheotherhand,experiments\nwith transformer architecture showed improved perfor-\nmance for all the instruments.",
                "This is mainly because\nthe transformer architecture with a multi-head attention\nmechanism helps to focus or attend to specific regions of\nthe visual representation for predominant instruments\nrecognition.",
                "Wealsoconductedanablationstudyofthearchitecture\nin order to gain a better understanding of the network\u2019s\nbehavior.",
                "The results are tabulated\ninTable5.TheoptimalparametersobtainedthroughMel-\nspectrogram analysis are applied to the modgdgram andtempogramarchitecturesthroughasimilarablationstudy.",
                "To summarize, the results show the potential of Swin-T\narchitectureandthepromiseofalternatevisualrepresen-\ntationsotherthantheconventionalMel-spectrogramsfor\npredominantinstrumentsrecognitiontasks.",
                "Thisshows\nthe importance of phase information in the proposed\nTable5AblationstudyoftheMel-spectrogramarchitectureshowingtheeffectofnumberofheads,patchsize,projectiondimension,\nandnumberofMLPnodes.",
                "Highestvaluesarehighlighted\nSL.No ArchitectureSpec.",
                "[10] customized the architecture of Han et al. and intro-\nduced two models, namely, single-layer and multi-layer\napproaches.",
                "We experimented with Vi-T and the\nrecentSwin-Tarchitectureswithadetailedablationstudy\nand our proposed experiments using Swin-T outperform\nexistingalgorithmswithverylesstrainableparameters.",
                "Musicalinstrumentrecognitioninuser-generatedvideosusing\namultimodalconvolutionalneuralnetworkarchitecture,(2017),\npp.226\u2013232."
            ],
            "training parameters": [],
            "learning rate": [
                "A constant learning rate of 0.0001 is used with\n\u03b21=0.5and \u03b22=0.9.",
                "For Vi-T, we used categorical cross-entropy loss function\nusing Adam optimizer, with a learning rate of 0.001 and\nweightdecayof0.0001,andthemini-batchsizewassetto\n256."
            ],
            "batch size": [
                "We choose a threshold value of 0.5 empir-\nically as it helps to recognize most of the predominant\ninstruments[ 1].\n5.3.1 Trainingconfiguration\nThe DNN network is trained with categorical cross-\nentropylossfunctionusingAdamoptimizerwithalearn-\ning rate of 0.001 and a mini- batch size of 128.",
                "For CNN\nnetworks, we choose a batch size of 128 and an Adam\noptimizer with a categorical cross-entropy loss function.",
                "For Swin-T we used categorical cross-entropy using\ntheAdamoptimizer,withalearningrateof0.001andgra-\ndient clip value of 0.5, and the mini-batch size was set to\n32."
            ],
            "ethical": [],
            "impact": [],
            "society": [
                "J.J.Bosch,J.Janer,F.Fuhrmann,P.Herrera,in Proc.ofthe13th\nInternationalSocietyforMusicInformationRetrievalConference,ISMIR,Porto,\nPortugal.",
                "S.Gururani,C.Summers,A.Lerch,in Proc.of19thInternationalSocietyfor\nMusicInformationRetrievalConferenceParis,France.",
                "J.S.G\u2019omez,J.Abe\u00dfer,E.Cano,in Proc.ofthe19thInternationalSocietyfor\nMusicInformationRetrievalConference,ISMIR,Paris,FranceSeptember23-27,\n2018.Jazzsoloinstrumentclassificationwithconvolutionalneural\nnetworks,sourceseparation,andtransferlearning,(2018),pp.577\u2013584.\nhttps://doi.org/10.5281/zenodo.1492481\n14. X.Li,K.Wang,J.Soraghan,J.Ren,in ProcofInternationalConferenceon\nComputationalIntelligenceinMusicSoundArtandDesign(PartofEvoStar) .",
                "M.Muller,T.Pratzlich,J.Driedger,in Proc.of13thInternationalSocietyfor\nMusicInformationRetrievalConference(ISMIR),Porto,Portugal,October\n8th-12th,2012 .Across-versionapproachforstabilizingtempo-based\nnoveltydetection,(2012),pp.427\u2013432\n23."
            ],
            "copyright": [
                "Ifmaterialisnotincludedinthearticle\u2019sCreativeCommonslicenceandyourintended\nuseisnotpermittedbystatutoryregulationorexceedsthepermitteduse,youwillneedtoobtainpermissiondirectlyfromthe\ncopyrightholder."
            ],
            "evaluation metrics": [],
            "metric": [
                "Thesemetricsare3.12%and12.72%relativelyhigherthanthose\nobtainedbythestate-of-the-artHan\u2019smodel.",
                "WaveGAN is trained for 2000 epochs on the three-sec\naudio files of each class to generate similar audio files\nbased on a similarity metric ( s)[45] with an acceptance\ncriterion of s>0.1.",
                "For the micro\naverages, we calculated the metrics globally, thus giving\nmore weight to the instrument with a higher number of\nappearances.",
                "While the proposed\nmethod of Voting-Swin-T, without data augmentation(TrainDB), reports micro and macro F1 score of 0.59 and\n0.60, respectively, the metrics improved to 0.66 and 0.62,\nrespectively, for the data augmentation scheme.",
                "6.5 Comparisontoexistingalgorithms\nThe performance metrics for various algorithms on the\nIRMAS corpus are reported in Table 7."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [
                "J.S.G\u2019omez,J.Abe\u00dfer,E.Cano,in Proc.ofthe19thInternationalSocietyfor\nMusicInformationRetrievalConference,ISMIR,Paris,FranceSeptember23-27,\n2018.Jazzsoloinstrumentclassificationwithconvolutionalneural\nnetworks,sourceseparation,andtransferlearning,(2018),pp.577\u2013584.\nhttps://doi.org/10.5281/zenodo.1492481\n14. X.Li,K.Wang,J.Soraghan,J.Ren,in ProcofInternationalConferenceon\nComputationalIntelligenceinMusicSoundArtandDesign(PartofEvoStar) ."
            ],
            "choral": [],
            "orchestral": [
                "*Correspondence: clekshmir04@gmail.com\nDepartmentofElectronicsandCommunicationEngineering,Collegeof\nEngineeringTrivandrum,APJAbdulKalamTechnologicalUniversity,\nTrivandrum,IndiaThe task of identifying the leading instrument in poly-\nphonic music is challenging due to the presence of inter-\nferingpartialsintheorchestralbackground."
            ],
            "electronic": [
                "*Correspondence: clekshmir04@gmail.com\nDepartmentofElectronicsandCommunicationEngineering,Collegeof\nEngineeringTrivandrum,APJAbdulKalamTechnologicalUniversity,\nTrivandrum,IndiaThe task of identifying the leading instrument in poly-\nphonic music is challenging due to the presence of inter-\nferingpartialsintheorchestralbackground."
            ],
            "pop": [],
            "Demo availability": [],
            "dataset": [
                "The\nproposedsystemissystematicallyevaluatedusingtheIRMASdatasetwithelevenclasses.",
                "Both approaches were trained\nand validated by the IRMAS dataset of polyphonic music\nexcerpts.",
                "It was found that\nbothsourceseparationandtransferlearningcouldsignif-\nicantly improve the recognition performance, especially\nfor a small dataset composed of highly similar musical\ninstruments.",
                "The pro-\nposedworkin[ 15]employedanattentionmechanismand\nmultiple-instance learning (MIL) framework to address\nthe challengeof weakly labeled instrument recognition in\ntheOpenMICdataset.",
                "All audio files in the IRMAS dataset are\nin a 16-bit stereo .wav format with a sampling rate of\n44,100 Hz.",
                "5 Performanceevaluation\n5.1 Dataset\nThe performance of the proposed system is evaluated\nusing the IRMAS (Instrument Recognition in Musical\nAudio Signals) dataset, developed by the Music Technol-\nogy Group (MTG) of Universitat Pompeu Fabra (UPF).",
                "All audio files in the IRMAS dataset are\nFig.3(a)BlockdiagramoftheproposedmethodofSwintransformer,(b)InternalarchitectureofSwin-TblockReghunathandRajan EURASIPJournalonAudio,Speech,andMusicProcessing         (2022) 2022:11 Page7of14\nin a 16-bit stereo .wav format with a sampling rate of\n44,100 Hz.",
                "IRMAS dataset [ 2] contains separate train-\ning and testing set of eleven classes.",
                "Thisdatasethastwodisadvan-\ntages when training models.",
                "Second, the dataset is not\nwell balanced in terms of either musical genre or instru-\nmentation.",
                "However, this may not be a problem if the\ndatasets were larger and the distribution represented the\nrealworld.",
                "The problem\nwith small datasets is that models trained with them\ndo not generalize well from the validation and test set\n[47].",
                "It reports a micro and macro F1 score of\n0.50 and 0.43 respectively, and it is evident that the pro-\nposedensembleframeworksoutperformthehand-craftedTable7PerformancecomparisononIRMASdataset.",
                "In [15], the usage of an attention layer was\nshown to improve classification results in the OpenMIC\ndataset when applied to a set of Mel-spectrogram fea-\ntures extracted from a pre-trained VGG net.",
                "Our proposed ensemble voting\ntechniqueoutperformedexistingalgorithmsandtheMTF\nDNNandSVMframeworkontheIRMASdatasetforboth\nthemicroandthemacroF1measure.\n7C o n c l u s i o n\nWe presented a transformer-based predominant instru-\nment recognition system using multiple visual repre-\nsentations.",
                "The proposed method is evaluated using the IRMAS\ndataset.",
                "Acknowledgements\nTheauthorswouldliketoacknowledgeJuanJ.Bosch,FerdinandFuhrmann,\nandPerfectoHerrera(MusicTechnologyGroup-UniversitatPompeuFabra)\nfordevelopingtheIRMASdatasetandmakingitpubliclyavailable.",
                "Availabilityofdataandmaterials\nThedatasetsgeneratedand/oranalyzedduringthecurrentstudyareavailable\ninthezenodorepository( https://www.upf.edu/web/mtg/irmas )andare\npubliclyavailable."
            ]
        },
        {
            "title": "Genre Recognition from Symbolic Music with CNNs: Performance and Explainability",
            "architecture": [
                "We propose an architecture for handling MIDI data that makes use \nof multiple resolutions of the input, called Multiple Sequence Resolution Network (MuSeReNet).",
                "These \ndifficulties, among others, have led to the development of \nnew approaches for processing music in both the audio \nand the symbolic domain, many of which take the form of \nspecialized neural network architectures such as WaveNets This article is part of the topical collection \u201cEvolutionary Art \nand Music\u201d guest edited by Aniko Ekart, Juan Romero and Tiago \nMartins.\n *",
                "However, finding an \noptimal architecture and set of hyper-parameters, such as \nnumber of layers, kernel size, number of kernels, etc. for \na given task remains a difficult problem that is still being \ntackled mostly experimentally.",
                "In addition, multiple network architecture search (NAS) \nalgorithms have been proposed in recent years, especially \nfor computer vision, such as Amoebanets [32], but these are \nbeyond the scope of this paper.",
                "The rest of the paper is structured as follows: Sec-\ntion\u00a0\u201c Related Work \u201d presents relevant literature for genre \nrecognition, in Section\u00a0\u201c Preliminaries \u201d we expose the \nreader to important concepts for 1D Convolutional Neural \nNetworks, and for music genres, in Section\u00a0\u201c Architecture\u201d \nwe show the proposed neural architecture, in Sect.",
                "The most up-to-date techniques for the \nclassification of music use neural networks on MFCCs or \nspectrograms [29, 47], some of them focus on feature selec-\ntion [39, 44], or introducing new architectures [22, 26, 51, \n52].",
                "Recently in multiple domains, there is a tendency to forgo \nfeature extraction stages of an information retrieval pipeline, \ninstead using a more complex neural network architecture, in \nwhich the first layers act as feature extractors.",
                "Architecture\nTrees are an effective representation of music since they are \nable to capture information, patterns, and structures at multi -\nple different time scales.",
                "Such an architecture is similar to U-nets [38] which \nis used for image segmentation.",
                "Experiments\nTo explore the effectiveness of our architecture for informa-\ntion retrieval from symbolic music and to check the com -\npatibility of 1D CNNs for the task, along with the effect of \nallocating resources to network depth or to kernel size we \nconducted a set of experiments.1\nData\nFor our experiments, we use the Lakh Pianoroll Dataset \nas presented by Dong et\u00a0al.",
                "The first case represents a \ntraditional CNN architecture, while the second represents \nMuSeReNets in which information flows from the leaves \nto the root (Fig.\u00a0 2).",
                "In general, all CNNs which were trained on sequences \nlonger than 256 samples surpassed Ferraro and Lemstr\u00f6m\u2019s \npattern recognition approach, as well as Liang et\u00a0al. model \nFig. 7  a Sequence architecture and b MuSeRe architecture used in \nexperiments SN Computer Science (2023) 4:106\n 106 Page 8 of 18\nSN Computer Science\nwith regards to F1 metric",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "We plan to further explore this idea, using \nnetwork architecture search to find a good baseline network \nand similarly to [45] find an efficient way to scale up neural \nnetworks for MIDI classification tasks.",
                "Which neural net architectures give rise to exploding \nand vanishing gradients?",
                "Regularized evolution for \nimage classifier architecture search."
            ],
            "training parameters": [],
            "learning rate": [
                "For finding \ntrainable parameters of networks which minimize the cross-\nentropy between predicted genres and real genres we use \nAdam [15] as the optimization algorithm during training \nwith a learning rate of /u1D6FC=10\u22125 and for the other hyper-\nparameters of the optimizer /u1D6FD1=0.9 , /u1D6FD2=0.999  and a batch \nsize of 32."
            ],
            "batch size": [],
            "ethical": [],
            "impact": [
                "Notably, this \napproach does not impact the rhythmic characteristics of \nthe pianoroll, since the only changes are made in the pitch \ndimension, so even if pitches are changed drastically for \nsome samples, they can still be considered to be \u201clocal\u201d."
            ],
            "society": [
                "In: 19th International Society for Music Information \nRetrieval Conference (ISMIR 2018) 2018.",
                "Transactions of the International \nSociety for Music Information Retrieval."
            ],
            "copyright": [
                "If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder."
            ],
            "evaluation metrics": [],
            "metric": [
                "Evaluation and\u00a0Post Processing\nWe evaluate our models on the held-out test set for each of \nthe MASD and topMAGD datasets by computing precision, \nrecall, and micro f1 metric.",
                "In general, all CNNs which were trained on sequences \nlonger than 256 samples surpassed Ferraro and Lemstr\u00f6m\u2019s \npattern recognition approach, as well as Liang et\u00a0al. model \nFig. 7  a Sequence architecture and b MuSeRe architecture used in \nexperiments SN Computer Science (2023) 4:106\n 106 Page 8 of 18\nSN Computer Science\nwith regards to F1 metric"
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [
                "Marsden A. Representing melodic patterns as networks of elabo-\nrations."
            ],
            "harmonic complexity": [],
            "expressiveness": [],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [],
            "orchestral": [],
            "electronic": [
                "It is interesting that genres such as Jazz which have lit-\ntle representation in the dataset are better classified than gen-\nres such as Electronic which has almost double the support.",
                "This could be due to distinguishing musical characteristics \nof each genre, which are apparent in symbolic representa-\ntions of music\u2014for instance, jazz music tends to have com-\nplex harmony and utilize more notes, while electronic music \ntends to contain loops of very few notes.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "We chose (b) and (c) as \nexamples of certain predictions, while (d) shows an errone-\nous prediction by the CNN (Electronic), and a relatively high \nvalue of 0.06 for the vocal genre which is interesting since \nthe introduction of the song features an a capella chorus.",
                "Finally, for Bohemian Rhapsody, the second half of the \npianoroll contributes more towards the Electronic genre, \nafter the piano part is introduced.",
                "The \ntracks are: Beethoven\u2014Moonlight Sonata, The Beatles\u2014Here Comes the Sun, Eminem\u2014The Real Slim Shady, Queen\u2014Bohemian Rhapsody\nBeethoven Beatles Eminem Queen\nInternational 0.69 Pop\u2013Rock 0.83 Rap 0.89 Electronic 0.54\nNew Age",
                "0.40 Jazz 0.06 Electronic 0.03 Pop\u2013Rock 0.20\nRap 0.25 RnB 0.04",
                "Finally, the first measure of the piano part of Bohe-\nmian Rhapsody along with its preceding measure contrib-\nute towards Electronic, while the third measure after the \npiano is introduced contributes towards Pop-Rock .",
                "For Bohemian Rhapsody-Electronic and Moonlight \nSonata-International, which are both erroneous predictions, \nGPX has shown two intervals as important features.",
                "This rule says that if both the minor third \nand the fifth appear at least half as often as the tonic, then \nthe sample is classified as Electronic, which makes intui-\ntive sense since Electronic music tends not to have complex \nharmony, and the rule represents the prevalence of a minor \ntriad in the pitches which appear in the pianoroll.",
                "For genres with more than 100 samples \nin the test set, in which the CNN does not perform well \n(Electronic, Latin, International, Rap) the generated pro-\ntotypes are, as expected, far from representative of each \ngenre, however, they are still useful for gaining insight \non what the CNN has learned.",
                "Spice Girls\u2014Wannabe (Pop-Rock)\nElectronic Toy-Box\u2014Tarzan & Jane (Pop-Rock, Electronic) George Michael - Fast Love (Electronic)\nCountry Session Americana - John Brown (Country) Olivia Newton-John - Everything Love Is (Pop-Rock, Country)",
                "By Me (Pop-Rock, Electronic, Rap) Bobby Brown - Don\u2019t Be Cruel (Pop-Rock, Jazz)\nVocal Nana Mouskouri - Habanera (International, Vocal)",
                "Donny Osmond - This Guy\u2019s In Love With You (Pop-Rock)\nElectronic Siniestro Total - C\u2019est Chic (Pop-Rock) Crystal Waters - 100% Pure Love (Electronic)",
                "International Brasilian Tropical Orchestra - Yesterday (International) Uniting Nations - Uniting Nations (Electronic)"
            ],
            "pop": [
                "The Million Song Dataset is the \nlargest currently available collection of audio features and \nmetadata for a million contemporary popular music tracks.",
                "We arbitrarily chose the \nsmallest kernel size k=9 which has the same number of \ntrainable parameters as a 3\u00d73 kernel which is popular for \ncomputer vision two-dimensional CNNs.",
                "On the one hand, the effect of the imbal-\nanced dataset is apparent in the network\u2019s performance for \nthe most common label (Pop-Rock) when compared to those \nwith fewer files in the dataset such as Blues, Reggae, and \nFolk.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "For Here comes the Sun the first quarter of the \npianoroll seems to contribute more towards a Jazz predic-\ntion, while the third quarter contributes towards a Pop-Rock  \nprediction.",
                "The \ntracks are: Beethoven\u2014Moonlight Sonata, The Beatles\u2014Here Comes the Sun, Eminem\u2014The Real Slim Shady, Queen\u2014Bohemian Rhapsody\nBeethoven Beatles Eminem Queen\nInternational 0.69 Pop\u2013Rock 0.83 Rap 0.89 Electronic 0.54\nNew Age",
                "0.40 Jazz 0.06 Electronic 0.03 Pop\u2013Rock 0.20\nRap 0.25 RnB 0.04",
                "Jazz 0.02 Vocal 0.06\nPop\u2013Rock 0.24 Country 0.037 Vocal 0.003 RnB 0.04\n2 https:// www. reddit.",
                "For \nHere Comes the Sun, the fifth and sixth bars, along with \ntheir repetition four bars later contribute more towards the \nPop-Rock genre.",
                "Finally, the first measure of the piano part of Bohe-\nmian Rhapsody along with its preceding measure contrib-\nute towards Electronic, while the third measure after the \npiano is introduced contributes towards Pop-Rock .",
                "Genetic Programming (GP), in general, generates a random \npopulation and evaluates the fitness of each individual, in \nterms of effectiveness in solving the problem, favoring the \nbetter individuals.",
                "For the genetic programming algorithm, we used a popu -\nlation size of 110 evolved over 110 generations, to attempt \nto mimic the classifier in a local neighborhood of 11,000 \nsamples /u1D702i .",
                "The feature importance \nis calculated as the number of appearances of each feature in \nthe final population, which consists of 110 algebraic expres-\nsions.",
                "For the other two sam-\nples, the best explainer was a constant function; however, the \nexistence of features in the final population can give us some \nmusical insight.",
                "For \nthe local sample set generated around Here Comes the Sun \nand The Real Slim Shady, the CNN classified 0.75 and 0.93 \nas Pop-Rock and Rap  respectively (percentage of positive \n/u1D702i ).",
                "Even though the best explainer for these two samples \nwas (essentially) a constant function, the prevalence of the \ntonic note as an important feature for Rap  classification in \nthe final population makes sense intuitively.",
                "For Here \nComes the Sun, we attribute the failure of GPX to the bias \nof the classifier towards the Pop-Rock genre which is a result \nof dataset imbalance.",
                "Furthermore, the prototype \nfor the Latin genre does not have any characteristics of Latin  \nmusic besides the language and would be considered Pop.",
                "Similarly, the prototype for Rap  could be considered RnB \n(which often contains Rap in modern music), while the \nprototype for New Age  could be considered New Wave/Pop \ninstead.",
                "For instance, the Pop-Rock genre \nis represented by a very diverse set of samples, ranging from \nHard Rock to Disco.",
                "A result of this is that almost half of the Fig. 11  Final programs gener -\nated by GPX as explanations \nfor the top prediction for each \nsample\nSN Computer Science (2023) 4:106 \n Page 15 of 18 106\nSN Computer Science\nselected prototypes and criticisms are labeled as Pop-Rock  \namong other labels.",
                "For \nPop-Rock the prototype chosen by MMD-critic is a power \nballad by Abba which is closer to Pop than Rock, which is \ninteresting when compared to the prototype selected from the ground-truth labels: a Nine Inch Nails song which is a \nlot closer to Rock.",
                "In parentheses are the ground-truth \nlabels\nGenre Test set prototype Test set criticism\nPop-Rock Nine Inch Nails\u2014Piggy (Pop-Rock)",
                "Spice Girls\u2014Wannabe (Pop-Rock)\nElectronic Toy-Box\u2014Tarzan & Jane (Pop-Rock, Electronic) George Michael - Fast Love (Electronic)\nCountry Session Americana - John Brown (Country) Olivia Newton-John - Everything Love Is (Pop-Rock, Country)",
                "If It\u2019s Over (Pop-Rock, RnB)",
                "Procol Harum - A Whiter Shade Of Pale (Pop-Rock, Jazz)",
                "By Me (Pop-Rock, Electronic, Rap) Bobby Brown - Don\u2019t Be Cruel (Pop-Rock, Jazz)\nVocal Nana Mouskouri - Habanera (International, Vocal)",
                "Reggae Johnnie Taylor - For Your Precious Love (Pop-Rock, RnB, Reg-\ngae)The Elgins - When A Man Loves A Woman (Pop-Rock, Coun-\ntry, Latin, Reggae)",
                "Blues Deborah Coleman - Long Time (Pop-Rock, Blues)",
                "In parentheses are the ground-truth labels\nGenre Black-box prototype Black-box criticism\nPop-Rock Abba - The Winner Takes It All (Pop-Rock, Vocal)",
                "Donny Osmond - This Guy\u2019s In Love With You (Pop-Rock)\nElectronic Siniestro Total - C\u2019est Chic (Pop-Rock) Crystal Waters - 100% Pure Love (Electronic)",
                "John Fogerty - Big Train (Pop-Rock)\nRnB Stevie Wonder -",
                "Whitney Houston - So Emotional (RnB)\nJazz Abba - Take A Chance On Me (Pop-Rock) Vince Guaraldi Trio - Christmas Time Is Here (Jazz)",
                "Rap Queen - We Will Rock You (Pop-Rock) Phish - Wading In The Velvet (Pop-Rock)",
                "Vocal Michael Crawford - The Phantom Of The Opera (Pop-Rock, \nVocal)Collin Raye - Little Rock (Country, Vocal)\nNew Age Lionel Richie - Hello (New Age) Enya - China Roses (New Age)",
                "Reggae The Elgins - When A Man Loves A Woman (Pop-Rock, RnB, \nReggae)Johnnie Taylor -",
                "For Your Precious Love (Pop-Rock, RnB, \nReggae)\nBlues Bill Quinn - He\u2019ll Have To Go (Country, Blues) Jim Reeves - He\u2019ll Have To Go (Country) SN Computer Science (2023) 4:106\n 106 Page 16 of 18\nSN Computer Science\nDiscussion\nIn this section, we showed how various explainability meth-\nods may be used in the context of symbolic music classifi-\ncation."
            ],
            "Demo availability": [],
            "dataset": [
                "Through our \nexperiments, we outperform the state-of-the-art for MIDI genre recognition on the topMAGD and MASD datasets.",
                "[4 ] used a machine learning approach, including \na simple neural network, on a custom dataset for successful \ngenre recognition in the symbolic domain.",
                "in [16] compute a \nsequence similarity between all pairs of channels of two \nMIDI files and then use a k-NN classifier for genre recog-\nnition, on a dataset of 100 songs and four genres.",
                "[53] extract features related to the melody and the bass \nthrough a musicological perspective, incorporating text clas -\nsification techniques and using Multinomial Naive Bayes \nas the principal probabilistic classifier, in a self-collected \ndataset of 273 records.",
                "These approaches were experimentally validated on rela -\ntively small datasets compared to, for example, the openly \navailable Lakh MIDI dataset",
                "Given a large enough dataset of n  such sequences, along \nwith their ground-truth labels D= {(X1,Y1),\u2026,(Xn,Yn)} , \nin the context of machine learning, the goal is to train an \nalgorithm F(X;/u1D703) to model the conditional distribution of \nlabels with respect to input sequences.",
                "Each classifier\u2019s performance is then \nmeasured on a test dataset which does not overlap with D.\n1D Convnets\nThe state-of-the-art approach for MIDI genre classification \npresented by Ferraro and Lemstr\u00f6m in [7 ] uses an algorithm \nfor recognizing patterns of notes in an input sequence and \nthen performs classification based on recognized patterns.",
                "Experiments\nTo explore the effectiveness of our architecture for informa-\ntion retrieval from symbolic music and to check the com -\npatibility of 1D CNNs for the task, along with the effect of \nallocating resources to network depth or to kernel size we \nconducted a set of experiments.1\nData\nFor our experiments, we use the Lakh Pianoroll Dataset \nas presented by Dong et\u00a0al.",
                "This dataset consists of pianoroll represen-\ntations of MIDI files in the Lakh MIDI Dataset presented \nby Raffel in [31].",
                "The LMD-matched subset \ncontains pianorolls that have been linked with the Million \nSong Dataset (MSD)",
                "The Million Song Dataset is the \nlargest currently available collection of audio features and \nmetadata for a million contemporary popular music tracks.",
                "We use labels acquired by MSD to construct the MASD and \ntopMAGD datasets presented by Schindler et\u00a0al. in [42], so \nwe can compare our results with existing work.",
                "At the time \nof writing, Ferraro and Lemstr\u00f6m in [7 ] have achieved the \nbest results with regards to genre classification of symbolic \nmusic for the MASD and topMAGD datasets.",
                "Finally, we \nrandomly split each dataset into a train and test set (.75/.25), \nwe use the train set for training our models and the test set \nfor evaluating them.",
                "Both datasets are imbalanced with regard to the number \nof files corresponding to each label (Figs.\u00a0 4 and 5 ).",
                "ndeep=nshallow ,\n1153 fo1+9fo1fo2+fo2+1153fo2+128=nshallow ,\nFig. 4  Number of files in the LPD dataset per label of the MASD \ndataset\nFig. 5  Number of files in the LPD dataset per label of the topMAGD \ndataset\nFig.",
                "In the \ncontext of our dataset, these lengths represent musical time \nfrom approximately 5 quarter notes to 170 quarter notes, or \n42 bars for a 4\n4 time signature (around one\u2013two minutes for \ntypical values of a song\u2019s tempo).",
                "Then, each network will \nconsist of log2l blocks stacked depth-wise, followed by a \nfully connected layer at the output, with as many sigmoid-\nactivated units as there are different labels in each dataset.",
                "Evaluation and\u00a0Post Processing\nWe evaluate our models on the held-out test set for each of \nthe MASD and topMAGD datasets by computing precision, \nrecall, and micro f1 metric.",
                "If no labels have a \nprobability greater than 0.5, then we assign as a single label \nthe element of the vector which has the maximum prob-\nability, since there are no unlabeled samples in the dataset.",
                "In addition, we present precision, recall, and F1 scores for \neach label in the topMAGD dataset for the best performing \nmodel (Table\u00a0 2).",
                "On the one hand, the effect of the imbal-\nanced dataset is apparent in the network\u2019s performance for \nthe most common label (Pop-Rock) when compared to those \nwith fewer files in the dataset such as Blues, Reggae, and \nFolk.",
                "It is interesting that genres such as Jazz which have lit-\ntle representation in the dataset are better classified than gen-\nres such as Electronic which has almost double the support.",
                "Furthermore, since genres \nthemselves are not well-defined terms, and their characteris-\ntics can vastly change over time, explanations of predictions \ncould be valuable for understanding both the model and the \ndataset.",
                "Table 1  Micro F1 scores on the test sets of the MASD and topMAGD \ndataset for each of our architectures P2\u20134 and P2\u20135 refer to the best \nperforming configuration of those presented in [7] and PiRhDy_GM \nrefer to the best performing configuration of those presented in [21]\nBold represents the best performance for each sequence length and \nfor each datasetLength Block Input MASD topMAGD\n64 Deep Sequence 0.258 0.620\nMuSeRe 0.265 0.622\nShallow Sequence 0.295 0.623\nMuSeRe 0.308 0.622\n128 Deep Sequence 0.315 0.624\nMuSeRe 0.317 0.631\nShallow Sequence 0.361 0.632\nMuSeRe 0.407 0.639\n256 Deep Sequence 0.411 0.654\nMuSeRe 0.404 0.639\nShallow Sequence 0.335 0.663\nMuSeRe 0.491 0.668\n512 Deep Sequence 0.456 0.661\nMuSeRe 0.374 0.653\nShallow Sequence 0.545 0.711\nMuSeRe 0.525 0.703\n1024 Deep Sequence 0.507 0.673\nMuSeRe 0.337 0.641\nShallow Sequence 0.581 0.777\nMuSeRe 0.526 0.737\n2048 Deep Sequence 0.456 0.696\nMuSeRe 0.264 0.627\nShallow Sequence 0.593 0.759\nMuSeRe 0.444 0.733\nP2\u20134 0.468 0.662\nP2\u20135 0.431 0.649\nPiRhDy_GM 0.471 0.668Table 2  Per label precision recall and F1-score on the test set for \nShallow Sequence model with input length 1024 (best performing \nmodel) on the topMAGD dataset\nLabel F1 Precision Recall Support\nPop-Rock 0.86 0.81 0.96 3705\nElectronic 0.58 0.74 0.47 557\nCountry 0.67 0.83 0.56 502\nRnB 0.61 0.92 0.45 432\nJazz 0.76 0.91 0.65 281\nLatin 0.45 0.78 0.32 338\nInternational 0.53 0.77 0.41 236\nRap 0.34 0.78 0.22 133\nVocal 0.65 0.90 0.51 150\nNew Age 0.66 0.94 0.51 116\nFolk 0.48 1.00 0.32 44\nReggae 0.48 1.00 0.31 38\nBlues 0.55 0.73 0.44 18\nMicro avg 0.78 0.81 0.74",
                "The results concern four hand-picked samples from the \nreddit MIDI dataset.2",
                "We chose \nMoonlight Sonata, since it is out of domain as its genre is not \nincluded in the dataset\u2019s labels.",
                "In music-theoretical terms, B\u266d appears as a \nDorian substitute and it would be interesting to explore the \ndistribution of the Dorian mode within our dataset of Inter -\nnational music, and determine if this is a bias learned by the classifier, or if it is an actual feature of the genre.",
                "For Here \nComes the Sun, we attribute the failure of GPX to the bias \nof the classifier towards the Pop-Rock genre which is a result \nof dataset imbalance.",
                "[14]\nMMD-critic is a methodology for analyzing the distribution \nof a dataset to find specific samples which are prototypes, \nand others which are criticisms.",
                "Regarding the results on the test set (Table\u00a0 5), we can \ngain some insight about the dataset and by extension the \nperformance of the CNNs.",
                "A second issue raised by Table\u00a0 5 is that of dataset \nimbalance.",
                "Finally, for those genres with very low \nsupport, we cannot expect MMD-critic to produce meaning-\nful explanations since it is a statistics-based approach that \nrequires a sufficiently large dataset.",
                "Furthermore, increasing input sequence length effectively \nreduces the number of (non-overlapping) sequences in the \ndataset while network size increases, which makes models \nmore prone to overfitting.",
                "In addition, even though we used one of the largest avail-\nable MIDI genre annotated datasets for training and evaluat-\ning our models, the dataset is by no means representative of \nall available music and suffers from poor class balance.",
                "For \nfuture work, we plan to augment our dataset by including files from other sources, such as the reddit4 MIDI dataset and \nautomatically acquire additional labels and annotations from \nonline sources such as The Echo Nest5 and Spotify APIs.",
                "Bertin-Mahieux T, Ellis DP, Whitman B, Lamere P. The million \nsong dataset 2011.",
                "Facilitating comprehensive \nbenchmarking experiments on the million song dataset."
            ]
        },
        {
            "title": "CycleDRUMS: automatic drum arrangement for bass lines using CycleGAN",
            "architecture": [
                "In addition, more solutions based on deep learning \ntechniques, such as RL-Duet [4 ]\u2014a deep reinforcement learning algorithm for online accompaniment generation\u2014or \nPopMAG, a transformer-based architecture which relies on a multi-track MIDI representation of music [5], continue to be \nstudied.",
                "In particular, we \ntrained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset [8 ] and the musdb18 dataset [9 ].",
                "To sum up, our main contributions are the following:\n\u2022 we trained a CycleGAN architecture on bass and drum mel-spectrograms in order to automatically generate drums \nthat follow the beat and sound credible for any given bass line;\n\u2022 our approach can generate drum arrangements with low computational resources and limited inference time, if \ncompared to other popular solutions for automatic music generation [12];\n\u2022 we developed a metric\u2014partially based on or correlated to human (and expert) judgment\u2014to automatically evaluate \nthe obtained results and the creativity of the proposed system, given the challenges of a quantitative assessment of \nmusic;\n\u2022 we compared our method to Pix2Pix, another popular image transfer network, showing that the music arrangement \nproblem can be better tackled with an unpaired approach and adding a cycle-consistency loss.",
                "The following contributions used MIDI, piano rolls, chord \nand note names to feed several deep learning architectures and tackle different aspects of the music generation problem.",
                "In [21], symbolic sequences of polyphonic music are modeled in an entirely general piano-roll representation, while the \nauthors of [22] propose a novel architecture to generate melodies satisfying positional constraints in the style of the \nsoprano parts of the J.S. Bach chorale harmonizations encoded in MIDI.",
                "[38] tested a model for unconditional audio synthesis based on generating one audio sample at a time, and \n[39] applied Restricted Boltzmann Machine and LSTM architectures to raw audio files in the frequency domain in order \nto generate music.",
                "The authors of [40] present a raw audio music generation model based on the WaveNet architecture, \nwhich takes the composition notes as a secondary input.",
                "Nonetheless, due to the \ncomputational resources required to model long-range dependencies in the time domain directly, either short samples \nof music can be generated or complex and large architectures and long inference time are required.",
                "As to \nthe arrangement generation task, the large majority of approaches proposed in the literature is based on a symbolic \nrepresentation of music: in [5 ], a novel multi-track MIDI representation (MuMIDI) is presented, which enables simultane -\nous multi-track generation in a single sequence and explicitly models the dependency of the notes from different tracks \nutilizing a Transformer-based architecture; in [4 ], a deep reinforcement learning algorithm for online accompaniment \ngeneration is described.",
                "It features a U-NET encoder\u2013decoder architecture with a bidirectional LSTM as hidden layer.",
                "The architecture assumes some underlying relationship between \ndomains and tries to learn it.",
                "In particular, we trained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset",
                "[7] to obtain:\nWe adopt the architecture from [56] for our generative networks, which have shown impressive neural style transfer \nand super-resolution results.",
                "Figure\u00a0 2 shows a schema summarizing the entire architecture.\n3.4  Automatic bass to\u00a0drums arrangement\nCycleDRUMS takes as input a set of N  music songs in the waveform domain X={xi}N\ni=1 , where /u1D431/u1D422 is a waveform whose \nnumber of samples depends on the sampling rate and the audio length.",
                "In the final stage of our pipeline, we fed CycleGAN architecture with the obtained dataset.",
                "The architecture assumes \nsome underlying relationship between domains and tries to learn it.",
                "For this reason, our training strategy is to pre-train the architecture with the artificially source-\nseparated FMA dataset and then fine-tune it with musdb18.",
                "Ultimately, instead of forcing a pre-\nexisting method to work in our specific scenario, we decided to replicate our experiments using the Pix2Pix architecture \n[11], another image-to-image translation network.",
                "At this website7 a private Sound Cloud playlist of some of the most exciting results is available, \nwhile at this link8 we uploaded some samples obtained with the Pix2Pix baseline architecture.",
                "Even with the promising results, some critical issues \nmust be addressed before a more compelling architecture can be developed.",
                "Moreover, the model architecture should be further improved to focus on longer dependencies and consider \nthe actual degradation of high frequencies."
            ],
            "training parameters": [],
            "learning rate": [
                "The Adam \noptimizer [ 59] was chosen both for the generators and the discriminators, with betas (0.5,\u00a00.999) and a learning rate equal \nto 0.0002.",
                "Hyperparameter Value\nEpochs 20\nWindow size 256\nPatch size 70 \u00d7 70\nLearning rate 2e\u22124\n/u1D706 10\n/u1D6FD (0.5, 0.999)\nReferences\n 1."
            ],
            "batch size": [
                "The batch size was set to 1."
            ],
            "ethical": [],
            "impact": [
                "Even though the discretization step introduces some distortion\u2014original spectrogram values are floats\u2014the impact \non the audio quality is negligible.",
                "S\u00e1nchez Fern\u00e1ndez LP , S\u00e1nchez P\u00e9rez LA, Carbajal Hern\u00e1ndez JJ, Rojo Ruiz A. Aircraft classification and acoustic impact estimation based \non real-time take-off noise measurements."
            ],
            "society": [],
            "copyright": [
                "If material is not included in \nthe article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder."
            ],
            "evaluation metrics": [],
            "metric": [
                "In the absence of an objective way of evaluating the output of both \ngenerative adversarial networks and generative music systems, we further defined a possible metric for the proposed \ntask, partially based on human (and expert) judgment.",
                "To sum up, our main contributions are the following:\n\u2022 we trained a CycleGAN architecture on bass and drum mel-spectrograms in order to automatically generate drums \nthat follow the beat and sound credible for any given bass line;\n\u2022 our approach can generate drum arrangements with low computational resources and limited inference time, if \ncompared to other popular solutions for automatic music generation [12];\n\u2022 we developed a metric\u2014partially based on or correlated to human (and expert) judgment\u2014to automatically evaluate \nthe obtained results and the creativity of the proposed system, given the challenges of a quantitative assessment of \nmusic;\n\u2022 we compared our method to Pix2Pix, another popular image transfer network, showing that the music arrangement \nproblem can be better tackled with an unpaired approach and adding a cycle-consistency loss.",
                "In [13], CNNs are used for generating melody as a series of MIDI notes either from scratch, by following a chord sequence, \nor by conditioning on the melody of previous bars, whereas in [14\u2013 17] LSTMs are used to generate musical notes, melo -\ndies, polyphonic music pieces, and long drum sequences under constraints imposed by metrical rhythm information and Vol.:(0123456789)Discover Artificial Intelligence             (2023) 3:4  | https://doi.org/10.1007/s44163-023-00047-7 \n Research\n1 3\na given bass sequence.",
                "[48], and (ii) the definition of an objective \nmetric and loss is a common problem to generative models such as GANs: as of now, generative models in the music \ndomain are evaluated based on the subjective response of a pool of listeners, because an objective metric for the raw \naudio representation has never been proposed so far.",
                "Just for the MIDI representation, a set of simple musically informed \nobjective metrics was proposed",
                "The /u1D706 weights for cycle losses were both equal to 10.\n4.3  Experimental setting\nEven though researchers proposed some effective metrics to predict how popular a song will become [60], there \nis an intrinsic difficulty in objectively evaluating artistic artifacts such as music.",
                "In light of the limits linked \nto this human-based approach, we propose a new metric that correlates well with human judgment.",
                "4.4  Metrics\nIf we consider as a general objective for a system the capacity to assist composers and musicians, rather than to \nautonomously generate music, we should also consider as an evaluation criteria the satisfaction of the composer, \nrather than the satisfaction of the auditors [1 ].",
                "This metric leverages the established Inception pre-trained model by getting a vector representation of each mel-\nspectrogram (i.e. each song), and uses these vectors to compare the distributions of generated and gold examples.",
                "Wang X, Takaki S, Yamagishi J. Neural source-filter waveform models for statistical parametric speech synthesis.",
                "Lee J, Lee J. Music popularity: metrics, characteristics, and audio-based prediction."
            ],
            "pitch accuracy": [],
            "rhythm accuracy": [],
            "melodic": [],
            "harmonic complexity": [],
            "expressiveness": [
                "Indeed, even if it is possible to use synthesizers \nto produce sounds from symbolic music, MIDI, music sheets, and piano rolls are not always easy to find or produce, and \nthey sometimes lack expressiveness."
            ],
            "types of music": [],
            "homophonic": [],
            "heterophonic": [],
            "a cappella": [],
            "solo": [],
            "choral": [
                "In [21], symbolic sequences of polyphonic music are modeled in an entirely general piano-roll representation, while the \nauthors of [22] propose a novel architecture to generate melodies satisfying positional constraints in the style of the \nsoprano parts of the J.S. Bach chorale harmonizations encoded in MIDI.",
                "In [23], RNNs are used for the prediction and \ncomposition of polyphonic music; in [24], highly convincing chorales in the style of Bach were automatically generated \nusing note names [25]; added higher-level structure on generated polyphonic music, whereas in [26] an end-to-end \ngenerative model capable of composing music conditioned on a specific mixture of composer styles was designed.",
                "Hadjeres G, Pachet F, Nielsen F. Deepbach: a steerable model for bach chorales generation."
            ],
            "orchestral": [],
            "electronic": [],
            "pop": [
                "Keywords Automatic music arrangement\u00a0\u00b7 Cycle-GAN\u00a0\u00b7 Deep learning\u00a0\u00b7 Source separation\u00a0\u00b7 Audio and speech \nprocessing\n1 Introduction\nThe development of home music production has brought significant innovations into the process of pop music composi-\ntion.",
                "In addition, more solutions based on deep learning \ntechniques, such as RL-Duet [4 ]\u2014a deep reinforcement learning algorithm for online accompaniment generation\u2014or \nPopMAG, a transformer-based architecture which relies on a multi-track MIDI representation of music [5], continue to be \nstudied.",
                "The results were then compared to Pix2Pix \n[11], another popular paired image-to-image translation network.",
                "To sum up, our main contributions are the following:\n\u2022 we trained a CycleGAN architecture on bass and drum mel-spectrograms in order to automatically generate drums \nthat follow the beat and sound credible for any given bass line;\n\u2022 our approach can generate drum arrangements with low computational resources and limited inference time, if \ncompared to other popular solutions for automatic music generation [12];\n\u2022 we developed a metric\u2014partially based on or correlated to human (and expert) judgment\u2014to automatically evaluate \nthe obtained results and the creativity of the proposed system, given the challenges of a quantitative assessment of \nmusic;\n\u2022 we compared our method to Pix2Pix, another popular image transfer network, showing that the music arrangement \nproblem can be better tackled with an unpaired approach and adding a cycle-consistency loss.",
                "Given the size of FMA, we chose to select only untrimmed songs tagged as either pop, soul-RnB, or indie-\nrock, for approximately 10,000 songs ( \u2248700 h of audio).",
                "The /u1D706 weights for cycle losses were both equal to 10.\n4.3  Experimental setting\nEven though researchers proposed some effective metrics to predict how popular a song will become [60], there \nis an intrinsic difficulty in objectively evaluating artistic artifacts such as music.",
                "We \nthen asked a professional guitarist who has been playing in a pop-rock band for more than ten years, a professional \ndrummer from the same band, and two pop and indie-rock music producers with more than four years of experience \nto manually annotate these samples, capturing the following musical dimensions: sound quality, contamination, \ncredibility, and whether the generated drums followed the beat.",
                "Ren Y, He J, Tan X, Qin T, Zhao Z, Liu T-Y. Popmag: pop music accompaniment generation.",
                "Zhu H, Liu Q, Yuan NJ, Qin C, Li J, Zhang K, Zhou G, Wei F, Xu Y, Chen E. Xiaoice band: a melody and arrangement generation framework for \npop music.",
                "Lee J, Lee J. Music popularity: metrics, characteristics, and audio-based prediction."
            ],
            "Demo availability": [],
            "dataset": [
                "In particular, we \ntrained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset [8 ] and the musdb18 dataset [9 ].",
                "Coming to the most relevant issues in the development of music generation systems, both the training and evalu-\nation of such systems have proven challenging, mainly because of the following reasons: (i) the available datasets for \nmusic generation tasks are challenging due to their inherent high-entropy",
                "After the source separation task is carried out on our song dataset, both the bass and drum waveforms are turned \ninto the corresponding mel-spectrograms using PyTorch Audio.1 PyTorch works very fast and is optimized to perform \nrobust GPU-accelerated conversion.",
                "In particular, we trained a CycleGAN architecture on 5s bass and drum samples (equivalent to 256\u00d7256 mel-spectrograms) coming from \nboth the Free Music Archive (FMA) dataset",
                "[8] and the musdb18 dataset [9].",
                "After the source separation task on \nour song dataset, the bass and drum waveforms are turned into the corresponding mel-spectrograms.",
                "In the final stage of our pipeline, we fed CycleGAN architecture with the obtained dataset.",
                "4  Experiments\n4.1  Dataset\nIt is important to carefully pick the dataset for the quality of the generated music samples.",
                "To train and test our model, \nwe decided to use the Free Music Archive2 (FMA), and the musdb183 dataset [9 ] that were both released in 2017.",
                "The \nFree Music Archive (FMA) is the largest publicly available dataset suitable for music information retrieval tasks [8 ].",
                "Finally, to better validate and fine-tune our model, we decided also to use the full musdb18 dataset.",
                "This rather small \ndataset comprises 100 tracks taken from the DSD100 dataset, 46 tracks from the MedleyDB, two tracks kindly provided by \nNative Instruments, and two tracks from the Canadian rock band The Easton Ellises.",
                "We used the 100 tracks taken from the DSD100 dataset to fine-tune \nthe model ( \u22486.5 h) and the remaining 50 songs to test it ( \u22483.5 h).",
                "For this reason, our training strategy is to pre-train the architecture with the artificially source-\nseparated FMA dataset and then fine-tune it with musdb18.",
                "A large, clean dataset of separated raw-audio sources remains a research objective.",
                "We trained our model on 2 Tesla V100 SXM2 GPUs with 32 GB of RAM for 12 epochs (FMA dataset) and fine-tuned it for \n20 more epochs (musdb18 dataset).",
                "Moreover, the cost and time required to manually \nannotate the dataset could become prohibitive even for relatively few samples (over 1000).",
                "At training time, we relied on the default network provided by the original \nauthors,6 we ran it on 2 Tesla V100 SXM2 GPUs with 32 GB of RAM for 50 epochs (FMA dataset), and we fine-tuned it for \n30 more epochs (musdb18 dataset).",
                "First and foremost, a more extensive and \ncleaner dataset of source-separated songs should be created.",
                "Data availability  The datasets generated by the survey research and analyzed during the current study are publicly available at the following \naddresses: https:// freem usica rchive.",
                "Hawthorne C, Stasyuk A, Roberts A, Simon I, Huang C-ZA, Dieleman S, Elsen E, Engel J, Eck D. Enabling factorized piano music modeling \nand generation with the maestro dataset."
            ]
        }
    ]
}